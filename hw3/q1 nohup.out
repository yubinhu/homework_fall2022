A.L.E: Arcade Learning Environment (version 0.7.5+db37282)
[Powered by Stella]



LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw3/cs285/scripts/../../data/q1_MsPacman-v0_10-10-2022_11-16-30 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw3/cs285/scripts/../../data/q1_MsPacman-v0_10-10-2022_11-16-30
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001996
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0019960403442382812
Done logging...




********** Iteration 1000 ************

Training agent...


********** Iteration 2000 ************

Training agent...


********** Iteration 3000 ************

Training agent...


********** Iteration 4000 ************

Training agent...


********** Iteration 5000 ************

Training agent...


********** Iteration 6000 ************

Training agent...


********** Iteration 7000 ************

Training agent...


********** Iteration 8000 ************

Training agent...


********** Iteration 9000 ************

Training agent...


********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) 435.593231
best mean reward -inf
running time 43.166912
Train_EnvstepsSoFar : 10001
Train_AverageReturn : 435.5932312011719
TimeSinceStart : 43.16691184043884
Done logging...




********** Iteration 11000 ************

Training agent...


********** Iteration 12000 ************

Training agent...


********** Iteration 13000 ************

Training agent...


********** Iteration 14000 ************

Training agent...


********** Iteration 15000 ************

Training agent...


********** Iteration 16000 ************

Training agent...


********** Iteration 17000 ************

Training agent...


********** Iteration 18000 ************

Training agent...


********** Iteration 19000 ************

Training agent...


********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) 408.600006
best mean reward 408.600006
running time 85.481899
Train_EnvstepsSoFar : 20001
Train_AverageReturn : 408.6000061035156
Train_BestReturn : 408.6000061035156
TimeSinceStart : 85.48189902305603
Done logging...




********** Iteration 21000 ************

Training agent...


********** Iteration 22000 ************

Training agent...


********** Iteration 23000 ************

Training agent...


********** Iteration 24000 ************

Training agent...


********** Iteration 25000 ************

Training agent...


********** Iteration 26000 ************

Training agent...


********** Iteration 27000 ************

Training agent...


********** Iteration 28000 ************

Training agent...


********** Iteration 29000 ************

Training agent...


********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) 409.000000
best mean reward 409.000000
running time 127.839321
Train_EnvstepsSoFar : 30001
Train_AverageReturn : 409.0
Train_BestReturn : 409.0
TimeSinceStart : 127.83932089805603
Done logging...




********** Iteration 31000 ************

Training agent...


********** Iteration 32000 ************

Training agent...


********** Iteration 33000 ************

Training agent...


********** Iteration 34000 ************

Training agent...


********** Iteration 35000 ************

Training agent...


********** Iteration 36000 ************

Training agent...


********** Iteration 37000 ************

Training agent...


********** Iteration 38000 ************

Training agent...


********** Iteration 39000 ************

Training agent...


********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) 432.399994
best mean reward 432.399994
running time 169.691314
Train_EnvstepsSoFar : 40001
Train_AverageReturn : 432.3999938964844
Train_BestReturn : 432.3999938964844
TimeSinceStart : 169.69131445884705
Done logging...




********** Iteration 41000 ************

Training agent...


********** Iteration 42000 ************

Training agent...


********** Iteration 43000 ************

Training agent...


********** Iteration 44000 ************

Training agent...


********** Iteration 45000 ************

Training agent...


********** Iteration 46000 ************

Training agent...


********** Iteration 47000 ************

Training agent...


********** Iteration 48000 ************

Training agent...


********** Iteration 49000 ************

Training agent...


********** Iteration 50000 ************

Training agent...

Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) 411.500000
best mean reward 432.399994
running time 211.395294
Train_EnvstepsSoFar : 50001
Train_AverageReturn : 411.5
Train_BestReturn : 432.3999938964844
TimeSinceStart : 211.39529371261597
Done logging...




********** Iteration 51000 ************

Training agent...


********** Iteration 52000 ************

Training agent...


********** Iteration 53000 ************

Training agent...


********** Iteration 54000 ************

Training agent...


********** Iteration 55000 ************

Training agent...


********** Iteration 56000 ************

Training agent...


********** Iteration 57000 ************

Training agent...


********** Iteration 58000 ************

Training agent...


********** Iteration 59000 ************

Training agent...


********** Iteration 60000 ************

Training agent...

Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) 468.299988
best mean reward 468.299988
running time 276.816509
Train_EnvstepsSoFar : 60001
Train_AverageReturn : 468.29998779296875
Train_BestReturn : 468.29998779296875
TimeSinceStart : 276.816508769989
Training Loss : 0.0931345522403717
Done logging...




********** Iteration 61000 ************

Training agent...


********** Iteration 62000 ************

Training agent...


********** Iteration 63000 ************

Training agent...


********** Iteration 64000 ************

Training agent...


********** Iteration 65000 ************

Training agent...


********** Iteration 66000 ************

Training agent...


********** Iteration 67000 ************

Training agent...


********** Iteration 68000 ************

Training agent...


********** Iteration 69000 ************

Training agent...


********** Iteration 70000 ************

Training agent...

Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) 461.899994
best mean reward 468.299988
running time 346.628123
Train_EnvstepsSoFar : 70001
Train_AverageReturn : 461.8999938964844
Train_BestReturn : 468.29998779296875
TimeSinceStart : 346.62812304496765
Training Loss : 0.0581555999815464
Done logging...




********** Iteration 71000 ************

Training agent...


********** Iteration 72000 ************

Training agent...


********** Iteration 73000 ************

Training agent...


********** Iteration 74000 ************

Training agent...


********** Iteration 75000 ************

Training agent...


********** Iteration 76000 ************

Training agent...


********** Iteration 77000 ************

Training agent...


********** Iteration 78000 ************

Training agent...


********** Iteration 79000 ************

Training agent...


********** Iteration 80000 ************

Training agent...

Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) 460.799988
best mean reward 468.299988
running time 417.651157
Train_EnvstepsSoFar : 80001
Train_AverageReturn : 460.79998779296875
Train_BestReturn : 468.29998779296875
TimeSinceStart : 417.65115666389465
Training Loss : 0.07844260334968567
Done logging...




********** Iteration 81000 ************

Training agent...


********** Iteration 82000 ************

Training agent...


********** Iteration 83000 ************

Training agent...


********** Iteration 84000 ************

Training agent...


********** Iteration 85000 ************

Training agent...


********** Iteration 86000 ************

Training agent...


********** Iteration 87000 ************

Training agent...


********** Iteration 88000 ************

Training agent...


********** Iteration 89000 ************

Training agent...


********** Iteration 90000 ************

Training agent...

Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) 481.799988
best mean reward 481.799988
running time 488.078182
Train_EnvstepsSoFar : 90001
Train_AverageReturn : 481.79998779296875
Train_BestReturn : 481.79998779296875
TimeSinceStart : 488.0781817436218
Training Loss : 0.0463506281375885
Done logging...




********** Iteration 91000 ************

Training agent...


********** Iteration 92000 ************

Training agent...


********** Iteration 93000 ************

Training agent...


********** Iteration 94000 ************

Training agent...


********** Iteration 95000 ************

Training agent...


********** Iteration 96000 ************

Training agent...


********** Iteration 97000 ************

Training agent...


********** Iteration 98000 ************

Training agent...


********** Iteration 99000 ************

Training agent...


********** Iteration 100000 ************

Training agent...

Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) 464.299988
best mean reward 481.799988
running time 562.199569
Train_EnvstepsSoFar : 100001
Train_AverageReturn : 464.29998779296875
Train_BestReturn : 481.79998779296875
TimeSinceStart : 562.1995685100555
Training Loss : 0.08919139951467514
Done logging...




********** Iteration 101000 ************

Training agent...


********** Iteration 102000 ************

Training agent...


********** Iteration 103000 ************

Training agent...


********** Iteration 104000 ************

Training agent...


********** Iteration 105000 ************

Training agent...


********** Iteration 106000 ************

Training agent...


********** Iteration 107000 ************

Training agent...


********** Iteration 108000 ************

Training agent...


********** Iteration 109000 ************

Training agent...


********** Iteration 110000 ************

Training agent...

Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) 449.799988
best mean reward 481.799988
running time 636.053352
Train_EnvstepsSoFar : 110001
Train_AverageReturn : 449.79998779296875
Train_BestReturn : 481.79998779296875
TimeSinceStart : 636.0533516407013
Training Loss : 0.07208693027496338
Done logging...




********** Iteration 111000 ************

Training agent...


********** Iteration 112000 ************

Training agent...


********** Iteration 113000 ************

Training agent...


********** Iteration 114000 ************

Training agent...


********** Iteration 115000 ************

Training agent...


********** Iteration 116000 ************

Training agent...


********** Iteration 117000 ************

Training agent...


********** Iteration 118000 ************

Training agent...


********** Iteration 119000 ************

Training agent...


********** Iteration 120000 ************

Training agent...

Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) 475.799988
best mean reward 481.799988
running time 708.925227
Train_EnvstepsSoFar : 120001
Train_AverageReturn : 475.79998779296875
Train_BestReturn : 481.79998779296875
TimeSinceStart : 708.9252271652222
Training Loss : 0.06364788115024567
Done logging...




********** Iteration 121000 ************

Training agent...


********** Iteration 122000 ************

Training agent...


********** Iteration 123000 ************

Training agent...


********** Iteration 124000 ************

Training agent...


********** Iteration 125000 ************

Training agent...


********** Iteration 126000 ************

Training agent...


********** Iteration 127000 ************

Training agent...


********** Iteration 128000 ************

Training agent...


********** Iteration 129000 ************

Training agent...


********** Iteration 130000 ************

Training agent...

Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) 479.000000
best mean reward 481.799988
running time 780.638138
Train_EnvstepsSoFar : 130001
Train_AverageReturn : 479.0
Train_BestReturn : 481.79998779296875
TimeSinceStart : 780.6381375789642
Training Loss : 0.038359809666872025
Done logging...




********** Iteration 131000 ************

Training agent...


********** Iteration 132000 ************

Training agent...


********** Iteration 133000 ************

Training agent...


********** Iteration 134000 ************

Training agent...


********** Iteration 135000 ************

Training agent...


********** Iteration 136000 ************

Training agent...


********** Iteration 137000 ************

Training agent...


********** Iteration 138000 ************

Training agent...


********** Iteration 139000 ************

Training agent...


********** Iteration 140000 ************

Training agent...

Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) 500.500000
best mean reward 500.500000
running time 852.341646
Train_EnvstepsSoFar : 140001
Train_AverageReturn : 500.5
Train_BestReturn : 500.5
TimeSinceStart : 852.3416457176208
Training Loss : 0.04466480761766434
Done logging...




********** Iteration 141000 ************

Training agent...


********** Iteration 142000 ************

Training agent...


********** Iteration 143000 ************

Training agent...


********** Iteration 144000 ************

Training agent...


********** Iteration 145000 ************

Training agent...


********** Iteration 146000 ************

Training agent...


********** Iteration 147000 ************

Training agent...


********** Iteration 148000 ************

Training agent...


********** Iteration 149000 ************

Training agent...


********** Iteration 150000 ************

Training agent...

Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) 497.600006
best mean reward 500.500000
running time 924.982751
Train_EnvstepsSoFar : 150001
Train_AverageReturn : 497.6000061035156
Train_BestReturn : 500.5
TimeSinceStart : 924.9827506542206
Training Loss : 0.08986123651266098
Done logging...




********** Iteration 151000 ************

Training agent...


********** Iteration 152000 ************

Training agent...


********** Iteration 153000 ************

Training agent...


********** Iteration 154000 ************

Training agent...


********** Iteration 155000 ************

Training agent...


********** Iteration 156000 ************

Training agent...


********** Iteration 157000 ************

Training agent...


********** Iteration 158000 ************

Training agent...


********** Iteration 159000 ************

Training agent...


********** Iteration 160000 ************

Training agent...

Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) 490.500000
best mean reward 500.500000
running time 996.930619
Train_EnvstepsSoFar : 160001
Train_AverageReturn : 490.5
Train_BestReturn : 500.5
TimeSinceStart : 996.9306192398071
Training Loss : 0.09464367479085922
Done logging...




********** Iteration 161000 ************

Training agent...


********** Iteration 162000 ************

Training agent...


********** Iteration 163000 ************

Training agent...


********** Iteration 164000 ************

Training agent...


********** Iteration 165000 ************

Training agent...


********** Iteration 166000 ************

Training agent...


********** Iteration 167000 ************

Training agent...


********** Iteration 168000 ************

Training agent...


********** Iteration 169000 ************

Training agent...


********** Iteration 170000 ************

Training agent...

Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) 515.000000
best mean reward 515.000000
running time 1069.950552
Train_EnvstepsSoFar : 170001
Train_AverageReturn : 515.0
Train_BestReturn : 515.0
TimeSinceStart : 1069.9505519866943
Training Loss : 0.11479660868644714
Done logging...




********** Iteration 171000 ************

Training agent...


********** Iteration 172000 ************

Training agent...


********** Iteration 173000 ************

Training agent...


********** Iteration 174000 ************

Training agent...


********** Iteration 175000 ************

Training agent...


********** Iteration 176000 ************

Training agent...


********** Iteration 177000 ************

Training agent...


********** Iteration 178000 ************

Training agent...


********** Iteration 179000 ************

Training agent...


********** Iteration 180000 ************

Training agent...

Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) 552.299988
best mean reward 552.299988
running time 1141.898340
Train_EnvstepsSoFar : 180001
Train_AverageReturn : 552.2999877929688
Train_BestReturn : 552.2999877929688
TimeSinceStart : 1141.898339509964
Training Loss : 0.12063391506671906
Done logging...




********** Iteration 181000 ************

Training agent...


********** Iteration 182000 ************

Training agent...


********** Iteration 183000 ************

Training agent...


********** Iteration 184000 ************

Training agent...


********** Iteration 185000 ************

Training agent...


********** Iteration 186000 ************

Training agent...


********** Iteration 187000 ************

Training agent...


********** Iteration 188000 ************

Training agent...


********** Iteration 189000 ************

Training agent...


********** Iteration 190000 ************

Training agent...

Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) 541.299988
best mean reward 552.299988
running time 1214.821674
Train_EnvstepsSoFar : 190001
Train_AverageReturn : 541.2999877929688
Train_BestReturn : 552.2999877929688
TimeSinceStart : 1214.8216738700867
Training Loss : 0.13364148139953613
Done logging...




********** Iteration 191000 ************

Training agent...


********** Iteration 192000 ************

Training agent...


********** Iteration 193000 ************

Training agent...


********** Iteration 194000 ************

Training agent...


********** Iteration 195000 ************

Training agent...


********** Iteration 196000 ************

Training agent...


********** Iteration 197000 ************

Training agent...


********** Iteration 198000 ************

Training agent...


********** Iteration 199000 ************

Training agent...


********** Iteration 200000 ************

Training agent...

Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) 541.500000
best mean reward 552.299988
running time 1287.810788
Train_EnvstepsSoFar : 200001
Train_AverageReturn : 541.5
Train_BestReturn : 552.2999877929688
TimeSinceStart : 1287.810788154602
Training Loss : 0.11395717412233353
Done logging...




********** Iteration 201000 ************

Training agent...


********** Iteration 202000 ************

Training agent...


********** Iteration 203000 ************

Training agent...


********** Iteration 204000 ************

Training agent...


********** Iteration 205000 ************

Training agent...


********** Iteration 206000 ************

Training agent...


********** Iteration 207000 ************

Training agent...


********** Iteration 208000 ************

Training agent...


********** Iteration 209000 ************

Training agent...


********** Iteration 210000 ************

Training agent...

Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) 553.900024
best mean reward 553.900024
running time 1359.931714
Train_EnvstepsSoFar : 210001
Train_AverageReturn : 553.9000244140625
Train_BestReturn : 553.9000244140625
TimeSinceStart : 1359.9317140579224
Training Loss : 0.032228514552116394
Done logging...




********** Iteration 211000 ************

Training agent...


********** Iteration 212000 ************

Training agent...


********** Iteration 213000 ************

Training agent...


********** Iteration 214000 ************

Training agent...


********** Iteration 215000 ************

Training agent...


********** Iteration 216000 ************

Training agent...


********** Iteration 217000 ************

Training agent...


********** Iteration 218000 ************

Training agent...


********** Iteration 219000 ************

Training agent...


********** Iteration 220000 ************

Training agent...

Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) 544.599976
best mean reward 553.900024
running time 1433.304457
Train_EnvstepsSoFar : 220001
Train_AverageReturn : 544.5999755859375
Train_BestReturn : 553.9000244140625
TimeSinceStart : 1433.3044567108154
Training Loss : 0.11613133549690247
Done logging...




********** Iteration 221000 ************

Training agent...


********** Iteration 222000 ************

Training agent...


********** Iteration 223000 ************

Training agent...


********** Iteration 224000 ************

Training agent...


********** Iteration 225000 ************

Training agent...


********** Iteration 226000 ************

Training agent...


********** Iteration 227000 ************

Training agent...


********** Iteration 228000 ************

Training agent...


********** Iteration 229000 ************

Training agent...


********** Iteration 230000 ************

Training agent...

Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) 522.400024
best mean reward 553.900024
running time 1506.432709
Train_EnvstepsSoFar : 230001
Train_AverageReturn : 522.4000244140625
Train_BestReturn : 553.9000244140625
TimeSinceStart : 1506.4327092170715
Training Loss : 0.26631367206573486
Done logging...




********** Iteration 231000 ************

Training agent...


********** Iteration 232000 ************

Training agent...


********** Iteration 233000 ************

Training agent...


********** Iteration 234000 ************

Training agent...


********** Iteration 235000 ************

Training agent...


********** Iteration 236000 ************

Training agent...


********** Iteration 237000 ************

Training agent...


********** Iteration 238000 ************

Training agent...


********** Iteration 239000 ************

Training agent...


********** Iteration 240000 ************

Training agent...

Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) 573.700012
best mean reward 573.700012
running time 1578.791238
Train_EnvstepsSoFar : 240001
Train_AverageReturn : 573.7000122070312
Train_BestReturn : 573.7000122070312
TimeSinceStart : 1578.7912375926971
Training Loss : 0.12654490768909454
Done logging...




********** Iteration 241000 ************

Training agent...


********** Iteration 242000 ************

Training agent...


********** Iteration 243000 ************

Training agent...


********** Iteration 244000 ************

Training agent...


********** Iteration 245000 ************

Training agent...


********** Iteration 246000 ************

Training agent...


********** Iteration 247000 ************

Training agent...


********** Iteration 248000 ************

Training agent...


********** Iteration 249000 ************

Training agent...


********** Iteration 250000 ************

Training agent...

Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) 614.799988
best mean reward 614.799988
running time 1652.258461
Train_EnvstepsSoFar : 250001
Train_AverageReturn : 614.7999877929688
Train_BestReturn : 614.7999877929688
TimeSinceStart : 1652.2584607601166
Training Loss : 0.12982173264026642
Done logging...




********** Iteration 251000 ************

Training agent...


********** Iteration 252000 ************

Training agent...


********** Iteration 253000 ************

Training agent...


********** Iteration 254000 ************

Training agent...


********** Iteration 255000 ************

Training agent...


********** Iteration 256000 ************

Training agent...


********** Iteration 257000 ************

Training agent...


********** Iteration 258000 ************

Training agent...


********** Iteration 259000 ************

Training agent...


********** Iteration 260000 ************

Training agent...

Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) 635.200012
best mean reward 635.200012
running time 1724.775449
Train_EnvstepsSoFar : 260001
Train_AverageReturn : 635.2000122070312
Train_BestReturn : 635.2000122070312
TimeSinceStart : 1724.7754490375519
Training Loss : 0.15872041881084442
Done logging...




********** Iteration 261000 ************

Training agent...


********** Iteration 262000 ************

Training agent...


********** Iteration 263000 ************

Training agent...


********** Iteration 264000 ************

Training agent...


********** Iteration 265000 ************

Training agent...


********** Iteration 266000 ************

Training agent...


********** Iteration 267000 ************

Training agent...


********** Iteration 268000 ************

Training agent...


********** Iteration 269000 ************

Training agent...


********** Iteration 270000 ************

Training agent...

Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) 601.400024
best mean reward 635.200012
running time 1797.623419
Train_EnvstepsSoFar : 270001
Train_AverageReturn : 601.4000244140625
Train_BestReturn : 635.2000122070312
TimeSinceStart : 1797.623419046402
Training Loss : 0.3446997404098511
Done logging...




********** Iteration 271000 ************

Training agent...


********** Iteration 272000 ************

Training agent...


********** Iteration 273000 ************

Training agent...


********** Iteration 274000 ************

Training agent...


********** Iteration 275000 ************

Training agent...


********** Iteration 276000 ************

Training agent...


********** Iteration 277000 ************

Training agent...


********** Iteration 278000 ************

Training agent...


********** Iteration 279000 ************

Training agent...


********** Iteration 280000 ************

Training agent...

Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) 583.500000
best mean reward 635.200012
running time 1870.083993
Train_EnvstepsSoFar : 280001
Train_AverageReturn : 583.5
Train_BestReturn : 635.2000122070312
TimeSinceStart : 1870.0839929580688
Training Loss : 0.12173525989055634
Done logging...




********** Iteration 281000 ************

Training agent...


********** Iteration 282000 ************

Training agent...


********** Iteration 283000 ************

Training agent...


********** Iteration 284000 ************

Training agent...


********** Iteration 285000 ************

Training agent...


********** Iteration 286000 ************

Training agent...


********** Iteration 287000 ************

Training agent...


********** Iteration 288000 ************

Training agent...


********** Iteration 289000 ************

Training agent...


********** Iteration 290000 ************

Training agent...

Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) 644.500000
best mean reward 644.500000
running time 1943.545760
Train_EnvstepsSoFar : 290001
Train_AverageReturn : 644.5
Train_BestReturn : 644.5
TimeSinceStart : 1943.5457603931427
Training Loss : 0.3647531270980835
Done logging...




********** Iteration 291000 ************

Training agent...


********** Iteration 292000 ************

Training agent...


********** Iteration 293000 ************

Training agent...


********** Iteration 294000 ************

Training agent...


********** Iteration 295000 ************

Training agent...


********** Iteration 296000 ************

Training agent...


********** Iteration 297000 ************

Training agent...


********** Iteration 298000 ************

Training agent...


********** Iteration 299000 ************

Training agent...


********** Iteration 300000 ************

Training agent...

Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) 671.900024
best mean reward 671.900024
running time 2016.933903
Train_EnvstepsSoFar : 300001
Train_AverageReturn : 671.9000244140625
Train_BestReturn : 671.9000244140625
TimeSinceStart : 2016.9339032173157
Training Loss : 0.13487374782562256
Done logging...




********** Iteration 301000 ************

Training agent...


********** Iteration 302000 ************

Training agent...


********** Iteration 303000 ************

Training agent...


********** Iteration 304000 ************

Training agent...


********** Iteration 305000 ************

Training agent...


********** Iteration 306000 ************

Training agent...


********** Iteration 307000 ************

Training agent...


********** Iteration 308000 ************

Training agent...


********** Iteration 309000 ************

Training agent...


********** Iteration 310000 ************

Training agent...

Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) 596.299988
best mean reward 671.900024
running time 2089.889753
Train_EnvstepsSoFar : 310001
Train_AverageReturn : 596.2999877929688
Train_BestReturn : 671.9000244140625
TimeSinceStart : 2089.889753341675
Training Loss : 0.19986823201179504
Done logging...




********** Iteration 311000 ************

Training agent...


********** Iteration 312000 ************

Training agent...


********** Iteration 313000 ************

Training agent...


********** Iteration 314000 ************

Training agent...


********** Iteration 315000 ************

Training agent...


********** Iteration 316000 ************

Training agent...


********** Iteration 317000 ************

Training agent...


********** Iteration 318000 ************

Training agent...


********** Iteration 319000 ************

Training agent...


********** Iteration 320000 ************

Training agent...

Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) 663.700012
best mean reward 671.900024
running time 2163.399210
Train_EnvstepsSoFar : 320001
Train_AverageReturn : 663.7000122070312
Train_BestReturn : 671.9000244140625
TimeSinceStart : 2163.3992099761963
Training Loss : 0.10796347260475159
Done logging...




********** Iteration 321000 ************

Training agent...


********** Iteration 322000 ************

Training agent...


********** Iteration 323000 ************

Training agent...


********** Iteration 324000 ************

Training agent...


********** Iteration 325000 ************

Training agent...


********** Iteration 326000 ************

Training agent...


********** Iteration 327000 ************

Training agent...


********** Iteration 328000 ************

Training agent...


********** Iteration 329000 ************

Training agent...


********** Iteration 330000 ************

Training agent...

Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) 664.000000
best mean reward 671.900024
running time 2235.998591
Train_EnvstepsSoFar : 330001
Train_AverageReturn : 664.0
Train_BestReturn : 671.9000244140625
TimeSinceStart : 2235.998591184616
Training Loss : 0.7052708268165588
Done logging...




********** Iteration 331000 ************

Training agent...


********** Iteration 332000 ************

Training agent...


********** Iteration 333000 ************

Training agent...


********** Iteration 334000 ************

Training agent...


********** Iteration 335000 ************

Training agent...


********** Iteration 336000 ************

Training agent...


********** Iteration 337000 ************

Training agent...


********** Iteration 338000 ************

Training agent...


********** Iteration 339000 ************

Training agent...


********** Iteration 340000 ************

Training agent...

Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) 661.200012
best mean reward 671.900024
running time 2309.679642
Train_EnvstepsSoFar : 340001
Train_AverageReturn : 661.2000122070312
Train_BestReturn : 671.9000244140625
TimeSinceStart : 2309.679641723633
Training Loss : 0.12569433450698853
Done logging...




********** Iteration 341000 ************

Training agent...


********** Iteration 342000 ************

Training agent...


********** Iteration 343000 ************

Training agent...


********** Iteration 344000 ************

Training agent...


********** Iteration 345000 ************

Training agent...


********** Iteration 346000 ************

Training agent...


********** Iteration 347000 ************

Training agent...


********** Iteration 348000 ************

Training agent...


********** Iteration 349000 ************

Training agent...


********** Iteration 350000 ************

Training agent...

Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) 749.700012
best mean reward 749.700012
running time 2383.474041
Train_EnvstepsSoFar : 350001
Train_AverageReturn : 749.7000122070312
Train_BestReturn : 749.7000122070312
TimeSinceStart : 2383.474041223526
Training Loss : 0.2890720069408417
Done logging...




********** Iteration 351000 ************

Training agent...


********** Iteration 352000 ************

Training agent...


********** Iteration 353000 ************

Training agent...


********** Iteration 354000 ************

Training agent...


********** Iteration 355000 ************

Training agent...


********** Iteration 356000 ************

Training agent...


********** Iteration 357000 ************

Training agent...


********** Iteration 358000 ************

Training agent...


********** Iteration 359000 ************

Training agent...


********** Iteration 360000 ************

Training agent...

Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) 809.500000
best mean reward 809.500000
running time 2457.607790
Train_EnvstepsSoFar : 360001
Train_AverageReturn : 809.5
Train_BestReturn : 809.5
TimeSinceStart : 2457.6077897548676
Training Loss : 0.22471249103546143
Done logging...




********** Iteration 361000 ************

Training agent...


********** Iteration 362000 ************

Training agent...


********** Iteration 363000 ************

Training agent...


********** Iteration 364000 ************

Training agent...


********** Iteration 365000 ************

Training agent...


********** Iteration 366000 ************

Training agent...


********** Iteration 367000 ************

Training agent...


********** Iteration 368000 ************

Training agent...


********** Iteration 369000 ************

Training agent...


********** Iteration 370000 ************

Training agent...

Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) 811.400024
best mean reward 811.400024
running time 2530.940456
Train_EnvstepsSoFar : 370001
Train_AverageReturn : 811.4000244140625
Train_BestReturn : 811.4000244140625
TimeSinceStart : 2530.9404561519623
Training Loss : 0.18641845881938934
Done logging...




********** Iteration 371000 ************

Training agent...


********** Iteration 372000 ************

Training agent...


********** Iteration 373000 ************

Training agent...


********** Iteration 374000 ************

Training agent...


********** Iteration 375000 ************

Training agent...


********** Iteration 376000 ************

Training agent...


********** Iteration 377000 ************

Training agent...


********** Iteration 378000 ************

Training agent...


********** Iteration 379000 ************

Training agent...


********** Iteration 380000 ************

Training agent...

Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) 762.099976
best mean reward 811.400024
running time 2604.045118
Train_EnvstepsSoFar : 380001
Train_AverageReturn : 762.0999755859375
Train_BestReturn : 811.4000244140625
TimeSinceStart : 2604.045117855072
Training Loss : 0.11458475142717361
Done logging...




********** Iteration 381000 ************

Training agent...


********** Iteration 382000 ************

Training agent...


********** Iteration 383000 ************

Training agent...


********** Iteration 384000 ************

Training agent...


********** Iteration 385000 ************

Training agent...


********** Iteration 386000 ************

Training agent...


********** Iteration 387000 ************

Training agent...


********** Iteration 388000 ************

Training agent...


********** Iteration 389000 ************

Training agent...


********** Iteration 390000 ************

Training agent...

Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) 768.700012
best mean reward 811.400024
running time 2677.624397
Train_EnvstepsSoFar : 390001
Train_AverageReturn : 768.7000122070312
Train_BestReturn : 811.4000244140625
TimeSinceStart : 2677.6243970394135
Training Loss : 0.306373655796051
Done logging...




********** Iteration 391000 ************

Training agent...


********** Iteration 392000 ************

Training agent...


********** Iteration 393000 ************

Training agent...


********** Iteration 394000 ************

Training agent...


********** Iteration 395000 ************

Training agent...


********** Iteration 396000 ************

Training agent...


********** Iteration 397000 ************

Training agent...


********** Iteration 398000 ************

Training agent...


********** Iteration 399000 ************

Training agent...


********** Iteration 400000 ************

Training agent...

Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) 738.200012
best mean reward 811.400024
running time 2750.569953
Train_EnvstepsSoFar : 400001
Train_AverageReturn : 738.2000122070312
Train_BestReturn : 811.4000244140625
TimeSinceStart : 2750.56995344162
Training Loss : 0.12568840384483337
Done logging...




********** Iteration 401000 ************

Training agent...


********** Iteration 402000 ************

Training agent...


********** Iteration 403000 ************

Training agent...


********** Iteration 404000 ************

Training agent...


********** Iteration 405000 ************

Training agent...


********** Iteration 406000 ************

Training agent...


********** Iteration 407000 ************

Training agent...


********** Iteration 408000 ************

Training agent...


********** Iteration 409000 ************

Training agent...


********** Iteration 410000 ************

Training agent...

Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) 789.299988
best mean reward 811.400024
running time 2823.989007
Train_EnvstepsSoFar : 410001
Train_AverageReturn : 789.2999877929688
Train_BestReturn : 811.4000244140625
TimeSinceStart : 2823.989006757736
Training Loss : 0.1441635638475418
Done logging...




********** Iteration 411000 ************

Training agent...


********** Iteration 412000 ************

Training agent...


********** Iteration 413000 ************

Training agent...


********** Iteration 414000 ************

Training agent...


********** Iteration 415000 ************

Training agent...


********** Iteration 416000 ************

Training agent...


********** Iteration 417000 ************

Training agent...


********** Iteration 418000 ************

Training agent...


********** Iteration 419000 ************

Training agent...


********** Iteration 420000 ************

Training agent...

Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) 811.500000
best mean reward 811.500000
running time 2896.938774
Train_EnvstepsSoFar : 420001
Train_AverageReturn : 811.5
Train_BestReturn : 811.5
TimeSinceStart : 2896.9387741088867
Training Loss : 0.30437910556793213
Done logging...




********** Iteration 421000 ************

Training agent...


********** Iteration 422000 ************

Training agent...


********** Iteration 423000 ************

Training agent...


********** Iteration 424000 ************

Training agent...


********** Iteration 425000 ************

Training agent...


********** Iteration 426000 ************

Training agent...


********** Iteration 427000 ************

Training agent...


********** Iteration 428000 ************

Training agent...


********** Iteration 429000 ************

Training agent...


********** Iteration 430000 ************

Training agent...

Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) 778.599976
best mean reward 811.500000
running time 2970.584442
Train_EnvstepsSoFar : 430001
Train_AverageReturn : 778.5999755859375
Train_BestReturn : 811.5
TimeSinceStart : 2970.5844416618347
Training Loss : 0.28412336111068726
Done logging...




********** Iteration 431000 ************

Training agent...


********** Iteration 432000 ************

Training agent...


********** Iteration 433000 ************

Training agent...


********** Iteration 434000 ************

Training agent...


********** Iteration 435000 ************

Training agent...


********** Iteration 436000 ************

Training agent...


********** Iteration 437000 ************

Training agent...


********** Iteration 438000 ************

Training agent...


********** Iteration 439000 ************

Training agent...


********** Iteration 440000 ************

Training agent...

Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) 839.700012
best mean reward 839.700012
running time 3044.451661
Train_EnvstepsSoFar : 440001
Train_AverageReturn : 839.7000122070312
Train_BestReturn : 839.7000122070312
TimeSinceStart : 3044.4516611099243
Training Loss : 0.3811824321746826
Done logging...




********** Iteration 441000 ************

Training agent...


********** Iteration 442000 ************

Training agent...


********** Iteration 443000 ************

Training agent...


********** Iteration 444000 ************

Training agent...


********** Iteration 445000 ************

Training agent...


********** Iteration 446000 ************

Training agent...


********** Iteration 447000 ************

Training agent...


********** Iteration 448000 ************

Training agent...


********** Iteration 449000 ************

Training agent...


********** Iteration 450000 ************

Training agent...

Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) 833.700012
best mean reward 839.700012
running time 3118.438874
Train_EnvstepsSoFar : 450001
Train_AverageReturn : 833.7000122070312
Train_BestReturn : 839.7000122070312
TimeSinceStart : 3118.4388744831085
Training Loss : 0.47096389532089233
Done logging...




********** Iteration 451000 ************

Training agent...


********** Iteration 452000 ************

Training agent...


********** Iteration 453000 ************

Training agent...


********** Iteration 454000 ************

Training agent...


********** Iteration 455000 ************

Training agent...


********** Iteration 456000 ************

Training agent...


********** Iteration 457000 ************

Training agent...


********** Iteration 458000 ************

Training agent...


********** Iteration 459000 ************

Training agent...


********** Iteration 460000 ************

Training agent...

Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) 864.000000
best mean reward 864.000000
running time 3192.917661
Train_EnvstepsSoFar : 460001
Train_AverageReturn : 864.0
Train_BestReturn : 864.0
TimeSinceStart : 3192.917661190033
Training Loss : 0.29889392852783203
Done logging...




********** Iteration 461000 ************

Training agent...


********** Iteration 462000 ************

Training agent...


********** Iteration 463000 ************

Training agent...


********** Iteration 464000 ************

Training agent...


********** Iteration 465000 ************

Training agent...


********** Iteration 466000 ************

Training agent...


********** Iteration 467000 ************

Training agent...


********** Iteration 468000 ************

Training agent...


********** Iteration 469000 ************

Training agent...


********** Iteration 470000 ************

Training agent...

Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) 830.799988
best mean reward 864.000000
running time 3267.685533
Train_EnvstepsSoFar : 470001
Train_AverageReturn : 830.7999877929688
Train_BestReturn : 864.0
TimeSinceStart : 3267.685533285141
Training Loss : 0.18686732649803162
Done logging...




********** Iteration 471000 ************

Training agent...


********** Iteration 472000 ************

Training agent...


********** Iteration 473000 ************

Training agent...


********** Iteration 474000 ************

Training agent...


********** Iteration 475000 ************

Training agent...


********** Iteration 476000 ************

Training agent...


********** Iteration 477000 ************

Training agent...


********** Iteration 478000 ************

Training agent...


********** Iteration 479000 ************

Training agent...


********** Iteration 480000 ************

Training agent...

Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) 800.400024
best mean reward 864.000000
running time 3341.044422
Train_EnvstepsSoFar : 480001
Train_AverageReturn : 800.4000244140625
Train_BestReturn : 864.0
TimeSinceStart : 3341.044422149658
Training Loss : 0.19304601848125458
Done logging...




********** Iteration 481000 ************

Training agent...


********** Iteration 482000 ************

Training agent...


********** Iteration 483000 ************

Training agent...


********** Iteration 484000 ************

Training agent...


********** Iteration 485000 ************

Training agent...


********** Iteration 486000 ************

Training agent...


********** Iteration 487000 ************

Training agent...


********** Iteration 488000 ************

Training agent...


********** Iteration 489000 ************

Training agent...


********** Iteration 490000 ************

Training agent...

Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) 819.200012
best mean reward 864.000000
running time 3415.279276
Train_EnvstepsSoFar : 490001
Train_AverageReturn : 819.2000122070312
Train_BestReturn : 864.0
TimeSinceStart : 3415.2792761325836
Training Loss : 0.2390466034412384
Done logging...




********** Iteration 491000 ************

Training agent...


********** Iteration 492000 ************

Training agent...


********** Iteration 493000 ************

Training agent...


********** Iteration 494000 ************

Training agent...


********** Iteration 495000 ************

Training agent...


********** Iteration 496000 ************

Training agent...


********** Iteration 497000 ************

Training agent...


********** Iteration 498000 ************

Training agent...


********** Iteration 499000 ************

Training agent...


********** Iteration 500000 ************

Training agent...

Beginning logging procedure...
Timestep 500001
mean reward (100 episodes) 817.700012
best mean reward 864.000000
running time 3489.182018
Train_EnvstepsSoFar : 500001
Train_AverageReturn : 817.7000122070312
Train_BestReturn : 864.0
TimeSinceStart : 3489.182017803192
Training Loss : 0.33053719997406006
Done logging...




********** Iteration 501000 ************

Training agent...


********** Iteration 502000 ************

Training agent...


********** Iteration 503000 ************

Training agent...


********** Iteration 504000 ************

Training agent...


********** Iteration 505000 ************

Training agent...


********** Iteration 506000 ************

Training agent...


********** Iteration 507000 ************

Training agent...


********** Iteration 508000 ************

Training agent...


********** Iteration 509000 ************

Training agent...


********** Iteration 510000 ************

Training agent...

Beginning logging procedure...
Timestep 510001
mean reward (100 episodes) 819.799988
best mean reward 864.000000
running time 3562.999932
Train_EnvstepsSoFar : 510001
Train_AverageReturn : 819.7999877929688
Train_BestReturn : 864.0
TimeSinceStart : 3562.9999322891235
Training Loss : 0.1392326056957245
Done logging...




********** Iteration 511000 ************

Training agent...


********** Iteration 512000 ************

Training agent...


********** Iteration 513000 ************

Training agent...


********** Iteration 514000 ************

Training agent...


********** Iteration 515000 ************

Training agent...


********** Iteration 516000 ************

Training agent...


********** Iteration 517000 ************

Training agent...


********** Iteration 518000 ************

Training agent...


********** Iteration 519000 ************

Training agent...


********** Iteration 520000 ************

Training agent...

Beginning logging procedure...
Timestep 520001
mean reward (100 episodes) 768.299988
best mean reward 864.000000
running time 3638.749420
Train_EnvstepsSoFar : 520001
Train_AverageReturn : 768.2999877929688
Train_BestReturn : 864.0
TimeSinceStart : 3638.7494196891785
Training Loss : 0.40363338589668274
Done logging...




********** Iteration 521000 ************

Training agent...


********** Iteration 522000 ************

Training agent...


********** Iteration 523000 ************

Training agent...


********** Iteration 524000 ************

Training agent...


********** Iteration 525000 ************

Training agent...


********** Iteration 526000 ************

Training agent...


********** Iteration 527000 ************

Training agent...


********** Iteration 528000 ************

Training agent...


********** Iteration 529000 ************

Training agent...


********** Iteration 530000 ************

Training agent...

Beginning logging procedure...
Timestep 530001
mean reward (100 episodes) 795.599976
best mean reward 864.000000
running time 3714.550126
Train_EnvstepsSoFar : 530001
Train_AverageReturn : 795.5999755859375
Train_BestReturn : 864.0
TimeSinceStart : 3714.5501255989075
Training Loss : 0.2835472822189331
Done logging...




********** Iteration 531000 ************

Training agent...


********** Iteration 532000 ************

Training agent...


********** Iteration 533000 ************

Training agent...


********** Iteration 534000 ************

Training agent...


********** Iteration 535000 ************

Training agent...


********** Iteration 536000 ************

Training agent...


********** Iteration 537000 ************

Training agent...


********** Iteration 538000 ************

Training agent...


********** Iteration 539000 ************

Training agent...


********** Iteration 540000 ************

Training agent...

Beginning logging procedure...
Timestep 540001
mean reward (100 episodes) 863.099976
best mean reward 864.000000
running time 3790.033641
Train_EnvstepsSoFar : 540001
Train_AverageReturn : 863.0999755859375
Train_BestReturn : 864.0
TimeSinceStart : 3790.0336406230927
Training Loss : 0.19356310367584229
Done logging...




********** Iteration 541000 ************

Training agent...


********** Iteration 542000 ************

Training agent...


********** Iteration 543000 ************

Training agent...


********** Iteration 544000 ************

Training agent...


********** Iteration 545000 ************

Training agent...


********** Iteration 546000 ************

Training agent...


********** Iteration 547000 ************

Training agent...


********** Iteration 548000 ************

Training agent...


********** Iteration 549000 ************

Training agent...


********** Iteration 550000 ************

Training agent...

Beginning logging procedure...
Timestep 550001
mean reward (100 episodes) 938.299988
best mean reward 938.299988
running time 3866.477877
Train_EnvstepsSoFar : 550001
Train_AverageReturn : 938.2999877929688
Train_BestReturn : 938.2999877929688
TimeSinceStart : 3866.477877140045
Training Loss : 0.3834921717643738
Done logging...




********** Iteration 551000 ************

Training agent...


********** Iteration 552000 ************

Training agent...


********** Iteration 553000 ************

Training agent...


********** Iteration 554000 ************

Training agent...


********** Iteration 555000 ************

Training agent...


********** Iteration 556000 ************

Training agent...


********** Iteration 557000 ************

Training agent...


********** Iteration 558000 ************

Training agent...


********** Iteration 559000 ************

Training agent...


********** Iteration 560000 ************

Training agent...

Beginning logging procedure...
Timestep 560001
mean reward (100 episodes) 905.799988
best mean reward 938.299988
running time 3943.854114
Train_EnvstepsSoFar : 560001
Train_AverageReturn : 905.7999877929688
Train_BestReturn : 938.2999877929688
TimeSinceStart : 3943.854113817215
Training Loss : 0.3802991807460785
Done logging...




********** Iteration 561000 ************

Training agent...


********** Iteration 562000 ************

Training agent...


********** Iteration 563000 ************

Training agent...


********** Iteration 564000 ************

Training agent...


********** Iteration 565000 ************

Training agent...


********** Iteration 566000 ************

Training agent...


********** Iteration 567000 ************

Training agent...


********** Iteration 568000 ************

Training agent...


********** Iteration 569000 ************

Training agent...


********** Iteration 570000 ************

Training agent...

Beginning logging procedure...
Timestep 570001
mean reward (100 episodes) 860.200012
best mean reward 938.299988
running time 4020.334906
Train_EnvstepsSoFar : 570001
Train_AverageReturn : 860.2000122070312
Train_BestReturn : 938.2999877929688
TimeSinceStart : 4020.334906101227
Training Loss : 0.20358410477638245
Done logging...




********** Iteration 571000 ************

Training agent...


********** Iteration 572000 ************

Training agent...


********** Iteration 573000 ************

Training agent...


********** Iteration 574000 ************

Training agent...


********** Iteration 575000 ************

Training agent...


********** Iteration 576000 ************

Training agent...


********** Iteration 577000 ************

Training agent...


********** Iteration 578000 ************

Training agent...


********** Iteration 579000 ************

Training agent...


********** Iteration 580000 ************

Training agent...

Beginning logging procedure...
Timestep 580001
mean reward (100 episodes) 883.599976
best mean reward 938.299988
running time 4096.972532
Train_EnvstepsSoFar : 580001
Train_AverageReturn : 883.5999755859375
Train_BestReturn : 938.2999877929688
TimeSinceStart : 4096.97253203392
Training Loss : 0.5270805358886719
Done logging...




********** Iteration 581000 ************

Training agent...


********** Iteration 582000 ************

Training agent...


********** Iteration 583000 ************

Training agent...


********** Iteration 584000 ************

Training agent...


********** Iteration 585000 ************

Training agent...


********** Iteration 586000 ************

Training agent...


********** Iteration 587000 ************

Training agent...


********** Iteration 588000 ************

Training agent...


********** Iteration 589000 ************

Training agent...


********** Iteration 590000 ************

Training agent...

Beginning logging procedure...
Timestep 590001
mean reward (100 episodes) 883.799988
best mean reward 938.299988
running time 4173.977002
Train_EnvstepsSoFar : 590001
Train_AverageReturn : 883.7999877929688
Train_BestReturn : 938.2999877929688
TimeSinceStart : 4173.977001905441
Training Loss : 0.6592468023300171
Done logging...




********** Iteration 591000 ************

Training agent...


********** Iteration 592000 ************

Training agent...


********** Iteration 593000 ************

Training agent...


********** Iteration 594000 ************

Training agent...


********** Iteration 595000 ************

Training agent...


********** Iteration 596000 ************

Training agent...


********** Iteration 597000 ************

Training agent...


********** Iteration 598000 ************

Training agent...


********** Iteration 599000 ************

Training agent...


********** Iteration 600000 ************

Training agent...

Beginning logging procedure...
Timestep 600001
mean reward (100 episodes) 869.700012
best mean reward 938.299988
running time 4249.885908
Train_EnvstepsSoFar : 600001
Train_AverageReturn : 869.7000122070312
Train_BestReturn : 938.2999877929688
TimeSinceStart : 4249.88590836525
Training Loss : 0.33912330865859985
Done logging...




********** Iteration 601000 ************

Training agent...


********** Iteration 602000 ************

Training agent...


********** Iteration 603000 ************

Training agent...


********** Iteration 604000 ************

Training agent...


********** Iteration 605000 ************

Training agent...


********** Iteration 606000 ************

Training agent...


********** Iteration 607000 ************

Training agent...


********** Iteration 608000 ************

Training agent...


********** Iteration 609000 ************

Training agent...


********** Iteration 610000 ************

Training agent...

Beginning logging procedure...
Timestep 610001
mean reward (100 episodes) 863.900024
best mean reward 938.299988
running time 4325.731477
Train_EnvstepsSoFar : 610001
Train_AverageReturn : 863.9000244140625
Train_BestReturn : 938.2999877929688
TimeSinceStart : 4325.731476545334
Training Loss : 0.17432403564453125
Done logging...




********** Iteration 611000 ************

Training agent...


********** Iteration 612000 ************

Training agent...


********** Iteration 613000 ************

Training agent...


********** Iteration 614000 ************

Training agent...


********** Iteration 615000 ************

Training agent...


********** Iteration 616000 ************

Training agent...


********** Iteration 617000 ************

Training agent...


********** Iteration 618000 ************

Training agent...


********** Iteration 619000 ************

Training agent...


********** Iteration 620000 ************

Training agent...

Beginning logging procedure...
Timestep 620001
mean reward (100 episodes) 910.500000
best mean reward 938.299988
running time 4401.766016
Train_EnvstepsSoFar : 620001
Train_AverageReturn : 910.5
Train_BestReturn : 938.2999877929688
TimeSinceStart : 4401.766016483307
Training Loss : 0.31824082136154175
Done logging...




********** Iteration 621000 ************

Training agent...


********** Iteration 622000 ************

Training agent...


********** Iteration 623000 ************

Training agent...


********** Iteration 624000 ************

Training agent...


********** Iteration 625000 ************

Training agent...


********** Iteration 626000 ************

Training agent...


********** Iteration 627000 ************

Training agent...


********** Iteration 628000 ************

Training agent...


********** Iteration 629000 ************

Training agent...


********** Iteration 630000 ************

Training agent...

Beginning logging procedure...
Timestep 630001
mean reward (100 episodes) 1035.599976
best mean reward 1035.599976
running time 4476.347570
Train_EnvstepsSoFar : 630001
Train_AverageReturn : 1035.5999755859375
Train_BestReturn : 1035.5999755859375
TimeSinceStart : 4476.3475704193115
Training Loss : 0.544831395149231
Done logging...




********** Iteration 631000 ************

Training agent...


********** Iteration 632000 ************

Training agent...


********** Iteration 633000 ************

Training agent...


********** Iteration 634000 ************

Training agent...


********** Iteration 635000 ************

Training agent...


********** Iteration 636000 ************

Training agent...


********** Iteration 637000 ************

Training agent...


********** Iteration 638000 ************

Training agent...


********** Iteration 639000 ************

Training agent...


********** Iteration 640000 ************

Training agent...

Beginning logging procedure...
Timestep 640001
mean reward (100 episodes) 1072.199951
best mean reward 1072.199951
running time 4552.562186
Train_EnvstepsSoFar : 640001
Train_AverageReturn : 1072.199951171875
Train_BestReturn : 1072.199951171875
TimeSinceStart : 4552.5621864795685
Training Loss : 0.2497774064540863
Done logging...




********** Iteration 641000 ************

Training agent...


********** Iteration 642000 ************

Training agent...


********** Iteration 643000 ************

Training agent...


********** Iteration 644000 ************

Training agent...


********** Iteration 645000 ************

Training agent...


********** Iteration 646000 ************

Training agent...


********** Iteration 647000 ************

Training agent...


********** Iteration 648000 ************

Training agent...


********** Iteration 649000 ************

Training agent...


********** Iteration 650000 ************

Training agent...

Beginning logging procedure...
Timestep 650001
mean reward (100 episodes) 1098.199951
best mean reward 1098.199951
running time 4628.800652
Train_EnvstepsSoFar : 650001
Train_AverageReturn : 1098.199951171875
Train_BestReturn : 1098.199951171875
TimeSinceStart : 4628.800652265549
Training Loss : 0.2443728744983673
Done logging...




********** Iteration 651000 ************

Training agent...


********** Iteration 652000 ************

Training agent...


********** Iteration 653000 ************

Training agent...


********** Iteration 654000 ************

Training agent...


********** Iteration 655000 ************

Training agent...


********** Iteration 656000 ************

Training agent...


********** Iteration 657000 ************

Training agent...


********** Iteration 658000 ************

Training agent...


********** Iteration 659000 ************

Training agent...


********** Iteration 660000 ************

Training agent...

Beginning logging procedure...
Timestep 660001
mean reward (100 episodes) 1101.199951
best mean reward 1101.199951
running time 4704.340204
Train_EnvstepsSoFar : 660001
Train_AverageReturn : 1101.199951171875
Train_BestReturn : 1101.199951171875
TimeSinceStart : 4704.34020447731
Training Loss : 0.5130530595779419
Done logging...




********** Iteration 661000 ************

Training agent...


********** Iteration 662000 ************

Training agent...


********** Iteration 663000 ************

Training agent...


********** Iteration 664000 ************

Training agent...


********** Iteration 665000 ************

Training agent...


********** Iteration 666000 ************

Training agent...


********** Iteration 667000 ************

Training agent...


********** Iteration 668000 ************

Training agent...


********** Iteration 669000 ************

Training agent...


********** Iteration 670000 ************

Training agent...

Beginning logging procedure...
Timestep 670001
mean reward (100 episodes) 1084.099976
best mean reward 1101.199951
running time 4780.212230
Train_EnvstepsSoFar : 670001
Train_AverageReturn : 1084.0999755859375
Train_BestReturn : 1101.199951171875
TimeSinceStart : 4780.212229728699
Training Loss : 0.4917534589767456
Done logging...




********** Iteration 671000 ************

Training agent...


********** Iteration 672000 ************

Training agent...


********** Iteration 673000 ************

Training agent...


********** Iteration 674000 ************

Training agent...


********** Iteration 675000 ************

Training agent...


********** Iteration 676000 ************

Training agent...


********** Iteration 677000 ************

Training agent...


********** Iteration 678000 ************

Training agent...


********** Iteration 679000 ************

Training agent...


********** Iteration 680000 ************

Training agent...

Beginning logging procedure...
Timestep 680001
mean reward (100 episodes) 1032.699951
best mean reward 1101.199951
running time 4856.804516
Train_EnvstepsSoFar : 680001
Train_AverageReturn : 1032.699951171875
Train_BestReturn : 1101.199951171875
TimeSinceStart : 4856.80451631546
Training Loss : 0.3539063036441803
Done logging...




********** Iteration 681000 ************

Training agent...


********** Iteration 682000 ************

Training agent...


********** Iteration 683000 ************

Training agent...


********** Iteration 684000 ************

Training agent...


********** Iteration 685000 ************

Training agent...


********** Iteration 686000 ************

Training agent...


********** Iteration 687000 ************

Training agent...


********** Iteration 688000 ************

Training agent...


********** Iteration 689000 ************

Training agent...


********** Iteration 690000 ************

Training agent...

Beginning logging procedure...
Timestep 690001
mean reward (100 episodes) 1138.599976
best mean reward 1138.599976
running time 4932.896899
Train_EnvstepsSoFar : 690001
Train_AverageReturn : 1138.5999755859375
Train_BestReturn : 1138.5999755859375
TimeSinceStart : 4932.896899223328
Training Loss : 0.5238468050956726
Done logging...




********** Iteration 691000 ************

Training agent...


********** Iteration 692000 ************

Training agent...


********** Iteration 693000 ************

Training agent...


********** Iteration 694000 ************

Training agent...


********** Iteration 695000 ************

Training agent...


********** Iteration 696000 ************

Training agent...


********** Iteration 697000 ************

Training agent...


********** Iteration 698000 ************

Training agent...


********** Iteration 699000 ************

Training agent...


********** Iteration 700000 ************

Training agent...

Beginning logging procedure...
Timestep 700001
mean reward (100 episodes) 1151.000000
best mean reward 1151.000000
running time 5008.563240
Train_EnvstepsSoFar : 700001
Train_AverageReturn : 1151.0
Train_BestReturn : 1151.0
TimeSinceStart : 5008.5632400512695
Training Loss : 0.24293342232704163
Done logging...




********** Iteration 701000 ************

Training agent...


********** Iteration 702000 ************

Training agent...


********** Iteration 703000 ************

Training agent...


********** Iteration 704000 ************

Training agent...


********** Iteration 705000 ************

Training agent...


********** Iteration 706000 ************

Training agent...


********** Iteration 707000 ************

Training agent...


********** Iteration 708000 ************

Training agent...


********** Iteration 709000 ************

Training agent...


********** Iteration 710000 ************

Training agent...

Beginning logging procedure...
Timestep 710001
mean reward (100 episodes) 1135.000000
best mean reward 1151.000000
running time 5085.462287
Train_EnvstepsSoFar : 710001
Train_AverageReturn : 1135.0
Train_BestReturn : 1151.0
TimeSinceStart : 5085.462286710739
Training Loss : 0.19208192825317383
Done logging...




********** Iteration 711000 ************

Training agent...


********** Iteration 712000 ************

Training agent...


********** Iteration 713000 ************

Training agent...


********** Iteration 714000 ************

Training agent...


********** Iteration 715000 ************

Training agent...


********** Iteration 716000 ************

Training agent...


********** Iteration 717000 ************

Training agent...


********** Iteration 718000 ************

Training agent...


********** Iteration 719000 ************

Training agent...


********** Iteration 720000 ************

Training agent...

Beginning logging procedure...
Timestep 720001
mean reward (100 episodes) 1145.199951
best mean reward 1151.000000
running time 5160.950678
Train_EnvstepsSoFar : 720001
Train_AverageReturn : 1145.199951171875
Train_BestReturn : 1151.0
TimeSinceStart : 5160.950677871704
Training Loss : 0.3097938895225525
Done logging...




********** Iteration 721000 ************

Training agent...


********** Iteration 722000 ************

Training agent...


********** Iteration 723000 ************

Training agent...


********** Iteration 724000 ************

Training agent...


********** Iteration 725000 ************

Training agent...


********** Iteration 726000 ************

Training agent...


********** Iteration 727000 ************

Training agent...


********** Iteration 728000 ************

Training agent...


********** Iteration 729000 ************

Training agent...


********** Iteration 730000 ************

Training agent...

Beginning logging procedure...
Timestep 730001
mean reward (100 episodes) 1164.199951
best mean reward 1164.199951
running time 5235.772318
Train_EnvstepsSoFar : 730001
Train_AverageReturn : 1164.199951171875
Train_BestReturn : 1164.199951171875
TimeSinceStart : 5235.772318124771
Training Loss : 0.2198905646800995
Done logging...




********** Iteration 731000 ************

Training agent...


********** Iteration 732000 ************

Training agent...


********** Iteration 733000 ************

Training agent...


********** Iteration 734000 ************

Training agent...


********** Iteration 735000 ************

Training agent...


********** Iteration 736000 ************

Training agent...


********** Iteration 737000 ************

Training agent...


********** Iteration 738000 ************

Training agent...


********** Iteration 739000 ************

Training agent...


********** Iteration 740000 ************

Training agent...

Beginning logging procedure...
Timestep 740001
mean reward (100 episodes) 1135.099976
best mean reward 1164.199951
running time 5310.449723
Train_EnvstepsSoFar : 740001
Train_AverageReturn : 1135.0999755859375
Train_BestReturn : 1164.199951171875
TimeSinceStart : 5310.449722766876
Training Loss : 0.649126410484314
Done logging...




********** Iteration 741000 ************

Training agent...


********** Iteration 742000 ************

Training agent...


********** Iteration 743000 ************

Training agent...


********** Iteration 744000 ************

Training agent...


********** Iteration 745000 ************

Training agent...


********** Iteration 746000 ************

Training agent...


********** Iteration 747000 ************

Training agent...


********** Iteration 748000 ************

Training agent...


********** Iteration 749000 ************

Training agent...


********** Iteration 750000 ************

Training agent...

Beginning logging procedure...
Timestep 750001
mean reward (100 episodes) 1153.699951
best mean reward 1164.199951
running time 5385.116079
Train_EnvstepsSoFar : 750001
Train_AverageReturn : 1153.699951171875
Train_BestReturn : 1164.199951171875
TimeSinceStart : 5385.116078615189
Training Loss : 0.3616302013397217
Done logging...




********** Iteration 751000 ************

Training agent...


********** Iteration 752000 ************

Training agent...


********** Iteration 753000 ************

Training agent...


********** Iteration 754000 ************

Training agent...


********** Iteration 755000 ************

Training agent...


********** Iteration 756000 ************

Training agent...


********** Iteration 757000 ************

Training agent...


********** Iteration 758000 ************

Training agent...


********** Iteration 759000 ************

Training agent...


********** Iteration 760000 ************

Training agent...

Beginning logging procedure...
Timestep 760001
mean reward (100 episodes) 1265.300049
best mean reward 1265.300049
running time 5460.163751
Train_EnvstepsSoFar : 760001
Train_AverageReturn : 1265.300048828125
Train_BestReturn : 1265.300048828125
TimeSinceStart : 5460.163750886917
Training Loss : 0.4075339436531067
Done logging...




********** Iteration 761000 ************

Training agent...


********** Iteration 762000 ************

Training agent...


********** Iteration 763000 ************

Training agent...


********** Iteration 764000 ************

Training agent...


********** Iteration 765000 ************

Training agent...


********** Iteration 766000 ************

Training agent...


********** Iteration 767000 ************

Training agent...


********** Iteration 768000 ************

Training agent...


********** Iteration 769000 ************

Training agent...


********** Iteration 770000 ************

Training agent...

Beginning logging procedure...
Timestep 770001
mean reward (100 episodes) 1251.099976
best mean reward 1265.300049
running time 5535.099948
Train_EnvstepsSoFar : 770001
Train_AverageReturn : 1251.0999755859375
Train_BestReturn : 1265.300048828125
TimeSinceStart : 5535.099947929382
Training Loss : 0.24489259719848633
Done logging...




********** Iteration 771000 ************

Training agent...


********** Iteration 772000 ************

Training agent...


********** Iteration 773000 ************

Training agent...


********** Iteration 774000 ************

Training agent...


********** Iteration 775000 ************

Training agent...


********** Iteration 776000 ************

Training agent...


********** Iteration 777000 ************

Training agent...


********** Iteration 778000 ************

Training agent...


********** Iteration 779000 ************

Training agent...


********** Iteration 780000 ************

Training agent...

Beginning logging procedure...
Timestep 780001
mean reward (100 episodes) 1345.199951
best mean reward 1345.199951
running time 5609.416426
Train_EnvstepsSoFar : 780001
Train_AverageReturn : 1345.199951171875
Train_BestReturn : 1345.199951171875
TimeSinceStart : 5609.416425943375
Training Loss : 0.29033827781677246
Done logging...




********** Iteration 781000 ************

Training agent...


********** Iteration 782000 ************

Training agent...


********** Iteration 783000 ************

Training agent...


********** Iteration 784000 ************

Training agent...


********** Iteration 785000 ************

Training agent...


********** Iteration 786000 ************

Training agent...


********** Iteration 787000 ************

Training agent...


********** Iteration 788000 ************

Training agent...


********** Iteration 789000 ************

Training agent...


********** Iteration 790000 ************

Training agent...

Beginning logging procedure...
Timestep 790001
mean reward (100 episodes) 1346.699951
best mean reward 1346.699951
running time 5684.503415
Train_EnvstepsSoFar : 790001
Train_AverageReturn : 1346.699951171875
Train_BestReturn : 1346.699951171875
TimeSinceStart : 5684.503415107727
Training Loss : 0.5615831613540649
Done logging...




********** Iteration 791000 ************

Training agent...


********** Iteration 792000 ************

Training agent...


********** Iteration 793000 ************

Training agent...


********** Iteration 794000 ************

Training agent...


********** Iteration 795000 ************

Training agent...


********** Iteration 796000 ************

Training agent...


********** Iteration 797000 ************

Training agent...


********** Iteration 798000 ************

Training agent...


********** Iteration 799000 ************

Training agent...


********** Iteration 800000 ************

Training agent...

Beginning logging procedure...
Timestep 800001
mean reward (100 episodes) 1379.300049
best mean reward 1379.300049
running time 5759.984180
Train_EnvstepsSoFar : 800001
Train_AverageReturn : 1379.300048828125
Train_BestReturn : 1379.300048828125
TimeSinceStart : 5759.984180212021
Training Loss : 0.5166763067245483
Done logging...




********** Iteration 801000 ************

Training agent...


********** Iteration 802000 ************

Training agent...


********** Iteration 803000 ************

Training agent...


********** Iteration 804000 ************

Training agent...


********** Iteration 805000 ************

Training agent...


********** Iteration 806000 ************

Training agent...


********** Iteration 807000 ************

Training agent...


********** Iteration 808000 ************

Training agent...


********** Iteration 809000 ************

Training agent...


********** Iteration 810000 ************

Training agent...

Beginning logging procedure...
Timestep 810001
mean reward (100 episodes) 1347.099976
best mean reward 1379.300049
running time 5835.031526
Train_EnvstepsSoFar : 810001
Train_AverageReturn : 1347.0999755859375
Train_BestReturn : 1379.300048828125
TimeSinceStart : 5835.031526327133
Training Loss : 0.38757506012916565
Done logging...




********** Iteration 811000 ************

Training agent...


********** Iteration 812000 ************

Training agent...


********** Iteration 813000 ************

Training agent...


********** Iteration 814000 ************

Training agent...


********** Iteration 815000 ************

Training agent...


********** Iteration 816000 ************

Training agent...


********** Iteration 817000 ************

Training agent...


********** Iteration 818000 ************

Training agent...


********** Iteration 819000 ************

Training agent...


********** Iteration 820000 ************

Training agent...

Beginning logging procedure...
Timestep 820001
mean reward (100 episodes) 1357.500000
best mean reward 1379.300049
running time 5910.061235
Train_EnvstepsSoFar : 820001
Train_AverageReturn : 1357.5
Train_BestReturn : 1379.300048828125
TimeSinceStart : 5910.061235427856
Training Loss : 0.5860251188278198
Done logging...




********** Iteration 821000 ************

Training agent...


********** Iteration 822000 ************

Training agent...


********** Iteration 823000 ************

Training agent...


********** Iteration 824000 ************

Training agent...


********** Iteration 825000 ************

Training agent...


********** Iteration 826000 ************

Training agent...


********** Iteration 827000 ************

Training agent...


********** Iteration 828000 ************

Training agent...


********** Iteration 829000 ************

Training agent...


********** Iteration 830000 ************

Training agent...

Beginning logging procedure...
Timestep 830001
mean reward (100 episodes) 1282.900024
best mean reward 1379.300049
running time 5985.149312
Train_EnvstepsSoFar : 830001
Train_AverageReturn : 1282.9000244140625
Train_BestReturn : 1379.300048828125
TimeSinceStart : 5985.149312257767
Training Loss : 0.36666226387023926
Done logging...




********** Iteration 831000 ************

Training agent...


********** Iteration 832000 ************

Training agent...


********** Iteration 833000 ************

Training agent...


********** Iteration 834000 ************

Training agent...


********** Iteration 835000 ************

Training agent...


********** Iteration 836000 ************

Training agent...


********** Iteration 837000 ************

Training agent...


********** Iteration 838000 ************

Training agent...


********** Iteration 839000 ************

Training agent...


********** Iteration 840000 ************

Training agent...

Beginning logging procedure...
Timestep 840001
mean reward (100 episodes) 1378.400024
best mean reward 1379.300049
running time 6060.227032
Train_EnvstepsSoFar : 840001
Train_AverageReturn : 1378.4000244140625
Train_BestReturn : 1379.300048828125
TimeSinceStart : 6060.227031946182
Training Loss : 0.4438108801841736
Done logging...




********** Iteration 841000 ************

Training agent...


********** Iteration 842000 ************

Training agent...


********** Iteration 843000 ************

Training agent...


********** Iteration 844000 ************

Training agent...


********** Iteration 845000 ************

Training agent...


********** Iteration 846000 ************

Training agent...


********** Iteration 847000 ************

Training agent...


********** Iteration 848000 ************

Training agent...


********** Iteration 849000 ************

Training agent...


********** Iteration 850000 ************

Training agent...

Beginning logging procedure...
Timestep 850001
mean reward (100 episodes) 1408.099976
best mean reward 1408.099976
running time 6135.261909
Train_EnvstepsSoFar : 850001
Train_AverageReturn : 1408.0999755859375
Train_BestReturn : 1408.0999755859375
TimeSinceStart : 6135.2619087696075
Training Loss : 0.5836178064346313
Done logging...




********** Iteration 851000 ************

Training agent...


********** Iteration 852000 ************

Training agent...


********** Iteration 853000 ************

Training agent...


********** Iteration 854000 ************

Training agent...


********** Iteration 855000 ************

Training agent...


********** Iteration 856000 ************

Training agent...


********** Iteration 857000 ************

Training agent...


********** Iteration 858000 ************

Training agent...


********** Iteration 859000 ************

Training agent...


********** Iteration 860000 ************

Training agent...

Beginning logging procedure...
Timestep 860001
mean reward (100 episodes) 1459.900024
best mean reward 1459.900024
running time 6210.700783
Train_EnvstepsSoFar : 860001
Train_AverageReturn : 1459.9000244140625
Train_BestReturn : 1459.9000244140625
TimeSinceStart : 6210.700782775879
Training Loss : 0.9449291229248047
Done logging...




********** Iteration 861000 ************

Training agent...


********** Iteration 862000 ************

Training agent...


********** Iteration 863000 ************

Training agent...


********** Iteration 864000 ************

Training agent...


********** Iteration 865000 ************

Training agent...


********** Iteration 866000 ************

Training agent...


********** Iteration 867000 ************

Training agent...


********** Iteration 868000 ************

Training agent...


********** Iteration 869000 ************

Training agent...


********** Iteration 870000 ************

Training agent...

Beginning logging procedure...
Timestep 870001
mean reward (100 episodes) 1396.699951
best mean reward 1459.900024
running time 6285.814258
Train_EnvstepsSoFar : 870001
Train_AverageReturn : 1396.699951171875
Train_BestReturn : 1459.9000244140625
TimeSinceStart : 6285.814257860184
Training Loss : 0.7709910869598389
Done logging...




********** Iteration 871000 ************

Training agent...


********** Iteration 872000 ************

Training agent...


********** Iteration 873000 ************

Training agent...


********** Iteration 874000 ************

Training agent...


********** Iteration 875000 ************

Training agent...


********** Iteration 876000 ************

Training agent...


********** Iteration 877000 ************

Training agent...


********** Iteration 878000 ************

Training agent...


********** Iteration 879000 ************

Training agent...


********** Iteration 880000 ************

Training agent...

Beginning logging procedure...
Timestep 880001
mean reward (100 episodes) 1420.000000
best mean reward 1459.900024
running time 6360.273822
Train_EnvstepsSoFar : 880001
Train_AverageReturn : 1420.0
Train_BestReturn : 1459.9000244140625
TimeSinceStart : 6360.2738218307495
Training Loss : 0.2667504847049713
Done logging...




********** Iteration 881000 ************

Training agent...


********** Iteration 882000 ************

Training agent...


********** Iteration 883000 ************

Training agent...


********** Iteration 884000 ************

Training agent...


********** Iteration 885000 ************

Training agent...


********** Iteration 886000 ************

Training agent...


********** Iteration 887000 ************

Training agent...


********** Iteration 888000 ************

Training agent...


********** Iteration 889000 ************

Training agent...


********** Iteration 890000 ************

Training agent...

Beginning logging procedure...
Timestep 890001
mean reward (100 episodes) 1461.000000
best mean reward 1461.000000
running time 6436.209579
Train_EnvstepsSoFar : 890001
Train_AverageReturn : 1461.0
Train_BestReturn : 1461.0
TimeSinceStart : 6436.209578514099
Training Loss : 0.44163990020751953
Done logging...




********** Iteration 891000 ************

Training agent...


********** Iteration 892000 ************

Training agent...


********** Iteration 893000 ************

Training agent...


********** Iteration 894000 ************

Training agent...


********** Iteration 895000 ************

Training agent...


********** Iteration 896000 ************

Training agent...


********** Iteration 897000 ************

Training agent...


********** Iteration 898000 ************

Training agent...


********** Iteration 899000 ************

Training agent...


********** Iteration 900000 ************

Training agent...

Beginning logging procedure...
Timestep 900001
mean reward (100 episodes) 1431.800049
best mean reward 1461.000000
running time 6512.485516
Train_EnvstepsSoFar : 900001
Train_AverageReturn : 1431.800048828125
Train_BestReturn : 1461.0
TimeSinceStart : 6512.485515832901
Training Loss : 0.61611008644104
Done logging...




********** Iteration 901000 ************

Training agent...


********** Iteration 902000 ************

Training agent...


********** Iteration 903000 ************

Training agent...


********** Iteration 904000 ************

Training agent...


********** Iteration 905000 ************

Training agent...


********** Iteration 906000 ************

Training agent...


********** Iteration 907000 ************

Training agent...


********** Iteration 908000 ************

Training agent...


********** Iteration 909000 ************

Training agent...


********** Iteration 910000 ************

Training agent...

Beginning logging procedure...
Timestep 910001
mean reward (100 episodes) 1411.599976
best mean reward 1461.000000
running time 6587.789124
Train_EnvstepsSoFar : 910001
Train_AverageReturn : 1411.5999755859375
Train_BestReturn : 1461.0
TimeSinceStart : 6587.789124250412
Training Loss : 0.4998432993888855
Done logging...




********** Iteration 911000 ************

Training agent...


********** Iteration 912000 ************

Training agent...


********** Iteration 913000 ************

Training agent...


********** Iteration 914000 ************

Training agent...


********** Iteration 915000 ************

Training agent...


********** Iteration 916000 ************

Training agent...


********** Iteration 917000 ************

Training agent...


********** Iteration 918000 ************

Training agent...


********** Iteration 919000 ************

Training agent...


********** Iteration 920000 ************

Training agent...

Beginning logging procedure...
Timestep 920001
mean reward (100 episodes) 1478.199951
best mean reward 1478.199951
running time 6663.186812
Train_EnvstepsSoFar : 920001
Train_AverageReturn : 1478.199951171875
Train_BestReturn : 1478.199951171875
TimeSinceStart : 6663.186811685562
Training Loss : 1.3888680934906006
Done logging...




********** Iteration 921000 ************

Training agent...


********** Iteration 922000 ************

Training agent...


********** Iteration 923000 ************

Training agent...


********** Iteration 924000 ************

Training agent...


********** Iteration 925000 ************

Training agent...


********** Iteration 926000 ************

Training agent...


********** Iteration 927000 ************

Training agent...


********** Iteration 928000 ************

Training agent...


********** Iteration 929000 ************

Training agent...


********** Iteration 930000 ************

Training agent...

Beginning logging procedure...
Timestep 930001
mean reward (100 episodes) 1481.199951
best mean reward 1481.199951
running time 6739.081358
Train_EnvstepsSoFar : 930001
Train_AverageReturn : 1481.199951171875
Train_BestReturn : 1481.199951171875
TimeSinceStart : 6739.08135843277
Training Loss : 0.7132951021194458
Done logging...




********** Iteration 931000 ************

Training agent...


********** Iteration 932000 ************

Training agent...


********** Iteration 933000 ************

Training agent...


********** Iteration 934000 ************

Training agent...


********** Iteration 935000 ************

Training agent...


********** Iteration 936000 ************

Training agent...


********** Iteration 937000 ************

Training agent...


********** Iteration 938000 ************

Training agent...


********** Iteration 939000 ************

Training agent...


********** Iteration 940000 ************

Training agent...

Beginning logging procedure...
Timestep 940001
mean reward (100 episodes) 1527.000000
best mean reward 1527.000000
running time 6814.830471
Train_EnvstepsSoFar : 940001
Train_AverageReturn : 1527.0
Train_BestReturn : 1527.0
TimeSinceStart : 6814.8304708004
Training Loss : 0.38276129961013794
Done logging...




********** Iteration 941000 ************

Training agent...


********** Iteration 942000 ************

Training agent...


********** Iteration 943000 ************

Training agent...


********** Iteration 944000 ************

Training agent...


********** Iteration 945000 ************

Training agent...


********** Iteration 946000 ************

Training agent...


********** Iteration 947000 ************

Training agent...


********** Iteration 948000 ************

Training agent...


********** Iteration 949000 ************

Training agent...


********** Iteration 950000 ************

Training agent...

Beginning logging procedure...
Timestep 950001
mean reward (100 episodes) 1547.199951
best mean reward 1547.199951
running time 6889.514599
Train_EnvstepsSoFar : 950001
Train_AverageReturn : 1547.199951171875
Train_BestReturn : 1547.199951171875
TimeSinceStart : 6889.514598608017
Training Loss : 0.4811428189277649
Done logging...




********** Iteration 951000 ************

Training agent...


********** Iteration 952000 ************

Training agent...


********** Iteration 953000 ************

Training agent...


********** Iteration 954000 ************

Training agent...


********** Iteration 955000 ************

Training agent...


********** Iteration 956000 ************

Training agent...


********** Iteration 957000 ************

Training agent...


********** Iteration 958000 ************

Training agent...


********** Iteration 959000 ************

Training agent...


********** Iteration 960000 ************

Training agent...

Beginning logging procedure...
Timestep 960001
mean reward (100 episodes) 1572.400024
best mean reward 1572.400024
running time 6965.600095
Train_EnvstepsSoFar : 960001
Train_AverageReturn : 1572.4000244140625
Train_BestReturn : 1572.4000244140625
TimeSinceStart : 6965.600095272064
Training Loss : 0.42387381196022034
Done logging...




********** Iteration 961000 ************

Training agent...


********** Iteration 962000 ************

Training agent...


********** Iteration 963000 ************

Training agent...


********** Iteration 964000 ************

Training agent...


********** Iteration 965000 ************

Training agent...


********** Iteration 966000 ************

Training agent...


********** Iteration 967000 ************

Training agent...


********** Iteration 968000 ************

Training agent...


********** Iteration 969000 ************

Training agent...


********** Iteration 970000 ************

Training agent...

Beginning logging procedure...
Timestep 970001
mean reward (100 episodes) 1487.900024
best mean reward 1572.400024
running time 7041.120636
Train_EnvstepsSoFar : 970001
Train_AverageReturn : 1487.9000244140625
Train_BestReturn : 1572.4000244140625
TimeSinceStart : 7041.120636224747
Training Loss : 0.6053211092948914
Done logging...




********** Iteration 971000 ************

Training agent...


********** Iteration 972000 ************

Training agent...


********** Iteration 973000 ************

Training agent...


********** Iteration 974000 ************

Training agent...


********** Iteration 975000 ************

Training agent...


********** Iteration 976000 ************

Training agent...


********** Iteration 977000 ************

Training agent...


********** Iteration 978000 ************

Training agent...


********** Iteration 979000 ************

Training agent...


********** Iteration 980000 ************

Training agent...

Beginning logging procedure...
Timestep 980001
mean reward (100 episodes) 1515.099976
best mean reward 1572.400024
running time 7117.672430
Train_EnvstepsSoFar : 980001
Train_AverageReturn : 1515.0999755859375
Train_BestReturn : 1572.4000244140625
TimeSinceStart : 7117.672430276871
Training Loss : 0.8978402018547058
Done logging...




********** Iteration 981000 ************

Training agent...


********** Iteration 982000 ************

Training agent...


********** Iteration 983000 ************

Training agent...


********** Iteration 984000 ************

Training agent...


********** Iteration 985000 ************

Training agent...


********** Iteration 986000 ************

Training agent...


********** Iteration 987000 ************

Training agent...


********** Iteration 988000 ************

Training agent...


********** Iteration 989000 ************

Training agent...


********** Iteration 990000 ************

Training agent...

Beginning logging procedure...
Timestep 990001
mean reward (100 episodes) 1478.800049
best mean reward 1572.400024
running time 7193.478447
Train_EnvstepsSoFar : 990001
Train_AverageReturn : 1478.800048828125
Train_BestReturn : 1572.4000244140625
TimeSinceStart : 7193.478446960449
Training Loss : 0.31608909368515015
Done logging...




********** Iteration 991000 ************

Training agent...


********** Iteration 992000 ************

Training agent...


********** Iteration 993000 ************

Training agent...


********** Iteration 994000 ************

Training agent...


********** Iteration 995000 ************

Training agent...


********** Iteration 996000 ************

Training agent...


********** Iteration 997000 ************

Training agent...


********** Iteration 998000 ************

Training agent...


********** Iteration 999000 ************

Training agent...


********** Iteration 1000000 ************

Training agent...

Beginning logging procedure...
Timestep 1000001
mean reward (100 episodes) 1532.199951
best mean reward 1572.400024
running time 7269.642625
Train_EnvstepsSoFar : 1000001
Train_AverageReturn : 1532.199951171875
Train_BestReturn : 1572.4000244140625
TimeSinceStart : 7269.642624616623
Training Loss : 0.41428667306900024
Done logging...




********** Iteration 1001000 ************

Training agent...


********** Iteration 1002000 ************

Training agent...


********** Iteration 1003000 ************

Training agent...


********** Iteration 1004000 ************

Training agent...


********** Iteration 1005000 ************

Training agent...


********** Iteration 1006000 ************

Training agent...


********** Iteration 1007000 ************

Training agent...


********** Iteration 1008000 ************

Training agent...


********** Iteration 1009000 ************

Training agent...


********** Iteration 1010000 ************

Training agent...

Beginning logging procedure...
Timestep 1010001
mean reward (100 episodes) 1549.599976
best mean reward 1572.400024
running time 7344.685644
Train_EnvstepsSoFar : 1010001
Train_AverageReturn : 1549.5999755859375
Train_BestReturn : 1572.4000244140625
TimeSinceStart : 7344.685644388199
Training Loss : 0.9382462501525879
Done logging...




********** Iteration 1011000 ************

Training agent...


********** Iteration 1012000 ************

Training agent...


********** Iteration 1013000 ************

Training agent...


********** Iteration 1014000 ************

Training agent...


********** Iteration 1015000 ************

Training agent...


********** Iteration 1016000 ************

Training agent...


********** Iteration 1017000 ************

Training agent...


********** Iteration 1018000 ************

Training agent...


********** Iteration 1019000 ************

Training agent...


********** Iteration 1020000 ************

Training agent...

Beginning logging procedure...
Timestep 1020001
mean reward (100 episodes) 1540.699951
best mean reward 1572.400024
running time 7420.388072
Train_EnvstepsSoFar : 1020001
Train_AverageReturn : 1540.699951171875
Train_BestReturn : 1572.4000244140625
TimeSinceStart : 7420.388072252274
Training Loss : 0.606036365032196
Done logging...




********** Iteration 1021000 ************

Training agent...


********** Iteration 1022000 ************

Training agent...


********** Iteration 1023000 ************

Training agent...


********** Iteration 1024000 ************

Training agent...


********** Iteration 1025000 ************

Training agent...


********** Iteration 1026000 ************

Training agent...


********** Iteration 1027000 ************

Training agent...


********** Iteration 1028000 ************

Training agent...


********** Iteration 1029000 ************

Training agent...


********** Iteration 1030000 ************

Training agent...

Beginning logging procedure...
Timestep 1030001
mean reward (100 episodes) 1605.599976
best mean reward 1605.599976
running time 7495.093980
Train_EnvstepsSoFar : 1030001
Train_AverageReturn : 1605.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 7495.09397983551
Training Loss : 0.33780336380004883
Done logging...




********** Iteration 1031000 ************

Training agent...


********** Iteration 1032000 ************

Training agent...


********** Iteration 1033000 ************

Training agent...


********** Iteration 1034000 ************

Training agent...


********** Iteration 1035000 ************

Training agent...


********** Iteration 1036000 ************

Training agent...


********** Iteration 1037000 ************

Training agent...


********** Iteration 1038000 ************

Training agent...


********** Iteration 1039000 ************

Training agent...


********** Iteration 1040000 ************

Training agent...

Beginning logging procedure...
Timestep 1040001
mean reward (100 episodes) 1591.099976
best mean reward 1605.599976
running time 7570.555781
Train_EnvstepsSoFar : 1040001
Train_AverageReturn : 1591.0999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 7570.555781364441
Training Loss : 0.2674504518508911
Done logging...




********** Iteration 1041000 ************

Training agent...


********** Iteration 1042000 ************

Training agent...


********** Iteration 1043000 ************

Training agent...


********** Iteration 1044000 ************

Training agent...


********** Iteration 1045000 ************

Training agent...


********** Iteration 1046000 ************

Training agent...


********** Iteration 1047000 ************

Training agent...


********** Iteration 1048000 ************

Training agent...


********** Iteration 1049000 ************

Training agent...


********** Iteration 1050000 ************

Training agent...

Beginning logging procedure...
Timestep 1050001
mean reward (100 episodes) 1523.599976
best mean reward 1605.599976
running time 7646.040420
Train_EnvstepsSoFar : 1050001
Train_AverageReturn : 1523.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 7646.040420293808
Training Loss : 0.8440496921539307
Done logging...




********** Iteration 1051000 ************

Training agent...


********** Iteration 1052000 ************

Training agent...


********** Iteration 1053000 ************

Training agent...


********** Iteration 1054000 ************

Training agent...


********** Iteration 1055000 ************

Training agent...


********** Iteration 1056000 ************

Training agent...


********** Iteration 1057000 ************

Training agent...


********** Iteration 1058000 ************

Training agent...


********** Iteration 1059000 ************

Training agent...


********** Iteration 1060000 ************

Training agent...

Beginning logging procedure...
Timestep 1060001
mean reward (100 episodes) 1550.400024
best mean reward 1605.599976
running time 7720.940116
Train_EnvstepsSoFar : 1060001
Train_AverageReturn : 1550.4000244140625
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 7720.94011592865
Training Loss : 0.8414146900177002
Done logging...




********** Iteration 1061000 ************

Training agent...


********** Iteration 1062000 ************

Training agent...


********** Iteration 1063000 ************

Training agent...


********** Iteration 1064000 ************

Training agent...


********** Iteration 1065000 ************

Training agent...


********** Iteration 1066000 ************

Training agent...


********** Iteration 1067000 ************

Training agent...


********** Iteration 1068000 ************

Training agent...


********** Iteration 1069000 ************

Training agent...


********** Iteration 1070000 ************

Training agent...

Beginning logging procedure...
Timestep 1070001
mean reward (100 episodes) 1568.500000
best mean reward 1605.599976
running time 7794.814690
Train_EnvstepsSoFar : 1070001
Train_AverageReturn : 1568.5
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 7794.8146896362305
Training Loss : 0.39205795526504517
Done logging...




********** Iteration 1071000 ************

Training agent...


********** Iteration 1072000 ************

Training agent...


********** Iteration 1073000 ************

Training agent...


********** Iteration 1074000 ************

Training agent...


********** Iteration 1075000 ************

Training agent...


********** Iteration 1076000 ************

Training agent...


********** Iteration 1077000 ************

Training agent...


********** Iteration 1078000 ************

Training agent...


********** Iteration 1079000 ************

Training agent...


********** Iteration 1080000 ************

Training agent...

Beginning logging procedure...
Timestep 1080001
mean reward (100 episodes) 1515.099976
best mean reward 1605.599976
running time 7869.425792
Train_EnvstepsSoFar : 1080001
Train_AverageReturn : 1515.0999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 7869.425791978836
Training Loss : 0.4542958438396454
Done logging...




********** Iteration 1081000 ************

Training agent...


********** Iteration 1082000 ************

Training agent...


********** Iteration 1083000 ************

Training agent...


********** Iteration 1084000 ************

Training agent...


********** Iteration 1085000 ************

Training agent...


********** Iteration 1086000 ************

Training agent...


********** Iteration 1087000 ************

Training agent...


********** Iteration 1088000 ************

Training agent...


********** Iteration 1089000 ************

Training agent...


********** Iteration 1090000 ************

Training agent...

Beginning logging procedure...
Timestep 1090001
mean reward (100 episodes) 1490.599976
best mean reward 1605.599976
running time 7944.411071
Train_EnvstepsSoFar : 1090001
Train_AverageReturn : 1490.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 7944.411071062088
Training Loss : 0.5496169924736023
Done logging...




********** Iteration 1091000 ************

Training agent...


********** Iteration 1092000 ************

Training agent...


********** Iteration 1093000 ************

Training agent...


********** Iteration 1094000 ************

Training agent...


********** Iteration 1095000 ************

Training agent...


********** Iteration 1096000 ************

Training agent...


********** Iteration 1097000 ************

Training agent...


********** Iteration 1098000 ************

Training agent...


********** Iteration 1099000 ************

Training agent...


********** Iteration 1100000 ************

Training agent...

Beginning logging procedure...
Timestep 1100001
mean reward (100 episodes) 1473.599976
best mean reward 1605.599976
running time 8018.609429
Train_EnvstepsSoFar : 1100001
Train_AverageReturn : 1473.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8018.609428882599
Training Loss : 0.8955391645431519
Done logging...




********** Iteration 1101000 ************

Training agent...


********** Iteration 1102000 ************

Training agent...


********** Iteration 1103000 ************

Training agent...


********** Iteration 1104000 ************

Training agent...


********** Iteration 1105000 ************

Training agent...


********** Iteration 1106000 ************

Training agent...


********** Iteration 1107000 ************

Training agent...


********** Iteration 1108000 ************

Training agent...


********** Iteration 1109000 ************

Training agent...


********** Iteration 1110000 ************

Training agent...

Beginning logging procedure...
Timestep 1110001
mean reward (100 episodes) 1493.599976
best mean reward 1605.599976
running time 8093.157616
Train_EnvstepsSoFar : 1110001
Train_AverageReturn : 1493.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8093.15761590004
Training Loss : 0.5813808441162109
Done logging...




********** Iteration 1111000 ************

Training agent...


********** Iteration 1112000 ************

Training agent...


********** Iteration 1113000 ************

Training agent...


********** Iteration 1114000 ************

Training agent...


********** Iteration 1115000 ************

Training agent...


********** Iteration 1116000 ************

Training agent...


********** Iteration 1117000 ************

Training agent...


********** Iteration 1118000 ************

Training agent...


********** Iteration 1119000 ************

Training agent...


********** Iteration 1120000 ************

Training agent...

Beginning logging procedure...
Timestep 1120001
mean reward (100 episodes) 1563.000000
best mean reward 1605.599976
running time 8167.954016
Train_EnvstepsSoFar : 1120001
Train_AverageReturn : 1563.0
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8167.9540157318115
Training Loss : 0.540847897529602
Done logging...




********** Iteration 1121000 ************

Training agent...


********** Iteration 1122000 ************

Training agent...


********** Iteration 1123000 ************

Training agent...


********** Iteration 1124000 ************

Training agent...


********** Iteration 1125000 ************

Training agent...


********** Iteration 1126000 ************

Training agent...


********** Iteration 1127000 ************

Training agent...


********** Iteration 1128000 ************

Training agent...


********** Iteration 1129000 ************

Training agent...


********** Iteration 1130000 ************

Training agent...

Beginning logging procedure...
Timestep 1130001
mean reward (100 episodes) 1517.199951
best mean reward 1605.599976
running time 8242.512748
Train_EnvstepsSoFar : 1130001
Train_AverageReturn : 1517.199951171875
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8242.512748241425
Training Loss : 0.1983254849910736
Done logging...




********** Iteration 1131000 ************

Training agent...


********** Iteration 1132000 ************

Training agent...


********** Iteration 1133000 ************

Training agent...


********** Iteration 1134000 ************

Training agent...


********** Iteration 1135000 ************

Training agent...


********** Iteration 1136000 ************

Training agent...


********** Iteration 1137000 ************

Training agent...


********** Iteration 1138000 ************

Training agent...


********** Iteration 1139000 ************

Training agent...


********** Iteration 1140000 ************

Training agent...

Beginning logging procedure...
Timestep 1140001
mean reward (100 episodes) 1485.800049
best mean reward 1605.599976
running time 8317.038721
Train_EnvstepsSoFar : 1140001
Train_AverageReturn : 1485.800048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8317.038721323013
Training Loss : 0.4394373893737793
Done logging...




********** Iteration 1141000 ************

Training agent...


********** Iteration 1142000 ************

Training agent...


********** Iteration 1143000 ************

Training agent...


********** Iteration 1144000 ************

Training agent...


********** Iteration 1145000 ************

Training agent...


********** Iteration 1146000 ************

Training agent...


********** Iteration 1147000 ************

Training agent...


********** Iteration 1148000 ************

Training agent...


********** Iteration 1149000 ************

Training agent...


********** Iteration 1150000 ************

Training agent...

Beginning logging procedure...
Timestep 1150001
mean reward (100 episodes) 1422.199951
best mean reward 1605.599976
running time 8391.744671
Train_EnvstepsSoFar : 1150001
Train_AverageReturn : 1422.199951171875
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8391.74467086792
Training Loss : 0.2891446352005005
Done logging...




********** Iteration 1151000 ************

Training agent...


********** Iteration 1152000 ************

Training agent...


********** Iteration 1153000 ************

Training agent...


********** Iteration 1154000 ************

Training agent...


********** Iteration 1155000 ************

Training agent...


********** Iteration 1156000 ************

Training agent...


********** Iteration 1157000 ************

Training agent...


********** Iteration 1158000 ************

Training agent...


********** Iteration 1159000 ************

Training agent...


********** Iteration 1160000 ************

Training agent...

Beginning logging procedure...
Timestep 1160001
mean reward (100 episodes) 1452.500000
best mean reward 1605.599976
running time 8466.458423
Train_EnvstepsSoFar : 1160001
Train_AverageReturn : 1452.5
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8466.458422660828
Training Loss : 0.9904704093933105
Done logging...




********** Iteration 1161000 ************

Training agent...


********** Iteration 1162000 ************

Training agent...


********** Iteration 1163000 ************

Training agent...


********** Iteration 1164000 ************

Training agent...


********** Iteration 1165000 ************

Training agent...


********** Iteration 1166000 ************

Training agent...


********** Iteration 1167000 ************

Training agent...


********** Iteration 1168000 ************

Training agent...


********** Iteration 1169000 ************

Training agent...


********** Iteration 1170000 ************

Training agent...

Beginning logging procedure...
Timestep 1170001
mean reward (100 episodes) 1419.800049
best mean reward 1605.599976
running time 8540.008482
Train_EnvstepsSoFar : 1170001
Train_AverageReturn : 1419.800048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8540.008482217789
Training Loss : 0.6577908992767334
Done logging...




********** Iteration 1171000 ************

Training agent...


********** Iteration 1172000 ************

Training agent...


********** Iteration 1173000 ************

Training agent...


********** Iteration 1174000 ************

Training agent...


********** Iteration 1175000 ************

Training agent...


********** Iteration 1176000 ************

Training agent...


********** Iteration 1177000 ************

Training agent...


********** Iteration 1178000 ************

Training agent...


********** Iteration 1179000 ************

Training agent...


********** Iteration 1180000 ************

Training agent...

Beginning logging procedure...
Timestep 1180001
mean reward (100 episodes) 1387.099976
best mean reward 1605.599976
running time 8614.889430
Train_EnvstepsSoFar : 1180001
Train_AverageReturn : 1387.0999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8614.889429569244
Training Loss : 1.1864200830459595
Done logging...




********** Iteration 1181000 ************

Training agent...


********** Iteration 1182000 ************

Training agent...


********** Iteration 1183000 ************

Training agent...


********** Iteration 1184000 ************

Training agent...


********** Iteration 1185000 ************

Training agent...


********** Iteration 1186000 ************

Training agent...


********** Iteration 1187000 ************

Training agent...


********** Iteration 1188000 ************

Training agent...


********** Iteration 1189000 ************

Training agent...


********** Iteration 1190000 ************

Training agent...

Beginning logging procedure...
Timestep 1190001
mean reward (100 episodes) 1441.900024
best mean reward 1605.599976
running time 8689.141394
Train_EnvstepsSoFar : 1190001
Train_AverageReturn : 1441.9000244140625
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8689.141393661499
Training Loss : 0.20175254344940186
Done logging...




********** Iteration 1191000 ************

Training agent...


********** Iteration 1192000 ************

Training agent...


********** Iteration 1193000 ************

Training agent...


********** Iteration 1194000 ************

Training agent...


********** Iteration 1195000 ************

Training agent...


********** Iteration 1196000 ************

Training agent...


********** Iteration 1197000 ************

Training agent...


********** Iteration 1198000 ************

Training agent...


********** Iteration 1199000 ************

Training agent...


********** Iteration 1200000 ************

Training agent...

Beginning logging procedure...
Timestep 1200001
mean reward (100 episodes) 1449.400024
best mean reward 1605.599976
running time 8763.233111
Train_EnvstepsSoFar : 1200001
Train_AverageReturn : 1449.4000244140625
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8763.233110904694
Training Loss : 0.3452211618423462
Done logging...




********** Iteration 1201000 ************

Training agent...


********** Iteration 1202000 ************

Training agent...


********** Iteration 1203000 ************

Training agent...


********** Iteration 1204000 ************

Training agent...


********** Iteration 1205000 ************

Training agent...


********** Iteration 1206000 ************

Training agent...


********** Iteration 1207000 ************

Training agent...


********** Iteration 1208000 ************

Training agent...


********** Iteration 1209000 ************

Training agent...


********** Iteration 1210000 ************

Training agent...

Beginning logging procedure...
Timestep 1210001
mean reward (100 episodes) 1547.599976
best mean reward 1605.599976
running time 8837.124498
Train_EnvstepsSoFar : 1210001
Train_AverageReturn : 1547.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8837.124497890472
Training Loss : 0.7823838591575623
Done logging...




********** Iteration 1211000 ************

Training agent...


********** Iteration 1212000 ************

Training agent...


********** Iteration 1213000 ************

Training agent...


********** Iteration 1214000 ************

Training agent...


********** Iteration 1215000 ************

Training agent...


********** Iteration 1216000 ************

Training agent...


********** Iteration 1217000 ************

Training agent...


********** Iteration 1218000 ************

Training agent...


********** Iteration 1219000 ************

Training agent...


********** Iteration 1220000 ************

Training agent...

Beginning logging procedure...
Timestep 1220001
mean reward (100 episodes) 1591.699951
best mean reward 1605.599976
running time 8911.254481
Train_EnvstepsSoFar : 1220001
Train_AverageReturn : 1591.699951171875
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8911.254481315613
Training Loss : 1.2572304010391235
Done logging...




********** Iteration 1221000 ************

Training agent...


********** Iteration 1222000 ************

Training agent...


********** Iteration 1223000 ************

Training agent...


********** Iteration 1224000 ************

Training agent...


********** Iteration 1225000 ************

Training agent...


********** Iteration 1226000 ************

Training agent...


********** Iteration 1227000 ************

Training agent...


********** Iteration 1228000 ************

Training agent...


********** Iteration 1229000 ************

Training agent...


********** Iteration 1230000 ************

Training agent...

Beginning logging procedure...
Timestep 1230001
mean reward (100 episodes) 1594.099976
best mean reward 1605.599976
running time 8985.053432
Train_EnvstepsSoFar : 1230001
Train_AverageReturn : 1594.0999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 8985.053431987762
Training Loss : 1.1328333616256714
Done logging...




********** Iteration 1231000 ************

Training agent...


********** Iteration 1232000 ************

Training agent...


********** Iteration 1233000 ************

Training agent...


********** Iteration 1234000 ************

Training agent...


********** Iteration 1235000 ************

Training agent...


********** Iteration 1236000 ************

Training agent...


********** Iteration 1237000 ************

Training agent...


********** Iteration 1238000 ************

Training agent...


********** Iteration 1239000 ************

Training agent...


********** Iteration 1240000 ************

Training agent...

Beginning logging procedure...
Timestep 1240001
mean reward (100 episodes) 1509.800049
best mean reward 1605.599976
running time 9057.915929
Train_EnvstepsSoFar : 1240001
Train_AverageReturn : 1509.800048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9057.915929317474
Training Loss : 0.23448854684829712
Done logging...




********** Iteration 1241000 ************

Training agent...


********** Iteration 1242000 ************

Training agent...


********** Iteration 1243000 ************

Training agent...


********** Iteration 1244000 ************

Training agent...


********** Iteration 1245000 ************

Training agent...


********** Iteration 1246000 ************

Training agent...


********** Iteration 1247000 ************

Training agent...


********** Iteration 1248000 ************

Training agent...


********** Iteration 1249000 ************

Training agent...


********** Iteration 1250000 ************

Training agent...

Beginning logging procedure...
Timestep 1250001
mean reward (100 episodes) 1475.699951
best mean reward 1605.599976
running time 9132.348187
Train_EnvstepsSoFar : 1250001
Train_AverageReturn : 1475.699951171875
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9132.348186731339
Training Loss : 0.3948349356651306
Done logging...




********** Iteration 1251000 ************

Training agent...


********** Iteration 1252000 ************

Training agent...


********** Iteration 1253000 ************

Training agent...


********** Iteration 1254000 ************

Training agent...


********** Iteration 1255000 ************

Training agent...


********** Iteration 1256000 ************

Training agent...


********** Iteration 1257000 ************

Training agent...


********** Iteration 1258000 ************

Training agent...


********** Iteration 1259000 ************

Training agent...


********** Iteration 1260000 ************

Training agent...

Beginning logging procedure...
Timestep 1260001
mean reward (100 episodes) 1420.300049
best mean reward 1605.599976
running time 9205.831509
Train_EnvstepsSoFar : 1260001
Train_AverageReturn : 1420.300048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9205.831509113312
Training Loss : 1.0857551097869873
Done logging...




********** Iteration 1261000 ************

Training agent...


********** Iteration 1262000 ************

Training agent...


********** Iteration 1263000 ************

Training agent...


********** Iteration 1264000 ************

Training agent...


********** Iteration 1265000 ************

Training agent...


********** Iteration 1266000 ************

Training agent...


********** Iteration 1267000 ************

Training agent...


********** Iteration 1268000 ************

Training agent...


********** Iteration 1269000 ************

Training agent...


********** Iteration 1270000 ************

Training agent...

Beginning logging procedure...
Timestep 1270001
mean reward (100 episodes) 1456.099976
best mean reward 1605.599976
running time 9279.041099
Train_EnvstepsSoFar : 1270001
Train_AverageReturn : 1456.0999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9279.041098594666
Training Loss : 0.17657214403152466
Done logging...




********** Iteration 1271000 ************

Training agent...


********** Iteration 1272000 ************

Training agent...


********** Iteration 1273000 ************

Training agent...


********** Iteration 1274000 ************

Training agent...


********** Iteration 1275000 ************

Training agent...


********** Iteration 1276000 ************

Training agent...


********** Iteration 1277000 ************

Training agent...


********** Iteration 1278000 ************

Training agent...


********** Iteration 1279000 ************

Training agent...


********** Iteration 1280000 ************

Training agent...

Beginning logging procedure...
Timestep 1280001
mean reward (100 episodes) 1495.099976
best mean reward 1605.599976
running time 9353.404531
Train_EnvstepsSoFar : 1280001
Train_AverageReturn : 1495.0999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9353.404531002045
Training Loss : 0.49308884143829346
Done logging...




********** Iteration 1281000 ************

Training agent...


********** Iteration 1282000 ************

Training agent...


********** Iteration 1283000 ************

Training agent...


********** Iteration 1284000 ************

Training agent...


********** Iteration 1285000 ************

Training agent...


********** Iteration 1286000 ************

Training agent...


********** Iteration 1287000 ************

Training agent...


********** Iteration 1288000 ************

Training agent...


********** Iteration 1289000 ************

Training agent...


********** Iteration 1290000 ************

Training agent...

Beginning logging procedure...
Timestep 1290001
mean reward (100 episodes) 1467.900024
best mean reward 1605.599976
running time 9427.985394
Train_EnvstepsSoFar : 1290001
Train_AverageReturn : 1467.9000244140625
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9427.985394239426
Training Loss : 0.49624139070510864
Done logging...




********** Iteration 1291000 ************

Training agent...


********** Iteration 1292000 ************

Training agent...


********** Iteration 1293000 ************

Training agent...


********** Iteration 1294000 ************

Training agent...


********** Iteration 1295000 ************

Training agent...


********** Iteration 1296000 ************

Training agent...


********** Iteration 1297000 ************

Training agent...


********** Iteration 1298000 ************

Training agent...


********** Iteration 1299000 ************

Training agent...


********** Iteration 1300000 ************

Training agent...

Beginning logging procedure...
Timestep 1300001
mean reward (100 episodes) 1507.300049
best mean reward 1605.599976
running time 9501.578973
Train_EnvstepsSoFar : 1300001
Train_AverageReturn : 1507.300048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9501.578973054886
Training Loss : 0.6464540362358093
Done logging...




********** Iteration 1301000 ************

Training agent...


********** Iteration 1302000 ************

Training agent...


********** Iteration 1303000 ************

Training agent...


********** Iteration 1304000 ************

Training agent...


********** Iteration 1305000 ************

Training agent...


********** Iteration 1306000 ************

Training agent...


********** Iteration 1307000 ************

Training agent...


********** Iteration 1308000 ************

Training agent...


********** Iteration 1309000 ************

Training agent...


********** Iteration 1310000 ************

Training agent...

Beginning logging procedure...
Timestep 1310001
mean reward (100 episodes) 1489.699951
best mean reward 1605.599976
running time 9575.263867
Train_EnvstepsSoFar : 1310001
Train_AverageReturn : 1489.699951171875
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9575.263866901398
Training Loss : 0.30436617136001587
Done logging...




********** Iteration 1311000 ************

Training agent...


********** Iteration 1312000 ************

Training agent...


********** Iteration 1313000 ************

Training agent...


********** Iteration 1314000 ************

Training agent...


********** Iteration 1315000 ************

Training agent...


********** Iteration 1316000 ************

Training agent...


********** Iteration 1317000 ************

Training agent...


********** Iteration 1318000 ************

Training agent...


********** Iteration 1319000 ************

Training agent...


********** Iteration 1320000 ************

Training agent...

Beginning logging procedure...
Timestep 1320001
mean reward (100 episodes) 1561.300049
best mean reward 1605.599976
running time 9649.143176
Train_EnvstepsSoFar : 1320001
Train_AverageReturn : 1561.300048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9649.143175840378
Training Loss : 1.3921176195144653
Done logging...




********** Iteration 1321000 ************

Training agent...


********** Iteration 1322000 ************

Training agent...


********** Iteration 1323000 ************

Training agent...


********** Iteration 1324000 ************

Training agent...


********** Iteration 1325000 ************

Training agent...


********** Iteration 1326000 ************

Training agent...


********** Iteration 1327000 ************

Training agent...


********** Iteration 1328000 ************

Training agent...


********** Iteration 1329000 ************

Training agent...


********** Iteration 1330000 ************

Training agent...

Beginning logging procedure...
Timestep 1330001
mean reward (100 episodes) 1550.900024
best mean reward 1605.599976
running time 9722.961405
Train_EnvstepsSoFar : 1330001
Train_AverageReturn : 1550.9000244140625
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9722.961404561996
Training Loss : 0.38189229369163513
Done logging...




********** Iteration 1331000 ************

Training agent...


********** Iteration 1332000 ************

Training agent...


********** Iteration 1333000 ************

Training agent...


********** Iteration 1334000 ************

Training agent...


********** Iteration 1335000 ************

Training agent...


********** Iteration 1336000 ************

Training agent...


********** Iteration 1337000 ************

Training agent...


********** Iteration 1338000 ************

Training agent...


********** Iteration 1339000 ************

Training agent...


********** Iteration 1340000 ************

Training agent...

Beginning logging procedure...
Timestep 1340001
mean reward (100 episodes) 1503.599976
best mean reward 1605.599976
running time 9796.938835
Train_EnvstepsSoFar : 1340001
Train_AverageReturn : 1503.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9796.938835382462
Training Loss : 0.9901824593544006
Done logging...




********** Iteration 1341000 ************

Training agent...


********** Iteration 1342000 ************

Training agent...


********** Iteration 1343000 ************

Training agent...


********** Iteration 1344000 ************

Training agent...


********** Iteration 1345000 ************

Training agent...


********** Iteration 1346000 ************

Training agent...


********** Iteration 1347000 ************

Training agent...


********** Iteration 1348000 ************

Training agent...


********** Iteration 1349000 ************

Training agent...


********** Iteration 1350000 ************

Training agent...

Beginning logging procedure...
Timestep 1350001
mean reward (100 episodes) 1496.800049
best mean reward 1605.599976
running time 9871.033240
Train_EnvstepsSoFar : 1350001
Train_AverageReturn : 1496.800048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9871.033239841461
Training Loss : 0.5885042548179626
Done logging...




********** Iteration 1351000 ************

Training agent...


********** Iteration 1352000 ************

Training agent...


********** Iteration 1353000 ************

Training agent...


********** Iteration 1354000 ************

Training agent...


********** Iteration 1355000 ************

Training agent...


********** Iteration 1356000 ************

Training agent...


********** Iteration 1357000 ************

Training agent...


********** Iteration 1358000 ************

Training agent...


********** Iteration 1359000 ************

Training agent...


********** Iteration 1360000 ************

Training agent...

Beginning logging procedure...
Timestep 1360001
mean reward (100 episodes) 1530.199951
best mean reward 1605.599976
running time 9944.327902
Train_EnvstepsSoFar : 1360001
Train_AverageReturn : 1530.199951171875
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 9944.327901601791
Training Loss : 1.1101818084716797
Done logging...




********** Iteration 1361000 ************

Training agent...


********** Iteration 1362000 ************

Training agent...


********** Iteration 1363000 ************

Training agent...


********** Iteration 1364000 ************

Training agent...


********** Iteration 1365000 ************

Training agent...


********** Iteration 1366000 ************

Training agent...


********** Iteration 1367000 ************

Training agent...


********** Iteration 1368000 ************

Training agent...


********** Iteration 1369000 ************

Training agent...


********** Iteration 1370000 ************

Training agent...

Beginning logging procedure...
Timestep 1370001
mean reward (100 episodes) 1528.099976
best mean reward 1605.599976
running time 10018.620697
Train_EnvstepsSoFar : 1370001
Train_AverageReturn : 1528.0999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10018.620697021484
Training Loss : 0.41310274600982666
Done logging...




********** Iteration 1371000 ************

Training agent...


********** Iteration 1372000 ************

Training agent...


********** Iteration 1373000 ************

Training agent...


********** Iteration 1374000 ************

Training agent...


********** Iteration 1375000 ************

Training agent...


********** Iteration 1376000 ************

Training agent...


********** Iteration 1377000 ************

Training agent...


********** Iteration 1378000 ************

Training agent...


********** Iteration 1379000 ************

Training agent...


********** Iteration 1380000 ************

Training agent...

Beginning logging procedure...
Timestep 1380001
mean reward (100 episodes) 1481.199951
best mean reward 1605.599976
running time 10091.221688
Train_EnvstepsSoFar : 1380001
Train_AverageReturn : 1481.199951171875
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10091.221688270569
Training Loss : 0.8472939729690552
Done logging...




********** Iteration 1381000 ************

Training agent...


********** Iteration 1382000 ************

Training agent...


********** Iteration 1383000 ************

Training agent...


********** Iteration 1384000 ************

Training agent...


********** Iteration 1385000 ************

Training agent...


********** Iteration 1386000 ************

Training agent...


********** Iteration 1387000 ************

Training agent...


********** Iteration 1388000 ************

Training agent...


********** Iteration 1389000 ************

Training agent...


********** Iteration 1390000 ************

Training agent...

Beginning logging procedure...
Timestep 1390001
mean reward (100 episodes) 1508.099976
best mean reward 1605.599976
running time 10164.947293
Train_EnvstepsSoFar : 1390001
Train_AverageReturn : 1508.0999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10164.947293043137
Training Loss : 0.29125654697418213
Done logging...




********** Iteration 1391000 ************

Training agent...


********** Iteration 1392000 ************

Training agent...


********** Iteration 1393000 ************

Training agent...


********** Iteration 1394000 ************

Training agent...


********** Iteration 1395000 ************

Training agent...


********** Iteration 1396000 ************

Training agent...


********** Iteration 1397000 ************

Training agent...


********** Iteration 1398000 ************

Training agent...


********** Iteration 1399000 ************

Training agent...


********** Iteration 1400000 ************

Training agent...

Beginning logging procedure...
Timestep 1400001
mean reward (100 episodes) 1488.400024
best mean reward 1605.599976
running time 10238.225603
Train_EnvstepsSoFar : 1400001
Train_AverageReturn : 1488.4000244140625
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10238.225603342056
Training Loss : 0.4243682026863098
Done logging...




********** Iteration 1401000 ************

Training agent...


********** Iteration 1402000 ************

Training agent...


********** Iteration 1403000 ************

Training agent...


********** Iteration 1404000 ************

Training agent...


********** Iteration 1405000 ************

Training agent...


********** Iteration 1406000 ************

Training agent...


********** Iteration 1407000 ************

Training agent...


********** Iteration 1408000 ************

Training agent...


********** Iteration 1409000 ************

Training agent...


********** Iteration 1410000 ************

Training agent...

Beginning logging procedure...
Timestep 1410001
mean reward (100 episodes) 1570.800049
best mean reward 1605.599976
running time 10310.677490
Train_EnvstepsSoFar : 1410001
Train_AverageReturn : 1570.800048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10310.67748951912
Training Loss : 0.17810893058776855
Done logging...




********** Iteration 1411000 ************

Training agent...


********** Iteration 1412000 ************

Training agent...


********** Iteration 1413000 ************

Training agent...


********** Iteration 1414000 ************

Training agent...


********** Iteration 1415000 ************

Training agent...


********** Iteration 1416000 ************

Training agent...


********** Iteration 1417000 ************

Training agent...


********** Iteration 1418000 ************

Training agent...


********** Iteration 1419000 ************

Training agent...


********** Iteration 1420000 ************

Training agent...

Beginning logging procedure...
Timestep 1420001
mean reward (100 episodes) 1521.599976
best mean reward 1605.599976
running time 10383.873154
Train_EnvstepsSoFar : 1420001
Train_AverageReturn : 1521.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10383.87315440178
Training Loss : 0.7181795835494995
Done logging...




********** Iteration 1421000 ************

Training agent...


********** Iteration 1422000 ************

Training agent...


********** Iteration 1423000 ************

Training agent...


********** Iteration 1424000 ************

Training agent...


********** Iteration 1425000 ************

Training agent...


********** Iteration 1426000 ************

Training agent...


********** Iteration 1427000 ************

Training agent...


********** Iteration 1428000 ************

Training agent...


********** Iteration 1429000 ************

Training agent...


********** Iteration 1430000 ************

Training agent...

Beginning logging procedure...
Timestep 1430001
mean reward (100 episodes) 1512.800049
best mean reward 1605.599976
running time 10456.704156
Train_EnvstepsSoFar : 1430001
Train_AverageReturn : 1512.800048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10456.704155921936
Training Loss : 0.6777074933052063
Done logging...




********** Iteration 1431000 ************

Training agent...


********** Iteration 1432000 ************

Training agent...


********** Iteration 1433000 ************

Training agent...


********** Iteration 1434000 ************

Training agent...


********** Iteration 1435000 ************

Training agent...


********** Iteration 1436000 ************

Training agent...


********** Iteration 1437000 ************

Training agent...


********** Iteration 1438000 ************

Training agent...


********** Iteration 1439000 ************

Training agent...


********** Iteration 1440000 ************

Training agent...

Beginning logging procedure...
Timestep 1440001
mean reward (100 episodes) 1511.000000
best mean reward 1605.599976
running time 10529.959365
Train_EnvstepsSoFar : 1440001
Train_AverageReturn : 1511.0
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10529.959364891052
Training Loss : 0.4756764769554138
Done logging...




********** Iteration 1441000 ************

Training agent...


********** Iteration 1442000 ************

Training agent...


********** Iteration 1443000 ************

Training agent...


********** Iteration 1444000 ************

Training agent...


********** Iteration 1445000 ************

Training agent...


********** Iteration 1446000 ************

Training agent...


********** Iteration 1447000 ************

Training agent...


********** Iteration 1448000 ************

Training agent...


********** Iteration 1449000 ************

Training agent...


********** Iteration 1450000 ************

Training agent...

Beginning logging procedure...
Timestep 1450001
mean reward (100 episodes) 1553.599976
best mean reward 1605.599976
running time 10603.181598
Train_EnvstepsSoFar : 1450001
Train_AverageReturn : 1553.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10603.181597709656
Training Loss : 0.3577768802642822
Done logging...




********** Iteration 1451000 ************

Training agent...


********** Iteration 1452000 ************

Training agent...


********** Iteration 1453000 ************

Training agent...


********** Iteration 1454000 ************

Training agent...


********** Iteration 1455000 ************

Training agent...


********** Iteration 1456000 ************

Training agent...


********** Iteration 1457000 ************

Training agent...


********** Iteration 1458000 ************

Training agent...


********** Iteration 1459000 ************

Training agent...


********** Iteration 1460000 ************

Training agent...

Beginning logging procedure...
Timestep 1460001
mean reward (100 episodes) 1563.699951
best mean reward 1605.599976
running time 10676.720305
Train_EnvstepsSoFar : 1460001
Train_AverageReturn : 1563.699951171875
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10676.720305204391
Training Loss : 0.9937411546707153
Done logging...




********** Iteration 1461000 ************

Training agent...


********** Iteration 1462000 ************

Training agent...


********** Iteration 1463000 ************

Training agent...


********** Iteration 1464000 ************

Training agent...


********** Iteration 1465000 ************

Training agent...


********** Iteration 1466000 ************

Training agent...


********** Iteration 1467000 ************

Training agent...


********** Iteration 1468000 ************

Training agent...


********** Iteration 1469000 ************

Training agent...


********** Iteration 1470000 ************

Training agent...

Beginning logging procedure...
Timestep 1470001
mean reward (100 episodes) 1517.900024
best mean reward 1605.599976
running time 10749.641355
Train_EnvstepsSoFar : 1470001
Train_AverageReturn : 1517.9000244140625
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10749.641354560852
Training Loss : 0.6105194091796875
Done logging...




********** Iteration 1471000 ************

Training agent...


********** Iteration 1472000 ************

Training agent...


********** Iteration 1473000 ************

Training agent...


********** Iteration 1474000 ************

Training agent...


********** Iteration 1475000 ************

Training agent...


********** Iteration 1476000 ************

Training agent...


********** Iteration 1477000 ************

Training agent...


********** Iteration 1478000 ************

Training agent...


********** Iteration 1479000 ************

Training agent...


********** Iteration 1480000 ************

Training agent...

Beginning logging procedure...
Timestep 1480001
mean reward (100 episodes) 1505.800049
best mean reward 1605.599976
running time 10823.869046
Train_EnvstepsSoFar : 1480001
Train_AverageReturn : 1505.800048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10823.869045734406
Training Loss : 1.2744319438934326
Done logging...




********** Iteration 1481000 ************

Training agent...


********** Iteration 1482000 ************

Training agent...


********** Iteration 1483000 ************

Training agent...


********** Iteration 1484000 ************

Training agent...


********** Iteration 1485000 ************

Training agent...


********** Iteration 1486000 ************

Training agent...


********** Iteration 1487000 ************

Training agent...


********** Iteration 1488000 ************

Training agent...


********** Iteration 1489000 ************

Training agent...


********** Iteration 1490000 ************

Training agent...

Beginning logging procedure...
Timestep 1490001
mean reward (100 episodes) 1500.800049
best mean reward 1605.599976
running time 10898.022063
Train_EnvstepsSoFar : 1490001
Train_AverageReturn : 1500.800048828125
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10898.02206325531
Training Loss : 0.503537654876709
Done logging...




********** Iteration 1491000 ************

Training agent...


********** Iteration 1492000 ************

Training agent...


********** Iteration 1493000 ************

Training agent...


********** Iteration 1494000 ************

Training agent...


********** Iteration 1495000 ************

Training agent...


********** Iteration 1496000 ************

Training agent...


********** Iteration 1497000 ************

Training agent...


********** Iteration 1498000 ************

Training agent...


********** Iteration 1499000 ************

Training agent...


********** Iteration 1500000 ************

Training agent...

Beginning logging procedure...
Timestep 1500001
mean reward (100 episodes) 1561.900024
best mean reward 1605.599976
running time 10971.558232
Train_EnvstepsSoFar : 1500001
Train_AverageReturn : 1561.9000244140625
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 10971.558231830597
Training Loss : 0.5440578460693359
Done logging...




********** Iteration 1501000 ************

Training agent...


********** Iteration 1502000 ************

Training agent...


********** Iteration 1503000 ************

Training agent...


********** Iteration 1504000 ************

Training agent...


********** Iteration 1505000 ************

Training agent...


********** Iteration 1506000 ************

Training agent...


********** Iteration 1507000 ************

Training agent...


********** Iteration 1508000 ************

Training agent...


********** Iteration 1509000 ************

Training agent...


********** Iteration 1510000 ************

Training agent...

Beginning logging procedure...
Timestep 1510001
mean reward (100 episodes) 1504.599976
best mean reward 1605.599976
running time 11046.650727
Train_EnvstepsSoFar : 1510001
Train_AverageReturn : 1504.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 11046.650726795197
Training Loss : 0.4331468343734741
Done logging...




********** Iteration 1511000 ************

Training agent...


********** Iteration 1512000 ************

Training agent...


********** Iteration 1513000 ************

Training agent...


********** Iteration 1514000 ************

Training agent...


********** Iteration 1515000 ************

Training agent...


********** Iteration 1516000 ************

Training agent...


********** Iteration 1517000 ************

Training agent...


********** Iteration 1518000 ************

Training agent...


********** Iteration 1519000 ************

Training agent...


********** Iteration 1520000 ************

Training agent...

Beginning logging procedure...
Timestep 1520001
mean reward (100 episodes) 1514.599976
best mean reward 1605.599976
running time 11121.413512
Train_EnvstepsSoFar : 1520001
Train_AverageReturn : 1514.5999755859375
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 11121.413511514664
Training Loss : 0.25094497203826904
Done logging...




********** Iteration 1521000 ************

Training agent...


********** Iteration 1522000 ************

Training agent...


********** Iteration 1523000 ************

Training agent...


********** Iteration 1524000 ************

Training agent...


********** Iteration 1525000 ************

Training agent...


********** Iteration 1526000 ************

Training agent...


********** Iteration 1527000 ************

Training agent...


********** Iteration 1528000 ************

Training agent...


********** Iteration 1529000 ************

Training agent...


********** Iteration 1530000 ************

Training agent...

Beginning logging procedure...
Timestep 1530001
mean reward (100 episodes) 1560.699951
best mean reward 1605.599976
running time 11195.033695
Train_EnvstepsSoFar : 1530001
Train_AverageReturn : 1560.699951171875
Train_BestReturn : 1605.5999755859375
TimeSinceStart : 11195.03369474411
Training Loss : 0.8845192790031433
Done logging...




********** Iteration 1531000 ************

Training agent...


********** Iteration 1532000 ************

Training agent...


********** Iteration 1533000 ************

Training agent...


********** Iteration 1534000 ************

Training agent...


********** Iteration 1535000 ************

Training agent...


********** Iteration 1536000 ************

Training agent...


********** Iteration 1537000 ************

Training agent...


********** Iteration 1538000 ************

Training agent...


********** Iteration 1539000 ************

Training agent...


********** Iteration 1540000 ************

Training agent...

Beginning logging procedure...
Timestep 1540001
mean reward (100 episodes) 1639.599976
best mean reward 1639.599976
running time 11268.477491
Train_EnvstepsSoFar : 1540001
Train_AverageReturn : 1639.5999755859375
Train_BestReturn : 1639.5999755859375
TimeSinceStart : 11268.477490901947
Training Loss : 0.497915655374527
Done logging...




********** Iteration 1541000 ************

Training agent...


********** Iteration 1542000 ************

Training agent...


********** Iteration 1543000 ************

Training agent...


********** Iteration 1544000 ************

Training agent...


********** Iteration 1545000 ************

Training agent...


********** Iteration 1546000 ************

Training agent...


********** Iteration 1547000 ************

Training agent...


********** Iteration 1548000 ************

Training agent...


********** Iteration 1549000 ************

Training agent...


********** Iteration 1550000 ************

Training agent...

Beginning logging procedure...
Timestep 1550001
mean reward (100 episodes) 1602.099976
best mean reward 1639.599976
running time 11342.120899
Train_EnvstepsSoFar : 1550001
Train_AverageReturn : 1602.0999755859375
Train_BestReturn : 1639.5999755859375
TimeSinceStart : 11342.120899438858
Training Loss : 0.32774221897125244
Done logging...




********** Iteration 1551000 ************

Training agent...


********** Iteration 1552000 ************

Training agent...


********** Iteration 1553000 ************

Training agent...


********** Iteration 1554000 ************

Training agent...


********** Iteration 1555000 ************

Training agent...


********** Iteration 1556000 ************

Training agent...


********** Iteration 1557000 ************

Training agent...


********** Iteration 1558000 ************

Training agent...


********** Iteration 1559000 ************

Training agent...


********** Iteration 1560000 ************

Training agent...

Beginning logging procedure...
Timestep 1560001
mean reward (100 episodes) 1554.199951
best mean reward 1639.599976
running time 11415.311056
Train_EnvstepsSoFar : 1560001
Train_AverageReturn : 1554.199951171875
Train_BestReturn : 1639.5999755859375
TimeSinceStart : 11415.311055898666
Training Loss : 0.5007085800170898
Done logging...




********** Iteration 1561000 ************

Training agent...


********** Iteration 1562000 ************

Training agent...


********** Iteration 1563000 ************

Training agent...


********** Iteration 1564000 ************

Training agent...


********** Iteration 1565000 ************

Training agent...


********** Iteration 1566000 ************

Training agent...


********** Iteration 1567000 ************

Training agent...


********** Iteration 1568000 ************

Training agent...


********** Iteration 1569000 ************

Training agent...


********** Iteration 1570000 ************

Training agent...

Beginning logging procedure...
Timestep 1570001
mean reward (100 episodes) 1509.199951
best mean reward 1639.599976
running time 11489.470334
Train_EnvstepsSoFar : 1570001
Train_AverageReturn : 1509.199951171875
Train_BestReturn : 1639.5999755859375
TimeSinceStart : 11489.47033405304
Training Loss : 0.3576846122741699
Done logging...




********** Iteration 1571000 ************

Training agent...


********** Iteration 1572000 ************

Training agent...


********** Iteration 1573000 ************

Training agent...


********** Iteration 1574000 ************

Training agent...


********** Iteration 1575000 ************

Training agent...


********** Iteration 1576000 ************

Training agent...


********** Iteration 1577000 ************

Training agent...


********** Iteration 1578000 ************

Training agent...


********** Iteration 1579000 ************

Training agent...


********** Iteration 1580000 ************

Training agent...

Beginning logging procedure...
Timestep 1580001
mean reward (100 episodes) 1531.599976
best mean reward 1639.599976
running time 11563.517387
Train_EnvstepsSoFar : 1580001
Train_AverageReturn : 1531.5999755859375
Train_BestReturn : 1639.5999755859375
TimeSinceStart : 11563.5173869133
Training Loss : 0.6666662096977234
Done logging...




********** Iteration 1581000 ************

Training agent...


********** Iteration 1582000 ************

Training agent...


********** Iteration 1583000 ************

Training agent...


********** Iteration 1584000 ************

Training agent...


********** Iteration 1585000 ************

Training agent...


********** Iteration 1586000 ************

Training agent...


********** Iteration 1587000 ************

Training agent...


********** Iteration 1588000 ************

Training agent...


********** Iteration 1589000 ************

Training agent...


********** Iteration 1590000 ************

Training agent...

Beginning logging procedure...
Timestep 1590001
mean reward (100 episodes) 1612.400024
best mean reward 1639.599976
running time 11637.818407
Train_EnvstepsSoFar : 1590001
Train_AverageReturn : 1612.4000244140625
Train_BestReturn : 1639.5999755859375
TimeSinceStart : 11637.818407297134
Training Loss : 0.46765315532684326
Done logging...




********** Iteration 1591000 ************

Training agent...


********** Iteration 1592000 ************

Training agent...


********** Iteration 1593000 ************

Training agent...


********** Iteration 1594000 ************

Training agent...


********** Iteration 1595000 ************

Training agent...


********** Iteration 1596000 ************

Training agent...


********** Iteration 1597000 ************

Training agent...


********** Iteration 1598000 ************

Training agent...


********** Iteration 1599000 ************

Training agent...


********** Iteration 1600000 ************

Training agent...

Beginning logging procedure...
Timestep 1600001
mean reward (100 episodes) 1662.199951
best mean reward 1662.199951
running time 11711.557619
Train_EnvstepsSoFar : 1600001
Train_AverageReturn : 1662.199951171875
Train_BestReturn : 1662.199951171875
TimeSinceStart : 11711.557619333267
Training Loss : 0.5606285929679871
Done logging...




********** Iteration 1601000 ************

Training agent...


********** Iteration 1602000 ************

Training agent...


********** Iteration 1603000 ************

Training agent...


********** Iteration 1604000 ************

Training agent...


********** Iteration 1605000 ************

Training agent...


********** Iteration 1606000 ************

Training agent...


********** Iteration 1607000 ************

Training agent...


********** Iteration 1608000 ************

Training agent...


********** Iteration 1609000 ************

Training agent...


********** Iteration 1610000 ************

Training agent...

Beginning logging procedure...
Timestep 1610001
mean reward (100 episodes) 1657.500000
best mean reward 1662.199951
running time 11785.416985
Train_EnvstepsSoFar : 1610001
Train_AverageReturn : 1657.5
Train_BestReturn : 1662.199951171875
TimeSinceStart : 11785.416985034943
Training Loss : 0.8161298632621765
Done logging...




********** Iteration 1611000 ************

Training agent...


********** Iteration 1612000 ************

Training agent...


********** Iteration 1613000 ************

Training agent...


********** Iteration 1614000 ************

Training agent...


********** Iteration 1615000 ************

Training agent...


********** Iteration 1616000 ************

Training agent...


********** Iteration 1617000 ************

Training agent...


********** Iteration 1618000 ************

Training agent...


********** Iteration 1619000 ************

Training agent...


********** Iteration 1620000 ************

Training agent...

Beginning logging procedure...
Timestep 1620001
mean reward (100 episodes) 1552.400024
best mean reward 1662.199951
running time 11859.431430
Train_EnvstepsSoFar : 1620001
Train_AverageReturn : 1552.4000244140625
Train_BestReturn : 1662.199951171875
TimeSinceStart : 11859.431429624557
Training Loss : 0.2875286936759949
Done logging...




********** Iteration 1621000 ************

Training agent...


********** Iteration 1622000 ************

Training agent...


********** Iteration 1623000 ************

Training agent...


********** Iteration 1624000 ************

Training agent...


********** Iteration 1625000 ************

Training agent...


********** Iteration 1626000 ************

Training agent...


********** Iteration 1627000 ************

Training agent...


********** Iteration 1628000 ************

Training agent...


********** Iteration 1629000 ************

Training agent...


********** Iteration 1630000 ************

Training agent...

Beginning logging procedure...
Timestep 1630001
mean reward (100 episodes) 1531.900024
best mean reward 1662.199951
running time 11933.303980
Train_EnvstepsSoFar : 1630001
Train_AverageReturn : 1531.9000244140625
Train_BestReturn : 1662.199951171875
TimeSinceStart : 11933.303980350494
Training Loss : 0.4615696370601654
Done logging...




********** Iteration 1631000 ************

Training agent...


********** Iteration 1632000 ************

Training agent...


********** Iteration 1633000 ************

Training agent...


********** Iteration 1634000 ************

Training agent...


********** Iteration 1635000 ************

Training agent...


********** Iteration 1636000 ************

Training agent...


********** Iteration 1637000 ************

Training agent...


********** Iteration 1638000 ************

Training agent...


********** Iteration 1639000 ************

Training agent...


********** Iteration 1640000 ************

Training agent...

Beginning logging procedure...
Timestep 1640001
mean reward (100 episodes) 1533.000000
best mean reward 1662.199951
running time 12006.761306
Train_EnvstepsSoFar : 1640001
Train_AverageReturn : 1533.0
Train_BestReturn : 1662.199951171875
TimeSinceStart : 12006.761305809021
Training Loss : 0.6752132177352905
Done logging...




********** Iteration 1641000 ************

Training agent...


********** Iteration 1642000 ************

Training agent...


********** Iteration 1643000 ************

Training agent...


********** Iteration 1644000 ************

Training agent...


********** Iteration 1645000 ************

Training agent...


********** Iteration 1646000 ************

Training agent...


********** Iteration 1647000 ************

Training agent...


********** Iteration 1648000 ************

Training agent...


********** Iteration 1649000 ************

Training agent...


********** Iteration 1650000 ************

Training agent...

Beginning logging procedure...
Timestep 1650001
mean reward (100 episodes) 1582.000000
best mean reward 1662.199951
running time 12080.458130
Train_EnvstepsSoFar : 1650001
Train_AverageReturn : 1582.0
Train_BestReturn : 1662.199951171875
TimeSinceStart : 12080.45813035965
Training Loss : 0.3596518337726593
Done logging...




********** Iteration 1651000 ************

Training agent...


********** Iteration 1652000 ************

Training agent...


********** Iteration 1653000 ************

Training agent...


********** Iteration 1654000 ************

Training agent...


********** Iteration 1655000 ************

Training agent...


********** Iteration 1656000 ************

Training agent...


********** Iteration 1657000 ************

Training agent...


********** Iteration 1658000 ************

Training agent...


********** Iteration 1659000 ************

Training agent...


********** Iteration 1660000 ************

Training agent...

Beginning logging procedure...
Timestep 1660001
mean reward (100 episodes) 1628.099976
best mean reward 1662.199951
running time 12154.679487
Train_EnvstepsSoFar : 1660001
Train_AverageReturn : 1628.0999755859375
Train_BestReturn : 1662.199951171875
TimeSinceStart : 12154.679487466812
Training Loss : 0.6109892129898071
Done logging...




********** Iteration 1661000 ************

Training agent...


********** Iteration 1662000 ************

Training agent...


********** Iteration 1663000 ************

Training agent...


********** Iteration 1664000 ************

Training agent...


********** Iteration 1665000 ************

Training agent...


********** Iteration 1666000 ************

Training agent...


********** Iteration 1667000 ************

Training agent...


********** Iteration 1668000 ************

Training agent...


********** Iteration 1669000 ************

Training agent...


********** Iteration 1670000 ************

Training agent...

Beginning logging procedure...
Timestep 1670001
mean reward (100 episodes) 1691.699951
best mean reward 1691.699951
running time 12228.587658
Train_EnvstepsSoFar : 1670001
Train_AverageReturn : 1691.699951171875
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12228.587657690048
Training Loss : 0.22464805841445923
Done logging...




********** Iteration 1671000 ************

Training agent...


********** Iteration 1672000 ************

Training agent...


********** Iteration 1673000 ************

Training agent...


********** Iteration 1674000 ************

Training agent...


********** Iteration 1675000 ************

Training agent...


********** Iteration 1676000 ************

Training agent...


********** Iteration 1677000 ************

Training agent...


********** Iteration 1678000 ************

Training agent...


********** Iteration 1679000 ************

Training agent...


********** Iteration 1680000 ************

Training agent...

Beginning logging procedure...
Timestep 1680001
mean reward (100 episodes) 1678.199951
best mean reward 1691.699951
running time 12302.057449
Train_EnvstepsSoFar : 1680001
Train_AverageReturn : 1678.199951171875
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12302.057449102402
Training Loss : 1.1995928287506104
Done logging...




********** Iteration 1681000 ************

Training agent...


********** Iteration 1682000 ************

Training agent...


********** Iteration 1683000 ************

Training agent...


********** Iteration 1684000 ************

Training agent...


********** Iteration 1685000 ************

Training agent...


********** Iteration 1686000 ************

Training agent...


********** Iteration 1687000 ************

Training agent...


********** Iteration 1688000 ************

Training agent...


********** Iteration 1689000 ************

Training agent...


********** Iteration 1690000 ************

Training agent...

Beginning logging procedure...
Timestep 1690001
mean reward (100 episodes) 1585.800049
best mean reward 1691.699951
running time 12376.848474
Train_EnvstepsSoFar : 1690001
Train_AverageReturn : 1585.800048828125
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12376.848474264145
Training Loss : 1.2760283946990967
Done logging...




********** Iteration 1691000 ************

Training agent...


********** Iteration 1692000 ************

Training agent...


********** Iteration 1693000 ************

Training agent...


********** Iteration 1694000 ************

Training agent...


********** Iteration 1695000 ************

Training agent...


********** Iteration 1696000 ************

Training agent...


********** Iteration 1697000 ************

Training agent...


********** Iteration 1698000 ************

Training agent...


********** Iteration 1699000 ************

Training agent...


********** Iteration 1700000 ************

Training agent...

Beginning logging procedure...
Timestep 1700001
mean reward (100 episodes) 1574.500000
best mean reward 1691.699951
running time 12450.738879
Train_EnvstepsSoFar : 1700001
Train_AverageReturn : 1574.5
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12450.738878965378
Training Loss : 0.6294441819190979
Done logging...




********** Iteration 1701000 ************

Training agent...


********** Iteration 1702000 ************

Training agent...


********** Iteration 1703000 ************

Training agent...


********** Iteration 1704000 ************

Training agent...


********** Iteration 1705000 ************

Training agent...


********** Iteration 1706000 ************

Training agent...


********** Iteration 1707000 ************

Training agent...


********** Iteration 1708000 ************

Training agent...


********** Iteration 1709000 ************

Training agent...


********** Iteration 1710000 ************

Training agent...

Beginning logging procedure...
Timestep 1710001
mean reward (100 episodes) 1574.699951
best mean reward 1691.699951
running time 12524.365164
Train_EnvstepsSoFar : 1710001
Train_AverageReturn : 1574.699951171875
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12524.36516404152
Training Loss : 0.38330331444740295
Done logging...




********** Iteration 1711000 ************

Training agent...


********** Iteration 1712000 ************

Training agent...


********** Iteration 1713000 ************

Training agent...


********** Iteration 1714000 ************

Training agent...


********** Iteration 1715000 ************

Training agent...


********** Iteration 1716000 ************

Training agent...


********** Iteration 1717000 ************

Training agent...


********** Iteration 1718000 ************

Training agent...


********** Iteration 1719000 ************

Training agent...


********** Iteration 1720000 ************

Training agent...

Beginning logging procedure...
Timestep 1720001
mean reward (100 episodes) 1628.599976
best mean reward 1691.699951
running time 12598.505057
Train_EnvstepsSoFar : 1720001
Train_AverageReturn : 1628.5999755859375
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12598.505056619644
Training Loss : 0.4640563726425171
Done logging...




********** Iteration 1721000 ************

Training agent...


********** Iteration 1722000 ************

Training agent...


********** Iteration 1723000 ************

Training agent...


********** Iteration 1724000 ************

Training agent...


********** Iteration 1725000 ************

Training agent...


********** Iteration 1726000 ************

Training agent...


********** Iteration 1727000 ************

Training agent...


********** Iteration 1728000 ************

Training agent...


********** Iteration 1729000 ************

Training agent...


********** Iteration 1730000 ************

Training agent...

Beginning logging procedure...
Timestep 1730001
mean reward (100 episodes) 1640.800049
best mean reward 1691.699951
running time 12672.452223
Train_EnvstepsSoFar : 1730001
Train_AverageReturn : 1640.800048828125
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12672.452222585678
Training Loss : 0.3649905025959015
Done logging...




********** Iteration 1731000 ************

Training agent...


********** Iteration 1732000 ************

Training agent...


********** Iteration 1733000 ************

Training agent...


********** Iteration 1734000 ************

Training agent...


********** Iteration 1735000 ************

Training agent...


********** Iteration 1736000 ************

Training agent...


********** Iteration 1737000 ************

Training agent...


********** Iteration 1738000 ************

Training agent...


********** Iteration 1739000 ************

Training agent...


********** Iteration 1740000 ************

Training agent...

Beginning logging procedure...
Timestep 1740001
mean reward (100 episodes) 1688.000000
best mean reward 1691.699951
running time 12746.282315
Train_EnvstepsSoFar : 1740001
Train_AverageReturn : 1688.0
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12746.282315015793
Training Loss : 0.25231748819351196
Done logging...




********** Iteration 1741000 ************

Training agent...


********** Iteration 1742000 ************

Training agent...


********** Iteration 1743000 ************

Training agent...


********** Iteration 1744000 ************

Training agent...


********** Iteration 1745000 ************

Training agent...


********** Iteration 1746000 ************

Training agent...


********** Iteration 1747000 ************

Training agent...


********** Iteration 1748000 ************

Training agent...


********** Iteration 1749000 ************

Training agent...


********** Iteration 1750000 ************

Training agent...

Beginning logging procedure...
Timestep 1750001
mean reward (100 episodes) 1684.500000
best mean reward 1691.699951
running time 12819.608158
Train_EnvstepsSoFar : 1750001
Train_AverageReturn : 1684.5
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12819.60815834999
Training Loss : 0.7271263599395752
Done logging...




********** Iteration 1751000 ************

Training agent...


********** Iteration 1752000 ************

Training agent...


********** Iteration 1753000 ************

Training agent...


********** Iteration 1754000 ************

Training agent...


********** Iteration 1755000 ************

Training agent...


********** Iteration 1756000 ************

Training agent...


********** Iteration 1757000 ************

Training agent...


********** Iteration 1758000 ************

Training agent...


********** Iteration 1759000 ************

Training agent...


********** Iteration 1760000 ************

Training agent...

Beginning logging procedure...
Timestep 1760001
mean reward (100 episodes) 1655.699951
best mean reward 1691.699951
running time 12893.529232
Train_EnvstepsSoFar : 1760001
Train_AverageReturn : 1655.699951171875
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12893.529231786728
Training Loss : 0.883023738861084
Done logging...




********** Iteration 1761000 ************

Training agent...


********** Iteration 1762000 ************

Training agent...


********** Iteration 1763000 ************

Training agent...


********** Iteration 1764000 ************

Training agent...


********** Iteration 1765000 ************

Training agent...


********** Iteration 1766000 ************

Training agent...


********** Iteration 1767000 ************

Training agent...


********** Iteration 1768000 ************

Training agent...


********** Iteration 1769000 ************

Training agent...


********** Iteration 1770000 ************

Training agent...

Beginning logging procedure...
Timestep 1770001
mean reward (100 episodes) 1624.300049
best mean reward 1691.699951
running time 12966.761471
Train_EnvstepsSoFar : 1770001
Train_AverageReturn : 1624.300048828125
Train_BestReturn : 1691.699951171875
TimeSinceStart : 12966.761471271515
Training Loss : 0.38345491886138916
Done logging...




********** Iteration 1771000 ************

Training agent...


********** Iteration 1772000 ************

Training agent...


********** Iteration 1773000 ************

Training agent...


********** Iteration 1774000 ************

Training agent...


********** Iteration 1775000 ************

Training agent...


********** Iteration 1776000 ************

Training agent...


********** Iteration 1777000 ************

Training agent...


********** Iteration 1778000 ************

Training agent...


********** Iteration 1779000 ************

Training agent...


********** Iteration 1780000 ************

Training agent...

Beginning logging procedure...
Timestep 1780001
mean reward (100 episodes) 1605.300049
best mean reward 1691.699951
running time 13040.593262
Train_EnvstepsSoFar : 1780001
Train_AverageReturn : 1605.300048828125
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13040.593261957169
Training Loss : 1.2785733938217163
Done logging...




********** Iteration 1781000 ************

Training agent...


********** Iteration 1782000 ************

Training agent...


********** Iteration 1783000 ************

Training agent...


********** Iteration 1784000 ************

Training agent...


********** Iteration 1785000 ************

Training agent...


********** Iteration 1786000 ************

Training agent...


********** Iteration 1787000 ************

Training agent...


********** Iteration 1788000 ************

Training agent...


********** Iteration 1789000 ************

Training agent...


********** Iteration 1790000 ************

Training agent...

Beginning logging procedure...
Timestep 1790001
mean reward (100 episodes) 1651.599976
best mean reward 1691.699951
running time 13112.890824
Train_EnvstepsSoFar : 1790001
Train_AverageReturn : 1651.5999755859375
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13112.890823602676
Training Loss : 0.4728040099143982
Done logging...




********** Iteration 1791000 ************

Training agent...


********** Iteration 1792000 ************

Training agent...


********** Iteration 1793000 ************

Training agent...


********** Iteration 1794000 ************

Training agent...


********** Iteration 1795000 ************

Training agent...


********** Iteration 1796000 ************

Training agent...


********** Iteration 1797000 ************

Training agent...


********** Iteration 1798000 ************

Training agent...


********** Iteration 1799000 ************

Training agent...


********** Iteration 1800000 ************

Training agent...

Beginning logging procedure...
Timestep 1800001
mean reward (100 episodes) 1668.199951
best mean reward 1691.699951
running time 13186.992705
Train_EnvstepsSoFar : 1800001
Train_AverageReturn : 1668.199951171875
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13186.992704868317
Training Loss : 0.45206236839294434
Done logging...




********** Iteration 1801000 ************

Training agent...


********** Iteration 1802000 ************

Training agent...


********** Iteration 1803000 ************

Training agent...


********** Iteration 1804000 ************

Training agent...


********** Iteration 1805000 ************

Training agent...


********** Iteration 1806000 ************

Training agent...


********** Iteration 1807000 ************

Training agent...


********** Iteration 1808000 ************

Training agent...


********** Iteration 1809000 ************

Training agent...


********** Iteration 1810000 ************

Training agent...

Beginning logging procedure...
Timestep 1810001
mean reward (100 episodes) 1657.000000
best mean reward 1691.699951
running time 13260.845477
Train_EnvstepsSoFar : 1810001
Train_AverageReturn : 1657.0
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13260.845477104187
Training Loss : 0.5252692699432373
Done logging...




********** Iteration 1811000 ************

Training agent...


********** Iteration 1812000 ************

Training agent...


********** Iteration 1813000 ************

Training agent...


********** Iteration 1814000 ************

Training agent...


********** Iteration 1815000 ************

Training agent...


********** Iteration 1816000 ************

Training agent...


********** Iteration 1817000 ************

Training agent...


********** Iteration 1818000 ************

Training agent...


********** Iteration 1819000 ************

Training agent...


********** Iteration 1820000 ************

Training agent...

Beginning logging procedure...
Timestep 1820001
mean reward (100 episodes) 1582.199951
best mean reward 1691.699951
running time 13334.498963
Train_EnvstepsSoFar : 1820001
Train_AverageReturn : 1582.199951171875
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13334.498963356018
Training Loss : 0.43646398186683655
Done logging...




********** Iteration 1821000 ************

Training agent...


********** Iteration 1822000 ************

Training agent...


********** Iteration 1823000 ************

Training agent...


********** Iteration 1824000 ************

Training agent...


********** Iteration 1825000 ************

Training agent...


********** Iteration 1826000 ************

Training agent...


********** Iteration 1827000 ************

Training agent...


********** Iteration 1828000 ************

Training agent...


********** Iteration 1829000 ************

Training agent...


********** Iteration 1830000 ************

Training agent...

Beginning logging procedure...
Timestep 1830001
mean reward (100 episodes) 1562.099976
best mean reward 1691.699951
running time 13407.820480
Train_EnvstepsSoFar : 1830001
Train_AverageReturn : 1562.0999755859375
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13407.820479631424
Training Loss : 0.4037872850894928
Done logging...




********** Iteration 1831000 ************

Training agent...


********** Iteration 1832000 ************

Training agent...


********** Iteration 1833000 ************

Training agent...


********** Iteration 1834000 ************

Training agent...


********** Iteration 1835000 ************

Training agent...


********** Iteration 1836000 ************

Training agent...


********** Iteration 1837000 ************

Training agent...


********** Iteration 1838000 ************

Training agent...


********** Iteration 1839000 ************

Training agent...


********** Iteration 1840000 ************

Training agent...

Beginning logging procedure...
Timestep 1840001
mean reward (100 episodes) 1597.900024
best mean reward 1691.699951
running time 13481.349692
Train_EnvstepsSoFar : 1840001
Train_AverageReturn : 1597.9000244140625
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13481.349692106247
Training Loss : 0.5539987087249756
Done logging...




********** Iteration 1841000 ************

Training agent...


********** Iteration 1842000 ************

Training agent...


********** Iteration 1843000 ************

Training agent...


********** Iteration 1844000 ************

Training agent...


********** Iteration 1845000 ************

Training agent...


********** Iteration 1846000 ************

Training agent...


********** Iteration 1847000 ************

Training agent...


********** Iteration 1848000 ************

Training agent...


********** Iteration 1849000 ************

Training agent...


********** Iteration 1850000 ************

Training agent...

Beginning logging procedure...
Timestep 1850001
mean reward (100 episodes) 1637.800049
best mean reward 1691.699951
running time 13556.343638
Train_EnvstepsSoFar : 1850001
Train_AverageReturn : 1637.800048828125
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13556.34363770485
Training Loss : 0.6628322601318359
Done logging...




********** Iteration 1851000 ************

Training agent...


********** Iteration 1852000 ************

Training agent...


********** Iteration 1853000 ************

Training agent...


********** Iteration 1854000 ************

Training agent...


********** Iteration 1855000 ************

Training agent...


********** Iteration 1856000 ************

Training agent...


********** Iteration 1857000 ************

Training agent...


********** Iteration 1858000 ************

Training agent...


********** Iteration 1859000 ************

Training agent...


********** Iteration 1860000 ************

Training agent...

Beginning logging procedure...
Timestep 1860001
mean reward (100 episodes) 1673.800049
best mean reward 1691.699951
running time 13630.548710
Train_EnvstepsSoFar : 1860001
Train_AverageReturn : 1673.800048828125
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13630.548709869385
Training Loss : 1.1794536113739014
Done logging...




********** Iteration 1861000 ************

Training agent...


********** Iteration 1862000 ************

Training agent...


********** Iteration 1863000 ************

Training agent...


********** Iteration 1864000 ************

Training agent...


********** Iteration 1865000 ************

Training agent...


********** Iteration 1866000 ************

Training agent...


********** Iteration 1867000 ************

Training agent...


********** Iteration 1868000 ************

Training agent...


********** Iteration 1869000 ************

Training agent...


********** Iteration 1870000 ************

Training agent...

Beginning logging procedure...
Timestep 1870001
mean reward (100 episodes) 1659.900024
best mean reward 1691.699951
running time 13704.962903
Train_EnvstepsSoFar : 1870001
Train_AverageReturn : 1659.9000244140625
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13704.962903499603
Training Loss : 0.3272877335548401
Done logging...




********** Iteration 1871000 ************

Training agent...


********** Iteration 1872000 ************

Training agent...


********** Iteration 1873000 ************

Training agent...


********** Iteration 1874000 ************

Training agent...


********** Iteration 1875000 ************

Training agent...


********** Iteration 1876000 ************

Training agent...


********** Iteration 1877000 ************

Training agent...


********** Iteration 1878000 ************

Training agent...


********** Iteration 1879000 ************

Training agent...


********** Iteration 1880000 ************

Training agent...

Beginning logging procedure...
Timestep 1880001
mean reward (100 episodes) 1670.099976
best mean reward 1691.699951
running time 13779.257821
Train_EnvstepsSoFar : 1880001
Train_AverageReturn : 1670.0999755859375
Train_BestReturn : 1691.699951171875
TimeSinceStart : 13779.257820606232
Training Loss : 0.43096137046813965
Done logging...




********** Iteration 1881000 ************

Training agent...


********** Iteration 1882000 ************

Training agent...


********** Iteration 1883000 ************

Training agent...


********** Iteration 1884000 ************

Training agent...


********** Iteration 1885000 ************

Training agent...


********** Iteration 1886000 ************

Training agent...


********** Iteration 1887000 ************

Training agent...


********** Iteration 1888000 ************

Training agent...


********** Iteration 1889000 ************

Training agent...


********** Iteration 1890000 ************

Training agent...

Beginning logging procedure...
Timestep 1890001
mean reward (100 episodes) 1691.900024
best mean reward 1691.900024
running time 13854.748291
Train_EnvstepsSoFar : 1890001
Train_AverageReturn : 1691.9000244140625
Train_BestReturn : 1691.9000244140625
TimeSinceStart : 13854.748290777206
Training Loss : 0.3293401002883911
Done logging...




********** Iteration 1891000 ************

Training agent...


********** Iteration 1892000 ************

Training agent...


********** Iteration 1893000 ************

Training agent...


********** Iteration 1894000 ************

Training agent...


********** Iteration 1895000 ************

Training agent...


********** Iteration 1896000 ************

Training agent...


********** Iteration 1897000 ************

Training agent...


********** Iteration 1898000 ************

Training agent...


********** Iteration 1899000 ************

Training agent...


********** Iteration 1900000 ************

Training agent...

Beginning logging procedure...
Timestep 1900001
mean reward (100 episodes) 1695.400024
best mean reward 1695.400024
running time 13929.673187
Train_EnvstepsSoFar : 1900001
Train_AverageReturn : 1695.4000244140625
Train_BestReturn : 1695.4000244140625
TimeSinceStart : 13929.673186540604
Training Loss : 0.6508017182350159
Done logging...




********** Iteration 1901000 ************

Training agent...


********** Iteration 1902000 ************

Training agent...


********** Iteration 1903000 ************

Training agent...


********** Iteration 1904000 ************

Training agent...


********** Iteration 1905000 ************

Training agent...


********** Iteration 1906000 ************

Training agent...


********** Iteration 1907000 ************

Training agent...


********** Iteration 1908000 ************

Training agent...


********** Iteration 1909000 ************

Training agent...


********** Iteration 1910000 ************

Training agent...

Beginning logging procedure...
Timestep 1910001
mean reward (100 episodes) 1678.699951
best mean reward 1695.400024
running time 14003.932954
Train_EnvstepsSoFar : 1910001
Train_AverageReturn : 1678.699951171875
Train_BestReturn : 1695.4000244140625
TimeSinceStart : 14003.932954072952
Training Loss : 1.3678572177886963
Done logging...




********** Iteration 1911000 ************

Training agent...


********** Iteration 1912000 ************

Training agent...


********** Iteration 1913000 ************

Training agent...


********** Iteration 1914000 ************

Training agent...


********** Iteration 1915000 ************

Training agent...


********** Iteration 1916000 ************

Training agent...


********** Iteration 1917000 ************

Training agent...


********** Iteration 1918000 ************

Training agent...


********** Iteration 1919000 ************

Training agent...


********** Iteration 1920000 ************

Training agent...

Beginning logging procedure...
Timestep 1920001
mean reward (100 episodes) 1689.300049
best mean reward 1695.400024
running time 14077.434442
Train_EnvstepsSoFar : 1920001
Train_AverageReturn : 1689.300048828125
Train_BestReturn : 1695.4000244140625
TimeSinceStart : 14077.434442043304
Training Loss : 1.2091866731643677
Done logging...




********** Iteration 1921000 ************

Training agent...


********** Iteration 1922000 ************

Training agent...


********** Iteration 1923000 ************

Training agent...


********** Iteration 1924000 ************

Training agent...


********** Iteration 1925000 ************

Training agent...


********** Iteration 1926000 ************

Training agent...


********** Iteration 1927000 ************

Training agent...


********** Iteration 1928000 ************

Training agent...


********** Iteration 1929000 ************

Training agent...


********** Iteration 1930000 ************

Training agent...

Beginning logging procedure...
Timestep 1930001
mean reward (100 episodes) 1731.300049
best mean reward 1731.300049
running time 14152.656853
Train_EnvstepsSoFar : 1930001
Train_AverageReturn : 1731.300048828125
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14152.656852960587
Training Loss : 0.7568382024765015
Done logging...




********** Iteration 1931000 ************

Training agent...


********** Iteration 1932000 ************

Training agent...


********** Iteration 1933000 ************

Training agent...


********** Iteration 1934000 ************

Training agent...


********** Iteration 1935000 ************

Training agent...


********** Iteration 1936000 ************

Training agent...


********** Iteration 1937000 ************

Training agent...


********** Iteration 1938000 ************

Training agent...


********** Iteration 1939000 ************

Training agent...


********** Iteration 1940000 ************

Training agent...

Beginning logging procedure...
Timestep 1940001
mean reward (100 episodes) 1721.400024
best mean reward 1731.300049
running time 14227.903260
Train_EnvstepsSoFar : 1940001
Train_AverageReturn : 1721.4000244140625
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14227.90325975418
Training Loss : 0.4164409637451172
Done logging...




********** Iteration 1941000 ************

Training agent...


********** Iteration 1942000 ************

Training agent...


********** Iteration 1943000 ************

Training agent...


********** Iteration 1944000 ************

Training agent...


********** Iteration 1945000 ************

Training agent...


********** Iteration 1946000 ************

Training agent...


********** Iteration 1947000 ************

Training agent...


********** Iteration 1948000 ************

Training agent...


********** Iteration 1949000 ************

Training agent...


********** Iteration 1950000 ************

Training agent...

Beginning logging procedure...
Timestep 1950001
mean reward (100 episodes) 1689.800049
best mean reward 1731.300049
running time 14304.218790
Train_EnvstepsSoFar : 1950001
Train_AverageReturn : 1689.800048828125
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14304.21879029274
Training Loss : 0.3877333998680115
Done logging...




********** Iteration 1951000 ************

Training agent...


********** Iteration 1952000 ************

Training agent...


********** Iteration 1953000 ************

Training agent...


********** Iteration 1954000 ************

Training agent...


********** Iteration 1955000 ************

Training agent...


********** Iteration 1956000 ************

Training agent...


********** Iteration 1957000 ************

Training agent...


********** Iteration 1958000 ************

Training agent...


********** Iteration 1959000 ************

Training agent...


********** Iteration 1960000 ************

Training agent...

Beginning logging procedure...
Timestep 1960001
mean reward (100 episodes) 1639.599976
best mean reward 1731.300049
running time 14381.483977
Train_EnvstepsSoFar : 1960001
Train_AverageReturn : 1639.5999755859375
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14381.48397731781
Training Loss : 0.09577867388725281
Done logging...




********** Iteration 1961000 ************

Training agent...


********** Iteration 1962000 ************

Training agent...


********** Iteration 1963000 ************

Training agent...


********** Iteration 1964000 ************

Training agent...


********** Iteration 1965000 ************

Training agent...


********** Iteration 1966000 ************

Training agent...


********** Iteration 1967000 ************

Training agent...


********** Iteration 1968000 ************

Training agent...


********** Iteration 1969000 ************

Training agent...


********** Iteration 1970000 ************

Training agent...

Beginning logging procedure...
Timestep 1970001
mean reward (100 episodes) 1665.400024
best mean reward 1731.300049
running time 14458.600422
Train_EnvstepsSoFar : 1970001
Train_AverageReturn : 1665.4000244140625
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14458.600421905518
Training Loss : 0.6453016996383667
Done logging...




********** Iteration 1971000 ************

Training agent...


********** Iteration 1972000 ************

Training agent...


********** Iteration 1973000 ************

Training agent...


********** Iteration 1974000 ************

Training agent...


********** Iteration 1975000 ************

Training agent...


********** Iteration 1976000 ************

Training agent...


********** Iteration 1977000 ************

Training agent...


********** Iteration 1978000 ************

Training agent...


********** Iteration 1979000 ************

Training agent...


********** Iteration 1980000 ************

Training agent...

Beginning logging procedure...
Timestep 1980001
mean reward (100 episodes) 1659.400024
best mean reward 1731.300049
running time 14535.792704
Train_EnvstepsSoFar : 1980001
Train_AverageReturn : 1659.4000244140625
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14535.792703866959
Training Loss : 0.9793776273727417
Done logging...




********** Iteration 1981000 ************

Training agent...


********** Iteration 1982000 ************

Training agent...


********** Iteration 1983000 ************

Training agent...


********** Iteration 1984000 ************

Training agent...


********** Iteration 1985000 ************

Training agent...


********** Iteration 1986000 ************

Training agent...


********** Iteration 1987000 ************

Training agent...


********** Iteration 1988000 ************

Training agent...


********** Iteration 1989000 ************

Training agent...


********** Iteration 1990000 ************

Training agent...

Beginning logging procedure...
Timestep 1990001
mean reward (100 episodes) 1652.699951
best mean reward 1731.300049
running time 14613.112950
Train_EnvstepsSoFar : 1990001
Train_AverageReturn : 1652.699951171875
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14613.112949848175
Training Loss : 0.5007922649383545
Done logging...




********** Iteration 1991000 ************

Training agent...


********** Iteration 1992000 ************

Training agent...


********** Iteration 1993000 ************

Training agent...


********** Iteration 1994000 ************

Training agent...


********** Iteration 1995000 ************

Training agent...


********** Iteration 1996000 ************

Training agent...


********** Iteration 1997000 ************

Training agent...


********** Iteration 1998000 ************

Training agent...


********** Iteration 1999000 ************

Training agent...


********** Iteration 2000000 ************

Training agent...

Beginning logging procedure...
Timestep 2000001
mean reward (100 episodes) 1617.699951
best mean reward 1731.300049
running time 14690.733808
Train_EnvstepsSoFar : 2000001
Train_AverageReturn : 1617.699951171875
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14690.7338078022
Training Loss : 0.9166531562805176
Done logging...




********** Iteration 2001000 ************

Training agent...


********** Iteration 2002000 ************

Training agent...


********** Iteration 2003000 ************

Training agent...


********** Iteration 2004000 ************

Training agent...


********** Iteration 2005000 ************

Training agent...


********** Iteration 2006000 ************

Training agent...


********** Iteration 2007000 ************

Training agent...


********** Iteration 2008000 ************

Training agent...


********** Iteration 2009000 ************

Training agent...


********** Iteration 2010000 ************

Training agent...

Beginning logging procedure...
Timestep 2010001
mean reward (100 episodes) 1632.900024
best mean reward 1731.300049
running time 14767.379172
Train_EnvstepsSoFar : 2010001
Train_AverageReturn : 1632.9000244140625
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14767.379171609879
Training Loss : 0.17873060703277588
Done logging...




********** Iteration 2011000 ************

Training agent...


********** Iteration 2012000 ************

Training agent...


********** Iteration 2013000 ************

Training agent...


********** Iteration 2014000 ************

Training agent...


********** Iteration 2015000 ************

Training agent...


********** Iteration 2016000 ************

Training agent...


********** Iteration 2017000 ************

Training agent...


********** Iteration 2018000 ************

Training agent...


********** Iteration 2019000 ************

Training agent...


********** Iteration 2020000 ************

Training agent...

Beginning logging procedure...
Timestep 2020001
mean reward (100 episodes) 1638.699951
best mean reward 1731.300049
running time 14844.833386
Train_EnvstepsSoFar : 2020001
Train_AverageReturn : 1638.699951171875
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14844.833385944366
Training Loss : 0.3130730986595154
Done logging...




********** Iteration 2021000 ************

Training agent...


********** Iteration 2022000 ************

Training agent...


********** Iteration 2023000 ************

Training agent...


********** Iteration 2024000 ************

Training agent...


********** Iteration 2025000 ************

Training agent...


********** Iteration 2026000 ************

Training agent...


********** Iteration 2027000 ************

Training agent...


********** Iteration 2028000 ************

Training agent...


********** Iteration 2029000 ************

Training agent...


********** Iteration 2030000 ************

Training agent...

Beginning logging procedure...
Timestep 2030001
mean reward (100 episodes) 1676.500000
best mean reward 1731.300049
running time 14925.042134
Train_EnvstepsSoFar : 2030001
Train_AverageReturn : 1676.5
Train_BestReturn : 1731.300048828125
TimeSinceStart : 14925.042133569717
Training Loss : 0.3498518466949463
Done logging...




********** Iteration 2031000 ************

Training agent...


********** Iteration 2032000 ************

Training agent...


********** Iteration 2033000 ************

Training agent...


********** Iteration 2034000 ************

Training agent...


********** Iteration 2035000 ************

Training agent...


********** Iteration 2036000 ************

Training agent...


********** Iteration 2037000 ************

Training agent...


********** Iteration 2038000 ************

Training agent...


********** Iteration 2039000 ************

Training agent...


********** Iteration 2040000 ************

Training agent...

Beginning logging procedure...
Timestep 2040001
mean reward (100 episodes) 1639.300049
best mean reward 1731.300049
running time 15004.191264
Train_EnvstepsSoFar : 2040001
Train_AverageReturn : 1639.300048828125
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15004.191264390945
Training Loss : 0.3764117956161499
Done logging...




********** Iteration 2041000 ************

Training agent...


********** Iteration 2042000 ************

Training agent...


********** Iteration 2043000 ************

Training agent...


********** Iteration 2044000 ************

Training agent...


********** Iteration 2045000 ************

Training agent...


********** Iteration 2046000 ************

Training agent...


********** Iteration 2047000 ************

Training agent...


********** Iteration 2048000 ************

Training agent...


********** Iteration 2049000 ************

Training agent...


********** Iteration 2050000 ************

Training agent...

Beginning logging procedure...
Timestep 2050001
mean reward (100 episodes) 1650.800049
best mean reward 1731.300049
running time 15083.775105
Train_EnvstepsSoFar : 2050001
Train_AverageReturn : 1650.800048828125
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15083.775104999542
Training Loss : 0.3979855477809906
Done logging...




********** Iteration 2051000 ************

Training agent...


********** Iteration 2052000 ************

Training agent...


********** Iteration 2053000 ************

Training agent...


********** Iteration 2054000 ************

Training agent...


********** Iteration 2055000 ************

Training agent...


********** Iteration 2056000 ************

Training agent...


********** Iteration 2057000 ************

Training agent...


********** Iteration 2058000 ************

Training agent...


********** Iteration 2059000 ************

Training agent...


********** Iteration 2060000 ************

Training agent...

Beginning logging procedure...
Timestep 2060001
mean reward (100 episodes) 1599.599976
best mean reward 1731.300049
running time 15164.088460
Train_EnvstepsSoFar : 2060001
Train_AverageReturn : 1599.5999755859375
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15164.088459968567
Training Loss : 0.41474252939224243
Done logging...




********** Iteration 2061000 ************

Training agent...


********** Iteration 2062000 ************

Training agent...


********** Iteration 2063000 ************

Training agent...


********** Iteration 2064000 ************

Training agent...


********** Iteration 2065000 ************

Training agent...


********** Iteration 2066000 ************

Training agent...


********** Iteration 2067000 ************

Training agent...


********** Iteration 2068000 ************

Training agent...


********** Iteration 2069000 ************

Training agent...


********** Iteration 2070000 ************

Training agent...

Beginning logging procedure...
Timestep 2070001
mean reward (100 episodes) 1639.400024
best mean reward 1731.300049
running time 15244.225856
Train_EnvstepsSoFar : 2070001
Train_AverageReturn : 1639.4000244140625
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15244.225855588913
Training Loss : 0.37376463413238525
Done logging...




********** Iteration 2071000 ************

Training agent...


********** Iteration 2072000 ************

Training agent...


********** Iteration 2073000 ************

Training agent...


********** Iteration 2074000 ************

Training agent...


********** Iteration 2075000 ************

Training agent...


********** Iteration 2076000 ************

Training agent...


********** Iteration 2077000 ************

Training agent...


********** Iteration 2078000 ************

Training agent...


********** Iteration 2079000 ************

Training agent...


********** Iteration 2080000 ************

Training agent...

Beginning logging procedure...
Timestep 2080001
mean reward (100 episodes) 1664.400024
best mean reward 1731.300049
running time 15324.401278
Train_EnvstepsSoFar : 2080001
Train_AverageReturn : 1664.4000244140625
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15324.401277542114
Training Loss : 0.4116450548171997
Done logging...




********** Iteration 2081000 ************

Training agent...


********** Iteration 2082000 ************

Training agent...


********** Iteration 2083000 ************

Training agent...


********** Iteration 2084000 ************

Training agent...


********** Iteration 2085000 ************

Training agent...


********** Iteration 2086000 ************

Training agent...


********** Iteration 2087000 ************

Training agent...


********** Iteration 2088000 ************

Training agent...


********** Iteration 2089000 ************

Training agent...


********** Iteration 2090000 ************

Training agent...

Beginning logging procedure...
Timestep 2090001
mean reward (100 episodes) 1679.300049
best mean reward 1731.300049
running time 15403.212269
Train_EnvstepsSoFar : 2090001
Train_AverageReturn : 1679.300048828125
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15403.212268590927
Training Loss : 2.2211740016937256
Done logging...




********** Iteration 2091000 ************

Training agent...


********** Iteration 2092000 ************

Training agent...


********** Iteration 2093000 ************

Training agent...


********** Iteration 2094000 ************

Training agent...


********** Iteration 2095000 ************

Training agent...


********** Iteration 2096000 ************

Training agent...


********** Iteration 2097000 ************

Training agent...


********** Iteration 2098000 ************

Training agent...


********** Iteration 2099000 ************

Training agent...


********** Iteration 2100000 ************

Training agent...

Beginning logging procedure...
Timestep 2100001
mean reward (100 episodes) 1667.400024
best mean reward 1731.300049
running time 15482.803594
Train_EnvstepsSoFar : 2100001
Train_AverageReturn : 1667.4000244140625
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15482.803594350815
Training Loss : 0.3464316129684448
Done logging...




********** Iteration 2101000 ************

Training agent...


********** Iteration 2102000 ************

Training agent...


********** Iteration 2103000 ************

Training agent...


********** Iteration 2104000 ************

Training agent...


********** Iteration 2105000 ************

Training agent...


********** Iteration 2106000 ************

Training agent...


********** Iteration 2107000 ************

Training agent...


********** Iteration 2108000 ************

Training agent...


********** Iteration 2109000 ************

Training agent...


********** Iteration 2110000 ************

Training agent...

Beginning logging procedure...
Timestep 2110001
mean reward (100 episodes) 1641.300049
best mean reward 1731.300049
running time 15560.860534
Train_EnvstepsSoFar : 2110001
Train_AverageReturn : 1641.300048828125
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15560.86053442955
Training Loss : 0.4779417812824249
Done logging...




********** Iteration 2111000 ************

Training agent...


********** Iteration 2112000 ************

Training agent...


********** Iteration 2113000 ************

Training agent...


********** Iteration 2114000 ************

Training agent...


********** Iteration 2115000 ************

Training agent...


********** Iteration 2116000 ************

Training agent...


********** Iteration 2117000 ************

Training agent...


********** Iteration 2118000 ************

Training agent...


********** Iteration 2119000 ************

Training agent...


********** Iteration 2120000 ************

Training agent...

Beginning logging procedure...
Timestep 2120001
mean reward (100 episodes) 1601.099976
best mean reward 1731.300049
running time 15638.752465
Train_EnvstepsSoFar : 2120001
Train_AverageReturn : 1601.0999755859375
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15638.75246500969
Training Loss : 0.3509978652000427
Done logging...




********** Iteration 2121000 ************

Training agent...


********** Iteration 2122000 ************

Training agent...


********** Iteration 2123000 ************

Training agent...


********** Iteration 2124000 ************

Training agent...


********** Iteration 2125000 ************

Training agent...


********** Iteration 2126000 ************

Training agent...


********** Iteration 2127000 ************

Training agent...


********** Iteration 2128000 ************

Training agent...


********** Iteration 2129000 ************

Training agent...


********** Iteration 2130000 ************

Training agent...

Beginning logging procedure...
Timestep 2130001
mean reward (100 episodes) 1578.500000
best mean reward 1731.300049
running time 15716.102807
Train_EnvstepsSoFar : 2130001
Train_AverageReturn : 1578.5
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15716.102807283401
Training Loss : 0.7298123240470886
Done logging...




********** Iteration 2131000 ************

Training agent...


********** Iteration 2132000 ************

Training agent...


********** Iteration 2133000 ************

Training agent...


********** Iteration 2134000 ************

Training agent...


********** Iteration 2135000 ************

Training agent...


********** Iteration 2136000 ************

Training agent...


********** Iteration 2137000 ************

Training agent...


********** Iteration 2138000 ************

Training agent...


********** Iteration 2139000 ************

Training agent...


********** Iteration 2140000 ************

Training agent...

Beginning logging procedure...
Timestep 2140001
mean reward (100 episodes) 1630.900024
best mean reward 1731.300049
running time 15792.308563
Train_EnvstepsSoFar : 2140001
Train_AverageReturn : 1630.9000244140625
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15792.308562994003
Training Loss : 1.2337915897369385
Done logging...




********** Iteration 2141000 ************

Training agent...


********** Iteration 2142000 ************

Training agent...


********** Iteration 2143000 ************

Training agent...


********** Iteration 2144000 ************

Training agent...


********** Iteration 2145000 ************

Training agent...


********** Iteration 2146000 ************

Training agent...


********** Iteration 2147000 ************

Training agent...


********** Iteration 2148000 ************

Training agent...


********** Iteration 2149000 ************

Training agent...


********** Iteration 2150000 ************

Training agent...

Beginning logging procedure...
Timestep 2150001
mean reward (100 episodes) 1699.699951
best mean reward 1731.300049
running time 15869.091622
Train_EnvstepsSoFar : 2150001
Train_AverageReturn : 1699.699951171875
Train_BestReturn : 1731.300048828125
TimeSinceStart : 15869.0916223526
Training Loss : 0.5484919548034668
Done logging...




********** Iteration 2151000 ************

Training agent...


********** Iteration 2152000 ************

Training agent...


********** Iteration 2153000 ************

Training agent...


********** Iteration 2154000 ************

Training agent...


********** Iteration 2155000 ************

Training agent...


********** Iteration 2156000 ************

Training agent...


********** Iteration 2157000 ************

Training agent...


********** Iteration 2158000 ************

Training agent...


********** Iteration 2159000 ************

Training agent...


********** Iteration 2160000 ************

Training agent...

Beginning logging procedure...
Timestep 2160001
mean reward (100 episodes) 1742.099976
best mean reward 1742.099976
running time 15945.704437
Train_EnvstepsSoFar : 2160001
Train_AverageReturn : 1742.0999755859375
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 15945.70443701744
Training Loss : 0.24745118618011475
Done logging...




********** Iteration 2161000 ************

Training agent...


********** Iteration 2162000 ************

Training agent...


********** Iteration 2163000 ************

Training agent...


********** Iteration 2164000 ************

Training agent...


********** Iteration 2165000 ************

Training agent...


********** Iteration 2166000 ************

Training agent...


********** Iteration 2167000 ************

Training agent...


********** Iteration 2168000 ************

Training agent...


********** Iteration 2169000 ************

Training agent...


********** Iteration 2170000 ************

Training agent...

Beginning logging procedure...
Timestep 2170001
mean reward (100 episodes) 1680.199951
best mean reward 1742.099976
running time 16024.246287
Train_EnvstepsSoFar : 2170001
Train_AverageReturn : 1680.199951171875
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16024.246286869049
Training Loss : 0.8997011184692383
Done logging...




********** Iteration 2171000 ************

Training agent...


********** Iteration 2172000 ************

Training agent...


********** Iteration 2173000 ************

Training agent...


********** Iteration 2174000 ************

Training agent...


********** Iteration 2175000 ************

Training agent...


********** Iteration 2176000 ************

Training agent...


********** Iteration 2177000 ************

Training agent...


********** Iteration 2178000 ************

Training agent...


********** Iteration 2179000 ************

Training agent...


********** Iteration 2180000 ************

Training agent...

Beginning logging procedure...
Timestep 2180001
mean reward (100 episodes) 1638.500000
best mean reward 1742.099976
running time 16100.730394
Train_EnvstepsSoFar : 2180001
Train_AverageReturn : 1638.5
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16100.730393886566
Training Loss : 0.2329498529434204
Done logging...




********** Iteration 2181000 ************

Training agent...


********** Iteration 2182000 ************

Training agent...


********** Iteration 2183000 ************

Training agent...


********** Iteration 2184000 ************

Training agent...


********** Iteration 2185000 ************

Training agent...


********** Iteration 2186000 ************

Training agent...


********** Iteration 2187000 ************

Training agent...


********** Iteration 2188000 ************

Training agent...


********** Iteration 2189000 ************

Training agent...


********** Iteration 2190000 ************

Training agent...

Beginning logging procedure...
Timestep 2190001
mean reward (100 episodes) 1591.099976
best mean reward 1742.099976
running time 16179.290020
Train_EnvstepsSoFar : 2190001
Train_AverageReturn : 1591.0999755859375
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16179.290019512177
Training Loss : 0.17167258262634277
Done logging...




********** Iteration 2191000 ************

Training agent...


********** Iteration 2192000 ************

Training agent...


********** Iteration 2193000 ************

Training agent...


********** Iteration 2194000 ************

Training agent...


********** Iteration 2195000 ************

Training agent...


********** Iteration 2196000 ************

Training agent...


********** Iteration 2197000 ************

Training agent...


********** Iteration 2198000 ************

Training agent...


********** Iteration 2199000 ************

Training agent...


********** Iteration 2200000 ************

Training agent...

Beginning logging procedure...
Timestep 2200001
mean reward (100 episodes) 1628.699951
best mean reward 1742.099976
running time 16259.396209
Train_EnvstepsSoFar : 2200001
Train_AverageReturn : 1628.699951171875
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16259.396208763123
Training Loss : 0.20217928290367126
Done logging...




********** Iteration 2201000 ************

Training agent...


********** Iteration 2202000 ************

Training agent...


********** Iteration 2203000 ************

Training agent...


********** Iteration 2204000 ************

Training agent...


********** Iteration 2205000 ************

Training agent...


********** Iteration 2206000 ************

Training agent...


********** Iteration 2207000 ************

Training agent...


********** Iteration 2208000 ************

Training agent...


********** Iteration 2209000 ************

Training agent...


********** Iteration 2210000 ************

Training agent...

Beginning logging procedure...
Timestep 2210001
mean reward (100 episodes) 1633.099976
best mean reward 1742.099976
running time 16336.863930
Train_EnvstepsSoFar : 2210001
Train_AverageReturn : 1633.0999755859375
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16336.863930225372
Training Loss : 0.23346777260303497
Done logging...




********** Iteration 2211000 ************

Training agent...


********** Iteration 2212000 ************

Training agent...


********** Iteration 2213000 ************

Training agent...


********** Iteration 2214000 ************

Training agent...


********** Iteration 2215000 ************

Training agent...


********** Iteration 2216000 ************

Training agent...


********** Iteration 2217000 ************

Training agent...


********** Iteration 2218000 ************

Training agent...


********** Iteration 2219000 ************

Training agent...


********** Iteration 2220000 ************

Training agent...

Beginning logging procedure...
Timestep 2220001
mean reward (100 episodes) 1680.400024
best mean reward 1742.099976
running time 16412.684738
Train_EnvstepsSoFar : 2220001
Train_AverageReturn : 1680.4000244140625
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16412.684737682343
Training Loss : 0.6251420974731445
Done logging...




********** Iteration 2221000 ************

Training agent...


********** Iteration 2222000 ************

Training agent...


********** Iteration 2223000 ************

Training agent...


********** Iteration 2224000 ************

Training agent...


********** Iteration 2225000 ************

Training agent...


********** Iteration 2226000 ************

Training agent...


********** Iteration 2227000 ************

Training agent...


********** Iteration 2228000 ************

Training agent...


********** Iteration 2229000 ************

Training agent...


********** Iteration 2230000 ************

Training agent...

Beginning logging procedure...
Timestep 2230001
mean reward (100 episodes) 1696.500000
best mean reward 1742.099976
running time 16488.672372
Train_EnvstepsSoFar : 2230001
Train_AverageReturn : 1696.5
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16488.672372102737
Training Loss : 0.3737723231315613
Done logging...




********** Iteration 2231000 ************

Training agent...


********** Iteration 2232000 ************

Training agent...


********** Iteration 2233000 ************

Training agent...


********** Iteration 2234000 ************

Training agent...


********** Iteration 2235000 ************

Training agent...


********** Iteration 2236000 ************

Training agent...


********** Iteration 2237000 ************

Training agent...


********** Iteration 2238000 ************

Training agent...


********** Iteration 2239000 ************

Training agent...


********** Iteration 2240000 ************

Training agent...

Beginning logging procedure...
Timestep 2240001
mean reward (100 episodes) 1672.699951
best mean reward 1742.099976
running time 16565.947008
Train_EnvstepsSoFar : 2240001
Train_AverageReturn : 1672.699951171875
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16565.947007894516
Training Loss : 0.2013338953256607
Done logging...




********** Iteration 2241000 ************

Training agent...


********** Iteration 2242000 ************

Training agent...


********** Iteration 2243000 ************

Training agent...


********** Iteration 2244000 ************

Training agent...


********** Iteration 2245000 ************

Training agent...


********** Iteration 2246000 ************

Training agent...


********** Iteration 2247000 ************

Training agent...


********** Iteration 2248000 ************

Training agent...


********** Iteration 2249000 ************

Training agent...


********** Iteration 2250000 ************

Training agent...

Beginning logging procedure...
Timestep 2250001
mean reward (100 episodes) 1653.199951
best mean reward 1742.099976
running time 16643.823908
Train_EnvstepsSoFar : 2250001
Train_AverageReturn : 1653.199951171875
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16643.823907613754
Training Loss : 0.20773297548294067
Done logging...




********** Iteration 2251000 ************

Training agent...


********** Iteration 2252000 ************

Training agent...


********** Iteration 2253000 ************

Training agent...


********** Iteration 2254000 ************

Training agent...


********** Iteration 2255000 ************

Training agent...


********** Iteration 2256000 ************

Training agent...


********** Iteration 2257000 ************

Training agent...


********** Iteration 2258000 ************

Training agent...


********** Iteration 2259000 ************

Training agent...


********** Iteration 2260000 ************

Training agent...

Beginning logging procedure...
Timestep 2260001
mean reward (100 episodes) 1676.400024
best mean reward 1742.099976
running time 16721.155433
Train_EnvstepsSoFar : 2260001
Train_AverageReturn : 1676.4000244140625
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16721.155433416367
Training Loss : 0.4247435927391052
Done logging...




********** Iteration 2261000 ************

Training agent...


********** Iteration 2262000 ************

Training agent...


********** Iteration 2263000 ************

Training agent...


********** Iteration 2264000 ************

Training agent...


********** Iteration 2265000 ************

Training agent...


********** Iteration 2266000 ************

Training agent...


********** Iteration 2267000 ************

Training agent...


********** Iteration 2268000 ************

Training agent...


********** Iteration 2269000 ************

Training agent...


********** Iteration 2270000 ************

Training agent...

Beginning logging procedure...
Timestep 2270001
mean reward (100 episodes) 1660.500000
best mean reward 1742.099976
running time 16797.985934
Train_EnvstepsSoFar : 2270001
Train_AverageReturn : 1660.5
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16797.98593401909
Training Loss : 0.311352014541626
Done logging...




********** Iteration 2271000 ************

Training agent...


********** Iteration 2272000 ************

Training agent...


********** Iteration 2273000 ************

Training agent...


********** Iteration 2274000 ************

Training agent...


********** Iteration 2275000 ************

Training agent...


********** Iteration 2276000 ************

Training agent...


********** Iteration 2277000 ************

Training agent...


********** Iteration 2278000 ************

Training agent...


********** Iteration 2279000 ************

Training agent...


********** Iteration 2280000 ************

Training agent...

Beginning logging procedure...
Timestep 2280001
mean reward (100 episodes) 1617.000000
best mean reward 1742.099976
running time 16874.571380
Train_EnvstepsSoFar : 2280001
Train_AverageReturn : 1617.0
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16874.571380376816
Training Loss : 0.1706341952085495
Done logging...




********** Iteration 2281000 ************

Training agent...


********** Iteration 2282000 ************

Training agent...


********** Iteration 2283000 ************

Training agent...


********** Iteration 2284000 ************

Training agent...


********** Iteration 2285000 ************

Training agent...


********** Iteration 2286000 ************

Training agent...


********** Iteration 2287000 ************

Training agent...


********** Iteration 2288000 ************

Training agent...


********** Iteration 2289000 ************

Training agent...


********** Iteration 2290000 ************

Training agent...

Beginning logging procedure...
Timestep 2290001
mean reward (100 episodes) 1587.000000
best mean reward 1742.099976
running time 16952.053796
Train_EnvstepsSoFar : 2290001
Train_AverageReturn : 1587.0
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 16952.053795814514
Training Loss : 0.3365694284439087
Done logging...




********** Iteration 2291000 ************

Training agent...


********** Iteration 2292000 ************

Training agent...


********** Iteration 2293000 ************

Training agent...


********** Iteration 2294000 ************

Training agent...


********** Iteration 2295000 ************

Training agent...


********** Iteration 2296000 ************

Training agent...


********** Iteration 2297000 ************

Training agent...


********** Iteration 2298000 ************

Training agent...


********** Iteration 2299000 ************

Training agent...


********** Iteration 2300000 ************

Training agent...

Beginning logging procedure...
Timestep 2300001
mean reward (100 episodes) 1584.300049
best mean reward 1742.099976
running time 17029.213788
Train_EnvstepsSoFar : 2300001
Train_AverageReturn : 1584.300048828125
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17029.213787794113
Training Loss : 1.487412929534912
Done logging...




********** Iteration 2301000 ************

Training agent...


********** Iteration 2302000 ************

Training agent...


********** Iteration 2303000 ************

Training agent...


********** Iteration 2304000 ************

Training agent...


********** Iteration 2305000 ************

Training agent...


********** Iteration 2306000 ************

Training agent...


********** Iteration 2307000 ************

Training agent...


********** Iteration 2308000 ************

Training agent...


********** Iteration 2309000 ************

Training agent...


********** Iteration 2310000 ************

Training agent...

Beginning logging procedure...
Timestep 2310001
mean reward (100 episodes) 1628.500000
best mean reward 1742.099976
running time 17107.407791
Train_EnvstepsSoFar : 2310001
Train_AverageReturn : 1628.5
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17107.407791376114
Training Loss : 0.12125815451145172
Done logging...




********** Iteration 2311000 ************

Training agent...


********** Iteration 2312000 ************

Training agent...


********** Iteration 2313000 ************

Training agent...


********** Iteration 2314000 ************

Training agent...


********** Iteration 2315000 ************

Training agent...


********** Iteration 2316000 ************

Training agent...


********** Iteration 2317000 ************

Training agent...


********** Iteration 2318000 ************

Training agent...


********** Iteration 2319000 ************

Training agent...


********** Iteration 2320000 ************

Training agent...

Beginning logging procedure...
Timestep 2320001
mean reward (100 episodes) 1617.000000
best mean reward 1742.099976
running time 17184.958052
Train_EnvstepsSoFar : 2320001
Train_AverageReturn : 1617.0
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17184.958052158356
Training Loss : 0.31706342101097107
Done logging...




********** Iteration 2321000 ************

Training agent...


********** Iteration 2322000 ************

Training agent...


********** Iteration 2323000 ************

Training agent...


********** Iteration 2324000 ************

Training agent...


********** Iteration 2325000 ************

Training agent...


********** Iteration 2326000 ************

Training agent...


********** Iteration 2327000 ************

Training agent...


********** Iteration 2328000 ************

Training agent...


********** Iteration 2329000 ************

Training agent...


********** Iteration 2330000 ************

Training agent...

Beginning logging procedure...
Timestep 2330001
mean reward (100 episodes) 1642.000000
best mean reward 1742.099976
running time 17262.277367
Train_EnvstepsSoFar : 2330001
Train_AverageReturn : 1642.0
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17262.277366876602
Training Loss : 0.2607128918170929
Done logging...




********** Iteration 2331000 ************

Training agent...


********** Iteration 2332000 ************

Training agent...


********** Iteration 2333000 ************

Training agent...


********** Iteration 2334000 ************

Training agent...


********** Iteration 2335000 ************

Training agent...


********** Iteration 2336000 ************

Training agent...


********** Iteration 2337000 ************

Training agent...


********** Iteration 2338000 ************

Training agent...


********** Iteration 2339000 ************

Training agent...


********** Iteration 2340000 ************

Training agent...

Beginning logging procedure...
Timestep 2340001
mean reward (100 episodes) 1693.900024
best mean reward 1742.099976
running time 17339.614935
Train_EnvstepsSoFar : 2340001
Train_AverageReturn : 1693.9000244140625
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17339.614934921265
Training Loss : 0.272399365901947
Done logging...




********** Iteration 2341000 ************

Training agent...


********** Iteration 2342000 ************

Training agent...


********** Iteration 2343000 ************

Training agent...


********** Iteration 2344000 ************

Training agent...


********** Iteration 2345000 ************

Training agent...


********** Iteration 2346000 ************

Training agent...


********** Iteration 2347000 ************

Training agent...


********** Iteration 2348000 ************

Training agent...


********** Iteration 2349000 ************

Training agent...


********** Iteration 2350000 ************

Training agent...

Beginning logging procedure...
Timestep 2350001
mean reward (100 episodes) 1658.400024
best mean reward 1742.099976
running time 17415.727845
Train_EnvstepsSoFar : 2350001
Train_AverageReturn : 1658.4000244140625
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17415.727845430374
Training Loss : 0.5631817579269409
Done logging...




********** Iteration 2351000 ************

Training agent...


********** Iteration 2352000 ************

Training agent...


********** Iteration 2353000 ************

Training agent...


********** Iteration 2354000 ************

Training agent...


********** Iteration 2355000 ************

Training agent...


********** Iteration 2356000 ************

Training agent...


********** Iteration 2357000 ************

Training agent...


********** Iteration 2358000 ************

Training agent...


********** Iteration 2359000 ************

Training agent...


********** Iteration 2360000 ************

Training agent...

Beginning logging procedure...
Timestep 2360001
mean reward (100 episodes) 1650.099976
best mean reward 1742.099976
running time 17492.455059
Train_EnvstepsSoFar : 2360001
Train_AverageReturn : 1650.0999755859375
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17492.455058813095
Training Loss : 0.2421087771654129
Done logging...




********** Iteration 2361000 ************

Training agent...


********** Iteration 2362000 ************

Training agent...


********** Iteration 2363000 ************

Training agent...


********** Iteration 2364000 ************

Training agent...


********** Iteration 2365000 ************

Training agent...


********** Iteration 2366000 ************

Training agent...


********** Iteration 2367000 ************

Training agent...


********** Iteration 2368000 ************

Training agent...


********** Iteration 2369000 ************

Training agent...


********** Iteration 2370000 ************

Training agent...

Beginning logging procedure...
Timestep 2370001
mean reward (100 episodes) 1647.400024
best mean reward 1742.099976
running time 17570.151807
Train_EnvstepsSoFar : 2370001
Train_AverageReturn : 1647.4000244140625
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17570.15180659294
Training Loss : 0.5357474088668823
Done logging...




********** Iteration 2371000 ************

Training agent...


********** Iteration 2372000 ************

Training agent...


********** Iteration 2373000 ************

Training agent...


********** Iteration 2374000 ************

Training agent...


********** Iteration 2375000 ************

Training agent...


********** Iteration 2376000 ************

Training agent...


********** Iteration 2377000 ************

Training agent...


********** Iteration 2378000 ************

Training agent...


********** Iteration 2379000 ************

Training agent...


********** Iteration 2380000 ************

Training agent...

Beginning logging procedure...
Timestep 2380001
mean reward (100 episodes) 1658.000000
best mean reward 1742.099976
running time 17646.842903
Train_EnvstepsSoFar : 2380001
Train_AverageReturn : 1658.0
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17646.84290289879
Training Loss : 0.5873957276344299
Done logging...




********** Iteration 2381000 ************

Training agent...


********** Iteration 2382000 ************

Training agent...


********** Iteration 2383000 ************

Training agent...


********** Iteration 2384000 ************

Training agent...


********** Iteration 2385000 ************

Training agent...


********** Iteration 2386000 ************

Training agent...


********** Iteration 2387000 ************

Training agent...


********** Iteration 2388000 ************

Training agent...


********** Iteration 2389000 ************

Training agent...


********** Iteration 2390000 ************

Training agent...

Beginning logging procedure...
Timestep 2390001
mean reward (100 episodes) 1623.099976
best mean reward 1742.099976
running time 17724.861378
Train_EnvstepsSoFar : 2390001
Train_AverageReturn : 1623.0999755859375
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17724.861377716064
Training Loss : 1.0926326513290405
Done logging...




********** Iteration 2391000 ************

Training agent...


********** Iteration 2392000 ************

Training agent...


********** Iteration 2393000 ************

Training agent...


********** Iteration 2394000 ************

Training agent...


********** Iteration 2395000 ************

Training agent...


********** Iteration 2396000 ************

Training agent...


********** Iteration 2397000 ************

Training agent...


********** Iteration 2398000 ************

Training agent...


********** Iteration 2399000 ************

Training agent...


********** Iteration 2400000 ************

Training agent...

Beginning logging procedure...
Timestep 2400001
mean reward (100 episodes) 1649.599976
best mean reward 1742.099976
running time 17801.391361
Train_EnvstepsSoFar : 2400001
Train_AverageReturn : 1649.5999755859375
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17801.39136147499
Training Loss : 0.5536493062973022
Done logging...




********** Iteration 2401000 ************

Training agent...


********** Iteration 2402000 ************

Training agent...


********** Iteration 2403000 ************

Training agent...


********** Iteration 2404000 ************

Training agent...


********** Iteration 2405000 ************

Training agent...


********** Iteration 2406000 ************

Training agent...


********** Iteration 2407000 ************

Training agent...


********** Iteration 2408000 ************

Training agent...


********** Iteration 2409000 ************

Training agent...


********** Iteration 2410000 ************

Training agent...

Beginning logging procedure...
Timestep 2410001
mean reward (100 episodes) 1640.300049
best mean reward 1742.099976
running time 17877.618926
Train_EnvstepsSoFar : 2410001
Train_AverageReturn : 1640.300048828125
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17877.61892604828
Training Loss : 1.0120270252227783
Done logging...




********** Iteration 2411000 ************

Training agent...


********** Iteration 2412000 ************

Training agent...


********** Iteration 2413000 ************

Training agent...


********** Iteration 2414000 ************

Training agent...


********** Iteration 2415000 ************

Training agent...


********** Iteration 2416000 ************

Training agent...


********** Iteration 2417000 ************

Training agent...


********** Iteration 2418000 ************

Training agent...


********** Iteration 2419000 ************

Training agent...


********** Iteration 2420000 ************

Training agent...

Beginning logging procedure...
Timestep 2420001
mean reward (100 episodes) 1668.800049
best mean reward 1742.099976
running time 17951.974368
Train_EnvstepsSoFar : 2420001
Train_AverageReturn : 1668.800048828125
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 17951.97436785698
Training Loss : 0.19278854131698608
Done logging...




********** Iteration 2421000 ************

Training agent...


********** Iteration 2422000 ************

Training agent...


********** Iteration 2423000 ************

Training agent...


********** Iteration 2424000 ************

Training agent...


********** Iteration 2425000 ************

Training agent...


********** Iteration 2426000 ************

Training agent...


********** Iteration 2427000 ************

Training agent...


********** Iteration 2428000 ************

Training agent...


********** Iteration 2429000 ************

Training agent...


********** Iteration 2430000 ************

Training agent...

Beginning logging procedure...
Timestep 2430001
mean reward (100 episodes) 1672.800049
best mean reward 1742.099976
running time 18025.995556
Train_EnvstepsSoFar : 2430001
Train_AverageReturn : 1672.800048828125
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 18025.995556116104
Training Loss : 0.20577079057693481
Done logging...




********** Iteration 2431000 ************

Training agent...


********** Iteration 2432000 ************

Training agent...


********** Iteration 2433000 ************

Training agent...


********** Iteration 2434000 ************

Training agent...


********** Iteration 2435000 ************

Training agent...


********** Iteration 2436000 ************

Training agent...


********** Iteration 2437000 ************

Training agent...


********** Iteration 2438000 ************

Training agent...


********** Iteration 2439000 ************

Training agent...


********** Iteration 2440000 ************

Training agent...

Beginning logging procedure...
Timestep 2440001
mean reward (100 episodes) 1669.199951
best mean reward 1742.099976
running time 18100.509708
Train_EnvstepsSoFar : 2440001
Train_AverageReturn : 1669.199951171875
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 18100.509707927704
Training Loss : 0.5904216766357422
Done logging...




********** Iteration 2441000 ************

Training agent...


********** Iteration 2442000 ************

Training agent...


********** Iteration 2443000 ************

Training agent...


********** Iteration 2444000 ************

Training agent...


********** Iteration 2445000 ************

Training agent...


********** Iteration 2446000 ************

Training agent...


********** Iteration 2447000 ************

Training agent...


********** Iteration 2448000 ************

Training agent...


********** Iteration 2449000 ************

Training agent...


********** Iteration 2450000 ************

Training agent...

Beginning logging procedure...
Timestep 2450001
mean reward (100 episodes) 1695.900024
best mean reward 1742.099976
running time 18174.643112
Train_EnvstepsSoFar : 2450001
Train_AverageReturn : 1695.9000244140625
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 18174.6431119442
Training Loss : 0.2132701724767685
Done logging...




********** Iteration 2451000 ************

Training agent...


********** Iteration 2452000 ************

Training agent...


********** Iteration 2453000 ************

Training agent...


********** Iteration 2454000 ************

Training agent...


********** Iteration 2455000 ************

Training agent...


********** Iteration 2456000 ************

Training agent...


********** Iteration 2457000 ************

Training agent...


********** Iteration 2458000 ************

Training agent...


********** Iteration 2459000 ************

Training agent...


********** Iteration 2460000 ************

Training agent...

Beginning logging procedure...
Timestep 2460001
mean reward (100 episodes) 1691.000000
best mean reward 1742.099976
running time 18249.182384
Train_EnvstepsSoFar : 2460001
Train_AverageReturn : 1691.0
Train_BestReturn : 1742.0999755859375
TimeSinceStart : 18249.18238377571
Training Loss : 0.662705659866333
Done logging...




********** Iteration 2461000 ************

Training agent...


********** Iteration 2462000 ************

Training agent...


********** Iteration 2463000 ************

Training agent...


********** Iteration 2464000 ************

Training agent...


********** Iteration 2465000 ************

Training agent...


********** Iteration 2466000 ************

Training agent...


********** Iteration 2467000 ************

Training agent...


********** Iteration 2468000 ************

Training agent...


********** Iteration 2469000 ************

Training agent...


********** Iteration 2470000 ************

Training agent...

Beginning logging procedure...
Timestep 2470001
mean reward (100 episodes) 1748.699951
best mean reward 1748.699951
running time 18323.903395
Train_EnvstepsSoFar : 2470001
Train_AverageReturn : 1748.699951171875
Train_BestReturn : 1748.699951171875
TimeSinceStart : 18323.903394937515
Training Loss : 0.5573907494544983
Done logging...




********** Iteration 2471000 ************

Training agent...


********** Iteration 2472000 ************

Training agent...


********** Iteration 2473000 ************

Training agent...


********** Iteration 2474000 ************

Training agent...


********** Iteration 2475000 ************

Training agent...


********** Iteration 2476000 ************

Training agent...


********** Iteration 2477000 ************

Training agent...


********** Iteration 2478000 ************

Training agent...


********** Iteration 2479000 ************

Training agent...


********** Iteration 2480000 ************

Training agent...

Beginning logging procedure...
Timestep 2480001
mean reward (100 episodes) 1758.500000
best mean reward 1758.500000
running time 18398.664115
Train_EnvstepsSoFar : 2480001
Train_AverageReturn : 1758.5
Train_BestReturn : 1758.5
TimeSinceStart : 18398.66411471367
Training Loss : 1.1826156377792358
Done logging...




********** Iteration 2481000 ************

Training agent...


********** Iteration 2482000 ************

Training agent...


********** Iteration 2483000 ************

Training agent...


********** Iteration 2484000 ************

Training agent...


********** Iteration 2485000 ************

Training agent...


********** Iteration 2486000 ************

Training agent...


********** Iteration 2487000 ************

Training agent...


********** Iteration 2488000 ************

Training agent...


********** Iteration 2489000 ************

Training agent...


********** Iteration 2490000 ************

Training agent...

Beginning logging procedure...
Timestep 2490001
mean reward (100 episodes) 1728.300049
best mean reward 1758.500000
running time 18473.547812
Train_EnvstepsSoFar : 2490001
Train_AverageReturn : 1728.300048828125
Train_BestReturn : 1758.5
TimeSinceStart : 18473.54781150818
Training Loss : 0.23068705201148987
Done logging...




********** Iteration 2491000 ************

Training agent...


********** Iteration 2492000 ************

Training agent...


********** Iteration 2493000 ************

Training agent...


********** Iteration 2494000 ************

Training agent...


********** Iteration 2495000 ************

Training agent...


********** Iteration 2496000 ************

Training agent...


********** Iteration 2497000 ************

Training agent...


********** Iteration 2498000 ************

Training agent...


********** Iteration 2499000 ************

Training agent...


********** Iteration 2500000 ************

Training agent...

Beginning logging procedure...
Timestep 2500001
mean reward (100 episodes) 1631.099976
best mean reward 1758.500000
running time 18547.521725
Train_EnvstepsSoFar : 2500001
Train_AverageReturn : 1631.0999755859375
Train_BestReturn : 1758.5
TimeSinceStart : 18547.521725177765
Training Loss : 0.5135414600372314
Done logging...




********** Iteration 2501000 ************

Training agent...


********** Iteration 2502000 ************

Training agent...


********** Iteration 2503000 ************

Training agent...


********** Iteration 2504000 ************

Training agent...


********** Iteration 2505000 ************

Training agent...


********** Iteration 2506000 ************

Training agent...


********** Iteration 2507000 ************

Training agent...


********** Iteration 2508000 ************

Training agent...


********** Iteration 2509000 ************

Training agent...


********** Iteration 2510000 ************

Training agent...

Beginning logging procedure...
Timestep 2510001
mean reward (100 episodes) 1541.500000
best mean reward 1758.500000
running time 18621.731989
Train_EnvstepsSoFar : 2510001
Train_AverageReturn : 1541.5
Train_BestReturn : 1758.5
TimeSinceStart : 18621.731989383698
Training Loss : 0.12068350613117218
Done logging...




********** Iteration 2511000 ************

Training agent...


********** Iteration 2512000 ************

Training agent...


********** Iteration 2513000 ************

Training agent...


********** Iteration 2514000 ************

Training agent...


********** Iteration 2515000 ************

Training agent...


********** Iteration 2516000 ************

Training agent...


********** Iteration 2517000 ************

Training agent...


********** Iteration 2518000 ************

Training agent...


********** Iteration 2519000 ************

Training agent...


********** Iteration 2520000 ************

Training agent...

Beginning logging procedure...
Timestep 2520001
mean reward (100 episodes) 1612.500000
best mean reward 1758.500000
running time 18695.039089
Train_EnvstepsSoFar : 2520001
Train_AverageReturn : 1612.5
Train_BestReturn : 1758.5
TimeSinceStart : 18695.03908920288
Training Loss : 0.2568992078304291
Done logging...




********** Iteration 2521000 ************

Training agent...


********** Iteration 2522000 ************

Training agent...


********** Iteration 2523000 ************

Training agent...


********** Iteration 2524000 ************

Training agent...


********** Iteration 2525000 ************

Training agent...


********** Iteration 2526000 ************

Training agent...


********** Iteration 2527000 ************

Training agent...


********** Iteration 2528000 ************

Training agent...


********** Iteration 2529000 ************

Training agent...


********** Iteration 2530000 ************

Training agent...

Beginning logging procedure...
Timestep 2530001
mean reward (100 episodes) 1656.699951
best mean reward 1758.500000
running time 18768.980545
Train_EnvstepsSoFar : 2530001
Train_AverageReturn : 1656.699951171875
Train_BestReturn : 1758.5
TimeSinceStart : 18768.980544805527
Training Loss : 0.40023839473724365
Done logging...




********** Iteration 2531000 ************

Training agent...


********** Iteration 2532000 ************

Training agent...


********** Iteration 2533000 ************

Training agent...


********** Iteration 2534000 ************

Training agent...


********** Iteration 2535000 ************

Training agent...


********** Iteration 2536000 ************

Training agent...


********** Iteration 2537000 ************

Training agent...


********** Iteration 2538000 ************

Training agent...


********** Iteration 2539000 ************

Training agent...


********** Iteration 2540000 ************

Training agent...

Beginning logging procedure...
Timestep 2540001
mean reward (100 episodes) 1652.500000
best mean reward 1758.500000
running time 18842.625055
Train_EnvstepsSoFar : 2540001
Train_AverageReturn : 1652.5
Train_BestReturn : 1758.5
TimeSinceStart : 18842.62505507469
Training Loss : 0.49938952922821045
Done logging...




********** Iteration 2541000 ************

Training agent...


********** Iteration 2542000 ************

Training agent...


********** Iteration 2543000 ************

Training agent...


********** Iteration 2544000 ************

Training agent...


********** Iteration 2545000 ************

Training agent...


********** Iteration 2546000 ************

Training agent...


********** Iteration 2547000 ************

Training agent...


********** Iteration 2548000 ************

Training agent...


********** Iteration 2549000 ************

Training agent...


********** Iteration 2550000 ************

Training agent...

Beginning logging procedure...
Timestep 2550001
mean reward (100 episodes) 1614.000000
best mean reward 1758.500000
running time 18915.512727
Train_EnvstepsSoFar : 2550001
Train_AverageReturn : 1614.0
Train_BestReturn : 1758.5
TimeSinceStart : 18915.512726783752
Training Loss : 0.5892249345779419
Done logging...




********** Iteration 2551000 ************

Training agent...


********** Iteration 2552000 ************

Training agent...


********** Iteration 2553000 ************

Training agent...


********** Iteration 2554000 ************

Training agent...


********** Iteration 2555000 ************

Training agent...


********** Iteration 2556000 ************

Training agent...


********** Iteration 2557000 ************

Training agent...


********** Iteration 2558000 ************

Training agent...


********** Iteration 2559000 ************

Training agent...


********** Iteration 2560000 ************

Training agent...

Beginning logging procedure...
Timestep 2560001
mean reward (100 episodes) 1591.500000
best mean reward 1758.500000
running time 18989.177533
Train_EnvstepsSoFar : 2560001
Train_AverageReturn : 1591.5
Train_BestReturn : 1758.5
TimeSinceStart : 18989.1775329113
Training Loss : 0.2915958762168884
Done logging...




********** Iteration 2561000 ************

Training agent...


********** Iteration 2562000 ************

Training agent...


********** Iteration 2563000 ************

Training agent...


********** Iteration 2564000 ************

Training agent...


********** Iteration 2565000 ************

Training agent...


********** Iteration 2566000 ************

Training agent...


********** Iteration 2567000 ************

Training agent...


********** Iteration 2568000 ************

Training agent...


********** Iteration 2569000 ************

Training agent...


********** Iteration 2570000 ************

Training agent...

Beginning logging procedure...
Timestep 2570001
mean reward (100 episodes) 1593.199951
best mean reward 1758.500000
running time 19062.021842
Train_EnvstepsSoFar : 2570001
Train_AverageReturn : 1593.199951171875
Train_BestReturn : 1758.5
TimeSinceStart : 19062.02184152603
Training Loss : 0.32263365387916565
Done logging...




********** Iteration 2571000 ************

Training agent...


********** Iteration 2572000 ************

Training agent...


********** Iteration 2573000 ************

Training agent...


********** Iteration 2574000 ************

Training agent...


********** Iteration 2575000 ************

Training agent...


********** Iteration 2576000 ************

Training agent...


********** Iteration 2577000 ************

Training agent...


********** Iteration 2578000 ************

Training agent...


********** Iteration 2579000 ************

Training agent...


********** Iteration 2580000 ************

Training agent...

Beginning logging procedure...
Timestep 2580001
mean reward (100 episodes) 1609.800049
best mean reward 1758.500000
running time 19135.079788
Train_EnvstepsSoFar : 2580001
Train_AverageReturn : 1609.800048828125
Train_BestReturn : 1758.5
TimeSinceStart : 19135.079788208008
Training Loss : 0.26054245233535767
Done logging...




********** Iteration 2581000 ************

Training agent...


********** Iteration 2582000 ************

Training agent...


********** Iteration 2583000 ************

Training agent...


********** Iteration 2584000 ************

Training agent...


********** Iteration 2585000 ************

Training agent...


********** Iteration 2586000 ************

Training agent...


********** Iteration 2587000 ************

Training agent...


********** Iteration 2588000 ************

Training agent...


********** Iteration 2589000 ************

Training agent...


********** Iteration 2590000 ************

Training agent...

Beginning logging procedure...
Timestep 2590001
mean reward (100 episodes) 1614.800049
best mean reward 1758.500000
running time 19208.795877
Train_EnvstepsSoFar : 2590001
Train_AverageReturn : 1614.800048828125
Train_BestReturn : 1758.5
TimeSinceStart : 19208.795877218246
Training Loss : 0.15262602269649506
Done logging...




********** Iteration 2591000 ************

Training agent...


********** Iteration 2592000 ************

Training agent...


********** Iteration 2593000 ************

Training agent...


********** Iteration 2594000 ************

Training agent...


********** Iteration 2595000 ************

Training agent...


********** Iteration 2596000 ************

Training agent...


********** Iteration 2597000 ************

Training agent...


********** Iteration 2598000 ************

Training agent...


********** Iteration 2599000 ************

Training agent...


********** Iteration 2600000 ************

Training agent...

Beginning logging procedure...
Timestep 2600001
mean reward (100 episodes) 1647.599976
best mean reward 1758.500000
running time 19280.667251
Train_EnvstepsSoFar : 2600001
Train_AverageReturn : 1647.5999755859375
Train_BestReturn : 1758.5
TimeSinceStart : 19280.667251348495
Training Loss : 0.5964576601982117
Done logging...




********** Iteration 2601000 ************

Training agent...


********** Iteration 2602000 ************

Training agent...


********** Iteration 2603000 ************

Training agent...


********** Iteration 2604000 ************

Training agent...


********** Iteration 2605000 ************

Training agent...


********** Iteration 2606000 ************

Training agent...


********** Iteration 2607000 ************

Training agent...


********** Iteration 2608000 ************

Training agent...


********** Iteration 2609000 ************

Training agent...


********** Iteration 2610000 ************

Training agent...

Beginning logging procedure...
Timestep 2610001
mean reward (100 episodes) 1635.599976
best mean reward 1758.500000
running time 19352.942320
Train_EnvstepsSoFar : 2610001
Train_AverageReturn : 1635.5999755859375
Train_BestReturn : 1758.5
TimeSinceStart : 19352.942320346832
Training Loss : 0.4488007426261902
Done logging...




********** Iteration 2611000 ************

Training agent...


********** Iteration 2612000 ************

Training agent...


********** Iteration 2613000 ************

Training agent...


********** Iteration 2614000 ************

Training agent...


********** Iteration 2615000 ************

Training agent...


********** Iteration 2616000 ************

Training agent...


********** Iteration 2617000 ************

Training agent...


********** Iteration 2618000 ************

Training agent...


********** Iteration 2619000 ************

Training agent...


********** Iteration 2620000 ************

Training agent...

Beginning logging procedure...
Timestep 2620001
mean reward (100 episodes) 1652.699951
best mean reward 1758.500000
running time 19426.508531
Train_EnvstepsSoFar : 2620001
Train_AverageReturn : 1652.699951171875
Train_BestReturn : 1758.5
TimeSinceStart : 19426.508531093597
Training Loss : 0.2616024911403656
Done logging...




********** Iteration 2621000 ************

Training agent...


********** Iteration 2622000 ************

Training agent...


********** Iteration 2623000 ************

Training agent...


********** Iteration 2624000 ************

Training agent...


********** Iteration 2625000 ************

Training agent...


********** Iteration 2626000 ************

Training agent...


********** Iteration 2627000 ************

Training agent...


********** Iteration 2628000 ************

Training agent...


********** Iteration 2629000 ************

Training agent...


********** Iteration 2630000 ************

Training agent...

Beginning logging procedure...
Timestep 2630001
mean reward (100 episodes) 1636.500000
best mean reward 1758.500000
running time 19499.168294
Train_EnvstepsSoFar : 2630001
Train_AverageReturn : 1636.5
Train_BestReturn : 1758.5
TimeSinceStart : 19499.16829419136
Training Loss : 1.2579264640808105
Done logging...




********** Iteration 2631000 ************

Training agent...


********** Iteration 2632000 ************

Training agent...


********** Iteration 2633000 ************

Training agent...


********** Iteration 2634000 ************

Training agent...


********** Iteration 2635000 ************

Training agent...


********** Iteration 2636000 ************

Training agent...


********** Iteration 2637000 ************

Training agent...


********** Iteration 2638000 ************

Training agent...


********** Iteration 2639000 ************

Training agent...


********** Iteration 2640000 ************

Training agent...

Beginning logging procedure...
Timestep 2640001
mean reward (100 episodes) 1650.699951
best mean reward 1758.500000
running time 19572.365060
Train_EnvstepsSoFar : 2640001
Train_AverageReturn : 1650.699951171875
Train_BestReturn : 1758.5
TimeSinceStart : 19572.36506009102
Training Loss : 0.7658828496932983
Done logging...




********** Iteration 2641000 ************

Training agent...


********** Iteration 2642000 ************

Training agent...


********** Iteration 2643000 ************

Training agent...


********** Iteration 2644000 ************

Training agent...


********** Iteration 2645000 ************

Training agent...


********** Iteration 2646000 ************

Training agent...


********** Iteration 2647000 ************

Training agent...


********** Iteration 2648000 ************

Training agent...


********** Iteration 2649000 ************

Training agent...


********** Iteration 2650000 ************

Training agent...

Beginning logging procedure...
Timestep 2650001
mean reward (100 episodes) 1632.099976
best mean reward 1758.500000
running time 19644.633847
Train_EnvstepsSoFar : 2650001
Train_AverageReturn : 1632.0999755859375
Train_BestReturn : 1758.5
TimeSinceStart : 19644.633847475052
Training Loss : 0.1422659158706665
Done logging...




********** Iteration 2651000 ************

Training agent...


********** Iteration 2652000 ************

Training agent...


********** Iteration 2653000 ************

Training agent...


********** Iteration 2654000 ************

Training agent...


********** Iteration 2655000 ************

Training agent...


********** Iteration 2656000 ************

Training agent...


********** Iteration 2657000 ************

Training agent...


********** Iteration 2658000 ************

Training agent...


********** Iteration 2659000 ************

Training agent...


********** Iteration 2660000 ************

Training agent...

Beginning logging procedure...
Timestep 2660001
mean reward (100 episodes) 1667.599976
best mean reward 1758.500000
running time 19716.416396
Train_EnvstepsSoFar : 2660001
Train_AverageReturn : 1667.5999755859375
Train_BestReturn : 1758.5
TimeSinceStart : 19716.416395902634
Training Loss : 0.5407561659812927
Done logging...




********** Iteration 2661000 ************

Training agent...


********** Iteration 2662000 ************

Training agent...


********** Iteration 2663000 ************

Training agent...


********** Iteration 2664000 ************

Training agent...


********** Iteration 2665000 ************

Training agent...


********** Iteration 2666000 ************

Training agent...


********** Iteration 2667000 ************

Training agent...


********** Iteration 2668000 ************

Training agent...


********** Iteration 2669000 ************

Training agent...


********** Iteration 2670000 ************

Training agent...

Beginning logging procedure...
Timestep 2670001
mean reward (100 episodes) 1633.300049
best mean reward 1758.500000
running time 19788.832515
Train_EnvstepsSoFar : 2670001
Train_AverageReturn : 1633.300048828125
Train_BestReturn : 1758.5
TimeSinceStart : 19788.832515001297
Training Loss : 0.26621267199516296
Done logging...




********** Iteration 2671000 ************

Training agent...


********** Iteration 2672000 ************

Training agent...


********** Iteration 2673000 ************

Training agent...


********** Iteration 2674000 ************

Training agent...


********** Iteration 2675000 ************

Training agent...


********** Iteration 2676000 ************

Training agent...


********** Iteration 2677000 ************

Training agent...


********** Iteration 2678000 ************

Training agent...


********** Iteration 2679000 ************

Training agent...


********** Iteration 2680000 ************

Training agent...

Beginning logging procedure...
Timestep 2680001
mean reward (100 episodes) 1616.599976
best mean reward 1758.500000
running time 19860.303143
Train_EnvstepsSoFar : 2680001
Train_AverageReturn : 1616.5999755859375
Train_BestReturn : 1758.5
TimeSinceStart : 19860.303143024445
Training Loss : 0.8422096967697144
Done logging...




********** Iteration 2681000 ************

Training agent...


********** Iteration 2682000 ************

Training agent...


********** Iteration 2683000 ************

Training agent...


********** Iteration 2684000 ************

Training agent...


********** Iteration 2685000 ************

Training agent...


********** Iteration 2686000 ************

Training agent...


********** Iteration 2687000 ************

Training agent...


********** Iteration 2688000 ************

Training agent...


********** Iteration 2689000 ************

Training agent...


********** Iteration 2690000 ************

Training agent...

Beginning logging procedure...
Timestep 2690001
mean reward (100 episodes) 1588.400024
best mean reward 1758.500000
running time 19931.267849
Train_EnvstepsSoFar : 2690001
Train_AverageReturn : 1588.4000244140625
Train_BestReturn : 1758.5
TimeSinceStart : 19931.267848730087
Training Loss : 0.37638553977012634
Done logging...




********** Iteration 2691000 ************

Training agent...


********** Iteration 2692000 ************

Training agent...


********** Iteration 2693000 ************

Training agent...


********** Iteration 2694000 ************

Training agent...


********** Iteration 2695000 ************

Training agent...


********** Iteration 2696000 ************

Training agent...


********** Iteration 2697000 ************

Training agent...


********** Iteration 2698000 ************

Training agent...


********** Iteration 2699000 ************

Training agent...


********** Iteration 2700000 ************

Training agent...

Beginning logging procedure...
Timestep 2700001
mean reward (100 episodes) 1647.699951
best mean reward 1758.500000
running time 20003.035230
Train_EnvstepsSoFar : 2700001
Train_AverageReturn : 1647.699951171875
Train_BestReturn : 1758.5
TimeSinceStart : 20003.035229682922
Training Loss : 0.1755302995443344
Done logging...




********** Iteration 2701000 ************

Training agent...


********** Iteration 2702000 ************

Training agent...


********** Iteration 2703000 ************

Training agent...


********** Iteration 2704000 ************

Training agent...


********** Iteration 2705000 ************

Training agent...


********** Iteration 2706000 ************

Training agent...


********** Iteration 2707000 ************

Training agent...


********** Iteration 2708000 ************

Training agent...


********** Iteration 2709000 ************

Training agent...


********** Iteration 2710000 ************

Training agent...

Beginning logging procedure...
Timestep 2710001
mean reward (100 episodes) 1677.400024
best mean reward 1758.500000
running time 20076.673496
Train_EnvstepsSoFar : 2710001
Train_AverageReturn : 1677.4000244140625
Train_BestReturn : 1758.5
TimeSinceStart : 20076.673496246338
Training Loss : 0.24986672401428223
Done logging...




********** Iteration 2711000 ************

Training agent...


********** Iteration 2712000 ************

Training agent...


********** Iteration 2713000 ************

Training agent...


********** Iteration 2714000 ************

Training agent...


********** Iteration 2715000 ************

Training agent...


********** Iteration 2716000 ************

Training agent...


********** Iteration 2717000 ************

Training agent...


********** Iteration 2718000 ************

Training agent...


********** Iteration 2719000 ************

Training agent...


********** Iteration 2720000 ************

Training agent...

Beginning logging procedure...
Timestep 2720001
mean reward (100 episodes) 1666.500000
best mean reward 1758.500000
running time 20149.743585
Train_EnvstepsSoFar : 2720001
Train_AverageReturn : 1666.5
Train_BestReturn : 1758.5
TimeSinceStart : 20149.74358510971
Training Loss : 0.3150438666343689
Done logging...




********** Iteration 2721000 ************

Training agent...


********** Iteration 2722000 ************

Training agent...


********** Iteration 2723000 ************

Training agent...


********** Iteration 2724000 ************

Training agent...


********** Iteration 2725000 ************

Training agent...


********** Iteration 2726000 ************

Training agent...


********** Iteration 2727000 ************

Training agent...


********** Iteration 2728000 ************

Training agent...


********** Iteration 2729000 ************

Training agent...


********** Iteration 2730000 ************

Training agent...

Beginning logging procedure...
Timestep 2730001
mean reward (100 episodes) 1657.400024
best mean reward 1758.500000
running time 20222.672441
Train_EnvstepsSoFar : 2730001
Train_AverageReturn : 1657.4000244140625
Train_BestReturn : 1758.5
TimeSinceStart : 20222.67244052887
Training Loss : 0.13740193843841553
Done logging...




********** Iteration 2731000 ************

Training agent...


********** Iteration 2732000 ************

Training agent...


********** Iteration 2733000 ************

Training agent...


********** Iteration 2734000 ************

Training agent...


********** Iteration 2735000 ************

Training agent...


********** Iteration 2736000 ************

Training agent...


********** Iteration 2737000 ************

Training agent...


********** Iteration 2738000 ************

Training agent...


********** Iteration 2739000 ************

Training agent...


********** Iteration 2740000 ************

Training agent...

Beginning logging procedure...
Timestep 2740001
mean reward (100 episodes) 1759.400024
best mean reward 1759.400024
running time 20295.637257
Train_EnvstepsSoFar : 2740001
Train_AverageReturn : 1759.4000244140625
Train_BestReturn : 1759.4000244140625
TimeSinceStart : 20295.637256860733
Training Loss : 0.16509069502353668
Done logging...




********** Iteration 2741000 ************

Training agent...


********** Iteration 2742000 ************

Training agent...


********** Iteration 2743000 ************

Training agent...


********** Iteration 2744000 ************

Training agent...


********** Iteration 2745000 ************

Training agent...


********** Iteration 2746000 ************

Training agent...


********** Iteration 2747000 ************

Training agent...


********** Iteration 2748000 ************

Training agent...


********** Iteration 2749000 ************

Training agent...


********** Iteration 2750000 ************

Training agent...

Beginning logging procedure...
Timestep 2750001
mean reward (100 episodes) 1749.599976
best mean reward 1759.400024
running time 20368.836097
Train_EnvstepsSoFar : 2750001
Train_AverageReturn : 1749.5999755859375
Train_BestReturn : 1759.4000244140625
TimeSinceStart : 20368.836097478867
Training Loss : 0.43018999695777893
Done logging...




********** Iteration 2751000 ************

Training agent...


********** Iteration 2752000 ************

Training agent...


********** Iteration 2753000 ************

Training agent...


********** Iteration 2754000 ************

Training agent...


********** Iteration 2755000 ************

Training agent...


********** Iteration 2756000 ************

Training agent...


********** Iteration 2757000 ************

Training agent...


********** Iteration 2758000 ************

Training agent...


********** Iteration 2759000 ************

Training agent...


********** Iteration 2760000 ************

Training agent...

Beginning logging procedure...
Timestep 2760001
mean reward (100 episodes) 1740.900024
best mean reward 1759.400024
running time 20441.173634
Train_EnvstepsSoFar : 2760001
Train_AverageReturn : 1740.9000244140625
Train_BestReturn : 1759.4000244140625
TimeSinceStart : 20441.173634052277
Training Loss : 0.1543537676334381
Done logging...




********** Iteration 2761000 ************

Training agent...


********** Iteration 2762000 ************

Training agent...


********** Iteration 2763000 ************

Training agent...


********** Iteration 2764000 ************

Training agent...


********** Iteration 2765000 ************

Training agent...


********** Iteration 2766000 ************

Training agent...


********** Iteration 2767000 ************

Training agent...


********** Iteration 2768000 ************

Training agent...


********** Iteration 2769000 ************

Training agent...


********** Iteration 2770000 ************

Training agent...

Beginning logging procedure...
Timestep 2770001
mean reward (100 episodes) 1691.400024
best mean reward 1759.400024
running time 20514.196193
Train_EnvstepsSoFar : 2770001
Train_AverageReturn : 1691.4000244140625
Train_BestReturn : 1759.4000244140625
TimeSinceStart : 20514.196192502975
Training Loss : 0.39182084798812866
Done logging...




********** Iteration 2771000 ************

Training agent...


********** Iteration 2772000 ************

Training agent...


********** Iteration 2773000 ************

Training agent...


********** Iteration 2774000 ************

Training agent...


********** Iteration 2775000 ************

Training agent...


********** Iteration 2776000 ************

Training agent...


********** Iteration 2777000 ************

Training agent...


********** Iteration 2778000 ************

Training agent...


********** Iteration 2779000 ************

Training agent...


********** Iteration 2780000 ************

Training agent...

Beginning logging procedure...
Timestep 2780001
mean reward (100 episodes) 1707.099976
best mean reward 1759.400024
running time 20587.241851
Train_EnvstepsSoFar : 2780001
Train_AverageReturn : 1707.0999755859375
Train_BestReturn : 1759.4000244140625
TimeSinceStart : 20587.241851091385
Training Loss : 0.35861706733703613
Done logging...




********** Iteration 2781000 ************

Training agent...


********** Iteration 2782000 ************

Training agent...


********** Iteration 2783000 ************

Training agent...


********** Iteration 2784000 ************

Training agent...


********** Iteration 2785000 ************

Training agent...


********** Iteration 2786000 ************

Training agent...


********** Iteration 2787000 ************

Training agent...


********** Iteration 2788000 ************

Training agent...


********** Iteration 2789000 ************

Training agent...


********** Iteration 2790000 ************

Training agent...

Beginning logging procedure...
Timestep 2790001
mean reward (100 episodes) 1767.099976
best mean reward 1767.099976
running time 20660.667135
Train_EnvstepsSoFar : 2790001
Train_AverageReturn : 1767.0999755859375
Train_BestReturn : 1767.0999755859375
TimeSinceStart : 20660.667135477066
Training Loss : 0.5097988843917847
Done logging...




********** Iteration 2791000 ************

Training agent...


********** Iteration 2792000 ************

Training agent...


********** Iteration 2793000 ************

Training agent...


********** Iteration 2794000 ************

Training agent...


********** Iteration 2795000 ************

Training agent...


********** Iteration 2796000 ************

Training agent...


********** Iteration 2797000 ************

Training agent...


********** Iteration 2798000 ************

Training agent...


********** Iteration 2799000 ************

Training agent...


********** Iteration 2800000 ************

Training agent...

Beginning logging procedure...
Timestep 2800001
mean reward (100 episodes) 1793.599976
best mean reward 1793.599976
running time 20734.129372
Train_EnvstepsSoFar : 2800001
Train_AverageReturn : 1793.5999755859375
Train_BestReturn : 1793.5999755859375
TimeSinceStart : 20734.129371643066
Training Loss : 0.3113482892513275
Done logging...




********** Iteration 2801000 ************

Training agent...


********** Iteration 2802000 ************

Training agent...


********** Iteration 2803000 ************

Training agent...


********** Iteration 2804000 ************

Training agent...


********** Iteration 2805000 ************

Training agent...


********** Iteration 2806000 ************

Training agent...


********** Iteration 2807000 ************

Training agent...


********** Iteration 2808000 ************

Training agent...


********** Iteration 2809000 ************

Training agent...


********** Iteration 2810000 ************

Training agent...

Beginning logging procedure...
Timestep 2810001
mean reward (100 episodes) 1766.500000
best mean reward 1793.599976
running time 20807.551293
Train_EnvstepsSoFar : 2810001
Train_AverageReturn : 1766.5
Train_BestReturn : 1793.5999755859375
TimeSinceStart : 20807.55129289627
Training Loss : 1.0574634075164795
Done logging...




********** Iteration 2811000 ************

Training agent...


********** Iteration 2812000 ************

Training agent...


********** Iteration 2813000 ************

Training agent...


********** Iteration 2814000 ************

Training agent...


********** Iteration 2815000 ************

Training agent...


********** Iteration 2816000 ************

Training agent...


********** Iteration 2817000 ************

Training agent...


********** Iteration 2818000 ************

Training agent...


********** Iteration 2819000 ************

Training agent...


********** Iteration 2820000 ************

Training agent...

Beginning logging procedure...
Timestep 2820001
mean reward (100 episodes) 1786.800049
best mean reward 1793.599976
running time 20881.302717
Train_EnvstepsSoFar : 2820001
Train_AverageReturn : 1786.800048828125
Train_BestReturn : 1793.5999755859375
TimeSinceStart : 20881.302717208862
Training Loss : 0.17175288498401642
Done logging...




********** Iteration 2821000 ************

Training agent...


********** Iteration 2822000 ************

Training agent...


********** Iteration 2823000 ************

Training agent...


********** Iteration 2824000 ************

Training agent...


********** Iteration 2825000 ************

Training agent...


********** Iteration 2826000 ************

Training agent...


********** Iteration 2827000 ************

Training agent...


********** Iteration 2828000 ************

Training agent...


********** Iteration 2829000 ************

Training agent...


********** Iteration 2830000 ************

Training agent...

Beginning logging procedure...
Timestep 2830001
mean reward (100 episodes) 1796.199951
best mean reward 1796.199951
running time 20954.483014
Train_EnvstepsSoFar : 2830001
Train_AverageReturn : 1796.199951171875
Train_BestReturn : 1796.199951171875
TimeSinceStart : 20954.483013629913
Training Loss : 0.3689991235733032
Done logging...




********** Iteration 2831000 ************

Training agent...


********** Iteration 2832000 ************

Training agent...


********** Iteration 2833000 ************

Training agent...


********** Iteration 2834000 ************

Training agent...


********** Iteration 2835000 ************

Training agent...


********** Iteration 2836000 ************

Training agent...


********** Iteration 2837000 ************

Training agent...


********** Iteration 2838000 ************

Training agent...


********** Iteration 2839000 ************

Training agent...


********** Iteration 2840000 ************

Training agent...

Beginning logging procedure...
Timestep 2840001
mean reward (100 episodes) 1767.300049
best mean reward 1796.199951
running time 21027.906511
Train_EnvstepsSoFar : 2840001
Train_AverageReturn : 1767.300048828125
Train_BestReturn : 1796.199951171875
TimeSinceStart : 21027.906510591507
Training Loss : 0.2438061684370041
Done logging...




********** Iteration 2841000 ************

Training agent...


********** Iteration 2842000 ************

Training agent...


********** Iteration 2843000 ************

Training agent...


********** Iteration 2844000 ************

Training agent...


********** Iteration 2845000 ************

Training agent...


********** Iteration 2846000 ************

Training agent...


********** Iteration 2847000 ************

Training agent...


********** Iteration 2848000 ************

Training agent...


********** Iteration 2849000 ************

Training agent...


********** Iteration 2850000 ************

Training agent...

Beginning logging procedure...
Timestep 2850001
mean reward (100 episodes) 1759.699951
best mean reward 1796.199951
running time 21101.197110
Train_EnvstepsSoFar : 2850001
Train_AverageReturn : 1759.699951171875
Train_BestReturn : 1796.199951171875
TimeSinceStart : 21101.197110414505
Training Loss : 0.45219218730926514
Done logging...




********** Iteration 2851000 ************

Training agent...


********** Iteration 2852000 ************

Training agent...


********** Iteration 2853000 ************

Training agent...


********** Iteration 2854000 ************

Training agent...


********** Iteration 2855000 ************

Training agent...


********** Iteration 2856000 ************

Training agent...


********** Iteration 2857000 ************

Training agent...


********** Iteration 2858000 ************

Training agent...


********** Iteration 2859000 ************

Training agent...


********** Iteration 2860000 ************

Training agent...

Beginning logging procedure...
Timestep 2860001
mean reward (100 episodes) 1775.500000
best mean reward 1796.199951
running time 21174.672079
Train_EnvstepsSoFar : 2860001
Train_AverageReturn : 1775.5
Train_BestReturn : 1796.199951171875
TimeSinceStart : 21174.672079324722
Training Loss : 1.4007076025009155
Done logging...




********** Iteration 2861000 ************

Training agent...


********** Iteration 2862000 ************

Training agent...


********** Iteration 2863000 ************

Training agent...


********** Iteration 2864000 ************

Training agent...


********** Iteration 2865000 ************

Training agent...


********** Iteration 2866000 ************

Training agent...


********** Iteration 2867000 ************

Training agent...


********** Iteration 2868000 ************

Training agent...


********** Iteration 2869000 ************

Training agent...


********** Iteration 2870000 ************

Training agent...

Beginning logging procedure...
Timestep 2870001
mean reward (100 episodes) 1756.199951
best mean reward 1796.199951
running time 21247.720526
Train_EnvstepsSoFar : 2870001
Train_AverageReturn : 1756.199951171875
Train_BestReturn : 1796.199951171875
TimeSinceStart : 21247.720526456833
Training Loss : 0.9230684041976929
Done logging...




********** Iteration 2871000 ************

Training agent...


********** Iteration 2872000 ************

Training agent...


********** Iteration 2873000 ************

Training agent...


********** Iteration 2874000 ************

Training agent...


********** Iteration 2875000 ************

Training agent...


********** Iteration 2876000 ************

Training agent...


********** Iteration 2877000 ************

Training agent...


********** Iteration 2878000 ************

Training agent...


********** Iteration 2879000 ************

Training agent...


********** Iteration 2880000 ************

Training agent...

Beginning logging procedure...
Timestep 2880001
mean reward (100 episodes) 1707.300049
best mean reward 1796.199951
running time 21320.788105
Train_EnvstepsSoFar : 2880001
Train_AverageReturn : 1707.300048828125
Train_BestReturn : 1796.199951171875
TimeSinceStart : 21320.788105249405
Training Loss : 1.107401728630066
Done logging...




********** Iteration 2881000 ************

Training agent...


********** Iteration 2882000 ************

Training agent...


********** Iteration 2883000 ************

Training agent...


********** Iteration 2884000 ************

Training agent...


********** Iteration 2885000 ************

Training agent...


********** Iteration 2886000 ************

Training agent...


********** Iteration 2887000 ************

Training agent...


********** Iteration 2888000 ************

Training agent...


********** Iteration 2889000 ************

Training agent...


********** Iteration 2890000 ************

Training agent...

Beginning logging procedure...
Timestep 2890001
mean reward (100 episodes) 1753.699951
best mean reward 1796.199951
running time 21394.189135
Train_EnvstepsSoFar : 2890001
Train_AverageReturn : 1753.699951171875
Train_BestReturn : 1796.199951171875
TimeSinceStart : 21394.189134836197
Training Loss : 0.7496938109397888
Done logging...




********** Iteration 2891000 ************

Training agent...


********** Iteration 2892000 ************

Training agent...


********** Iteration 2893000 ************

Training agent...


********** Iteration 2894000 ************

Training agent...


********** Iteration 2895000 ************

Training agent...


********** Iteration 2896000 ************

Training agent...


********** Iteration 2897000 ************

Training agent...


********** Iteration 2898000 ************

Training agent...


********** Iteration 2899000 ************

Training agent...


********** Iteration 2900000 ************

Training agent...

Beginning logging procedure...
Timestep 2900001
mean reward (100 episodes) 1820.000000
best mean reward 1820.000000
running time 21467.420679
Train_EnvstepsSoFar : 2900001
Train_AverageReturn : 1820.0
Train_BestReturn : 1820.0
TimeSinceStart : 21467.420679330826
Training Loss : 0.2896858751773834
Done logging...




********** Iteration 2901000 ************

Training agent...


********** Iteration 2902000 ************

Training agent...


********** Iteration 2903000 ************

Training agent...


********** Iteration 2904000 ************

Training agent...


********** Iteration 2905000 ************

Training agent...


********** Iteration 2906000 ************

Training agent...


********** Iteration 2907000 ************

Training agent...


********** Iteration 2908000 ************

Training agent...


********** Iteration 2909000 ************

Training agent...


********** Iteration 2910000 ************

Training agent...

Beginning logging procedure...
Timestep 2910001
mean reward (100 episodes) 1830.300049
best mean reward 1830.300049
running time 21539.777960
Train_EnvstepsSoFar : 2910001
Train_AverageReturn : 1830.300048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 21539.777960300446
Training Loss : 0.21184080839157104
Done logging...




********** Iteration 2911000 ************

Training agent...


********** Iteration 2912000 ************

Training agent...


********** Iteration 2913000 ************

Training agent...


********** Iteration 2914000 ************

Training agent...


********** Iteration 2915000 ************

Training agent...


********** Iteration 2916000 ************

Training agent...


********** Iteration 2917000 ************

Training agent...


********** Iteration 2918000 ************

Training agent...


********** Iteration 2919000 ************

Training agent...


********** Iteration 2920000 ************

Training agent...

Beginning logging procedure...
Timestep 2920001
mean reward (100 episodes) 1775.000000
best mean reward 1830.300049
running time 21612.695054
Train_EnvstepsSoFar : 2920001
Train_AverageReturn : 1775.0
Train_BestReturn : 1830.300048828125
TimeSinceStart : 21612.69505429268
Training Loss : 0.2936892807483673
Done logging...




********** Iteration 2921000 ************

Training agent...


********** Iteration 2922000 ************

Training agent...


********** Iteration 2923000 ************

Training agent...


********** Iteration 2924000 ************

Training agent...


********** Iteration 2925000 ************

Training agent...


********** Iteration 2926000 ************

Training agent...


********** Iteration 2927000 ************

Training agent...


********** Iteration 2928000 ************

Training agent...


********** Iteration 2929000 ************

Training agent...


********** Iteration 2930000 ************

Training agent...

Beginning logging procedure...
Timestep 2930001
mean reward (100 episodes) 1737.300049
best mean reward 1830.300049
running time 21685.985313
Train_EnvstepsSoFar : 2930001
Train_AverageReturn : 1737.300048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 21685.985313415527
Training Loss : 0.14697587490081787
Done logging...




********** Iteration 2931000 ************

Training agent...


********** Iteration 2932000 ************

Training agent...


********** Iteration 2933000 ************

Training agent...


********** Iteration 2934000 ************

Training agent...


********** Iteration 2935000 ************

Training agent...


********** Iteration 2936000 ************

Training agent...


********** Iteration 2937000 ************

Training agent...


********** Iteration 2938000 ************

Training agent...


********** Iteration 2939000 ************

Training agent...


********** Iteration 2940000 ************

Training agent...

Beginning logging procedure...
Timestep 2940001
mean reward (100 episodes) 1751.099976
best mean reward 1830.300049
running time 21758.869259
Train_EnvstepsSoFar : 2940001
Train_AverageReturn : 1751.0999755859375
Train_BestReturn : 1830.300048828125
TimeSinceStart : 21758.869259357452
Training Loss : 0.16282932460308075
Done logging...




********** Iteration 2941000 ************

Training agent...


********** Iteration 2942000 ************

Training agent...


********** Iteration 2943000 ************

Training agent...


********** Iteration 2944000 ************

Training agent...


********** Iteration 2945000 ************

Training agent...


********** Iteration 2946000 ************

Training agent...


********** Iteration 2947000 ************

Training agent...


********** Iteration 2948000 ************

Training agent...


********** Iteration 2949000 ************

Training agent...


********** Iteration 2950000 ************

Training agent...

Beginning logging procedure...
Timestep 2950001
mean reward (100 episodes) 1752.199951
best mean reward 1830.300049
running time 21831.366008
Train_EnvstepsSoFar : 2950001
Train_AverageReturn : 1752.199951171875
Train_BestReturn : 1830.300048828125
TimeSinceStart : 21831.36600804329
Training Loss : 0.450246661901474
Done logging...




********** Iteration 2951000 ************

Training agent...


********** Iteration 2952000 ************

Training agent...


********** Iteration 2953000 ************

Training agent...


********** Iteration 2954000 ************

Training agent...


********** Iteration 2955000 ************

Training agent...


********** Iteration 2956000 ************

Training agent...


********** Iteration 2957000 ************

Training agent...


********** Iteration 2958000 ************

Training agent...


********** Iteration 2959000 ************

Training agent...


********** Iteration 2960000 ************

Training agent...

Beginning logging procedure...
Timestep 2960001
mean reward (100 episodes) 1771.500000
best mean reward 1830.300049
running time 21903.804422
Train_EnvstepsSoFar : 2960001
Train_AverageReturn : 1771.5
Train_BestReturn : 1830.300048828125
TimeSinceStart : 21903.804421901703
Training Loss : 1.3192614316940308
Done logging...




********** Iteration 2961000 ************

Training agent...


********** Iteration 2962000 ************

Training agent...


********** Iteration 2963000 ************

Training agent...


********** Iteration 2964000 ************

Training agent...


********** Iteration 2965000 ************

Training agent...


********** Iteration 2966000 ************

Training agent...


********** Iteration 2967000 ************

Training agent...


********** Iteration 2968000 ************

Training agent...


********** Iteration 2969000 ************

Training agent...


********** Iteration 2970000 ************

Training agent...

Beginning logging procedure...
Timestep 2970001
mean reward (100 episodes) 1827.300049
best mean reward 1830.300049
running time 21975.959789
Train_EnvstepsSoFar : 2970001
Train_AverageReturn : 1827.300048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 21975.959789037704
Training Loss : 0.5313085317611694
Done logging...




********** Iteration 2971000 ************

Training agent...


********** Iteration 2972000 ************

Training agent...


********** Iteration 2973000 ************

Training agent...


********** Iteration 2974000 ************

Training agent...


********** Iteration 2975000 ************

Training agent...


********** Iteration 2976000 ************

Training agent...


********** Iteration 2977000 ************

Training agent...


********** Iteration 2978000 ************

Training agent...


********** Iteration 2979000 ************

Training agent...


********** Iteration 2980000 ************

Training agent...

Beginning logging procedure...
Timestep 2980001
mean reward (100 episodes) 1806.800049
best mean reward 1830.300049
running time 22047.150055
Train_EnvstepsSoFar : 2980001
Train_AverageReturn : 1806.800048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22047.15005517006
Training Loss : 0.31765151023864746
Done logging...




********** Iteration 2981000 ************

Training agent...


********** Iteration 2982000 ************

Training agent...


********** Iteration 2983000 ************

Training agent...


********** Iteration 2984000 ************

Training agent...


********** Iteration 2985000 ************

Training agent...


********** Iteration 2986000 ************

Training agent...


********** Iteration 2987000 ************

Training agent...


********** Iteration 2988000 ************

Training agent...


********** Iteration 2989000 ************

Training agent...


********** Iteration 2990000 ************

Training agent...

Beginning logging procedure...
Timestep 2990001
mean reward (100 episodes) 1792.300049
best mean reward 1830.300049
running time 22119.113363
Train_EnvstepsSoFar : 2990001
Train_AverageReturn : 1792.300048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22119.113362789154
Training Loss : 0.22838038206100464
Done logging...




********** Iteration 2991000 ************

Training agent...


********** Iteration 2992000 ************

Training agent...


********** Iteration 2993000 ************

Training agent...


********** Iteration 2994000 ************

Training agent...


********** Iteration 2995000 ************

Training agent...


********** Iteration 2996000 ************

Training agent...


********** Iteration 2997000 ************

Training agent...


********** Iteration 2998000 ************

Training agent...


********** Iteration 2999000 ************

Training agent...


********** Iteration 3000000 ************

Training agent...

Beginning logging procedure...
Timestep 3000001
mean reward (100 episodes) 1729.400024
best mean reward 1830.300049
running time 22192.017547
Train_EnvstepsSoFar : 3000001
Train_AverageReturn : 1729.4000244140625
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22192.017547130585
Training Loss : 0.16138450801372528
Done logging...




********** Iteration 3001000 ************

Training agent...


********** Iteration 3002000 ************

Training agent...


********** Iteration 3003000 ************

Training agent...


********** Iteration 3004000 ************

Training agent...


********** Iteration 3005000 ************

Training agent...


********** Iteration 3006000 ************

Training agent...


********** Iteration 3007000 ************

Training agent...


********** Iteration 3008000 ************

Training agent...


********** Iteration 3009000 ************

Training agent...


********** Iteration 3010000 ************

Training agent...

Beginning logging procedure...
Timestep 3010001
mean reward (100 episodes) 1770.800049
best mean reward 1830.300049
running time 22264.031629
Train_EnvstepsSoFar : 3010001
Train_AverageReturn : 1770.800048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22264.03162908554
Training Loss : 1.0312578678131104
Done logging...




********** Iteration 3011000 ************

Training agent...


********** Iteration 3012000 ************

Training agent...


********** Iteration 3013000 ************

Training agent...


********** Iteration 3014000 ************

Training agent...


********** Iteration 3015000 ************

Training agent...


********** Iteration 3016000 ************

Training agent...


********** Iteration 3017000 ************

Training agent...


********** Iteration 3018000 ************

Training agent...


********** Iteration 3019000 ************

Training agent...


********** Iteration 3020000 ************

Training agent...

Beginning logging procedure...
Timestep 3020001
mean reward (100 episodes) 1774.699951
best mean reward 1830.300049
running time 22336.432451
Train_EnvstepsSoFar : 3020001
Train_AverageReturn : 1774.699951171875
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22336.432451486588
Training Loss : 0.38945847749710083
Done logging...




********** Iteration 3021000 ************

Training agent...


********** Iteration 3022000 ************

Training agent...


********** Iteration 3023000 ************

Training agent...


********** Iteration 3024000 ************

Training agent...


********** Iteration 3025000 ************

Training agent...


********** Iteration 3026000 ************

Training agent...


********** Iteration 3027000 ************

Training agent...


********** Iteration 3028000 ************

Training agent...


********** Iteration 3029000 ************

Training agent...


********** Iteration 3030000 ************

Training agent...

Beginning logging procedure...
Timestep 3030001
mean reward (100 episodes) 1813.500000
best mean reward 1830.300049
running time 22407.940837
Train_EnvstepsSoFar : 3030001
Train_AverageReturn : 1813.5
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22407.940836906433
Training Loss : 0.3812362551689148
Done logging...




********** Iteration 3031000 ************

Training agent...


********** Iteration 3032000 ************

Training agent...


********** Iteration 3033000 ************

Training agent...


********** Iteration 3034000 ************

Training agent...


********** Iteration 3035000 ************

Training agent...


********** Iteration 3036000 ************

Training agent...


********** Iteration 3037000 ************

Training agent...


********** Iteration 3038000 ************

Training agent...


********** Iteration 3039000 ************

Training agent...


********** Iteration 3040000 ************

Training agent...

Beginning logging procedure...
Timestep 3040001
mean reward (100 episodes) 1776.199951
best mean reward 1830.300049
running time 22479.648847
Train_EnvstepsSoFar : 3040001
Train_AverageReturn : 1776.199951171875
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22479.6488468647
Training Loss : 0.6209836006164551
Done logging...




********** Iteration 3041000 ************

Training agent...


********** Iteration 3042000 ************

Training agent...


********** Iteration 3043000 ************

Training agent...


********** Iteration 3044000 ************

Training agent...


********** Iteration 3045000 ************

Training agent...


********** Iteration 3046000 ************

Training agent...


********** Iteration 3047000 ************

Training agent...


********** Iteration 3048000 ************

Training agent...


********** Iteration 3049000 ************

Training agent...


********** Iteration 3050000 ************

Training agent...

Beginning logging procedure...
Timestep 3050001
mean reward (100 episodes) 1727.099976
best mean reward 1830.300049
running time 22551.174853
Train_EnvstepsSoFar : 3050001
Train_AverageReturn : 1727.0999755859375
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22551.17485332489
Training Loss : 1.5128151178359985
Done logging...




********** Iteration 3051000 ************

Training agent...


********** Iteration 3052000 ************

Training agent...


********** Iteration 3053000 ************

Training agent...


********** Iteration 3054000 ************

Training agent...


********** Iteration 3055000 ************

Training agent...


********** Iteration 3056000 ************

Training agent...


********** Iteration 3057000 ************

Training agent...


********** Iteration 3058000 ************

Training agent...


********** Iteration 3059000 ************

Training agent...


********** Iteration 3060000 ************

Training agent...

Beginning logging procedure...
Timestep 3060001
mean reward (100 episodes) 1694.800049
best mean reward 1830.300049
running time 22623.516306
Train_EnvstepsSoFar : 3060001
Train_AverageReturn : 1694.800048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22623.516305923462
Training Loss : 1.35782790184021
Done logging...




********** Iteration 3061000 ************

Training agent...


********** Iteration 3062000 ************

Training agent...


********** Iteration 3063000 ************

Training agent...


********** Iteration 3064000 ************

Training agent...


********** Iteration 3065000 ************

Training agent...


********** Iteration 3066000 ************

Training agent...


********** Iteration 3067000 ************

Training agent...


********** Iteration 3068000 ************

Training agent...


********** Iteration 3069000 ************

Training agent...


********** Iteration 3070000 ************

Training agent...

Beginning logging procedure...
Timestep 3070001
mean reward (100 episodes) 1723.800049
best mean reward 1830.300049
running time 22695.171408
Train_EnvstepsSoFar : 3070001
Train_AverageReturn : 1723.800048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22695.171408176422
Training Loss : 0.2321302443742752
Done logging...




********** Iteration 3071000 ************

Training agent...


********** Iteration 3072000 ************

Training agent...


********** Iteration 3073000 ************

Training agent...


********** Iteration 3074000 ************

Training agent...


********** Iteration 3075000 ************

Training agent...


********** Iteration 3076000 ************

Training agent...


********** Iteration 3077000 ************

Training agent...


********** Iteration 3078000 ************

Training agent...


********** Iteration 3079000 ************

Training agent...


********** Iteration 3080000 ************

Training agent...

Beginning logging procedure...
Timestep 3080001
mean reward (100 episodes) 1749.000000
best mean reward 1830.300049
running time 22766.434113
Train_EnvstepsSoFar : 3080001
Train_AverageReturn : 1749.0
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22766.434113025665
Training Loss : 0.23163649439811707
Done logging...




********** Iteration 3081000 ************

Training agent...


********** Iteration 3082000 ************

Training agent...


********** Iteration 3083000 ************

Training agent...


********** Iteration 3084000 ************

Training agent...


********** Iteration 3085000 ************

Training agent...


********** Iteration 3086000 ************

Training agent...


********** Iteration 3087000 ************

Training agent...


********** Iteration 3088000 ************

Training agent...


********** Iteration 3089000 ************

Training agent...


********** Iteration 3090000 ************

Training agent...

Beginning logging procedure...
Timestep 3090001
mean reward (100 episodes) 1695.900024
best mean reward 1830.300049
running time 22838.431520
Train_EnvstepsSoFar : 3090001
Train_AverageReturn : 1695.9000244140625
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22838.43151950836
Training Loss : 0.34956976771354675
Done logging...




********** Iteration 3091000 ************

Training agent...


********** Iteration 3092000 ************

Training agent...


********** Iteration 3093000 ************

Training agent...


********** Iteration 3094000 ************

Training agent...


********** Iteration 3095000 ************

Training agent...


********** Iteration 3096000 ************

Training agent...


********** Iteration 3097000 ************

Training agent...


********** Iteration 3098000 ************

Training agent...


********** Iteration 3099000 ************

Training agent...


********** Iteration 3100000 ************

Training agent...

Beginning logging procedure...
Timestep 3100001
mean reward (100 episodes) 1700.199951
best mean reward 1830.300049
running time 22910.177665
Train_EnvstepsSoFar : 3100001
Train_AverageReturn : 1700.199951171875
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22910.177664995193
Training Loss : 0.15405450761318207
Done logging...




********** Iteration 3101000 ************

Training agent...


********** Iteration 3102000 ************

Training agent...


********** Iteration 3103000 ************

Training agent...


********** Iteration 3104000 ************

Training agent...


********** Iteration 3105000 ************

Training agent...


********** Iteration 3106000 ************

Training agent...


********** Iteration 3107000 ************

Training agent...


********** Iteration 3108000 ************

Training agent...


********** Iteration 3109000 ************

Training agent...


********** Iteration 3110000 ************

Training agent...

Beginning logging procedure...
Timestep 3110001
mean reward (100 episodes) 1651.099976
best mean reward 1830.300049
running time 22982.134046
Train_EnvstepsSoFar : 3110001
Train_AverageReturn : 1651.0999755859375
Train_BestReturn : 1830.300048828125
TimeSinceStart : 22982.13404560089
Training Loss : 0.5873323082923889
Done logging...




********** Iteration 3111000 ************

Training agent...


********** Iteration 3112000 ************

Training agent...


********** Iteration 3113000 ************

Training agent...


********** Iteration 3114000 ************

Training agent...


********** Iteration 3115000 ************

Training agent...


********** Iteration 3116000 ************

Training agent...


********** Iteration 3117000 ************

Training agent...


********** Iteration 3118000 ************

Training agent...


********** Iteration 3119000 ************

Training agent...


********** Iteration 3120000 ************

Training agent...

Beginning logging procedure...
Timestep 3120001
mean reward (100 episodes) 1690.000000
best mean reward 1830.300049
running time 23053.892368
Train_EnvstepsSoFar : 3120001
Train_AverageReturn : 1690.0
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23053.892367601395
Training Loss : 0.2713334560394287
Done logging...




********** Iteration 3121000 ************

Training agent...


********** Iteration 3122000 ************

Training agent...


********** Iteration 3123000 ************

Training agent...


********** Iteration 3124000 ************

Training agent...


********** Iteration 3125000 ************

Training agent...


********** Iteration 3126000 ************

Training agent...


********** Iteration 3127000 ************

Training agent...


********** Iteration 3128000 ************

Training agent...


********** Iteration 3129000 ************

Training agent...


********** Iteration 3130000 ************

Training agent...

Beginning logging procedure...
Timestep 3130001
mean reward (100 episodes) 1718.900024
best mean reward 1830.300049
running time 23125.398923
Train_EnvstepsSoFar : 3130001
Train_AverageReturn : 1718.9000244140625
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23125.39892268181
Training Loss : 0.2629489004611969
Done logging...




********** Iteration 3131000 ************

Training agent...


********** Iteration 3132000 ************

Training agent...


********** Iteration 3133000 ************

Training agent...


********** Iteration 3134000 ************

Training agent...


********** Iteration 3135000 ************

Training agent...


********** Iteration 3136000 ************

Training agent...


********** Iteration 3137000 ************

Training agent...


********** Iteration 3138000 ************

Training agent...


********** Iteration 3139000 ************

Training agent...


********** Iteration 3140000 ************

Training agent...

Beginning logging procedure...
Timestep 3140001
mean reward (100 episodes) 1673.400024
best mean reward 1830.300049
running time 23197.089049
Train_EnvstepsSoFar : 3140001
Train_AverageReturn : 1673.4000244140625
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23197.089049100876
Training Loss : 0.46648621559143066
Done logging...




********** Iteration 3141000 ************

Training agent...


********** Iteration 3142000 ************

Training agent...


********** Iteration 3143000 ************

Training agent...


********** Iteration 3144000 ************

Training agent...


********** Iteration 3145000 ************

Training agent...


********** Iteration 3146000 ************

Training agent...


********** Iteration 3147000 ************

Training agent...


********** Iteration 3148000 ************

Training agent...


********** Iteration 3149000 ************

Training agent...


********** Iteration 3150000 ************

Training agent...

Beginning logging procedure...
Timestep 3150001
mean reward (100 episodes) 1675.599976
best mean reward 1830.300049
running time 23269.140062
Train_EnvstepsSoFar : 3150001
Train_AverageReturn : 1675.5999755859375
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23269.140062093735
Training Loss : 0.17990824580192566
Done logging...




********** Iteration 3151000 ************

Training agent...


********** Iteration 3152000 ************

Training agent...


********** Iteration 3153000 ************

Training agent...


********** Iteration 3154000 ************

Training agent...


********** Iteration 3155000 ************

Training agent...


********** Iteration 3156000 ************

Training agent...


********** Iteration 3157000 ************

Training agent...


********** Iteration 3158000 ************

Training agent...


********** Iteration 3159000 ************

Training agent...


********** Iteration 3160000 ************

Training agent...

Beginning logging procedure...
Timestep 3160001
mean reward (100 episodes) 1594.099976
best mean reward 1830.300049
running time 23341.626332
Train_EnvstepsSoFar : 3160001
Train_AverageReturn : 1594.0999755859375
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23341.6263320446
Training Loss : 0.512969970703125
Done logging...




********** Iteration 3161000 ************

Training agent...


********** Iteration 3162000 ************

Training agent...


********** Iteration 3163000 ************

Training agent...


********** Iteration 3164000 ************

Training agent...


********** Iteration 3165000 ************

Training agent...


********** Iteration 3166000 ************

Training agent...


********** Iteration 3167000 ************

Training agent...


********** Iteration 3168000 ************

Training agent...


********** Iteration 3169000 ************

Training agent...


********** Iteration 3170000 ************

Training agent...

Beginning logging procedure...
Timestep 3170001
mean reward (100 episodes) 1622.199951
best mean reward 1830.300049
running time 23413.737557
Train_EnvstepsSoFar : 3170001
Train_AverageReturn : 1622.199951171875
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23413.737557172775
Training Loss : 0.26504451036453247
Done logging...




********** Iteration 3171000 ************

Training agent...


********** Iteration 3172000 ************

Training agent...


********** Iteration 3173000 ************

Training agent...


********** Iteration 3174000 ************

Training agent...


********** Iteration 3175000 ************

Training agent...


********** Iteration 3176000 ************

Training agent...


********** Iteration 3177000 ************

Training agent...


********** Iteration 3178000 ************

Training agent...


********** Iteration 3179000 ************

Training agent...


********** Iteration 3180000 ************

Training agent...

Beginning logging procedure...
Timestep 3180001
mean reward (100 episodes) 1630.400024
best mean reward 1830.300049
running time 23486.316552
Train_EnvstepsSoFar : 3180001
Train_AverageReturn : 1630.4000244140625
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23486.316551923752
Training Loss : 1.5005491971969604
Done logging...




********** Iteration 3181000 ************

Training agent...


********** Iteration 3182000 ************

Training agent...


********** Iteration 3183000 ************

Training agent...


********** Iteration 3184000 ************

Training agent...


********** Iteration 3185000 ************

Training agent...


********** Iteration 3186000 ************

Training agent...


********** Iteration 3187000 ************

Training agent...


********** Iteration 3188000 ************

Training agent...


********** Iteration 3189000 ************

Training agent...


********** Iteration 3190000 ************

Training agent...

Beginning logging procedure...
Timestep 3190001
mean reward (100 episodes) 1686.199951
best mean reward 1830.300049
running time 23558.405346
Train_EnvstepsSoFar : 3190001
Train_AverageReturn : 1686.199951171875
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23558.405345916748
Training Loss : 0.1081293597817421
Done logging...




********** Iteration 3191000 ************

Training agent...


********** Iteration 3192000 ************

Training agent...


********** Iteration 3193000 ************

Training agent...


********** Iteration 3194000 ************

Training agent...


********** Iteration 3195000 ************

Training agent...


********** Iteration 3196000 ************

Training agent...


********** Iteration 3197000 ************

Training agent...


********** Iteration 3198000 ************

Training agent...


********** Iteration 3199000 ************

Training agent...


********** Iteration 3200000 ************

Training agent...

Beginning logging procedure...
Timestep 3200001
mean reward (100 episodes) 1711.900024
best mean reward 1830.300049
running time 23629.099413
Train_EnvstepsSoFar : 3200001
Train_AverageReturn : 1711.9000244140625
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23629.09941291809
Training Loss : 0.5171652436256409
Done logging...




********** Iteration 3201000 ************

Training agent...


********** Iteration 3202000 ************

Training agent...


********** Iteration 3203000 ************

Training agent...


********** Iteration 3204000 ************

Training agent...


********** Iteration 3205000 ************

Training agent...


********** Iteration 3206000 ************

Training agent...


********** Iteration 3207000 ************

Training agent...


********** Iteration 3208000 ************

Training agent...


********** Iteration 3209000 ************

Training agent...


********** Iteration 3210000 ************

Training agent...

Beginning logging procedure...
Timestep 3210001
mean reward (100 episodes) 1714.300049
best mean reward 1830.300049
running time 23701.619566
Train_EnvstepsSoFar : 3210001
Train_AverageReturn : 1714.300048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23701.619565725327
Training Loss : 0.2675325572490692
Done logging...




********** Iteration 3211000 ************

Training agent...


********** Iteration 3212000 ************

Training agent...


********** Iteration 3213000 ************

Training agent...


********** Iteration 3214000 ************

Training agent...


********** Iteration 3215000 ************

Training agent...


********** Iteration 3216000 ************

Training agent...


********** Iteration 3217000 ************

Training agent...


********** Iteration 3218000 ************

Training agent...


********** Iteration 3219000 ************

Training agent...


********** Iteration 3220000 ************

Training agent...

Beginning logging procedure...
Timestep 3220001
mean reward (100 episodes) 1703.800049
best mean reward 1830.300049
running time 23773.473084
Train_EnvstepsSoFar : 3220001
Train_AverageReturn : 1703.800048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23773.47308421135
Training Loss : 0.2957580089569092
Done logging...




********** Iteration 3221000 ************

Training agent...


********** Iteration 3222000 ************

Training agent...


********** Iteration 3223000 ************

Training agent...


********** Iteration 3224000 ************

Training agent...


********** Iteration 3225000 ************

Training agent...


********** Iteration 3226000 ************

Training agent...


********** Iteration 3227000 ************

Training agent...


********** Iteration 3228000 ************

Training agent...


********** Iteration 3229000 ************

Training agent...


********** Iteration 3230000 ************

Training agent...

Beginning logging procedure...
Timestep 3230001
mean reward (100 episodes) 1728.699951
best mean reward 1830.300049
running time 23845.879208
Train_EnvstepsSoFar : 3230001
Train_AverageReturn : 1728.699951171875
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23845.879207611084
Training Loss : 0.2231692522764206
Done logging...




********** Iteration 3231000 ************

Training agent...


********** Iteration 3232000 ************

Training agent...


********** Iteration 3233000 ************

Training agent...


********** Iteration 3234000 ************

Training agent...


********** Iteration 3235000 ************

Training agent...


********** Iteration 3236000 ************

Training agent...


********** Iteration 3237000 ************

Training agent...


********** Iteration 3238000 ************

Training agent...


********** Iteration 3239000 ************

Training agent...


********** Iteration 3240000 ************

Training agent...

Beginning logging procedure...
Timestep 3240001
mean reward (100 episodes) 1690.099976
best mean reward 1830.300049
running time 23917.847390
Train_EnvstepsSoFar : 3240001
Train_AverageReturn : 1690.0999755859375
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23917.847390413284
Training Loss : 1.1339212656021118
Done logging...




********** Iteration 3241000 ************

Training agent...


********** Iteration 3242000 ************

Training agent...


********** Iteration 3243000 ************

Training agent...


********** Iteration 3244000 ************

Training agent...


********** Iteration 3245000 ************

Training agent...


********** Iteration 3246000 ************

Training agent...


********** Iteration 3247000 ************

Training agent...


********** Iteration 3248000 ************

Training agent...


********** Iteration 3249000 ************

Training agent...


********** Iteration 3250000 ************

Training agent...

Beginning logging procedure...
Timestep 3250001
mean reward (100 episodes) 1740.099976
best mean reward 1830.300049
running time 23988.876127
Train_EnvstepsSoFar : 3250001
Train_AverageReturn : 1740.0999755859375
Train_BestReturn : 1830.300048828125
TimeSinceStart : 23988.876126527786
Training Loss : 0.5274360775947571
Done logging...




********** Iteration 3251000 ************

Training agent...


********** Iteration 3252000 ************

Training agent...


********** Iteration 3253000 ************

Training agent...


********** Iteration 3254000 ************

Training agent...


********** Iteration 3255000 ************

Training agent...


********** Iteration 3256000 ************

Training agent...


********** Iteration 3257000 ************

Training agent...


********** Iteration 3258000 ************

Training agent...


********** Iteration 3259000 ************

Training agent...


********** Iteration 3260000 ************

Training agent...

Beginning logging procedure...
Timestep 3260001
mean reward (100 episodes) 1737.199951
best mean reward 1830.300049
running time 24060.498233
Train_EnvstepsSoFar : 3260001
Train_AverageReturn : 1737.199951171875
Train_BestReturn : 1830.300048828125
TimeSinceStart : 24060.49823284149
Training Loss : 0.5634177327156067
Done logging...




********** Iteration 3261000 ************

Training agent...


********** Iteration 3262000 ************

Training agent...


********** Iteration 3263000 ************

Training agent...


********** Iteration 3264000 ************

Training agent...


********** Iteration 3265000 ************

Training agent...


********** Iteration 3266000 ************

Training agent...


********** Iteration 3267000 ************

Training agent...


********** Iteration 3268000 ************

Training agent...


********** Iteration 3269000 ************

Training agent...


********** Iteration 3270000 ************

Training agent...

Beginning logging procedure...
Timestep 3270001
mean reward (100 episodes) 1770.699951
best mean reward 1830.300049
running time 24132.880987
Train_EnvstepsSoFar : 3270001
Train_AverageReturn : 1770.699951171875
Train_BestReturn : 1830.300048828125
TimeSinceStart : 24132.880987405777
Training Loss : 0.323076069355011
Done logging...




********** Iteration 3271000 ************

Training agent...


********** Iteration 3272000 ************

Training agent...


********** Iteration 3273000 ************

Training agent...


********** Iteration 3274000 ************

Training agent...


********** Iteration 3275000 ************

Training agent...


********** Iteration 3276000 ************

Training agent...


********** Iteration 3277000 ************

Training agent...


********** Iteration 3278000 ************

Training agent...


********** Iteration 3279000 ************

Training agent...


********** Iteration 3280000 ************

Training agent...

Beginning logging procedure...
Timestep 3280001
mean reward (100 episodes) 1735.400024
best mean reward 1830.300049
running time 24204.324382
Train_EnvstepsSoFar : 3280001
Train_AverageReturn : 1735.4000244140625
Train_BestReturn : 1830.300048828125
TimeSinceStart : 24204.324381828308
Training Loss : 0.16203638911247253
Done logging...




********** Iteration 3281000 ************

Training agent...


********** Iteration 3282000 ************

Training agent...


********** Iteration 3283000 ************

Training agent...


********** Iteration 3284000 ************

Training agent...


********** Iteration 3285000 ************

Training agent...


********** Iteration 3286000 ************

Training agent...


********** Iteration 3287000 ************

Training agent...


********** Iteration 3288000 ************

Training agent...


********** Iteration 3289000 ************

Training agent...


********** Iteration 3290000 ************

Training agent...

Beginning logging procedure...
Timestep 3290001
mean reward (100 episodes) 1720.300049
best mean reward 1830.300049
running time 24276.653496
Train_EnvstepsSoFar : 3290001
Train_AverageReturn : 1720.300048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 24276.653495550156
Training Loss : 0.3072943389415741
Done logging...




********** Iteration 3291000 ************

Training agent...


********** Iteration 3292000 ************

Training agent...


********** Iteration 3293000 ************

Training agent...


********** Iteration 3294000 ************

Training agent...


********** Iteration 3295000 ************

Training agent...


********** Iteration 3296000 ************

Training agent...


********** Iteration 3297000 ************

Training agent...


********** Iteration 3298000 ************

Training agent...


********** Iteration 3299000 ************

Training agent...


********** Iteration 3300000 ************

Training agent...

Beginning logging procedure...
Timestep 3300001
mean reward (100 episodes) 1717.500000
best mean reward 1830.300049
running time 24349.102738
Train_EnvstepsSoFar : 3300001
Train_AverageReturn : 1717.5
Train_BestReturn : 1830.300048828125
TimeSinceStart : 24349.102738142014
Training Loss : 0.6700533032417297
Done logging...




********** Iteration 3301000 ************

Training agent...


********** Iteration 3302000 ************

Training agent...


********** Iteration 3303000 ************

Training agent...


********** Iteration 3304000 ************

Training agent...


********** Iteration 3305000 ************

Training agent...


********** Iteration 3306000 ************

Training agent...


********** Iteration 3307000 ************

Training agent...


********** Iteration 3308000 ************

Training agent...


********** Iteration 3309000 ************

Training agent...


********** Iteration 3310000 ************

Training agent...

Beginning logging procedure...
Timestep 3310001
mean reward (100 episodes) 1707.900024
best mean reward 1830.300049
running time 24421.628170
Train_EnvstepsSoFar : 3310001
Train_AverageReturn : 1707.9000244140625
Train_BestReturn : 1830.300048828125
TimeSinceStart : 24421.62816953659
Training Loss : 0.2894093990325928
Done logging...




********** Iteration 3311000 ************

Training agent...


********** Iteration 3312000 ************

Training agent...


********** Iteration 3313000 ************

Training agent...


********** Iteration 3314000 ************

Training agent...


********** Iteration 3315000 ************

Training agent...


********** Iteration 3316000 ************

Training agent...


********** Iteration 3317000 ************

Training agent...


********** Iteration 3318000 ************

Training agent...


********** Iteration 3319000 ************

Training agent...


********** Iteration 3320000 ************

Training agent...

Beginning logging procedure...
Timestep 3320001
mean reward (100 episodes) 1704.000000
best mean reward 1830.300049
running time 24493.246440
Train_EnvstepsSoFar : 3320001
Train_AverageReturn : 1704.0
Train_BestReturn : 1830.300048828125
TimeSinceStart : 24493.246439933777
Training Loss : 0.2793270945549011
Done logging...




********** Iteration 3321000 ************

Training agent...


********** Iteration 3322000 ************

Training agent...


********** Iteration 3323000 ************

Training agent...


********** Iteration 3324000 ************

Training agent...


********** Iteration 3325000 ************

Training agent...


********** Iteration 3326000 ************

Training agent...


********** Iteration 3327000 ************

Training agent...


********** Iteration 3328000 ************

Training agent...


********** Iteration 3329000 ************

Training agent...


********** Iteration 3330000 ************

Training agent...

Beginning logging procedure...
Timestep 3330001
mean reward (100 episodes) 1751.500000
best mean reward 1830.300049
running time 24564.980124
Train_EnvstepsSoFar : 3330001
Train_AverageReturn : 1751.5
Train_BestReturn : 1830.300048828125
TimeSinceStart : 24564.980123519897
Training Loss : 0.3046095371246338
Done logging...




********** Iteration 3331000 ************

Training agent...


********** Iteration 3332000 ************

Training agent...


********** Iteration 3333000 ************

Training agent...


********** Iteration 3334000 ************

Training agent...


********** Iteration 3335000 ************

Training agent...


********** Iteration 3336000 ************

Training agent...


********** Iteration 3337000 ************

Training agent...


********** Iteration 3338000 ************

Training agent...


********** Iteration 3339000 ************

Training agent...


********** Iteration 3340000 ************

Training agent...

Beginning logging procedure...
Timestep 3340001
mean reward (100 episodes) 1764.199951
best mean reward 1830.300049
running time 24636.531147
Train_EnvstepsSoFar : 3340001
Train_AverageReturn : 1764.199951171875
Train_BestReturn : 1830.300048828125
TimeSinceStart : 24636.531147241592
Training Loss : 1.8392174243927002
Done logging...




********** Iteration 3341000 ************

Training agent...


********** Iteration 3342000 ************

Training agent...


********** Iteration 3343000 ************

Training agent...


********** Iteration 3344000 ************

Training agent...


********** Iteration 3345000 ************

Training agent...


********** Iteration 3346000 ************

Training agent...


********** Iteration 3347000 ************

Training agent...


********** Iteration 3348000 ************

Training agent...


********** Iteration 3349000 ************

Training agent...


********** Iteration 3350000 ************

Training agent...

Beginning logging procedure...
Timestep 3350001
mean reward (100 episodes) 1776.300049
best mean reward 1830.300049
running time 24707.842625
Train_EnvstepsSoFar : 3350001
Train_AverageReturn : 1776.300048828125
Train_BestReturn : 1830.300048828125
TimeSinceStart : 24707.842624664307
Training Loss : 0.48125141859054565
Done logging...




********** Iteration 3351000 ************

Training agent...


********** Iteration 3352000 ************

Training agent...


********** Iteration 3353000 ************

Training agent...


********** Iteration 3354000 ************

Training agent...


********** Iteration 3355000 ************

Training agent...


********** Iteration 3356000 ************

Training agent...


********** Iteration 3357000 ************

Training agent...


********** Iteration 3358000 ************

Training agent...


********** Iteration 3359000 ************

Training agent...


********** Iteration 3360000 ************

Training agent...

Beginning logging procedure...
Timestep 3360001
mean reward (100 episodes) 1830.800049
best mean reward 1830.800049
running time 24780.198588
Train_EnvstepsSoFar : 3360001
Train_AverageReturn : 1830.800048828125
Train_BestReturn : 1830.800048828125
TimeSinceStart : 24780.19858789444
Training Loss : 1.5298306941986084
Done logging...




********** Iteration 3361000 ************

Training agent...


********** Iteration 3362000 ************

Training agent...


********** Iteration 3363000 ************

Training agent...


********** Iteration 3364000 ************

Training agent...


********** Iteration 3365000 ************

Training agent...


********** Iteration 3366000 ************

Training agent...


********** Iteration 3367000 ************

Training agent...


********** Iteration 3368000 ************

Training agent...


********** Iteration 3369000 ************

Training agent...


********** Iteration 3370000 ************

Training agent...

Beginning logging procedure...
Timestep 3370001
mean reward (100 episodes) 1821.400024
best mean reward 1830.800049
running time 24851.818594
Train_EnvstepsSoFar : 3370001
Train_AverageReturn : 1821.4000244140625
Train_BestReturn : 1830.800048828125
TimeSinceStart : 24851.818593502045
Training Loss : 0.5797960758209229
Done logging...




********** Iteration 3371000 ************

Training agent...


********** Iteration 3372000 ************

Training agent...


********** Iteration 3373000 ************

Training agent...


********** Iteration 3374000 ************

Training agent...


********** Iteration 3375000 ************

Training agent...


********** Iteration 3376000 ************

Training agent...


********** Iteration 3377000 ************

Training agent...


********** Iteration 3378000 ************

Training agent...


********** Iteration 3379000 ************

Training agent...


********** Iteration 3380000 ************

Training agent...

Beginning logging procedure...
Timestep 3380001
mean reward (100 episodes) 1835.199951
best mean reward 1835.199951
running time 24922.722073
Train_EnvstepsSoFar : 3380001
Train_AverageReturn : 1835.199951171875
Train_BestReturn : 1835.199951171875
TimeSinceStart : 24922.722073316574
Training Loss : 0.23383110761642456
Done logging...




********** Iteration 3381000 ************

Training agent...


********** Iteration 3382000 ************

Training agent...


********** Iteration 3383000 ************

Training agent...


********** Iteration 3384000 ************

Training agent...


********** Iteration 3385000 ************

Training agent...


********** Iteration 3386000 ************

Training agent...


********** Iteration 3387000 ************

Training agent...


********** Iteration 3388000 ************

Training agent...


********** Iteration 3389000 ************

Training agent...


********** Iteration 3390000 ************

Training agent...

Beginning logging procedure...
Timestep 3390001
mean reward (100 episodes) 1816.599976
best mean reward 1835.199951
running time 24994.565504
Train_EnvstepsSoFar : 3390001
Train_AverageReturn : 1816.5999755859375
Train_BestReturn : 1835.199951171875
TimeSinceStart : 24994.565503835678
Training Loss : 0.5102496147155762
Done logging...




********** Iteration 3391000 ************

Training agent...


********** Iteration 3392000 ************

Training agent...


********** Iteration 3393000 ************

Training agent...


********** Iteration 3394000 ************

Training agent...


********** Iteration 3395000 ************

Training agent...


********** Iteration 3396000 ************

Training agent...


********** Iteration 3397000 ************

Training agent...


********** Iteration 3398000 ************

Training agent...


********** Iteration 3399000 ************

Training agent...


********** Iteration 3400000 ************

Training agent...

Beginning logging procedure...
Timestep 3400001
mean reward (100 episodes) 1839.099976
best mean reward 1839.099976
running time 25066.264655
Train_EnvstepsSoFar : 3400001
Train_AverageReturn : 1839.0999755859375
Train_BestReturn : 1839.0999755859375
TimeSinceStart : 25066.26465535164
Training Loss : 2.1019749641418457
Done logging...




********** Iteration 3401000 ************

Training agent...


********** Iteration 3402000 ************

Training agent...


********** Iteration 3403000 ************

Training agent...


********** Iteration 3404000 ************

Training agent...


********** Iteration 3405000 ************

Training agent...


********** Iteration 3406000 ************

Training agent...


********** Iteration 3407000 ************

Training agent...


********** Iteration 3408000 ************

Training agent...


********** Iteration 3409000 ************

Training agent...


********** Iteration 3410000 ************

Training agent...

Beginning logging procedure...
Timestep 3410001
mean reward (100 episodes) 1853.500000
best mean reward 1853.500000
running time 25138.462765
Train_EnvstepsSoFar : 3410001
Train_AverageReturn : 1853.5
Train_BestReturn : 1853.5
TimeSinceStart : 25138.46276473999
Training Loss : 0.4226343333721161
Done logging...




********** Iteration 3411000 ************

Training agent...


********** Iteration 3412000 ************

Training agent...


********** Iteration 3413000 ************

Training agent...


********** Iteration 3414000 ************

Training agent...


********** Iteration 3415000 ************

Training agent...


********** Iteration 3416000 ************

Training agent...


********** Iteration 3417000 ************

Training agent...


********** Iteration 3418000 ************

Training agent...


********** Iteration 3419000 ************

Training agent...


********** Iteration 3420000 ************

Training agent...

Beginning logging procedure...
Timestep 3420001
mean reward (100 episodes) 1672.199951
best mean reward 1853.500000
running time 25209.944429
Train_EnvstepsSoFar : 3420001
Train_AverageReturn : 1672.199951171875
Train_BestReturn : 1853.5
TimeSinceStart : 25209.944429397583
Training Loss : 0.11649393290281296
Done logging...




********** Iteration 3421000 ************

Training agent...


********** Iteration 3422000 ************

Training agent...


********** Iteration 3423000 ************

Training agent...


********** Iteration 3424000 ************

Training agent...


********** Iteration 3425000 ************

Training agent...


********** Iteration 3426000 ************

Training agent...


********** Iteration 3427000 ************

Training agent...


********** Iteration 3428000 ************

Training agent...


********** Iteration 3429000 ************

Training agent...


********** Iteration 3430000 ************

Training agent...

Beginning logging procedure...
Timestep 3430001
mean reward (100 episodes) 1715.500000
best mean reward 1853.500000
running time 25282.018819
Train_EnvstepsSoFar : 3430001
Train_AverageReturn : 1715.5
Train_BestReturn : 1853.5
TimeSinceStart : 25282.018818616867
Training Loss : 0.3094598948955536
Done logging...




********** Iteration 3431000 ************

Training agent...


********** Iteration 3432000 ************

Training agent...


********** Iteration 3433000 ************

Training agent...


********** Iteration 3434000 ************

Training agent...


********** Iteration 3435000 ************

Training agent...


********** Iteration 3436000 ************

Training agent...


********** Iteration 3437000 ************

Training agent...


********** Iteration 3438000 ************

Training agent...


********** Iteration 3439000 ************

Training agent...


********** Iteration 3440000 ************

Training agent...

Beginning logging procedure...
Timestep 3440001
mean reward (100 episodes) 1690.099976
best mean reward 1853.500000
running time 25353.846325
Train_EnvstepsSoFar : 3440001
Train_AverageReturn : 1690.0999755859375
Train_BestReturn : 1853.5
TimeSinceStart : 25353.846324920654
Training Loss : 1.4495482444763184
Done logging...




********** Iteration 3441000 ************

Training agent...


********** Iteration 3442000 ************

Training agent...


********** Iteration 3443000 ************

Training agent...


********** Iteration 3444000 ************

Training agent...


********** Iteration 3445000 ************

Training agent...


********** Iteration 3446000 ************

Training agent...


********** Iteration 3447000 ************

Training agent...


********** Iteration 3448000 ************

Training agent...


********** Iteration 3449000 ************

Training agent...


********** Iteration 3450000 ************

Training agent...

Beginning logging procedure...
Timestep 3450001
mean reward (100 episodes) 1699.599976
best mean reward 1853.500000
running time 25425.584060
Train_EnvstepsSoFar : 3450001
Train_AverageReturn : 1699.5999755859375
Train_BestReturn : 1853.5
TimeSinceStart : 25425.58405971527
Training Loss : 0.3851515054702759
Done logging...




********** Iteration 3451000 ************

Training agent...


********** Iteration 3452000 ************

Training agent...


********** Iteration 3453000 ************

Training agent...


********** Iteration 3454000 ************

Training agent...


********** Iteration 3455000 ************

Training agent...


********** Iteration 3456000 ************

Training agent...


********** Iteration 3457000 ************

Training agent...


********** Iteration 3458000 ************

Training agent...


********** Iteration 3459000 ************

Training agent...


********** Iteration 3460000 ************

Training agent...

Beginning logging procedure...
Timestep 3460001
mean reward (100 episodes) 1633.699951
best mean reward 1853.500000
running time 25497.478733
Train_EnvstepsSoFar : 3460001
Train_AverageReturn : 1633.699951171875
Train_BestReturn : 1853.5
TimeSinceStart : 25497.478732585907
Training Loss : 2.705326557159424
Done logging...




********** Iteration 3461000 ************

Training agent...


********** Iteration 3462000 ************

Training agent...


********** Iteration 3463000 ************

Training agent...


********** Iteration 3464000 ************

Training agent...


********** Iteration 3465000 ************

Training agent...


********** Iteration 3466000 ************

Training agent...


********** Iteration 3467000 ************

Training agent...


********** Iteration 3468000 ************

Training agent...


********** Iteration 3469000 ************

Training agent...


********** Iteration 3470000 ************

Training agent...

Beginning logging procedure...
Timestep 3470001
mean reward (100 episodes) 1692.900024
best mean reward 1853.500000
running time 25569.162119
Train_EnvstepsSoFar : 3470001
Train_AverageReturn : 1692.9000244140625
Train_BestReturn : 1853.5
TimeSinceStart : 25569.16211938858
Training Loss : 0.1601075828075409
Done logging...




********** Iteration 3471000 ************

Training agent...


********** Iteration 3472000 ************

Training agent...


********** Iteration 3473000 ************

Training agent...


********** Iteration 3474000 ************

Training agent...


********** Iteration 3475000 ************

Training agent...


********** Iteration 3476000 ************

Training agent...


********** Iteration 3477000 ************

Training agent...


********** Iteration 3478000 ************

Training agent...


********** Iteration 3479000 ************

Training agent...


********** Iteration 3480000 ************

Training agent...

Beginning logging procedure...
Timestep 3480001
mean reward (100 episodes) 1749.599976
best mean reward 1853.500000
running time 25640.528259
Train_EnvstepsSoFar : 3480001
Train_AverageReturn : 1749.5999755859375
Train_BestReturn : 1853.5
TimeSinceStart : 25640.528259277344
Training Loss : 0.15445607900619507
Done logging...




********** Iteration 3481000 ************

Training agent...


********** Iteration 3482000 ************

Training agent...


********** Iteration 3483000 ************

Training agent...


********** Iteration 3484000 ************

Training agent...


********** Iteration 3485000 ************

Training agent...


********** Iteration 3486000 ************

Training agent...


********** Iteration 3487000 ************

Training agent...


********** Iteration 3488000 ************

Training agent...


********** Iteration 3489000 ************

Training agent...


********** Iteration 3490000 ************

Training agent...

Beginning logging procedure...
Timestep 3490001
mean reward (100 episodes) 1729.300049
best mean reward 1853.500000
running time 25711.814540
Train_EnvstepsSoFar : 3490001
Train_AverageReturn : 1729.300048828125
Train_BestReturn : 1853.5
TimeSinceStart : 25711.814539670944
Training Loss : 0.12799961864948273
Done logging...




********** Iteration 3491000 ************

Training agent...


********** Iteration 3492000 ************

Training agent...


********** Iteration 3493000 ************

Training agent...


********** Iteration 3494000 ************

Training agent...


********** Iteration 3495000 ************

Training agent...


********** Iteration 3496000 ************

Training agent...


********** Iteration 3497000 ************

Training agent...


********** Iteration 3498000 ************

Training agent...


********** Iteration 3499000 ************

Training agent...


********** Iteration 3500000 ************

Training agent...

Beginning logging procedure...
Timestep 3500001
mean reward (100 episodes) 1791.099976
best mean reward 1853.500000
running time 25783.332792
Train_EnvstepsSoFar : 3500001
Train_AverageReturn : 1791.0999755859375
Train_BestReturn : 1853.5
TimeSinceStart : 25783.332792043686
Training Loss : 0.7404220700263977
Done logging...




********** Iteration 3501000 ************

Training agent...


********** Iteration 3502000 ************

Training agent...


********** Iteration 3503000 ************

Training agent...


********** Iteration 3504000 ************

Training agent...


********** Iteration 3505000 ************

Training agent...


********** Iteration 3506000 ************

Training agent...


********** Iteration 3507000 ************

Training agent...


********** Iteration 3508000 ************

Training agent...


********** Iteration 3509000 ************

Training agent...


********** Iteration 3510000 ************

Training agent...

Beginning logging procedure...
Timestep 3510001
mean reward (100 episodes) 1743.699951
best mean reward 1853.500000
running time 25854.587648
Train_EnvstepsSoFar : 3510001
Train_AverageReturn : 1743.699951171875
Train_BestReturn : 1853.5
TimeSinceStart : 25854.587647676468
Training Loss : 0.14344504475593567
Done logging...




********** Iteration 3511000 ************

Training agent...


********** Iteration 3512000 ************

Training agent...


********** Iteration 3513000 ************

Training agent...


********** Iteration 3514000 ************

Training agent...


********** Iteration 3515000 ************

Training agent...


********** Iteration 3516000 ************

Training agent...


********** Iteration 3517000 ************

Training agent...


********** Iteration 3518000 ************

Training agent...


********** Iteration 3519000 ************

Training agent...


********** Iteration 3520000 ************

Training agent...

Beginning logging procedure...
Timestep 3520001
mean reward (100 episodes) 1830.199951
best mean reward 1853.500000
running time 25925.987489
Train_EnvstepsSoFar : 3520001
Train_AverageReturn : 1830.199951171875
Train_BestReturn : 1853.5
TimeSinceStart : 25925.98748898506
Training Loss : 0.4474570155143738
Done logging...




********** Iteration 3521000 ************

Training agent...


********** Iteration 3522000 ************

Training agent...


********** Iteration 3523000 ************

Training agent...


********** Iteration 3524000 ************

Training agent...


********** Iteration 3525000 ************

Training agent...


********** Iteration 3526000 ************

Training agent...


********** Iteration 3527000 ************

Training agent...


********** Iteration 3528000 ************

Training agent...


********** Iteration 3529000 ************

Training agent...


********** Iteration 3530000 ************

Training agent...

Beginning logging procedure...
Timestep 3530001
mean reward (100 episodes) 1717.699951
best mean reward 1853.500000
running time 25997.115270
Train_EnvstepsSoFar : 3530001
Train_AverageReturn : 1717.699951171875
Train_BestReturn : 1853.5
TimeSinceStart : 25997.115270376205
Training Loss : 0.3389453589916229
Done logging...




********** Iteration 3531000 ************

Training agent...


********** Iteration 3532000 ************

Training agent...


********** Iteration 3533000 ************

Training agent...


********** Iteration 3534000 ************

Training agent...


********** Iteration 3535000 ************

Training agent...


********** Iteration 3536000 ************

Training agent...


********** Iteration 3537000 ************

Training agent...


********** Iteration 3538000 ************

Training agent...


********** Iteration 3539000 ************

Training agent...


********** Iteration 3540000 ************

Training agent...

Beginning logging procedure...
Timestep 3540001
mean reward (100 episodes) 1664.099976
best mean reward 1853.500000
running time 26068.202321
Train_EnvstepsSoFar : 3540001
Train_AverageReturn : 1664.0999755859375
Train_BestReturn : 1853.5
TimeSinceStart : 26068.20232129097
Training Loss : 0.17838872969150543
Done logging...




********** Iteration 3541000 ************

Training agent...


********** Iteration 3542000 ************

Training agent...


********** Iteration 3543000 ************

Training agent...


********** Iteration 3544000 ************

Training agent...


********** Iteration 3545000 ************

Training agent...


********** Iteration 3546000 ************

Training agent...


********** Iteration 3547000 ************

Training agent...


********** Iteration 3548000 ************

Training agent...


********** Iteration 3549000 ************

Training agent...


********** Iteration 3550000 ************

Training agent...

Beginning logging procedure...
Timestep 3550001
mean reward (100 episodes) 1650.099976
best mean reward 1853.500000
running time 26139.511037
Train_EnvstepsSoFar : 3550001
Train_AverageReturn : 1650.0999755859375
Train_BestReturn : 1853.5
TimeSinceStart : 26139.5110373497
Training Loss : 1.6672548055648804
Done logging...




********** Iteration 3551000 ************

Training agent...


********** Iteration 3552000 ************

Training agent...


********** Iteration 3553000 ************

Training agent...


********** Iteration 3554000 ************

Training agent...


********** Iteration 3555000 ************

Training agent...


********** Iteration 3556000 ************

Training agent...


********** Iteration 3557000 ************

Training agent...


********** Iteration 3558000 ************

Training agent...


********** Iteration 3559000 ************

Training agent...


********** Iteration 3560000 ************

Training agent...

Beginning logging procedure...
Timestep 3560001
mean reward (100 episodes) 1612.300049
best mean reward 1853.500000
running time 26211.318583
Train_EnvstepsSoFar : 3560001
Train_AverageReturn : 1612.300048828125
Train_BestReturn : 1853.5
TimeSinceStart : 26211.31858253479
Training Loss : 0.6420201063156128
Done logging...




********** Iteration 3561000 ************

Training agent...


********** Iteration 3562000 ************

Training agent...


********** Iteration 3563000 ************

Training agent...


********** Iteration 3564000 ************

Training agent...


********** Iteration 3565000 ************

Training agent...


********** Iteration 3566000 ************

Training agent...


********** Iteration 3567000 ************

Training agent...


********** Iteration 3568000 ************

Training agent...


********** Iteration 3569000 ************

Training agent...


********** Iteration 3570000 ************

Training agent...

Beginning logging procedure...
Timestep 3570001
mean reward (100 episodes) 1624.300049
best mean reward 1853.500000
running time 26282.809266
Train_EnvstepsSoFar : 3570001
Train_AverageReturn : 1624.300048828125
Train_BestReturn : 1853.5
TimeSinceStart : 26282.809265851974
Training Loss : 0.5519863367080688
Done logging...




********** Iteration 3571000 ************

Training agent...


********** Iteration 3572000 ************

Training agent...


********** Iteration 3573000 ************

Training agent...


********** Iteration 3574000 ************

Training agent...


********** Iteration 3575000 ************

Training agent...


********** Iteration 3576000 ************

Training agent...


********** Iteration 3577000 ************

Training agent...


********** Iteration 3578000 ************

Training agent...


********** Iteration 3579000 ************

Training agent...


********** Iteration 3580000 ************

Training agent...

Beginning logging procedure...
Timestep 3580001
mean reward (100 episodes) 1697.199951
best mean reward 1853.500000
running time 26354.691917
Train_EnvstepsSoFar : 3580001
Train_AverageReturn : 1697.199951171875
Train_BestReturn : 1853.5
TimeSinceStart : 26354.691916942596
Training Loss : 0.7116240859031677
Done logging...




********** Iteration 3581000 ************

Training agent...


********** Iteration 3582000 ************

Training agent...


********** Iteration 3583000 ************

Training agent...


********** Iteration 3584000 ************

Training agent...


********** Iteration 3585000 ************

Training agent...


********** Iteration 3586000 ************

Training agent...


********** Iteration 3587000 ************

Training agent...


********** Iteration 3588000 ************

Training agent...


********** Iteration 3589000 ************

Training agent...


********** Iteration 3590000 ************

Training agent...

Beginning logging procedure...
Timestep 3590001
mean reward (100 episodes) 1699.599976
best mean reward 1853.500000
running time 26426.539831
Train_EnvstepsSoFar : 3590001
Train_AverageReturn : 1699.5999755859375
Train_BestReturn : 1853.5
TimeSinceStart : 26426.53983092308
Training Loss : 0.17136059701442719
Done logging...




********** Iteration 3591000 ************

Training agent...


********** Iteration 3592000 ************

Training agent...


********** Iteration 3593000 ************

Training agent...


********** Iteration 3594000 ************

Training agent...


********** Iteration 3595000 ************

Training agent...


********** Iteration 3596000 ************

Training agent...


********** Iteration 3597000 ************

Training agent...


********** Iteration 3598000 ************

Training agent...


********** Iteration 3599000 ************

Training agent...


********** Iteration 3600000 ************

Training agent...

Beginning logging procedure...
Timestep 3600001
mean reward (100 episodes) 1713.500000
best mean reward 1853.500000
running time 26497.903299
Train_EnvstepsSoFar : 3600001
Train_AverageReturn : 1713.5
Train_BestReturn : 1853.5
TimeSinceStart : 26497.903299331665
Training Loss : 0.1816914975643158
Done logging...




********** Iteration 3601000 ************

Training agent...


********** Iteration 3602000 ************

Training agent...


********** Iteration 3603000 ************

Training agent...


********** Iteration 3604000 ************

Training agent...


********** Iteration 3605000 ************

Training agent...


********** Iteration 3606000 ************

Training agent...


********** Iteration 3607000 ************

Training agent...


********** Iteration 3608000 ************

Training agent...


********** Iteration 3609000 ************

Training agent...


********** Iteration 3610000 ************

Training agent...

Beginning logging procedure...
Timestep 3610001
mean reward (100 episodes) 1752.900024
best mean reward 1853.500000
running time 26568.913628
Train_EnvstepsSoFar : 3610001
Train_AverageReturn : 1752.9000244140625
Train_BestReturn : 1853.5
TimeSinceStart : 26568.91362786293
Training Loss : 0.42576780915260315
Done logging...




********** Iteration 3611000 ************

Training agent...


********** Iteration 3612000 ************

Training agent...


********** Iteration 3613000 ************

Training agent...


********** Iteration 3614000 ************

Training agent...


********** Iteration 3615000 ************

Training agent...


********** Iteration 3616000 ************

Training agent...


********** Iteration 3617000 ************

Training agent...


********** Iteration 3618000 ************

Training agent...


********** Iteration 3619000 ************

Training agent...


********** Iteration 3620000 ************

Training agent...

Beginning logging procedure...
Timestep 3620001
mean reward (100 episodes) 1736.599976
best mean reward 1853.500000
running time 26640.158810
Train_EnvstepsSoFar : 3620001
Train_AverageReturn : 1736.5999755859375
Train_BestReturn : 1853.5
TimeSinceStart : 26640.158809900284
Training Loss : 1.9276353120803833
Done logging...




********** Iteration 3621000 ************

Training agent...


********** Iteration 3622000 ************

Training agent...


********** Iteration 3623000 ************

Training agent...


********** Iteration 3624000 ************

Training agent...


********** Iteration 3625000 ************

Training agent...


********** Iteration 3626000 ************

Training agent...


********** Iteration 3627000 ************

Training agent...


********** Iteration 3628000 ************

Training agent...


********** Iteration 3629000 ************

Training agent...


********** Iteration 3630000 ************

Training agent...

Beginning logging procedure...
Timestep 3630001
mean reward (100 episodes) 1690.500000
best mean reward 1853.500000
running time 26711.618763
Train_EnvstepsSoFar : 3630001
Train_AverageReturn : 1690.5
Train_BestReturn : 1853.5
TimeSinceStart : 26711.618763446808
Training Loss : 0.1729913055896759
Done logging...




********** Iteration 3631000 ************

Training agent...


********** Iteration 3632000 ************

Training agent...


********** Iteration 3633000 ************

Training agent...


********** Iteration 3634000 ************

Training agent...


********** Iteration 3635000 ************

Training agent...


********** Iteration 3636000 ************

Training agent...


********** Iteration 3637000 ************

Training agent...


********** Iteration 3638000 ************

Training agent...


********** Iteration 3639000 ************

Training agent...


********** Iteration 3640000 ************

Training agent...

Beginning logging procedure...
Timestep 3640001
mean reward (100 episodes) 1727.699951
best mean reward 1853.500000
running time 26782.343822
Train_EnvstepsSoFar : 3640001
Train_AverageReturn : 1727.699951171875
Train_BestReturn : 1853.5
TimeSinceStart : 26782.343822479248
Training Loss : 0.736223578453064
Done logging...




********** Iteration 3641000 ************

Training agent...


********** Iteration 3642000 ************

Training agent...


********** Iteration 3643000 ************

Training agent...


********** Iteration 3644000 ************

Training agent...


********** Iteration 3645000 ************

Training agent...


********** Iteration 3646000 ************

Training agent...


********** Iteration 3647000 ************

Training agent...


********** Iteration 3648000 ************

Training agent...


********** Iteration 3649000 ************

Training agent...


********** Iteration 3650000 ************

Training agent...

Beginning logging procedure...
Timestep 3650001
mean reward (100 episodes) 1775.800049
best mean reward 1853.500000
running time 26853.333658
Train_EnvstepsSoFar : 3650001
Train_AverageReturn : 1775.800048828125
Train_BestReturn : 1853.5
TimeSinceStart : 26853.333657979965
Training Loss : 0.2489815354347229
Done logging...




********** Iteration 3651000 ************

Training agent...


********** Iteration 3652000 ************

Training agent...


********** Iteration 3653000 ************

Training agent...


********** Iteration 3654000 ************

Training agent...


********** Iteration 3655000 ************

Training agent...


********** Iteration 3656000 ************

Training agent...


********** Iteration 3657000 ************

Training agent...


********** Iteration 3658000 ************

Training agent...


********** Iteration 3659000 ************

Training agent...


********** Iteration 3660000 ************

Training agent...

Beginning logging procedure...
Timestep 3660001
mean reward (100 episodes) 1734.199951
best mean reward 1853.500000
running time 26924.995794
Train_EnvstepsSoFar : 3660001
Train_AverageReturn : 1734.199951171875
Train_BestReturn : 1853.5
TimeSinceStart : 26924.995794057846
Training Loss : 0.2051226943731308
Done logging...




********** Iteration 3661000 ************

Training agent...


********** Iteration 3662000 ************

Training agent...


********** Iteration 3663000 ************

Training agent...


********** Iteration 3664000 ************

Training agent...


********** Iteration 3665000 ************

Training agent...


********** Iteration 3666000 ************

Training agent...


********** Iteration 3667000 ************

Training agent...


********** Iteration 3668000 ************

Training agent...


********** Iteration 3669000 ************

Training agent...


********** Iteration 3670000 ************

Training agent...

Beginning logging procedure...
Timestep 3670001
mean reward (100 episodes) 1756.800049
best mean reward 1853.500000
running time 26996.636490
Train_EnvstepsSoFar : 3670001
Train_AverageReturn : 1756.800048828125
Train_BestReturn : 1853.5
TimeSinceStart : 26996.636489868164
Training Loss : 0.5049393177032471
Done logging...




********** Iteration 3671000 ************

Training agent...


********** Iteration 3672000 ************

Training agent...


********** Iteration 3673000 ************

Training agent...


********** Iteration 3674000 ************

Training agent...


********** Iteration 3675000 ************

Training agent...


********** Iteration 3676000 ************

Training agent...


********** Iteration 3677000 ************

Training agent...


********** Iteration 3678000 ************

Training agent...


********** Iteration 3679000 ************

Training agent...


********** Iteration 3680000 ************

Training agent...

Beginning logging procedure...
Timestep 3680001
mean reward (100 episodes) 1718.000000
best mean reward 1853.500000
running time 27067.991293
Train_EnvstepsSoFar : 3680001
Train_AverageReturn : 1718.0
Train_BestReturn : 1853.5
TimeSinceStart : 27067.99129295349
Training Loss : 0.27499401569366455
Done logging...




********** Iteration 3681000 ************

Training agent...


********** Iteration 3682000 ************

Training agent...


********** Iteration 3683000 ************

Training agent...


********** Iteration 3684000 ************

Training agent...


********** Iteration 3685000 ************

Training agent...


********** Iteration 3686000 ************

Training agent...


********** Iteration 3687000 ************

Training agent...


********** Iteration 3688000 ************

Training agent...


********** Iteration 3689000 ************

Training agent...


********** Iteration 3690000 ************

Training agent...

Beginning logging procedure...
Timestep 3690001
mean reward (100 episodes) 1690.599976
best mean reward 1853.500000
running time 27138.780283
Train_EnvstepsSoFar : 3690001
Train_AverageReturn : 1690.5999755859375
Train_BestReturn : 1853.5
TimeSinceStart : 27138.780282974243
Training Loss : 0.45432475209236145
Done logging...




********** Iteration 3691000 ************

Training agent...


********** Iteration 3692000 ************

Training agent...


********** Iteration 3693000 ************

Training agent...


********** Iteration 3694000 ************

Training agent...


********** Iteration 3695000 ************

Training agent...


********** Iteration 3696000 ************

Training agent...


********** Iteration 3697000 ************

Training agent...


********** Iteration 3698000 ************

Training agent...


********** Iteration 3699000 ************

Training agent...


********** Iteration 3700000 ************

Training agent...

Beginning logging procedure...
Timestep 3700001
mean reward (100 episodes) 1698.699951
best mean reward 1853.500000
running time 27210.256976
Train_EnvstepsSoFar : 3700001
Train_AverageReturn : 1698.699951171875
Train_BestReturn : 1853.5
TimeSinceStart : 27210.256976366043
Training Loss : 0.6284408569335938
Done logging...




********** Iteration 3701000 ************

Training agent...


********** Iteration 3702000 ************

Training agent...


********** Iteration 3703000 ************

Training agent...


********** Iteration 3704000 ************

Training agent...


********** Iteration 3705000 ************

Training agent...


********** Iteration 3706000 ************

Training agent...


********** Iteration 3707000 ************

Training agent...


********** Iteration 3708000 ************

Training agent...


********** Iteration 3709000 ************

Training agent...


********** Iteration 3710000 ************

Training agent...

Beginning logging procedure...
Timestep 3710001
mean reward (100 episodes) 1718.099976
best mean reward 1853.500000
running time 27281.314988
Train_EnvstepsSoFar : 3710001
Train_AverageReturn : 1718.0999755859375
Train_BestReturn : 1853.5
TimeSinceStart : 27281.314987897873
Training Loss : 0.5431569814682007
Done logging...




********** Iteration 3711000 ************

Training agent...


********** Iteration 3712000 ************

Training agent...


********** Iteration 3713000 ************

Training agent...


********** Iteration 3714000 ************

Training agent...


********** Iteration 3715000 ************

Training agent...


********** Iteration 3716000 ************

Training agent...


********** Iteration 3717000 ************

Training agent...


********** Iteration 3718000 ************

Training agent...


********** Iteration 3719000 ************

Training agent...


********** Iteration 3720000 ************

Training agent...

Beginning logging procedure...
Timestep 3720001
mean reward (100 episodes) 1709.500000
best mean reward 1853.500000
running time 27352.724384
Train_EnvstepsSoFar : 3720001
Train_AverageReturn : 1709.5
Train_BestReturn : 1853.5
TimeSinceStart : 27352.72438430786
Training Loss : 0.15833573043346405
Done logging...




********** Iteration 3721000 ************

Training agent...


********** Iteration 3722000 ************

Training agent...


********** Iteration 3723000 ************

Training agent...


********** Iteration 3724000 ************

Training agent...


********** Iteration 3725000 ************

Training agent...


********** Iteration 3726000 ************

Training agent...


********** Iteration 3727000 ************

Training agent...


********** Iteration 3728000 ************

Training agent...


********** Iteration 3729000 ************

Training agent...


********** Iteration 3730000 ************

Training agent...

Beginning logging procedure...
Timestep 3730001
mean reward (100 episodes) 1649.800049
best mean reward 1853.500000
running time 27424.313859
Train_EnvstepsSoFar : 3730001
Train_AverageReturn : 1649.800048828125
Train_BestReturn : 1853.5
TimeSinceStart : 27424.31385922432
Training Loss : 0.2888226807117462
Done logging...




********** Iteration 3731000 ************

Training agent...


********** Iteration 3732000 ************

Training agent...


********** Iteration 3733000 ************

Training agent...


********** Iteration 3734000 ************

Training agent...


********** Iteration 3735000 ************

Training agent...


********** Iteration 3736000 ************

Training agent...


********** Iteration 3737000 ************

Training agent...


********** Iteration 3738000 ************

Training agent...


********** Iteration 3739000 ************

Training agent...


********** Iteration 3740000 ************

Training agent...

Beginning logging procedure...
Timestep 3740001
mean reward (100 episodes) 1661.199951
best mean reward 1853.500000
running time 27495.279440
Train_EnvstepsSoFar : 3740001
Train_AverageReturn : 1661.199951171875
Train_BestReturn : 1853.5
TimeSinceStart : 27495.279439926147
Training Loss : 1.3536592721939087
Done logging...




********** Iteration 3741000 ************

Training agent...


********** Iteration 3742000 ************

Training agent...


********** Iteration 3743000 ************

Training agent...


********** Iteration 3744000 ************

Training agent...


********** Iteration 3745000 ************

Training agent...


********** Iteration 3746000 ************

Training agent...


********** Iteration 3747000 ************

Training agent...


********** Iteration 3748000 ************

Training agent...


********** Iteration 3749000 ************

Training agent...


********** Iteration 3750000 ************

Training agent...

Beginning logging procedure...
Timestep 3750001
mean reward (100 episodes) 1724.300049
best mean reward 1853.500000
running time 27566.335121
Train_EnvstepsSoFar : 3750001
Train_AverageReturn : 1724.300048828125
Train_BestReturn : 1853.5
TimeSinceStart : 27566.335120677948
Training Loss : 0.13719899952411652
Done logging...




********** Iteration 3751000 ************

Training agent...


********** Iteration 3752000 ************

Training agent...


********** Iteration 3753000 ************

Training agent...


********** Iteration 3754000 ************

Training agent...


********** Iteration 3755000 ************

Training agent...


********** Iteration 3756000 ************

Training agent...


********** Iteration 3757000 ************

Training agent...


********** Iteration 3758000 ************

Training agent...


********** Iteration 3759000 ************

Training agent...


********** Iteration 3760000 ************

Training agent...

Beginning logging procedure...
Timestep 3760001
mean reward (100 episodes) 1815.900024
best mean reward 1853.500000
running time 27636.850406
Train_EnvstepsSoFar : 3760001
Train_AverageReturn : 1815.9000244140625
Train_BestReturn : 1853.5
TimeSinceStart : 27636.850405931473
Training Loss : 0.8265101909637451
Done logging...




********** Iteration 3761000 ************

Training agent...


********** Iteration 3762000 ************

Training agent...


********** Iteration 3763000 ************

Training agent...


********** Iteration 3764000 ************

Training agent...


********** Iteration 3765000 ************

Training agent...


********** Iteration 3766000 ************

Training agent...


********** Iteration 3767000 ************

Training agent...


********** Iteration 3768000 ************

Training agent...


********** Iteration 3769000 ************

Training agent...


********** Iteration 3770000 ************

Training agent...

Beginning logging procedure...
Timestep 3770001
mean reward (100 episodes) 1874.300049
best mean reward 1874.300049
running time 27708.493188
Train_EnvstepsSoFar : 3770001
Train_AverageReturn : 1874.300048828125
Train_BestReturn : 1874.300048828125
TimeSinceStart : 27708.493187904358
Training Loss : 1.6541213989257812
Done logging...




********** Iteration 3771000 ************

Training agent...


********** Iteration 3772000 ************

Training agent...


********** Iteration 3773000 ************

Training agent...


********** Iteration 3774000 ************

Training agent...


********** Iteration 3775000 ************

Training agent...


********** Iteration 3776000 ************

Training agent...


********** Iteration 3777000 ************

Training agent...


********** Iteration 3778000 ************

Training agent...


********** Iteration 3779000 ************

Training agent...


********** Iteration 3780000 ************

Training agent...

Beginning logging procedure...
Timestep 3780001
mean reward (100 episodes) 1783.300049
best mean reward 1874.300049
running time 27779.451524
Train_EnvstepsSoFar : 3780001
Train_AverageReturn : 1783.300048828125
Train_BestReturn : 1874.300048828125
TimeSinceStart : 27779.45152449608
Training Loss : 0.3425822854042053
Done logging...




********** Iteration 3781000 ************

Training agent...


********** Iteration 3782000 ************

Training agent...


********** Iteration 3783000 ************

Training agent...


********** Iteration 3784000 ************

Training agent...


********** Iteration 3785000 ************

Training agent...


********** Iteration 3786000 ************

Training agent...


********** Iteration 3787000 ************

Training agent...


********** Iteration 3788000 ************

Training agent...


********** Iteration 3789000 ************

Training agent...


********** Iteration 3790000 ************

Training agent...

Beginning logging procedure...
Timestep 3790001
mean reward (100 episodes) 1666.599976
best mean reward 1874.300049
running time 27851.183193
Train_EnvstepsSoFar : 3790001
Train_AverageReturn : 1666.5999755859375
Train_BestReturn : 1874.300048828125
TimeSinceStart : 27851.183193445206
Training Loss : 0.47696688771247864
Done logging...




********** Iteration 3791000 ************

Training agent...


********** Iteration 3792000 ************

Training agent...


********** Iteration 3793000 ************

Training agent...


********** Iteration 3794000 ************

Training agent...


********** Iteration 3795000 ************

Training agent...


********** Iteration 3796000 ************

Training agent...


********** Iteration 3797000 ************

Training agent...


********** Iteration 3798000 ************

Training agent...


********** Iteration 3799000 ************

Training agent...


********** Iteration 3800000 ************

Training agent...

Beginning logging procedure...
Timestep 3800001
mean reward (100 episodes) 1604.800049
best mean reward 1874.300049
running time 27922.233302
Train_EnvstepsSoFar : 3800001
Train_AverageReturn : 1604.800048828125
Train_BestReturn : 1874.300048828125
TimeSinceStart : 27922.233301877975
Training Loss : 0.1974746733903885
Done logging...




********** Iteration 3801000 ************

Training agent...


********** Iteration 3802000 ************

Training agent...


********** Iteration 3803000 ************

Training agent...


********** Iteration 3804000 ************

Training agent...


********** Iteration 3805000 ************

Training agent...


********** Iteration 3806000 ************

Training agent...


********** Iteration 3807000 ************

Training agent...


********** Iteration 3808000 ************

Training agent...


********** Iteration 3809000 ************

Training agent...


********** Iteration 3810000 ************

Training agent...

Beginning logging procedure...
Timestep 3810001
mean reward (100 episodes) 1637.699951
best mean reward 1874.300049
running time 27994.008124
Train_EnvstepsSoFar : 3810001
Train_AverageReturn : 1637.699951171875
Train_BestReturn : 1874.300048828125
TimeSinceStart : 27994.008123874664
Training Loss : 0.21518252789974213
Done logging...




********** Iteration 3811000 ************

Training agent...


********** Iteration 3812000 ************

Training agent...


********** Iteration 3813000 ************

Training agent...


********** Iteration 3814000 ************

Training agent...


********** Iteration 3815000 ************

Training agent...


********** Iteration 3816000 ************

Training agent...


********** Iteration 3817000 ************

Training agent...


********** Iteration 3818000 ************

Training agent...


********** Iteration 3819000 ************

Training agent...


********** Iteration 3820000 ************

Training agent...

Beginning logging procedure...
Timestep 3820001
mean reward (100 episodes) 1791.000000
best mean reward 1874.300049
running time 28065.343089
Train_EnvstepsSoFar : 3820001
Train_AverageReturn : 1791.0
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28065.34308886528
Training Loss : 0.1225719079375267
Done logging...




********** Iteration 3821000 ************

Training agent...


********** Iteration 3822000 ************

Training agent...


********** Iteration 3823000 ************

Training agent...


********** Iteration 3824000 ************

Training agent...


********** Iteration 3825000 ************

Training agent...


********** Iteration 3826000 ************

Training agent...


********** Iteration 3827000 ************

Training agent...


********** Iteration 3828000 ************

Training agent...


********** Iteration 3829000 ************

Training agent...


********** Iteration 3830000 ************

Training agent...

Beginning logging procedure...
Timestep 3830001
mean reward (100 episodes) 1768.900024
best mean reward 1874.300049
running time 28136.274720
Train_EnvstepsSoFar : 3830001
Train_AverageReturn : 1768.9000244140625
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28136.274719953537
Training Loss : 0.10382456332445145
Done logging...




********** Iteration 3831000 ************

Training agent...


********** Iteration 3832000 ************

Training agent...


********** Iteration 3833000 ************

Training agent...


********** Iteration 3834000 ************

Training agent...


********** Iteration 3835000 ************

Training agent...


********** Iteration 3836000 ************

Training agent...


********** Iteration 3837000 ************

Training agent...


********** Iteration 3838000 ************

Training agent...


********** Iteration 3839000 ************

Training agent...


********** Iteration 3840000 ************

Training agent...

Beginning logging procedure...
Timestep 3840001
mean reward (100 episodes) 1750.900024
best mean reward 1874.300049
running time 28206.319405
Train_EnvstepsSoFar : 3840001
Train_AverageReturn : 1750.9000244140625
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28206.31940484047
Training Loss : 0.22399425506591797
Done logging...




********** Iteration 3841000 ************

Training agent...


********** Iteration 3842000 ************

Training agent...


********** Iteration 3843000 ************

Training agent...


********** Iteration 3844000 ************

Training agent...


********** Iteration 3845000 ************

Training agent...


********** Iteration 3846000 ************

Training agent...


********** Iteration 3847000 ************

Training agent...


********** Iteration 3848000 ************

Training agent...


********** Iteration 3849000 ************

Training agent...


********** Iteration 3850000 ************

Training agent...

Beginning logging procedure...
Timestep 3850001
mean reward (100 episodes) 1721.199951
best mean reward 1874.300049
running time 28277.536878
Train_EnvstepsSoFar : 3850001
Train_AverageReturn : 1721.199951171875
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28277.53687787056
Training Loss : 0.14047664403915405
Done logging...




********** Iteration 3851000 ************

Training agent...


********** Iteration 3852000 ************

Training agent...


********** Iteration 3853000 ************

Training agent...


********** Iteration 3854000 ************

Training agent...


********** Iteration 3855000 ************

Training agent...


********** Iteration 3856000 ************

Training agent...


********** Iteration 3857000 ************

Training agent...


********** Iteration 3858000 ************

Training agent...


********** Iteration 3859000 ************

Training agent...


********** Iteration 3860000 ************

Training agent...

Beginning logging procedure...
Timestep 3860001
mean reward (100 episodes) 1657.300049
best mean reward 1874.300049
running time 28347.893945
Train_EnvstepsSoFar : 3860001
Train_AverageReturn : 1657.300048828125
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28347.893945217133
Training Loss : 0.3866468369960785
Done logging...




********** Iteration 3861000 ************

Training agent...


********** Iteration 3862000 ************

Training agent...


********** Iteration 3863000 ************

Training agent...


********** Iteration 3864000 ************

Training agent...


********** Iteration 3865000 ************

Training agent...


********** Iteration 3866000 ************

Training agent...


********** Iteration 3867000 ************

Training agent...


********** Iteration 3868000 ************

Training agent...


********** Iteration 3869000 ************

Training agent...


********** Iteration 3870000 ************

Training agent...

Beginning logging procedure...
Timestep 3870001
mean reward (100 episodes) 1626.400024
best mean reward 1874.300049
running time 28419.379132
Train_EnvstepsSoFar : 3870001
Train_AverageReturn : 1626.4000244140625
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28419.379132032394
Training Loss : 1.284503698348999
Done logging...




********** Iteration 3871000 ************

Training agent...


********** Iteration 3872000 ************

Training agent...


********** Iteration 3873000 ************

Training agent...


********** Iteration 3874000 ************

Training agent...


********** Iteration 3875000 ************

Training agent...


********** Iteration 3876000 ************

Training agent...


********** Iteration 3877000 ************

Training agent...


********** Iteration 3878000 ************

Training agent...


********** Iteration 3879000 ************

Training agent...


********** Iteration 3880000 ************

Training agent...

Beginning logging procedure...
Timestep 3880001
mean reward (100 episodes) 1542.800049
best mean reward 1874.300049
running time 28490.523936
Train_EnvstepsSoFar : 3880001
Train_AverageReturn : 1542.800048828125
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28490.52393579483
Training Loss : 1.0872559547424316
Done logging...




********** Iteration 3881000 ************

Training agent...


********** Iteration 3882000 ************

Training agent...


********** Iteration 3883000 ************

Training agent...


********** Iteration 3884000 ************

Training agent...


********** Iteration 3885000 ************

Training agent...


********** Iteration 3886000 ************

Training agent...


********** Iteration 3887000 ************

Training agent...


********** Iteration 3888000 ************

Training agent...


********** Iteration 3889000 ************

Training agent...


********** Iteration 3890000 ************

Training agent...

Beginning logging procedure...
Timestep 3890001
mean reward (100 episodes) 1590.500000
best mean reward 1874.300049
running time 28561.522763
Train_EnvstepsSoFar : 3890001
Train_AverageReturn : 1590.5
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28561.52276301384
Training Loss : 0.3068648874759674
Done logging...




********** Iteration 3891000 ************

Training agent...


********** Iteration 3892000 ************

Training agent...


********** Iteration 3893000 ************

Training agent...


********** Iteration 3894000 ************

Training agent...


********** Iteration 3895000 ************

Training agent...


********** Iteration 3896000 ************

Training agent...


********** Iteration 3897000 ************

Training agent...


********** Iteration 3898000 ************

Training agent...


********** Iteration 3899000 ************

Training agent...


********** Iteration 3900000 ************

Training agent...

Beginning logging procedure...
Timestep 3900001
mean reward (100 episodes) 1650.599976
best mean reward 1874.300049
running time 28632.411228
Train_EnvstepsSoFar : 3900001
Train_AverageReturn : 1650.5999755859375
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28632.41122841835
Training Loss : 0.44748520851135254
Done logging...




********** Iteration 3901000 ************

Training agent...


********** Iteration 3902000 ************

Training agent...


********** Iteration 3903000 ************

Training agent...


********** Iteration 3904000 ************

Training agent...


********** Iteration 3905000 ************

Training agent...


********** Iteration 3906000 ************

Training agent...


********** Iteration 3907000 ************

Training agent...


********** Iteration 3908000 ************

Training agent...


********** Iteration 3909000 ************

Training agent...


********** Iteration 3910000 ************

Training agent...

Beginning logging procedure...
Timestep 3910001
mean reward (100 episodes) 1701.199951
best mean reward 1874.300049
running time 28706.794196
Train_EnvstepsSoFar : 3910001
Train_AverageReturn : 1701.199951171875
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28706.794195652008
Training Loss : 0.14124491810798645
Done logging...




********** Iteration 3911000 ************

Training agent...


********** Iteration 3912000 ************

Training agent...


********** Iteration 3913000 ************

Training agent...


********** Iteration 3914000 ************

Training agent...


********** Iteration 3915000 ************

Training agent...


********** Iteration 3916000 ************

Training agent...


********** Iteration 3917000 ************

Training agent...


********** Iteration 3918000 ************

Training agent...


********** Iteration 3919000 ************

Training agent...


********** Iteration 3920000 ************

Training agent...

Beginning logging procedure...
Timestep 3920001
mean reward (100 episodes) 1688.000000
best mean reward 1874.300049
running time 28779.472157
Train_EnvstepsSoFar : 3920001
Train_AverageReturn : 1688.0
Train_BestReturn : 1874.300048828125
TimeSinceStart : 28779.472157478333
Training Loss : 1.6570103168487549
Done logging...




********** Iteration 3921000 ************

Training agent...


********** Iteration 3922000 ************

Training agent...


********** Iteration 3923000 ************

Training agent...


********** Iteration 3924000 ************

Training agent...


********** Iteration 3925000 ************

Training agent...


********** Iteration 3926000 ************

Training agent...


********** Iteration 3927000 ************

Training agent...


********** Iteration 3928000 ************

Training agent...


********** Iteration 3929000 ************

Training agent...


********** Iteration 3930000 ************

Training agent...

Beginning logging procedure...
Timestep 3930001
mean reward (100 episodes) 1633.900024
best mean reward 1874.300049
running time 28850.790727
