


LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_supervised_lam0.1_PointmassEasy-v0_22-11-2022_20-25-48 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_supervised_lam0.1_PointmassEasy-v0_22-11-2022_20-25-48
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001846
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0018458366394042969
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 11.005952
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -50.0
TimeSinceStart : 11.00595211982727
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 22.069411
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -50.0
TimeSinceStart : 22.069410800933838
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -48.737705
best mean reward -inf
running time 50.260016
At timestep:     50 / 1000At timestep:     89 / 1000At timestep:     139 / 1000At timestep:     189 / 1000At timestep:     239 / 1000At timestep:     289 / 1000At timestep:     339 / 1000At timestep:     389 / 1000At timestep:     439 / 1000At timestep:     489 / 1000At timestep:     539 / 1000At timestep:     589 / 1000At timestep:     639 / 1000At timestep:     689 / 1000At timestep:     739 / 1000At timestep:     789 / 1000At timestep:     839 / 1000At timestep:     889 / 1000At timestep:     939 / 1000At timestep:     989 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -48.73770523071289
TimeSinceStart : 50.26001596450806
Exploration Critic Loss : 1680500.0
Exploitation Critic Loss : 0.05832605063915253
Exploration Model Loss : 112.24382781982422
Actor Loss : 1.1971311569213867
Eval_AverageReturn : -49.42856979370117
Eval_StdReturn : 2.555506706237793
Eval_MaxReturn : -38.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 49.476190476190474
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -40.408165
best mean reward -inf
running time 82.214831
At timestep:     46 / 1000At timestep:     96 / 1000At timestep:     146 / 1000At timestep:     196 / 1000At timestep:     246 / 1000At timestep:     296 / 1000At timestep:     311 / 1000At timestep:     361 / 1000At timestep:     397 / 1000At timestep:     431 / 1000At timestep:     481 / 1000At timestep:     531 / 1000At timestep:     559 / 1000At timestep:     585 / 1000At timestep:     616 / 1000At timestep:     663 / 1000At timestep:     708 / 1000At timestep:     756 / 1000At timestep:     806 / 1000At timestep:     856 / 1000At timestep:     906 / 1000At timestep:     956 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -40.408164978027344
TimeSinceStart : 82.21483063697815
Exploration Critic Loss : 18514852.0
Exploitation Critic Loss : 0.09691893309354782
Exploration Model Loss : 114.60838317871094
Actor Loss : 0.9505338668823242
Eval_AverageReturn : -43.21739196777344
Eval_StdReturn : 10.206376075744629
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 43.69565217391305
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -29.430000
best mean reward -29.430000
running time 115.812650
At timestep:     27 / 1000At timestep:     77 / 1000At timestep:     99 / 1000At timestep:     117 / 1000At timestep:     167 / 1000At timestep:     217 / 1000At timestep:     267 / 1000At timestep:     295 / 1000At timestep:     345 / 1000At timestep:     395 / 1000At timestep:     434 / 1000At timestep:     484 / 1000At timestep:     530 / 1000At timestep:     568 / 1000At timestep:     601 / 1000At timestep:     651 / 1000At timestep:     691 / 1000At timestep:     738 / 1000At timestep:     752 / 1000At timestep:     802 / 1000At timestep:     833 / 1000At timestep:     883 / 1000At timestep:     916 / 1000At timestep:     966 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -29.43000030517578
Train_BestReturn : -29.43000030517578
TimeSinceStart : 115.81265044212341
Exploration Critic Loss : 10362358.0
Exploitation Critic Loss : 0.08194288611412048
Exploration Model Loss : 102.07210540771484
Actor Loss : 0.7677150964736938
Eval_AverageReturn : -40.119998931884766
Eval_StdReturn : 11.809556007385254
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 40.64
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -21.650000
best mean reward -21.650000
running time 150.311387
At timestep:     41 / 1000At timestep:     67 / 1000At timestep:     117 / 1000At timestep:     146 / 1000At timestep:     180 / 1000At timestep:     226 / 1000At timestep:     276 / 1000At timestep:     307 / 1000At timestep:     333 / 1000At timestep:     354 / 1000At timestep:     378 / 1000At timestep:     428 / 1000At timestep:     464 / 1000At timestep:     514 / 1000At timestep:     545 / 1000At timestep:     595 / 1000At timestep:     618 / 1000At timestep:     668 / 1000At timestep:     718 / 1000At timestep:     738 / 1000At timestep:     788 / 1000At timestep:     809 / 1000At timestep:     838 / 1000At timestep:     873 / 1000At timestep:     917 / 1000At timestep:     951 / 1000At timestep:     977 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -21.649999618530273
Train_BestReturn : -21.649999618530273
TimeSinceStart : 150.31138706207275
Exploration Critic Loss : 15170705.0
Exploitation Critic Loss : 0.07716916501522064
Exploration Model Loss : 143.99261474609375
Actor Loss : 0.6741933822631836
Eval_AverageReturn : -35.96428680419922
Eval_StdReturn : 11.393864631652832
Eval_MaxReturn : -19.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 36.67857142857143
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -18.080000
best mean reward -18.080000
running time 187.496795
At timestep:     41 / 1000At timestep:     77 / 1000At timestep:     127 / 1000At timestep:     177 / 1000At timestep:     225 / 1000At timestep:     262 / 1000At timestep:     284 / 1000At timestep:     316 / 1000At timestep:     338 / 1000At timestep:     365 / 1000At timestep:     393 / 1000At timestep:     430 / 1000At timestep:     456 / 1000At timestep:     481 / 1000At timestep:     524 / 1000At timestep:     547 / 1000At timestep:     590 / 1000At timestep:     621 / 1000At timestep:     661 / 1000At timestep:     677 / 1000At timestep:     715 / 1000At timestep:     741 / 1000At timestep:     778 / 1000At timestep:     797 / 1000At timestep:     822 / 1000At timestep:     842 / 1000At timestep:     870 / 1000At timestep:     919 / 1000At timestep:     964 / 1000At timestep:     999 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -18.079999923706055
Train_BestReturn : -18.079999923706055
TimeSinceStart : 187.49679470062256
Exploration Critic Loss : 7923456.0
Exploitation Critic Loss : 0.18183165788650513
Exploration Model Loss : 56.38685989379883
Actor Loss : 0.6274076104164124
Eval_AverageReturn : -32.25806427001953
Eval_StdReturn : 9.824156761169434
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 33.193548387096776
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -16.950001
best mean reward -16.950001
running time 225.201869
At timestep:     47 / 1000At timestep:     82 / 1000At timestep:     101 / 1000At timestep:     134 / 1000At timestep:     149 / 1000At timestep:     169 / 1000At timestep:     205 / 1000At timestep:     237 / 1000At timestep:     277 / 1000At timestep:     302 / 1000At timestep:     336 / 1000At timestep:     371 / 1000At timestep:     392 / 1000At timestep:     442 / 1000At timestep:     463 / 1000At timestep:     494 / 1000At timestep:     534 / 1000At timestep:     552 / 1000At timestep:     587 / 1000At timestep:     607 / 1000At timestep:     628 / 1000At timestep:     648 / 1000At timestep:     684 / 1000At timestep:     715 / 1000At timestep:     743 / 1000At timestep:     768 / 1000At timestep:     813 / 1000At timestep:     863 / 1000At timestep:     891 / 1000At timestep:     941 / 1000At timestep:     962 / 1000At timestep:     984 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -16.950000762939453
Train_BestReturn : -16.950000762939453
TimeSinceStart : 225.20186924934387
Exploration Critic Loss : 3648139.75
Exploitation Critic Loss : 0.14708887040615082
Exploration Model Loss : 27.801055908203125
Actor Loss : 0.6346514225006104
Eval_AverageReturn : -30.030303955078125
Eval_StdReturn : 10.246905326843262
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 30.939393939393938
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -17.080000
best mean reward -16.950001
running time 263.391292
At timestep:     29 / 1000At timestep:     59 / 1000At timestep:     99 / 1000At timestep:     123 / 1000At timestep:     156 / 1000At timestep:     171 / 1000At timestep:     220 / 1000At timestep:     237 / 1000At timestep:     253 / 1000At timestep:     284 / 1000At timestep:     310 / 1000At timestep:     329 / 1000At timestep:     355 / 1000At timestep:     398 / 1000At timestep:     417 / 1000At timestep:     436 / 1000At timestep:     468 / 1000At timestep:     498 / 1000At timestep:     520 / 1000At timestep:     540 / 1000At timestep:     564 / 1000At timestep:     584 / 1000At timestep:     612 / 1000At timestep:     639 / 1000At timestep:     659 / 1000At timestep:     692 / 1000At timestep:     715 / 1000At timestep:     748 / 1000At timestep:     772 / 1000At timestep:     818 / 1000At timestep:     847 / 1000At timestep:     882 / 1000At timestep:     915 / 1000At timestep:     964 / 1000At timestep:     980 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -17.079999923706055
Train_BestReturn : -16.950000762939453
TimeSinceStart : 263.39129185676575
Exploration Critic Loss : 2591684.0
Exploitation Critic Loss : 0.12323454767465591
Exploration Model Loss : 31.293182373046875
Actor Loss : 0.5658423900604248
Eval_AverageReturn : -26.83333396911621
Eval_StdReturn : 9.013877868652344
Eval_MaxReturn : -14.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 27.833333333333332
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -17.670000
best mean reward -16.950001
running time 302.261605
At timestep:     22 / 1000At timestep:     52 / 1000At timestep:     75 / 1000At timestep:     96 / 1000At timestep:     115 / 1000At timestep:     156 / 1000At timestep:     180 / 1000At timestep:     217 / 1000At timestep:     236 / 1000At timestep:     257 / 1000At timestep:     280 / 1000At timestep:     305 / 1000At timestep:     329 / 1000At timestep:     359 / 1000At timestep:     377 / 1000At timestep:     401 / 1000At timestep:     430 / 1000At timestep:     480 / 1000At timestep:     530 / 1000At timestep:     558 / 1000At timestep:     574 / 1000At timestep:     610 / 1000At timestep:     643 / 1000At timestep:     693 / 1000At timestep:     714 / 1000At timestep:     735 / 1000At timestep:     758 / 1000At timestep:     796 / 1000At timestep:     813 / 1000At timestep:     834 / 1000At timestep:     857 / 1000At timestep:     872 / 1000At timestep:     915 / 1000At timestep:     932 / 1000At timestep:     956 / 1000At timestep:     976 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -17.670000076293945
Train_BestReturn : -16.950000762939453
TimeSinceStart : 302.2616045475006
Exploration Critic Loss : 3488483.25
Exploitation Critic Loss : 0.1082846075296402
Exploration Model Loss : 37.283935546875
Actor Loss : 0.6204441785812378
Eval_AverageReturn : -26.29729652404785
Eval_StdReturn : 9.844368934631348
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 27.216216216216218
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -17.770000
best mean reward -16.950001
running time 342.539528
At timestep:     25 / 1000At timestep:     45 / 1000At timestep:     69 / 1000At timestep:     82 / 1000At timestep:     110 / 1000At timestep:     125 / 1000At timestep:     148 / 1000At timestep:     163 / 1000At timestep:     194 / 1000At timestep:     211 / 1000At timestep:     226 / 1000At timestep:     245 / 1000At timestep:     288 / 1000At timestep:     320 / 1000At timestep:     339 / 1000At timestep:     365 / 1000At timestep:     379 / 1000At timestep:     395 / 1000At timestep:     414 / 1000At timestep:     430 / 1000At timestep:     448 / 1000At timestep:     477 / 1000At timestep:     508 / 1000At timestep:     523 / 1000At timestep:     559 / 1000At timestep:     582 / 1000At timestep:     608 / 1000At timestep:     635 / 1000At timestep:     652 / 1000At timestep:     688 / 1000At timestep:     703 / 1000At timestep:     723 / 1000At timestep:     769 / 1000At timestep:     804 / 1000At timestep:     823 / 1000At timestep:     873 / 1000At timestep:     885 / 1000At timestep:     909 / 1000At timestep:     931 / 1000At timestep:     956 / 1000At timestep:     992 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -17.770000457763672
Train_BestReturn : -16.950000762939453
TimeSinceStart : 342.5395276546478
Exploration Critic Loss : 5900722.0
Exploitation Critic Loss : 0.1438073366880417
Exploration Model Loss : 53.85322570800781
Actor Loss : 0.5492095351219177
Eval_AverageReturn : -23.190475463867188
Eval_StdReturn : 9.152772903442383
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 24.166666666666668
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -17.540001
best mean reward -16.950001
running time 382.532126
At timestep:     34 / 1000At timestep:     56 / 1000At timestep:     83 / 1000At timestep:     98 / 1000At timestep:     119 / 1000At timestep:     154 / 1000At timestep:     204 / 1000At timestep:     222 / 1000At timestep:     255 / 1000At timestep:     280 / 1000At timestep:     297 / 1000At timestep:     317 / 1000At timestep:     334 / 1000At timestep:     355 / 1000At timestep:     380 / 1000At timestep:     395 / 1000At timestep:     411 / 1000At timestep:     433 / 1000At timestep:     452 / 1000At timestep:     469 / 1000At timestep:     485 / 1000At timestep:     510 / 1000At timestep:     560 / 1000At timestep:     579 / 1000At timestep:     607 / 1000At timestep:     620 / 1000At timestep:     645 / 1000At timestep:     658 / 1000At timestep:     679 / 1000At timestep:     700 / 1000At timestep:     715 / 1000At timestep:     740 / 1000At timestep:     789 / 1000At timestep:     804 / 1000At timestep:     848 / 1000At timestep:     873 / 1000At timestep:     892 / 1000At timestep:     913 / 1000At timestep:     953 / 1000At timestep:     980 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -17.540000915527344
Train_BestReturn : -16.950000762939453
TimeSinceStart : 382.5321259498596
Exploration Critic Loss : 2267962.25
Exploitation Critic Loss : 0.14549803733825684
Exploration Model Loss : 26.669418334960938
Actor Loss : 0.5991028547286987
Eval_AverageReturn : -23.609756469726562
Eval_StdReturn : 9.994823455810547
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 24.5609756097561
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -17.700001
best mean reward -16.950001
running time 422.525202
At timestep:     18 / 1000At timestep:     48 / 1000At timestep:     84 / 1000At timestep:     111 / 1000At timestep:     135 / 1000At timestep:     183 / 1000At timestep:     206 / 1000At timestep:     240 / 1000At timestep:     258 / 1000At timestep:     277 / 1000At timestep:     310 / 1000At timestep:     345 / 1000At timestep:     365 / 1000At timestep:     383 / 1000At timestep:     410 / 1000At timestep:     428 / 1000At timestep:     448 / 1000At timestep:     469 / 1000At timestep:     498 / 1000At timestep:     524 / 1000At timestep:     542 / 1000At timestep:     561 / 1000At timestep:     582 / 1000At timestep:     626 / 1000At timestep:     676 / 1000At timestep:     694 / 1000At timestep:     735 / 1000At timestep:     777 / 1000At timestep:     805 / 1000At timestep:     825 / 1000At timestep:     858 / 1000At timestep:     878 / 1000At timestep:     900 / 1000At timestep:     920 / 1000At timestep:     943 / 1000At timestep:     967 / 1000At timestep:     989 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -17.700000762939453
Train_BestReturn : -16.950000762939453
TimeSinceStart : 422.5252015590668
Exploration Critic Loss : 2392080.25
Exploitation Critic Loss : 0.13050921261310577
Exploration Model Loss : 35.385833740234375
Actor Loss : 0.5792319774627686
Eval_AverageReturn : -25.605262756347656
Eval_StdReturn : 8.986948013305664
Eval_MaxReturn : -17.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 26.57894736842105
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -17.570000
best mean reward -16.950001
running time 461.956196
At timestep:     19 / 1000At timestep:     35 / 1000At timestep:     50 / 1000At timestep:     68 / 1000At timestep:     84 / 1000At timestep:     99 / 1000At timestep:     113 / 1000At timestep:     131 / 1000At timestep:     154 / 1000At timestep:     171 / 1000At timestep:     195 / 1000At timestep:     222 / 1000At timestep:     270 / 1000At timestep:     299 / 1000At timestep:     318 / 1000At timestep:     342 / 1000At timestep:     357 / 1000At timestep:     391 / 1000At timestep:     410 / 1000At timestep:     437 / 1000At timestep:     452 / 1000At timestep:     469 / 1000At timestep:     486 / 1000At timestep:     504 / 1000At timestep:     530 / 1000At timestep:     555 / 1000At timestep:     577 / 1000At timestep:     608 / 1000At timestep:     627 / 1000At timestep:     660 / 1000At timestep:     677 / 1000At timestep:     706 / 1000At timestep:     722 / 1000At timestep:     768 / 1000At timestep:     788 / 1000At timestep:     800 / 1000At timestep:     842 / 1000At timestep:     863 / 1000At timestep:     892 / 1000At timestep:     909 / 1000At timestep:     934 / 1000At timestep:     955 / 1000At timestep:     993 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -17.56999969482422
Train_BestReturn : -16.950000762939453
TimeSinceStart : 461.9561958312988
Exploration Critic Loss : 3599179.0
Exploitation Critic Loss : 0.1448514759540558
Exploration Model Loss : 52.88653564453125
Actor Loss : 0.6017561554908752
Eval_AverageReturn : -21.886363983154297
Eval_StdReturn : 8.53659439086914
Eval_MaxReturn : -11.0
Eval_MinReturn : -47.0
Eval_AverageEpLen : 22.886363636363637
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -17.660000
best mean reward -16.950001
running time 502.510713
At timestep:     31 / 1000At timestep:     56 / 1000At timestep:     80 / 1000At timestep:     97 / 1000At timestep:     122 / 1000At timestep:     155 / 1000At timestep:     176 / 1000At timestep:     196 / 1000At timestep:     224 / 1000At timestep:     247 / 1000At timestep:     268 / 1000At timestep:     287 / 1000At timestep:     301 / 1000At timestep:     319 / 1000At timestep:     340 / 1000At timestep:     362 / 1000At timestep:     392 / 1000At timestep:     412 / 1000At timestep:     455 / 1000At timestep:     471 / 1000At timestep:     489 / 1000At timestep:     521 / 1000At timestep:     550 / 1000At timestep:     575 / 1000At timestep:     593 / 1000At timestep:     613 / 1000At timestep:     633 / 1000At timestep:     654 / 1000At timestep:     675 / 1000At timestep:     695 / 1000At timestep:     736 / 1000At timestep:     776 / 1000At timestep:     797 / 1000At timestep:     817 / 1000At timestep:     838 / 1000At timestep:     885 / 1000At timestep:     916 / 1000At timestep:     930 / 1000At timestep:     946 / 1000At timestep:     964 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -17.65999984741211
Train_BestReturn : -16.950000762939453
TimeSinceStart : 502.51071310043335
Exploration Critic Loss : 1595546.0
Exploitation Critic Loss : 0.12679634988307953
Exploration Model Loss : 26.491958618164062
Actor Loss : 0.532463550567627
Eval_AverageReturn : -23.536584854125977
Eval_StdReturn : 8.234661102294922
Eval_MaxReturn : -13.0
Eval_MinReturn : -46.0
Eval_AverageEpLen : 24.536585365853657
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -17.559999
best mean reward -16.950001
running time 543.388314
At timestep:     30 / 1000At timestep:     59 / 1000At timestep:     78 / 1000At timestep:     92 / 1000At timestep:     116 / 1000At timestep:     135 / 1000At timestep:     167 / 1000At timestep:     193 / 1000At timestep:     217 / 1000At timestep:     249 / 1000At timestep:     276 / 1000At timestep:     296 / 1000At timestep:     314 / 1000At timestep:     351 / 1000At timestep:     372 / 1000At timestep:     404 / 1000At timestep:     447 / 1000At timestep:     468 / 1000At timestep:     491 / 1000At timestep:     518 / 1000At timestep:     542 / 1000At timestep:     564 / 1000At timestep:     587 / 1000At timestep:     605 / 1000At timestep:     630 / 1000At timestep:     647 / 1000At timestep:     674 / 1000At timestep:     698 / 1000At timestep:     720 / 1000At timestep:     739 / 1000At timestep:     754 / 1000At timestep:     789 / 1000At timestep:     802 / 1000At timestep:     816 / 1000At timestep:     844 / 1000At timestep:     888 / 1000At timestep:     927 / 1000At timestep:     947 / 1000At timestep:     969 / 1000At timestep:     986 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -17.559999465942383
Train_BestReturn : -16.950000762939453
TimeSinceStart : 543.3883137702942
Exploration Critic Loss : 1607765.5
Exploitation Critic Loss : 0.17251357436180115
Exploration Model Loss : 35.29875183105469
Actor Loss : 0.5218167901039124
Eval_AverageReturn : -23.512195587158203
Eval_StdReturn : 7.49348258972168
Eval_MaxReturn : -12.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 24.51219512195122
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -18.040001
best mean reward -16.950001
running time 583.486812
At timestep:     26 / 1000At timestep:     49 / 1000At timestep:     62 / 1000At timestep:     83 / 1000At timestep:     109 / 1000At timestep:     129 / 1000At timestep:     152 / 1000At timestep:     173 / 1000At timestep:     198 / 1000At timestep:     233 / 1000At timestep:     268 / 1000At timestep:     289 / 1000At timestep:     310 / 1000At timestep:     330 / 1000At timestep:     359 / 1000At timestep:     375 / 1000At timestep:     394 / 1000At timestep:     417 / 1000At timestep:     440 / 1000At timestep:     457 / 1000At timestep:     483 / 1000At timestep:     509 / 1000At timestep:     526 / 1000At timestep:     542 / 1000At timestep:     582 / 1000At timestep:     595 / 1000At timestep:     618 / 1000At timestep:     639 / 1000At timestep:     662 / 1000At timestep:     684 / 1000At timestep:     708 / 1000At timestep:     727 / 1000At timestep:     745 / 1000At timestep:     764 / 1000At timestep:     788 / 1000At timestep:     814 / 1000At timestep:     836 / 1000At timestep:     851 / 1000At timestep:     862 / 1000At timestep:     888 / 1000At timestep:     911 / 1000At timestep:     938 / 1000At timestep:     987 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -18.040000915527344
Train_BestReturn : -16.950000762939453
TimeSinceStart : 583.4868121147156
Exploration Critic Loss : 1770559.0
Exploitation Critic Loss : 0.08725355565547943
Exploration Model Loss : 52.842498779296875
Actor Loss : 0.6398781538009644
Eval_AverageReturn : -21.93181800842285
Eval_StdReturn : 6.837056636810303
Eval_MaxReturn : -10.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 22.931818181818183
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -18.500000
best mean reward -16.950001
running time 624.920094
At timestep:     16 / 1000At timestep:     34 / 1000At timestep:     48 / 1000At timestep:     63 / 1000At timestep:     110 / 1000At timestep:     128 / 1000At timestep:     146 / 1000At timestep:     170 / 1000At timestep:     193 / 1000At timestep:     207 / 1000At timestep:     220 / 1000At timestep:     242 / 1000At timestep:     263 / 1000At timestep:     281 / 1000At timestep:     299 / 1000At timestep:     325 / 1000At timestep:     351 / 1000At timestep:     368 / 1000At timestep:     411 / 1000At timestep:     426 / 1000At timestep:     437 / 1000At timestep:     455 / 1000At timestep:     478 / 1000At timestep:     500 / 1000At timestep:     520 / 1000At timestep:     545 / 1000At timestep:     572 / 1000At timestep:     588 / 1000At timestep:     601 / 1000At timestep:     622 / 1000At timestep:     644 / 1000At timestep:     666 / 1000At timestep:     680 / 1000At timestep:     712 / 1000At timestep:     731 / 1000At timestep:     763 / 1000At timestep:     783 / 1000At timestep:     818 / 1000At timestep:     843 / 1000At timestep:     870 / 1000At timestep:     890 / 1000At timestep:     906 / 1000At timestep:     932 / 1000At timestep:     953 / 1000At timestep:     979 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -18.5
Train_BestReturn : -16.950000762939453
TimeSinceStart : 624.9200944900513
Exploration Critic Loss : 946158.875
Exploitation Critic Loss : 0.06336686760187149
Exploration Model Loss : 26.478500366210938
Actor Loss : 0.552458643913269
Eval_AverageReturn : -20.7608699798584
Eval_StdReturn : 7.217685699462891
Eval_MaxReturn : -10.0
Eval_MinReturn : -46.0
Eval_AverageEpLen : 21.76086956521739
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.610001
best mean reward -16.950001
running time 666.769983
At timestep:     18 / 1000At timestep:     42 / 1000At timestep:     56 / 1000At timestep:     76 / 1000At timestep:     92 / 1000At timestep:     111 / 1000At timestep:     142 / 1000At timestep:     162 / 1000At timestep:     182 / 1000At timestep:     215 / 1000At timestep:     239 / 1000At timestep:     265 / 1000At timestep:     283 / 1000At timestep:     299 / 1000At timestep:     323 / 1000At timestep:     352 / 1000At timestep:     375 / 1000At timestep:     398 / 1000At timestep:     417 / 1000At timestep:     454 / 1000At timestep:     472 / 1000At timestep:     491 / 1000At timestep:     508 / 1000At timestep:     523 / 1000At timestep:     538 / 1000At timestep:     558 / 1000At timestep:     576 / 1000At timestep:     600 / 1000At timestep:     615 / 1000At timestep:     635 / 1000At timestep:     656 / 1000At timestep:     675 / 1000At timestep:     697 / 1000At timestep:     715 / 1000At timestep:     735 / 1000At timestep:     755 / 1000At timestep:     773 / 1000At timestep:     793 / 1000At timestep:     817 / 1000At timestep:     835 / 1000At timestep:     866 / 1000At timestep:     884 / 1000At timestep:     902 / 1000At timestep:     927 / 1000At timestep:     945 / 1000At timestep:     968 / 1000At timestep:     991 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.610000610351562
Train_BestReturn : -16.950000762939453
TimeSinceStart : 666.7699825763702
Exploration Critic Loss : 1115683.5
Exploitation Critic Loss : 0.11186922341585159
Exploration Model Loss : 35.291717529296875
Actor Loss : 0.5229227542877197
Eval_AverageReturn : -20.125
Eval_StdReturn : 4.80288553237915
Eval_MaxReturn : -13.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 21.125
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.379999
best mean reward -16.950001
running time 708.870629
At timestep:     29 / 1000At timestep:     62 / 1000At timestep:     80 / 1000At timestep:     107 / 1000At timestep:     126 / 1000At timestep:     156 / 1000At timestep:     176 / 1000At timestep:     193 / 1000At timestep:     219 / 1000At timestep:     243 / 1000At timestep:     257 / 1000At timestep:     274 / 1000At timestep:     293 / 1000At timestep:     327 / 1000At timestep:     352 / 1000At timestep:     375 / 1000At timestep:     393 / 1000At timestep:     412 / 1000At timestep:     425 / 1000At timestep:     445 / 1000At timestep:     473 / 1000At timestep:     492 / 1000At timestep:     506 / 1000At timestep:     530 / 1000At timestep:     560 / 1000At timestep:     576 / 1000At timestep:     595 / 1000At timestep:     610 / 1000At timestep:     648 / 1000At timestep:     677 / 1000At timestep:     698 / 1000At timestep:     722 / 1000At timestep:     755 / 1000At timestep:     776 / 1000At timestep:     814 / 1000At timestep:     847 / 1000At timestep:     864 / 1000At timestep:     886 / 1000At timestep:     900 / 1000At timestep:     917 / 1000At timestep:     933 / 1000At timestep:     956 / 1000At timestep:     975 / 1000At timestep:     990 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.3799991607666
Train_BestReturn : -16.950000762939453
TimeSinceStart : 708.8706293106079
Exploration Critic Loss : 1069408.0
Exploitation Critic Loss : 0.11255063861608505
Exploration Model Loss : 52.841522216796875
Actor Loss : 0.4678459167480469
Eval_AverageReturn : -21.311111450195312
Eval_StdReturn : 6.739015102386475
Eval_MaxReturn : -12.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 22.31111111111111
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.820000
best mean reward -16.950001
running time 750.288069
At timestep:     13 / 1000At timestep:     42 / 1000At timestep:     67 / 1000At timestep:     87 / 1000At timestep:     112 / 1000At timestep:     132 / 1000At timestep:     170 / 1000At timestep:     184 / 1000At timestep:     200 / 1000At timestep:     240 / 1000At timestep:     266 / 1000At timestep:     287 / 1000At timestep:     315 / 1000At timestep:     336 / 1000At timestep:     350 / 1000At timestep:     374 / 1000At timestep:     405 / 1000At timestep:     425 / 1000At timestep:     446 / 1000At timestep:     465 / 1000At timestep:     482 / 1000At timestep:     503 / 1000At timestep:     530 / 1000At timestep:     547 / 1000At timestep:     559 / 1000At timestep:     572 / 1000At timestep:     587 / 1000At timestep:     607 / 1000At timestep:     633 / 1000At timestep:     652 / 1000At timestep:     680 / 1000At timestep:     704 / 1000At timestep:     722 / 1000At timestep:     740 / 1000At timestep:     755 / 1000At timestep:     772 / 1000At timestep:     786 / 1000At timestep:     797 / 1000At timestep:     821 / 1000At timestep:     839 / 1000At timestep:     857 / 1000At timestep:     874 / 1000At timestep:     894 / 1000At timestep:     915 / 1000At timestep:     937 / 1000At timestep:     957 / 1000At timestep:     979 / 1000At timestep:     999 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.81999969482422
Train_BestReturn : -16.950000762939453
TimeSinceStart : 750.288069486618
Exploration Critic Loss : 1180373.5
Exploitation Critic Loss : 0.07508838921785355
Exploration Model Loss : 26.478317260742188
Actor Loss : 0.47214195132255554
Eval_AverageReturn : -19.795917510986328
Eval_StdReturn : 5.934952259063721
Eval_MaxReturn : -10.0
Eval_MinReturn : -39.0
Eval_AverageEpLen : 20.79591836734694
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.900000
best mean reward -16.950001
running time 793.025813
At timestep:     21 / 1000At timestep:     39 / 1000At timestep:     54 / 1000At timestep:     71 / 1000At timestep:     83 / 1000At timestep:     100 / 1000At timestep:     124 / 1000At timestep:     146 / 1000At timestep:     161 / 1000At timestep:     202 / 1000At timestep:     229 / 1000At timestep:     252 / 1000At timestep:     270 / 1000At timestep:     286 / 1000At timestep:     304 / 1000At timestep:     324 / 1000At timestep:     357 / 1000At timestep:     389 / 1000At timestep:     401 / 1000At timestep:     422 / 1000At timestep:     446 / 1000At timestep:     465 / 1000At timestep:     494 / 1000At timestep:     515 / 1000At timestep:     532 / 1000At timestep:     552 / 1000At timestep:     570 / 1000At timestep:     614 / 1000At timestep:     641 / 1000At timestep:     659 / 1000At timestep:     682 / 1000At timestep:     711 / 1000At timestep:     738 / 1000At timestep:     760 / 1000At timestep:     790 / 1000At timestep:     808 / 1000At timestep:     836 / 1000At timestep:     849 / 1000At timestep:     871 / 1000At timestep:     900 / 1000At timestep:     917 / 1000At timestep:     944 / 1000At timestep:     966 / 1000At timestep:     987 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.899999618530273
Train_BestReturn : -16.950000762939453
TimeSinceStart : 793.0258131027222
Exploration Critic Loss : 1144479.125
Exploitation Critic Loss : 0.12795081734657288
Exploration Model Loss : 35.291595458984375
Actor Loss : 0.5375176072120667
Eval_AverageReturn : -21.244443893432617
Eval_StdReturn : 6.857779026031494
Eval_MaxReturn : -11.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 22.244444444444444
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -17.450001
best mean reward -16.950001
running time 839.159613
At timestep:     11 / 1000At timestep:     36 / 1000At timestep:     52 / 1000At timestep:     67 / 1000At timestep:     83 / 1000At timestep:     98 / 1000At timestep:     119 / 1000At timestep:     140 / 1000At timestep:     156 / 1000At timestep:     173 / 1000At timestep:     201 / 1000At timestep:     218 / 1000At timestep:     244 / 1000At timestep:     268 / 1000At timestep:     294 / 1000At timestep:     314 / 1000At timestep:     330 / 1000At timestep:     345 / 1000At timestep:     358 / 1000At timestep:     377 / 1000At timestep:     409 / 1000At timestep:     431 / 1000At timestep:     448 / 1000At timestep:     469 / 1000At timestep:     490 / 1000At timestep:     511 / 1000At timestep:     529 / 1000At timestep:     544 / 1000At timestep:     570 / 1000At timestep:     612 / 1000At timestep:     628 / 1000At timestep:     658 / 1000At timestep:     678 / 1000At timestep:     700 / 1000At timestep:     722 / 1000At timestep:     738 / 1000At timestep:     753 / 1000At timestep:     779 / 1000At timestep:     801 / 1000At timestep:     820 / 1000At timestep:     840 / 1000At timestep:     863 / 1000At timestep:     877 / 1000At timestep:     891 / 1000At timestep:     913 / 1000At timestep:     929 / 1000At timestep:     953 / 1000At timestep:     966 / 1000At timestep:     989 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -17.450000762939453
Train_BestReturn : -16.950000762939453
TimeSinceStart : 839.1596131324768
Exploration Critic Loss : 1051459.5
Exploitation Critic Loss : 0.1256813108921051
Exploration Model Loss : 52.841644287109375
Actor Loss : 0.5186761617660522
Eval_AverageReturn : -19.1200008392334
Eval_StdReturn : 5.5951409339904785
Eval_MaxReturn : -10.0
Eval_MinReturn : -41.0
Eval_AverageEpLen : 20.12
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -17.790001
best mean reward -16.950001
running time 883.720267
At timestep:     19 / 1000At timestep:     36 / 1000At timestep:     60 / 1000At timestep:     76 / 1000At timestep:     96 / 1000At timestep:     113 / 1000At timestep:     140 / 1000At timestep:     161 / 1000At timestep:     182 / 1000At timestep:     197 / 1000At timestep:     227 / 1000At timestep:     257 / 1000At timestep:     272 / 1000At timestep:     290 / 1000At timestep:     316 / 1000At timestep:     335 / 1000At timestep:     361 / 1000At timestep:     394 / 1000At timestep:     413 / 1000At timestep:     437 / 1000At timestep:     466 / 1000At timestep:     490 / 1000At timestep:     504 / 1000At timestep:     520 / 1000At timestep:     559 / 1000At timestep:     584 / 1000At timestep:     603 / 1000At timestep:     622 / 1000At timestep:     649 / 1000At timestep:     667 / 1000At timestep:     684 / 1000At timestep:     701 / 1000At timestep:     719 / 1000At timestep:     742 / 1000At timestep:     764 / 1000At timestep:     778 / 1000At timestep:     800 / 1000At timestep:     816 / 1000At timestep:     833 / 1000At timestep:     848 / 1000At timestep:     869 / 1000At timestep:     888 / 1000At timestep:     904 / 1000At timestep:     936 / 1000At timestep:     958 / 1000At timestep:     973 / 1000At timestep:     985 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -17.790000915527344
Train_BestReturn : -16.950000762939453
TimeSinceStart : 883.7202668190002
Exploration Critic Loss : 1367612.0
Exploitation Critic Loss : 0.14397570490837097
Exploration Model Loss : 26.478195190429688
Actor Loss : 0.5446180105209351
Eval_AverageReturn : -19.83333396911621
Eval_StdReturn : 5.7312493324279785
Eval_MaxReturn : -11.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 20.833333333333332
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -18.160000
best mean reward -16.950001
running time 927.270797
At timestep:     13 / 1000At timestep:     35 / 1000At timestep:     56 / 1000At timestep:     68 / 1000At timestep:     86 / 1000At timestep:     101 / 1000At timestep:     137 / 1000At timestep:     159 / 1000At timestep:     178 / 1000At timestep:     194 / 1000At timestep:     217 / 1000At timestep:     241 / 1000At timestep:     263 / 1000At timestep:     280 / 1000At timestep:     297 / 1000At timestep:     312 / 1000At timestep:     332 / 1000At timestep:     347 / 1000At timestep:     368 / 1000At timestep:     386 / 1000At timestep:     403 / 1000At timestep:     420 / 1000At timestep:     434 / 1000At timestep:     450 / 1000At timestep:     479 / 1000At timestep:     500 / 1000At timestep:     516 / 1000At timestep:     532 / 1000At timestep:     553 / 1000At timestep:     570 / 1000At timestep:     591 / 1000At timestep:     619 / 1000At timestep:     635 / 1000At timestep:     661 / 1000At timestep:     686 / 1000At timestep:     703 / 1000At timestep:     730 / 1000At timestep:     756 / 1000At timestep:     768 / 1000At timestep:     789 / 1000At timestep:     808 / 1000At timestep:     827 / 1000At timestep:     842 / 1000At timestep:     861 / 1000At timestep:     886 / 1000At timestep:     906 / 1000At timestep:     929 / 1000At timestep:     946 / 1000At timestep:     964 / 1000At timestep:     984 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -18.15999984741211
Train_BestReturn : -16.950000762939453
TimeSinceStart : 927.270797252655
Exploration Critic Loss : 834384.6875
Exploitation Critic Loss : 0.06218625232577324
Exploration Model Loss : 35.29148864746094
Actor Loss : 0.5661760568618774
Eval_AverageReturn : -18.60784339904785
Eval_StdReturn : 4.652970790863037
Eval_MaxReturn : -11.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 19.607843137254903
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -17.770000
best mean reward -16.950001
running time 970.775584
At timestep:     18 / 1000At timestep:     40 / 1000At timestep:     56 / 1000At timestep:     79 / 1000At timestep:     94 / 1000At timestep:     112 / 1000At timestep:     126 / 1000At timestep:     143 / 1000At timestep:     167 / 1000At timestep:     181 / 1000At timestep:     201 / 1000At timestep:     220 / 1000At timestep:     243 / 1000At timestep:     259 / 1000At timestep:     284 / 1000At timestep:     303 / 1000At timestep:     323 / 1000At timestep:     361 / 1000At timestep:     387 / 1000At timestep:     406 / 1000At timestep:     427 / 1000At timestep:     449 / 1000At timestep:     465 / 1000At timestep:     480 / 1000At timestep:     501 / 1000At timestep:     519 / 1000At timestep:     534 / 1000At timestep:     550 / 1000At timestep:     573 / 1000At timestep:     597 / 1000At timestep:     626 / 1000At timestep:     641 / 1000At timestep:     661 / 1000At timestep:     682 / 1000At timestep:     702 / 1000At timestep:     725 / 1000At timestep:     741 / 1000At timestep:     758 / 1000At timestep:     769 / 1000At timestep:     785 / 1000At timestep:     805 / 1000At timestep:     826 / 1000At timestep:     838 / 1000At timestep:     853 / 1000At timestep:     880 / 1000At timestep:     894 / 1000At timestep:     920 / 1000At timestep:     942 / 1000At timestep:     968 / 1000At timestep:     988 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -17.770000457763672
Train_BestReturn : -16.950000762939453
TimeSinceStart : 970.7755839824677
Exploration Critic Loss : 1025215.375
Exploitation Critic Loss : 0.07498473674058914
Exploration Model Loss : 52.84173583984375
Actor Loss : 0.49531635642051697
Eval_AverageReturn : -18.647058486938477
Eval_StdReturn : 4.866143703460693
Eval_MaxReturn : -10.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 19.647058823529413
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -17.580000
best mean reward -16.950001
running time 1013.205975
At timestep:     20 / 1000At timestep:     44 / 1000At timestep:     60 / 1000At timestep:     73 / 1000At timestep:     89 / 1000At timestep:     121 / 1000At timestep:     155 / 1000At timestep:     172 / 1000At timestep:     186 / 1000At timestep:     202 / 1000At timestep:     219 / 1000At timestep:     247 / 1000At timestep:     267 / 1000At timestep:     290 / 1000At timestep:     306 / 1000At timestep:     324 / 1000At timestep:     338 / 1000At timestep:     357 / 1000At timestep:     383 / 1000At timestep:     417 / 1000At timestep:     438 / 1000At timestep:     457 / 1000At timestep:     485 / 1000At timestep:     505 / 1000At timestep:     522 / 1000At timestep:     537 / 1000At timestep:     560 / 1000At timestep:     581 / 1000At timestep:     601 / 1000At timestep:     625 / 1000At timestep:     644 / 1000At timestep:     660 / 1000At timestep:     678 / 1000At timestep:     700 / 1000At timestep:     727 / 1000At timestep:     748 / 1000At timestep:     768 / 1000At timestep:     783 / 1000At timestep:     803 / 1000At timestep:     819 / 1000At timestep:     840 / 1000At timestep:     864 / 1000At timestep:     889 / 1000At timestep:     902 / 1000At timestep:     920 / 1000At timestep:     941 / 1000At timestep:     961 / 1000At timestep:     989 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -17.579999923706055
Train_BestReturn : -16.950000762939453
TimeSinceStart : 1013.205974817276
Exploration Critic Loss : 778504.0625
Exploitation Critic Loss : 0.07722893357276917
Exploration Model Loss : 26.478057861328125
Actor Loss : 0.4716183841228485
Eval_AverageReturn : -19.489795684814453
Eval_StdReturn : 5.119480609893799
Eval_MaxReturn : -12.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 20.489795918367346
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -17.000000
best mean reward -16.950001
running time 1056.506362
At timestep:     21 / 1000At timestep:     35 / 1000At timestep:     53 / 1000At timestep:     74 / 1000At timestep:     91 / 1000At timestep:     106 / 1000At timestep:     124 / 1000At timestep:     139 / 1000At timestep:     157 / 1000At timestep:     182 / 1000At timestep:     206 / 1000At timestep:     221 / 1000At timestep:     243 / 1000At timestep:     259 / 1000At timestep:     275 / 1000At timestep:     292 / 1000At timestep:     310 / 1000At timestep:     334 / 1000At timestep:     352 / 1000At timestep:     371 / 1000At timestep:     393 / 1000At timestep:     412 / 1000At timestep:     436 / 1000At timestep:     459 / 1000At timestep:     481 / 1000At timestep:     497 / 1000At timestep:     516 / 1000At timestep:     547 / 1000At timestep:     561 / 1000At timestep:     577 / 1000At timestep:     599 / 1000At timestep:     617 / 1000At timestep:     635 / 1000At timestep:     659 / 1000At timestep:     683 / 1000At timestep:     700 / 1000At timestep:     737 / 1000At timestep:     759 / 1000At timestep:     793 / 1000At timestep:     812 / 1000At timestep:     833 / 1000At timestep:     847 / 1000At timestep:     865 / 1000At timestep:     877 / 1000At timestep:     899 / 1000At timestep:     920 / 1000At timestep:     941 / 1000At timestep:     959 / 1000At timestep:     984 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -17.0
Train_BestReturn : -16.950000762939453
TimeSinceStart : 1056.5063619613647
Exploration Critic Loss : 872303.9375
Exploitation Critic Loss : 0.06939682364463806
Exploration Model Loss : 35.29136657714844
Actor Loss : 0.5597177743911743
Eval_AverageReturn : -19.1200008392334
Eval_StdReturn : 4.806828498840332
Eval_MaxReturn : -11.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 20.12
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -17.450001
best mean reward -16.950001
running time 1099.877418
At timestep:     17 / 1000At timestep:     47 / 1000At timestep:     60 / 1000At timestep:     82 / 1000At timestep:     102 / 1000At timestep:     118 / 1000At timestep:     141 / 1000At timestep:     159 / 1000At timestep:     175 / 1000At timestep:     201 / 1000At timestep:     214 / 1000At timestep:     229 / 1000At timestep:     247 / 1000At timestep:     264 / 1000At timestep:     285 / 1000At timestep:     303 / 1000At timestep:     319 / 1000At timestep:     340 / 1000At timestep:     355 / 1000At timestep:     373 / 1000At timestep:     388 / 1000At timestep:     403 / 1000At timestep:     421 / 1000At timestep:     449 / 1000At timestep:     467 / 1000At timestep:     504 / 1000At timestep:     529 / 1000At timestep:     540 / 1000At timestep:     570 / 1000At timestep:     589 / 1000At timestep:     632 / 1000At timestep:     661 / 1000At timestep:     686 / 1000At timestep:     705 / 1000At timestep:     723 / 1000At timestep:     744 / 1000At timestep:     765 / 1000At timestep:     787 / 1000At timestep:     803 / 1000At timestep:     827 / 1000At timestep:     844 / 1000At timestep:     873 / 1000At timestep:     896 / 1000At timestep:     912 / 1000At timestep:     935 / 1000At timestep:     960 / 1000At timestep:     977 / 1000At timestep:     991 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -17.450000762939453
Train_BestReturn : -16.950000762939453
TimeSinceStart : 1099.8774180412292
Exploration Critic Loss : 741678.25
Exploitation Critic Loss : 0.07340484857559204
Exploration Model Loss : 52.84185791015625
Actor Loss : 0.4601447284221649
Eval_AverageReturn : -19.612245559692383
Eval_StdReturn : 6.150547981262207
Eval_MaxReturn : -10.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 20.612244897959183
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -17.350000
best mean reward -16.950001
running time 1143.341194
At timestep:     14 / 1000At timestep:     30 / 1000At timestep:     70 / 1000At timestep:     87 / 1000At timestep:     108 / 1000At timestep:     125 / 1000At timestep:     145 / 1000At timestep:     163 / 1000At timestep:     177 / 1000At timestep:     193 / 1000At timestep:     233 / 1000At timestep:     259 / 1000At timestep:     285 / 1000At timestep:     300 / 1000At timestep:     321 / 1000At timestep:     337 / 1000At timestep:     368 / 1000At timestep:     391 / 1000At timestep:     411 / 1000At timestep:     427 / 1000At timestep:     442 / 1000At timestep:     460 / 1000At timestep:     479 / 1000At timestep:     503 / 1000At timestep:     514 / 1000At timestep:     545 / 1000At timestep:     562 / 1000At timestep:     579 / 1000At timestep:     592 / 1000At timestep:     611 / 1000At timestep:     631 / 1000At timestep:     648 / 1000At timestep:     666 / 1000At timestep:     685 / 1000At timestep:     706 / 1000At timestep:     720 / 1000At timestep:     745 / 1000At timestep:     764 / 1000At timestep:     790 / 1000At timestep:     810 / 1000At timestep:     831 / 1000At timestep:     847 / 1000At timestep:     865 / 1000At timestep:     885 / 1000At timestep:     905 / 1000At timestep:     929 / 1000At timestep:     957 / 1000At timestep:     973 / 1000At timestep:     999 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -17.350000381469727
Train_BestReturn : -16.950000762939453
TimeSinceStart : 1143.341194152832
Exploration Critic Loss : 1179450.75
Exploitation Critic Loss : 0.1042836457490921
Exploration Model Loss : 26.477951049804688
Actor Loss : 0.45428889989852905
Eval_AverageReturn : -19.520000457763672
Eval_StdReturn : 6.0074615478515625
Eval_MaxReturn : -10.0
Eval_MinReturn : -39.0
Eval_AverageEpLen : 20.52
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -16.420000
best mean reward -16.420000
running time 1187.032406
At timestep:     11 / 1000At timestep:     27 / 1000At timestep:     47 / 1000At timestep:     63 / 1000At timestep:     82 / 1000At timestep:     109 / 1000At timestep:     127 / 1000At timestep:     145 / 1000At timestep:     166 / 1000At timestep:     181 / 1000At timestep:     201 / 1000At timestep:     225 / 1000At timestep:     241 / 1000At timestep:     262 / 1000At timestep:     288 / 1000At timestep:     304 / 1000At timestep:     326 / 1000At timestep:     358 / 1000At timestep:     382 / 1000At timestep:     402 / 1000At timestep:     424 / 1000At timestep:     442 / 1000At timestep:     459 / 1000At timestep:     477 / 1000At timestep:     500 / 1000At timestep:     526 / 1000At timestep:     539 / 1000At timestep:     557 / 1000At timestep:     573 / 1000At timestep:     591 / 1000At timestep:     615 / 1000At timestep:     636 / 1000At timestep:     650 / 1000At timestep:     673 / 1000At timestep:     690 / 1000At timestep:     723 / 1000At timestep:     740 / 1000At timestep:     755 / 1000At timestep:     781 / 1000At timestep:     801 / 1000At timestep:     816 / 1000At timestep:     835 / 1000At timestep:     849 / 1000At timestep:     873 / 1000At timestep:     892 / 1000At timestep:     920 / 1000At timestep:     945 / 1000At timestep:     963 / 1000At timestep:     983 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -16.420000076293945
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1187.032405614853
Exploration Critic Loss : 1241262.5
Exploitation Critic Loss : 0.08162404596805573
Exploration Model Loss : 35.291229248046875
Actor Loss : 0.5363930463790894
Eval_AverageReturn : -19.020000457763672
Eval_StdReturn : 4.62813138961792
Eval_MaxReturn : -10.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 20.02
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -16.520000
best mean reward -16.420000
running time 1230.621756
At timestep:     14 / 1000At timestep:     46 / 1000At timestep:     63 / 1000At timestep:     76 / 1000At timestep:     94 / 1000At timestep:     118 / 1000At timestep:     137 / 1000At timestep:     159 / 1000At timestep:     186 / 1000At timestep:     204 / 1000At timestep:     225 / 1000At timestep:     248 / 1000At timestep:     271 / 1000At timestep:     285 / 1000At timestep:     307 / 1000At timestep:     326 / 1000At timestep:     350 / 1000At timestep:     386 / 1000At timestep:     412 / 1000At timestep:     439 / 1000At timestep:     458 / 1000At timestep:     478 / 1000At timestep:     495 / 1000At timestep:     521 / 1000At timestep:     536 / 1000At timestep:     552 / 1000At timestep:     573 / 1000At timestep:     589 / 1000At timestep:     610 / 1000At timestep:     630 / 1000At timestep:     647 / 1000At timestep:     665 / 1000At timestep:     682 / 1000At timestep:     724 / 1000At timestep:     748 / 1000At timestep:     769 / 1000At timestep:     786 / 1000At timestep:     799 / 1000At timestep:     816 / 1000At timestep:     830 / 1000At timestep:     859 / 1000At timestep:     903 / 1000At timestep:     927 / 1000At timestep:     943 / 1000At timestep:     965 / 1000At timestep:     983 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -16.520000457763672
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1230.6217558383942
Exploration Critic Loss : 1059183.75
Exploitation Critic Loss : 0.07201528549194336
Exploration Model Loss : 52.84197998046875
Actor Loss : 0.5006673336029053
Eval_AverageReturn : -20.404254913330078
Eval_StdReturn : 6.648178577423096
Eval_MaxReturn : -12.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 21.404255319148938
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -16.830000
best mean reward -16.420000
running time 1273.383716
At timestep:     19 / 1000At timestep:     38 / 1000At timestep:     70 / 1000At timestep:     96 / 1000At timestep:     123 / 1000At timestep:     146 / 1000At timestep:     167 / 1000At timestep:     184 / 1000At timestep:     204 / 1000At timestep:     224 / 1000At timestep:     242 / 1000At timestep:     258 / 1000At timestep:     281 / 1000At timestep:     302 / 1000At timestep:     327 / 1000At timestep:     346 / 1000At timestep:     362 / 1000At timestep:     388 / 1000At timestep:     419 / 1000At timestep:     443 / 1000At timestep:     459 / 1000At timestep:     481 / 1000At timestep:     496 / 1000At timestep:     513 / 1000At timestep:     532 / 1000At timestep:     549 / 1000At timestep:     563 / 1000At timestep:     580 / 1000At timestep:     597 / 1000At timestep:     614 / 1000At timestep:     629 / 1000At timestep:     647 / 1000At timestep:     661 / 1000At timestep:     682 / 1000At timestep:     713 / 1000At timestep:     730 / 1000At timestep:     741 / 1000At timestep:     759 / 1000At timestep:     779 / 1000At timestep:     797 / 1000At timestep:     808 / 1000At timestep:     829 / 1000At timestep:     853 / 1000At timestep:     873 / 1000At timestep:     890 / 1000At timestep:     903 / 1000At timestep:     912 / 1000At timestep:     935 / 1000At timestep:     961 / 1000At timestep:     979 / 1000At timestep:     996 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -16.829999923706055
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1273.3837156295776
Exploration Critic Loss : 1154239.75
Exploitation Critic Loss : 0.07992768287658691
Exploration Model Loss : 26.477828979492188
Actor Loss : 0.4523501992225647
Eval_AverageReturn : -18.480770111083984
Eval_StdReturn : 4.877303123474121
Eval_MaxReturn : -8.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.48076923076923
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -16.870001
best mean reward -16.420000
running time 1318.643530
At timestep:     18 / 1000At timestep:     33 / 1000At timestep:     57 / 1000At timestep:     78 / 1000At timestep:     92 / 1000At timestep:     107 / 1000At timestep:     130 / 1000At timestep:     148 / 1000At timestep:     170 / 1000At timestep:     194 / 1000At timestep:     217 / 1000At timestep:     234 / 1000At timestep:     250 / 1000At timestep:     263 / 1000At timestep:     290 / 1000At timestep:     310 / 1000At timestep:     326 / 1000At timestep:     346 / 1000At timestep:     362 / 1000At timestep:     384 / 1000At timestep:     399 / 1000At timestep:     415 / 1000At timestep:     431 / 1000At timestep:     451 / 1000At timestep:     476 / 1000At timestep:     497 / 1000At timestep:     516 / 1000At timestep:     532 / 1000At timestep:     562 / 1000At timestep:     578 / 1000At timestep:     607 / 1000At timestep:     623 / 1000At timestep:     646 / 1000At timestep:     667 / 1000At timestep:     691 / 1000At timestep:     706 / 1000At timestep:     726 / 1000At timestep:     747 / 1000At timestep:     767 / 1000At timestep:     787 / 1000At timestep:     811 / 1000At timestep:     842 / 1000At timestep:     863 / 1000At timestep:     883 / 1000At timestep:     901 / 1000At timestep:     914 / 1000At timestep:     930 / 1000At timestep:     948 / 1000At timestep:     963 / 1000At timestep:     998 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -16.8700008392334
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1318.6435301303864
Exploration Critic Loss : 888129.0
Exploitation Critic Loss : 0.08917336165904999
Exploration Model Loss : 35.29112243652344
Actor Loss : 0.503015398979187
Eval_AverageReturn : -18.960784912109375
Eval_StdReturn : 4.715268611907959
Eval_MaxReturn : -12.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.96078431372549
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -17.760000
best mean reward -16.420000
running time 1361.804082
At timestep:     32 / 1000At timestep:     59 / 1000At timestep:     79 / 1000At timestep:     102 / 1000At timestep:     126 / 1000At timestep:     150 / 1000At timestep:     162 / 1000At timestep:     190 / 1000At timestep:     208 / 1000At timestep:     229 / 1000At timestep:     249 / 1000At timestep:     268 / 1000At timestep:     283 / 1000At timestep:     295 / 1000At timestep:     316 / 1000At timestep:     340 / 1000At timestep:     358 / 1000At timestep:     397 / 1000At timestep:     415 / 1000At timestep:     439 / 1000At timestep:     461 / 1000At timestep:     475 / 1000At timestep:     498 / 1000At timestep:     521 / 1000At timestep:     540 / 1000At timestep:     577 / 1000At timestep:     600 / 1000At timestep:     623 / 1000At timestep:     643 / 1000At timestep:     660 / 1000At timestep:     676 / 1000At timestep:     698 / 1000At timestep:     715 / 1000At timestep:     728 / 1000At timestep:     745 / 1000At timestep:     770 / 1000At timestep:     796 / 1000At timestep:     813 / 1000At timestep:     827 / 1000At timestep:     846 / 1000At timestep:     865 / 1000At timestep:     880 / 1000At timestep:     898 / 1000At timestep:     915 / 1000At timestep:     933 / 1000At timestep:     966 / 1000At timestep:     983 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -17.760000228881836
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1361.804081916809
Exploration Critic Loss : 1111699.875
Exploitation Critic Loss : 0.08163376152515411
Exploration Model Loss : 52.84211730957031
Actor Loss : 0.5018426775932312
Eval_AverageReturn : -20.375
Eval_StdReturn : 6.594268321990967
Eval_MaxReturn : -11.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 21.375
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -17.840000
best mean reward -16.420000
running time 1404.786106
At timestep:     17 / 1000At timestep:     44 / 1000At timestep:     63 / 1000At timestep:     88 / 1000At timestep:     112 / 1000At timestep:     127 / 1000At timestep:     148 / 1000At timestep:     172 / 1000At timestep:     196 / 1000At timestep:     212 / 1000At timestep:     229 / 1000At timestep:     245 / 1000At timestep:     265 / 1000At timestep:     288 / 1000At timestep:     304 / 1000At timestep:     318 / 1000At timestep:     335 / 1000At timestep:     358 / 1000At timestep:     375 / 1000At timestep:     397 / 1000At timestep:     425 / 1000At timestep:     447 / 1000At timestep:     459 / 1000At timestep:     477 / 1000At timestep:     496 / 1000At timestep:     516 / 1000At timestep:     539 / 1000At timestep:     558 / 1000At timestep:     578 / 1000At timestep:     594 / 1000At timestep:     613 / 1000At timestep:     627 / 1000At timestep:     647 / 1000At timestep:     666 / 1000At timestep:     685 / 1000At timestep:     712 / 1000At timestep:     739 / 1000At timestep:     756 / 1000At timestep:     773 / 1000At timestep:     802 / 1000At timestep:     815 / 1000At timestep:     837 / 1000At timestep:     853 / 1000At timestep:     875 / 1000At timestep:     894 / 1000At timestep:     909 / 1000At timestep:     936 / 1000At timestep:     958 / 1000At timestep:     971 / 1000At timestep:     993 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -17.84000015258789
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1404.7861058712006
Exploration Critic Loss : 551585.0
Exploitation Critic Loss : 0.10066372156143188
Exploration Model Loss : 26.477218627929688
Actor Loss : 0.5721678137779236
Eval_AverageReturn : -18.80392074584961
Eval_StdReturn : 4.205596923828125
Eval_MaxReturn : -11.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 19.80392156862745
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -17.540001
best mean reward -16.420000
running time 1448.134489
At timestep:     14 / 1000At timestep:     32 / 1000At timestep:     48 / 1000At timestep:     66 / 1000At timestep:     83 / 1000At timestep:     112 / 1000At timestep:     127 / 1000At timestep:     147 / 1000At timestep:     166 / 1000At timestep:     190 / 1000At timestep:     213 / 1000At timestep:     231 / 1000At timestep:     243 / 1000At timestep:     257 / 1000At timestep:     286 / 1000At timestep:     309 / 1000At timestep:     320 / 1000At timestep:     336 / 1000At timestep:     351 / 1000At timestep:     366 / 1000At timestep:     383 / 1000At timestep:     415 / 1000At timestep:     430 / 1000At timestep:     448 / 1000At timestep:     468 / 1000At timestep:     503 / 1000At timestep:     526 / 1000At timestep:     549 / 1000At timestep:     564 / 1000At timestep:     585 / 1000At timestep:     605 / 1000At timestep:     618 / 1000At timestep:     636 / 1000At timestep:     657 / 1000At timestep:     677 / 1000At timestep:     699 / 1000At timestep:     712 / 1000At timestep:     729 / 1000At timestep:     752 / 1000At timestep:     772 / 1000At timestep:     790 / 1000At timestep:     810 / 1000At timestep:     826 / 1000At timestep:     848 / 1000At timestep:     865 / 1000At timestep:     886 / 1000At timestep:     903 / 1000At timestep:     924 / 1000At timestep:     945 / 1000At timestep:     961 / 1000At timestep:     977 / 1000At timestep:     997 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -17.540000915527344
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1448.134488582611
Exploration Critic Loss : 972115.1875
Exploitation Critic Loss : 0.10338708758354187
Exploration Model Loss : 35.29046630859375
Actor Loss : 0.4610220789909363
Eval_AverageReturn : -18.320755004882812
Eval_StdReturn : 4.796907901763916
Eval_MaxReturn : -10.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.32075471698113
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -17.740000
best mean reward -16.420000
running time 1491.587757
At timestep:     22 / 1000At timestep:     47 / 1000At timestep:     62 / 1000At timestep:     84 / 1000At timestep:     99 / 1000At timestep:     112 / 1000At timestep:     131 / 1000At timestep:     148 / 1000At timestep:     160 / 1000At timestep:     175 / 1000At timestep:     189 / 1000At timestep:     212 / 1000At timestep:     232 / 1000At timestep:     259 / 1000At timestep:     274 / 1000At timestep:     295 / 1000At timestep:     317 / 1000At timestep:     330 / 1000At timestep:     345 / 1000At timestep:     363 / 1000At timestep:     387 / 1000At timestep:     404 / 1000At timestep:     425 / 1000At timestep:     440 / 1000At timestep:     458 / 1000At timestep:     471 / 1000At timestep:     486 / 1000At timestep:     499 / 1000At timestep:     516 / 1000At timestep:     545 / 1000At timestep:     569 / 1000At timestep:     582 / 1000At timestep:     602 / 1000At timestep:     618 / 1000At timestep:     645 / 1000At timestep:     665 / 1000At timestep:     679 / 1000At timestep:     698 / 1000At timestep:     716 / 1000At timestep:     729 / 1000At timestep:     740 / 1000At timestep:     759 / 1000At timestep:     774 / 1000At timestep:     799 / 1000At timestep:     816 / 1000At timestep:     838 / 1000At timestep:     863 / 1000At timestep:     892 / 1000At timestep:     907 / 1000At timestep:     925 / 1000At timestep:     939 / 1000At timestep:     968 / 1000At timestep:     991 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -17.739999771118164
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1491.5877566337585
Exploration Critic Loss : 906957.375
Exploitation Critic Loss : 0.09380603581666946
Exploration Model Loss : 52.842742919921875
Actor Loss : 0.5399953126907349
Eval_AverageReturn : -17.592592239379883
Eval_StdReturn : 4.847750663757324
Eval_MaxReturn : -10.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 18.59259259259259
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -17.270000
best mean reward -16.420000
running time 1535.514786
At timestep:     20 / 1000At timestep:     51 / 1000At timestep:     67 / 1000At timestep:     89 / 1000At timestep:     108 / 1000At timestep:     127 / 1000At timestep:     143 / 1000At timestep:     163 / 1000At timestep:     177 / 1000At timestep:     194 / 1000At timestep:     213 / 1000At timestep:     231 / 1000At timestep:     260 / 1000At timestep:     279 / 1000At timestep:     296 / 1000At timestep:     315 / 1000At timestep:     331 / 1000At timestep:     342 / 1000At timestep:     380 / 1000At timestep:     393 / 1000At timestep:     406 / 1000At timestep:     428 / 1000At timestep:     446 / 1000At timestep:     466 / 1000At timestep:     490 / 1000At timestep:     508 / 1000At timestep:     530 / 1000At timestep:     549 / 1000At timestep:     572 / 1000At timestep:     589 / 1000At timestep:     618 / 1000At timestep:     635 / 1000At timestep:     652 / 1000At timestep:     668 / 1000At timestep:     698 / 1000At timestep:     715 / 1000At timestep:     727 / 1000At timestep:     744 / 1000At timestep:     762 / 1000At timestep:     779 / 1000At timestep:     797 / 1000At timestep:     810 / 1000At timestep:     825 / 1000At timestep:     840 / 1000At timestep:     858 / 1000At timestep:     869 / 1000At timestep:     890 / 1000At timestep:     912 / 1000At timestep:     925 / 1000At timestep:     944 / 1000At timestep:     964 / 1000At timestep:     981 / 1000At timestep:     996 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -17.270000457763672
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1535.5147862434387
Exploration Critic Loss : 941618.625
Exploitation Critic Loss : 0.09660427272319794
Exploration Model Loss : 26.477081298828125
Actor Loss : 0.46250686049461365
Eval_AverageReturn : -17.685184478759766
Eval_StdReturn : 5.116509437561035
Eval_MaxReturn : -10.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 18.685185185185187
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -16.910000
best mean reward -16.420000
running time 1579.468537
At timestep:     19 / 1000At timestep:     32 / 1000At timestep:     50 / 1000At timestep:     72 / 1000At timestep:     85 / 1000At timestep:     105 / 1000At timestep:     124 / 1000At timestep:     145 / 1000At timestep:     163 / 1000At timestep:     174 / 1000At timestep:     189 / 1000At timestep:     211 / 1000At timestep:     229 / 1000At timestep:     245 / 1000At timestep:     261 / 1000At timestep:     281 / 1000At timestep:     308 / 1000At timestep:     323 / 1000At timestep:     340 / 1000At timestep:     356 / 1000At timestep:     378 / 1000At timestep:     392 / 1000At timestep:     411 / 1000At timestep:     434 / 1000At timestep:     450 / 1000At timestep:     470 / 1000At timestep:     484 / 1000At timestep:     502 / 1000At timestep:     524 / 1000At timestep:     550 / 1000At timestep:     565 / 1000At timestep:     581 / 1000At timestep:     596 / 1000At timestep:     624 / 1000At timestep:     646 / 1000At timestep:     659 / 1000At timestep:     677 / 1000At timestep:     697 / 1000At timestep:     711 / 1000At timestep:     727 / 1000At timestep:     743 / 1000At timestep:     767 / 1000At timestep:     796 / 1000At timestep:     823 / 1000At timestep:     840 / 1000At timestep:     858 / 1000At timestep:     871 / 1000At timestep:     889 / 1000At timestep:     910 / 1000At timestep:     944 / 1000At timestep:     959 / 1000At timestep:     992 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -16.90999984741211
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1579.4685370922089
Exploration Critic Loss : 632510.125
Exploitation Critic Loss : 0.10035672783851624
Exploration Model Loss : 35.29022216796875
Actor Loss : 0.4552749991416931
Eval_AverageReturn : -18.056604385375977
Eval_StdReturn : 4.988345623016357
Eval_MaxReturn : -10.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 19.056603773584907
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -16.940001
best mean reward -16.420000
running time 1624.036256
At timestep:     35 / 1000At timestep:     55 / 1000At timestep:     73 / 1000At timestep:     92 / 1000At timestep:     106 / 1000At timestep:     125 / 1000At timestep:     147 / 1000At timestep:     176 / 1000At timestep:     196 / 1000At timestep:     214 / 1000At timestep:     228 / 1000At timestep:     243 / 1000At timestep:     267 / 1000At timestep:     282 / 1000At timestep:     309 / 1000At timestep:     326 / 1000At timestep:     338 / 1000At timestep:     349 / 1000At timestep:     367 / 1000At timestep:     387 / 1000At timestep:     407 / 1000At timestep:     425 / 1000At timestep:     445 / 1000At timestep:     461 / 1000At timestep:     482 / 1000At timestep:     504 / 1000At timestep:     524 / 1000At timestep:     539 / 1000At timestep:     561 / 1000At timestep:     575 / 1000At timestep:     590 / 1000At timestep:     610 / 1000At timestep:     626 / 1000At timestep:     651 / 1000At timestep:     674 / 1000At timestep:     690 / 1000At timestep:     707 / 1000At timestep:     728 / 1000At timestep:     745 / 1000At timestep:     763 / 1000At timestep:     784 / 1000At timestep:     804 / 1000At timestep:     824 / 1000At timestep:     839 / 1000At timestep:     853 / 1000At timestep:     871 / 1000At timestep:     887 / 1000At timestep:     907 / 1000At timestep:     927 / 1000At timestep:     947 / 1000At timestep:     969 / 1000At timestep:     992 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -16.940000534057617
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1624.0362558364868
Exploration Critic Loss : 572463.75
Exploitation Critic Loss : 0.07474373281002045
Exploration Model Loss : 52.843017578125
Actor Loss : 0.5350193977355957
Eval_AverageReturn : -18.132076263427734
Eval_StdReturn : 4.184597492218018
Eval_MaxReturn : -10.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.132075471698112
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -16.910000
best mean reward -16.420000
running time 1669.351955
At timestep:     21 / 1000At timestep:     38 / 1000At timestep:     61 / 1000At timestep:     75 / 1000At timestep:     95 / 1000At timestep:     119 / 1000At timestep:     140 / 1000At timestep:     157 / 1000At timestep:     172 / 1000At timestep:     194 / 1000At timestep:     212 / 1000At timestep:     225 / 1000At timestep:     245 / 1000At timestep:     267 / 1000At timestep:     288 / 1000At timestep:     306 / 1000At timestep:     321 / 1000At timestep:     336 / 1000At timestep:     356 / 1000At timestep:     373 / 1000At timestep:     394 / 1000At timestep:     413 / 1000At timestep:     436 / 1000At timestep:     467 / 1000At timestep:     488 / 1000At timestep:     503 / 1000At timestep:     516 / 1000At timestep:     530 / 1000At timestep:     550 / 1000At timestep:     571 / 1000At timestep:     593 / 1000At timestep:     612 / 1000At timestep:     636 / 1000At timestep:     656 / 1000At timestep:     669 / 1000At timestep:     684 / 1000At timestep:     729 / 1000At timestep:     758 / 1000At timestep:     774 / 1000At timestep:     799 / 1000At timestep:     828 / 1000At timestep:     839 / 1000At timestep:     861 / 1000At timestep:     881 / 1000At timestep:     905 / 1000At timestep:     920 / 1000At timestep:     939 / 1000At timestep:     974 / 1000At timestep:     997 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -16.90999984741211
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1669.3519549369812
Exploration Critic Loss : 855516.5625
Exploitation Critic Loss : 0.11826709657907486
Exploration Model Loss : 26.476806640625
Actor Loss : 0.37753820419311523
Eval_AverageReturn : -19.280000686645508
Eval_StdReturn : 5.939831733703613
Eval_MaxReturn : -10.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 20.28
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -17.070000
best mean reward -16.420000
running time 1711.805431
At timestep:     15 / 1000At timestep:     36 / 1000At timestep:     68 / 1000At timestep:     90 / 1000At timestep:     107 / 1000At timestep:     135 / 1000At timestep:     153 / 1000At timestep:     172 / 1000At timestep:     193 / 1000At timestep:     211 / 1000At timestep:     226 / 1000At timestep:     241 / 1000At timestep:     261 / 1000At timestep:     279 / 1000At timestep:     295 / 1000At timestep:     314 / 1000At timestep:     330 / 1000At timestep:     345 / 1000At timestep:     366 / 1000At timestep:     382 / 1000At timestep:     406 / 1000At timestep:     423 / 1000At timestep:     435 / 1000At timestep:     454 / 1000At timestep:     468 / 1000At timestep:     489 / 1000At timestep:     504 / 1000At timestep:     527 / 1000At timestep:     577 / 1000At timestep:     597 / 1000At timestep:     620 / 1000At timestep:     643 / 1000At timestep:     660 / 1000At timestep:     682 / 1000At timestep:     698 / 1000At timestep:     712 / 1000At timestep:     731 / 1000At timestep:     749 / 1000At timestep:     771 / 1000At timestep:     790 / 1000At timestep:     804 / 1000At timestep:     825 / 1000At timestep:     842 / 1000At timestep:     858 / 1000At timestep:     878 / 1000At timestep:     896 / 1000At timestep:     913 / 1000At timestep:     931 / 1000At timestep:     949 / 1000At timestep:     968 / 1000At timestep:     985 / 1000At timestep:     997 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -17.06999969482422
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1711.8054313659668
Exploration Critic Loss : 974746.625
Exploitation Critic Loss : 0.08861375600099564
Exploration Model Loss : 35.29010009765625
Actor Loss : 0.3894542157649994
Eval_AverageReturn : -18.132076263427734
Eval_StdReturn : 5.720000743865967
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 19.11320754716981
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -17.420000
best mean reward -16.420000
running time 1755.668707
At timestep:     29 / 1000At timestep:     47 / 1000At timestep:     79 / 1000At timestep:     95 / 1000At timestep:     119 / 1000At timestep:     135 / 1000At timestep:     163 / 1000At timestep:     189 / 1000At timestep:     208 / 1000At timestep:     226 / 1000At timestep:     237 / 1000At timestep:     264 / 1000At timestep:     286 / 1000At timestep:     295 / 1000At timestep:     318 / 1000At timestep:     341 / 1000At timestep:     358 / 1000At timestep:     377 / 1000At timestep:     391 / 1000At timestep:     409 / 1000At timestep:     426 / 1000At timestep:     451 / 1000At timestep:     469 / 1000At timestep:     487 / 1000At timestep:     509 / 1000At timestep:     526 / 1000At timestep:     545 / 1000At timestep:     560 / 1000At timestep:     599 / 1000At timestep:     632 / 1000At timestep:     663 / 1000At timestep:     690 / 1000At timestep:     719 / 1000At timestep:     738 / 1000At timestep:     759 / 1000At timestep:     785 / 1000At timestep:     802 / 1000At timestep:     814 / 1000At timestep:     835 / 1000At timestep:     870 / 1000At timestep:     897 / 1000At timestep:     915 / 1000At timestep:     931 / 1000At timestep:     948 / 1000At timestep:     965 / 1000At timestep:     984 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -17.420000076293945
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1755.668706893921
Exploration Critic Loss : 713751.0625
Exploitation Critic Loss : 0.08519312739372253
Exploration Model Loss : 52.8431396484375
Actor Loss : 0.5588160753250122
Eval_AverageReturn : -20.36170196533203
Eval_StdReturn : 6.3662190437316895
Eval_MaxReturn : -8.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 21.361702127659573
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -17.650000
best mean reward -16.420000
running time 1798.523458
At timestep:     27 / 1000At timestep:     46 / 1000At timestep:     66 / 1000At timestep:     85 / 1000At timestep:     102 / 1000At timestep:     121 / 1000At timestep:     139 / 1000At timestep:     163 / 1000At timestep:     178 / 1000At timestep:     190 / 1000At timestep:     218 / 1000At timestep:     243 / 1000At timestep:     259 / 1000At timestep:     277 / 1000At timestep:     297 / 1000At timestep:     314 / 1000At timestep:     329 / 1000At timestep:     344 / 1000At timestep:     364 / 1000At timestep:     379 / 1000At timestep:     404 / 1000At timestep:     422 / 1000At timestep:     442 / 1000At timestep:     463 / 1000At timestep:     482 / 1000At timestep:     500 / 1000At timestep:     523 / 1000At timestep:     540 / 1000At timestep:     559 / 1000At timestep:     575 / 1000At timestep:     590 / 1000At timestep:     619 / 1000At timestep:     641 / 1000At timestep:     656 / 1000At timestep:     681 / 1000At timestep:     706 / 1000At timestep:     731 / 1000At timestep:     760 / 1000At timestep:     786 / 1000At timestep:     803 / 1000At timestep:     818 / 1000At timestep:     834 / 1000At timestep:     866 / 1000At timestep:     895 / 1000At timestep:     913 / 1000At timestep:     934 / 1000At timestep:     955 / 1000At timestep:     968 / 1000At timestep:     988 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -17.649999618530273
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1798.5234582424164
Exploration Critic Loss : 466861.3125
Exploitation Critic Loss : 0.07233408093452454
Exploration Model Loss : 26.476699829101562
Actor Loss : 0.5595221519470215
Eval_AverageReturn : -19.1200008392334
Eval_StdReturn : 4.676066875457764
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 20.12
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -17.850000
best mean reward -16.420000
running time 1842.007499
At timestep:     15 / 1000At timestep:     36 / 1000At timestep:     51 / 1000At timestep:     72 / 1000At timestep:     87 / 1000At timestep:     102 / 1000At timestep:     117 / 1000At timestep:     131 / 1000At timestep:     146 / 1000At timestep:     161 / 1000At timestep:     178 / 1000At timestep:     200 / 1000At timestep:     225 / 1000At timestep:     248 / 1000At timestep:     268 / 1000At timestep:     283 / 1000At timestep:     312 / 1000At timestep:     333 / 1000At timestep:     355 / 1000At timestep:     371 / 1000At timestep:     390 / 1000At timestep:     409 / 1000At timestep:     433 / 1000At timestep:     448 / 1000At timestep:     459 / 1000At timestep:     476 / 1000At timestep:     496 / 1000At timestep:     514 / 1000At timestep:     536 / 1000At timestep:     551 / 1000At timestep:     570 / 1000At timestep:     587 / 1000At timestep:     604 / 1000At timestep:     631 / 1000At timestep:     656 / 1000At timestep:     669 / 1000At timestep:     685 / 1000At timestep:     710 / 1000At timestep:     736 / 1000At timestep:     753 / 1000At timestep:     765 / 1000At timestep:     781 / 1000At timestep:     799 / 1000At timestep:     817 / 1000At timestep:     840 / 1000At timestep:     867 / 1000At timestep:     889 / 1000At timestep:     904 / 1000At timestep:     921 / 1000At timestep:     942 / 1000At timestep:     962 / 1000At timestep:     981 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -17.850000381469727
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1842.00749874115
Exploration Critic Loss : 409183.0625
Exploitation Critic Loss : 0.08415365219116211
Exploration Model Loss : 35.28997802734375
Actor Loss : 0.48941105604171753
Eval_AverageReturn : -17.88679313659668
Eval_StdReturn : 4.123839378356934
Eval_MaxReturn : -10.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 18.88679245283019
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -17.410000
best mean reward -16.420000
running time 1885.572711
At timestep:     20 / 1000At timestep:     37 / 1000At timestep:     55 / 1000At timestep:     68 / 1000At timestep:     89 / 1000At timestep:     107 / 1000At timestep:     123 / 1000At timestep:     140 / 1000At timestep:     159 / 1000At timestep:     171 / 1000At timestep:     189 / 1000At timestep:     209 / 1000At timestep:     221 / 1000At timestep:     237 / 1000At timestep:     261 / 1000At timestep:     273 / 1000At timestep:     290 / 1000At timestep:     307 / 1000At timestep:     324 / 1000At timestep:     346 / 1000At timestep:     364 / 1000At timestep:     380 / 1000At timestep:     400 / 1000At timestep:     411 / 1000At timestep:     426 / 1000At timestep:     448 / 1000At timestep:     463 / 1000At timestep:     490 / 1000At timestep:     504 / 1000At timestep:     531 / 1000At timestep:     550 / 1000At timestep:     571 / 1000At timestep:     589 / 1000At timestep:     610 / 1000At timestep:     630 / 1000At timestep:     646 / 1000At timestep:     667 / 1000At timestep:     683 / 1000At timestep:     703 / 1000At timestep:     723 / 1000At timestep:     737 / 1000At timestep:     753 / 1000At timestep:     767 / 1000At timestep:     790 / 1000At timestep:     807 / 1000At timestep:     830 / 1000At timestep:     856 / 1000At timestep:     870 / 1000At timestep:     888 / 1000At timestep:     905 / 1000At timestep:     921 / 1000At timestep:     944 / 1000At timestep:     964 / 1000At timestep:     982 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -17.40999984741211
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1885.5727107524872
Exploration Critic Loss : 706975.0
Exploitation Critic Loss : 0.08256678283214569
Exploration Model Loss : 52.84326171875
Actor Loss : 0.5230638980865479
Eval_AverageReturn : -17.254545211791992
Eval_StdReturn : 3.6839613914489746
Eval_MaxReturn : -10.0
Eval_MinReturn : -26.0
Eval_AverageEpLen : 18.254545454545454
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -17.610001
best mean reward -16.420000
running time 1929.318972
At timestep:     15 / 1000At timestep:     30 / 1000At timestep:     54 / 1000At timestep:     77 / 1000At timestep:     96 / 1000At timestep:     117 / 1000At timestep:     136 / 1000At timestep:     153 / 1000At timestep:     177 / 1000At timestep:     199 / 1000At timestep:     222 / 1000At timestep:     237 / 1000At timestep:     252 / 1000At timestep:     274 / 1000At timestep:     286 / 1000At timestep:     301 / 1000At timestep:     317 / 1000At timestep:     335 / 1000At timestep:     355 / 1000At timestep:     377 / 1000At timestep:     391 / 1000At timestep:     411 / 1000At timestep:     425 / 1000At timestep:     448 / 1000At timestep:     463 / 1000At timestep:     478 / 1000At timestep:     504 / 1000At timestep:     518 / 1000At timestep:     532 / 1000At timestep:     551 / 1000At timestep:     571 / 1000At timestep:     590 / 1000At timestep:     612 / 1000At timestep:     628 / 1000At timestep:     656 / 1000At timestep:     672 / 1000At timestep:     688 / 1000At timestep:     702 / 1000At timestep:     729 / 1000At timestep:     747 / 1000At timestep:     773 / 1000At timestep:     793 / 1000At timestep:     810 / 1000At timestep:     825 / 1000At timestep:     839 / 1000At timestep:     860 / 1000At timestep:     880 / 1000At timestep:     892 / 1000At timestep:     909 / 1000At timestep:     933 / 1000At timestep:     951 / 1000At timestep:     973 / 1000At timestep:     997 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -17.610000610351562
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1929.3189718723297
Exploration Critic Loss : 377193.15625
Exploitation Critic Loss : 0.07833019644021988
Exploration Model Loss : 26.476577758789062
Actor Loss : 0.463268518447876
Eval_AverageReturn : -17.703702926635742
Eval_StdReturn : 4.098914623260498
Eval_MaxReturn : -11.0
Eval_MinReturn : -27.0
Eval_AverageEpLen : 18.703703703703702
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -18.030001
best mean reward -16.420000
running time 1973.345501
At timestep:     21 / 1000At timestep:     45 / 1000At timestep:     59 / 1000At timestep:     85 / 1000At timestep:     104 / 1000At timestep:     116 / 1000At timestep:     130 / 1000At timestep:     148 / 1000At timestep:     165 / 1000At timestep:     179 / 1000At timestep:     198 / 1000At timestep:     213 / 1000At timestep:     235 / 1000At timestep:     250 / 1000At timestep:     271 / 1000At timestep:     293 / 1000At timestep:     312 / 1000At timestep:     334 / 1000At timestep:     349 / 1000At timestep:     370 / 1000At timestep:     386 / 1000At timestep:     413 / 1000At timestep:     441 / 1000At timestep:     459 / 1000At timestep:     478 / 1000At timestep:     494 / 1000At timestep:     516 / 1000At timestep:     530 / 1000At timestep:     560 / 1000At timestep:     580 / 1000At timestep:     603 / 1000At timestep:     617 / 1000At timestep:     635 / 1000At timestep:     650 / 1000At timestep:     669 / 1000At timestep:     698 / 1000At timestep:     716 / 1000At timestep:     736 / 1000At timestep:     749 / 1000At timestep:     778 / 1000At timestep:     802 / 1000At timestep:     818 / 1000At timestep:     839 / 1000At timestep:     863 / 1000At timestep:     880 / 1000At timestep:     893 / 1000At timestep:     914 / 1000At timestep:     932 / 1000At timestep:     949 / 1000At timestep:     967 / 1000At timestep:     981 / 1000At timestep:     1000 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_easy_supervised_lam0.1_PointmassEasy-v0_22-11-2022_20-25-48/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -18.030000686645508
Train_BestReturn : -16.420000076293945
TimeSinceStart : 1973.345501422882
Exploration Critic Loss : 465479.6875
Exploitation Critic Loss : 0.10927200317382812
Exploration Model Loss : 35.28985595703125
Actor Loss : 0.4336017370223999
Eval_AverageReturn : -18.230770111083984
Eval_StdReturn : 4.474781513214111
Eval_MaxReturn : -11.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 19.23076923076923
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_supervised_lam1_PointmassEasy-v0_22-11-2022_20-59-28 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_supervised_lam1_PointmassEasy-v0_22-11-2022_20-59-28
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001826
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.001825571060180664
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 12.128048
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -50.0
TimeSinceStart : 12.128048181533813
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 24.439149
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -50.0
TimeSinceStart : 24.439149379730225
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 55.526326
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     390 / 1000At timestep:     440 / 1000At timestep:     490 / 1000At timestep:     540 / 1000At timestep:     590 / 1000At timestep:     640 / 1000At timestep:     690 / 1000At timestep:     740 / 1000At timestep:     790 / 1000At timestep:     840 / 1000At timestep:     890 / 1000At timestep:     940 / 1000At timestep:     990 / 1000At timestep:     1040 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -50.0
TimeSinceStart : 55.526326417922974
Exploration Critic Loss : 465165.28125
Exploitation Critic Loss : 0.06619924306869507
Exploration Model Loss : 30.187185287475586
Actor Loss : 1.1806795597076416
Eval_AverageReturn : -49.47618865966797
Eval_StdReturn : 2.3425474166870117
Eval_MaxReturn : -39.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 49.523809523809526
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -49.650002
best mean reward -inf
running time 87.520866
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     246 / 1000At timestep:     296 / 1000At timestep:     346 / 1000At timestep:     396 / 1000At timestep:     446 / 1000At timestep:     496 / 1000At timestep:     546 / 1000At timestep:     596 / 1000At timestep:     646 / 1000At timestep:     696 / 1000At timestep:     746 / 1000At timestep:     796 / 1000At timestep:     846 / 1000At timestep:     896 / 1000At timestep:     946 / 1000At timestep:     996 / 1000At timestep:     1046 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -49.650001525878906
TimeSinceStart : 87.52086591720581
Exploration Critic Loss : 7405095.0
Exploitation Critic Loss : 0.050987113267183304
Exploration Model Loss : 33.84922790527344
Actor Loss : 0.9429558515548706
Eval_AverageReturn : -49.761905670166016
Eval_StdReturn : 1.0647941827774048
Eval_MaxReturn : -45.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 49.80952380952381
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -47.959999
best mean reward -47.959999
running time 118.871782
At timestep:     50 / 1000At timestep:     95 / 1000At timestep:     145 / 1000At timestep:     185 / 1000At timestep:     235 / 1000At timestep:     285 / 1000At timestep:     335 / 1000At timestep:     385 / 1000At timestep:     435 / 1000At timestep:     485 / 1000At timestep:     535 / 1000At timestep:     585 / 1000At timestep:     635 / 1000At timestep:     685 / 1000At timestep:     712 / 1000At timestep:     762 / 1000At timestep:     812 / 1000At timestep:     862 / 1000At timestep:     912 / 1000At timestep:     962 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -47.959999084472656
Train_BestReturn : -47.959999084472656
TimeSinceStart : 118.87178206443787
Exploration Critic Loss : 7364995.0
Exploitation Critic Loss : 0.11456845700740814
Exploration Model Loss : 36.868125915527344
Actor Loss : 0.814009964466095
Eval_AverageReturn : -48.0476188659668
Eval_StdReturn : 5.576106548309326
Eval_MaxReturn : -26.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 48.19047619047619
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -41.009998
best mean reward -41.009998
running time 152.271777
At timestep:     22 / 1000At timestep:     72 / 1000At timestep:     101 / 1000At timestep:     147 / 1000At timestep:     197 / 1000At timestep:     247 / 1000At timestep:     297 / 1000At timestep:     347 / 1000At timestep:     397 / 1000At timestep:     443 / 1000At timestep:     493 / 1000At timestep:     543 / 1000At timestep:     593 / 1000At timestep:     639 / 1000At timestep:     689 / 1000At timestep:     739 / 1000At timestep:     777 / 1000At timestep:     805 / 1000At timestep:     855 / 1000At timestep:     905 / 1000At timestep:     930 / 1000At timestep:     980 / 1000At timestep:     1030 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -41.0099983215332
Train_BestReturn : -41.0099983215332
TimeSinceStart : 152.27177739143372
Exploration Critic Loss : 6519652.0
Exploitation Critic Loss : 0.07451418042182922
Exploration Model Loss : 48.66120910644531
Actor Loss : 0.672512412071228
Eval_AverageReturn : -44.39130401611328
Eval_StdReturn : 9.444294929504395
Eval_MaxReturn : -21.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 44.78260869565217
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -27.360001
best mean reward -27.360001
running time 189.061824
At timestep:     50 / 1000At timestep:     90 / 1000At timestep:     140 / 1000At timestep:     190 / 1000At timestep:     235 / 1000At timestep:     275 / 1000At timestep:     319 / 1000At timestep:     369 / 1000At timestep:     419 / 1000At timestep:     457 / 1000At timestep:     477 / 1000At timestep:     500 / 1000At timestep:     543 / 1000At timestep:     582 / 1000At timestep:     625 / 1000At timestep:     658 / 1000At timestep:     685 / 1000At timestep:     735 / 1000At timestep:     768 / 1000At timestep:     798 / 1000At timestep:     827 / 1000At timestep:     873 / 1000At timestep:     923 / 1000At timestep:     952 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -27.360000610351562
Train_BestReturn : -27.360000610351562
TimeSinceStart : 189.06182432174683
Exploration Critic Loss : 5130309.0
Exploitation Critic Loss : 0.11881372332572937
Exploration Model Loss : 39.87996292114258
Actor Loss : 0.6047775745391846
Eval_AverageReturn : -39.31999969482422
Eval_StdReturn : 9.63626480102539
Eval_MaxReturn : -19.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 40.08
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -18.700001
best mean reward -18.700001
running time 227.092356
At timestep:     16 / 1000At timestep:     47 / 1000At timestep:     83 / 1000At timestep:     133 / 1000At timestep:     167 / 1000At timestep:     212 / 1000At timestep:     259 / 1000At timestep:     309 / 1000At timestep:     337 / 1000At timestep:     387 / 1000At timestep:     417 / 1000At timestep:     442 / 1000At timestep:     489 / 1000At timestep:     520 / 1000At timestep:     570 / 1000At timestep:     602 / 1000At timestep:     624 / 1000At timestep:     657 / 1000At timestep:     704 / 1000At timestep:     743 / 1000At timestep:     793 / 1000At timestep:     843 / 1000At timestep:     870 / 1000At timestep:     920 / 1000At timestep:     954 / 1000At timestep:     971 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -18.700000762939453
Train_BestReturn : -18.700000762939453
TimeSinceStart : 227.0923557281494
Exploration Critic Loss : 4356705.0
Exploitation Critic Loss : 0.17937199771404266
Exploration Model Loss : 22.912002563476562
Actor Loss : 0.5579283833503723
Eval_AverageReturn : -36.96296310424805
Eval_StdReturn : 11.2001371383667
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 37.7037037037037
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -17.590000
best mean reward -17.590000
running time 265.074887
At timestep:     50 / 1000At timestep:     89 / 1000At timestep:     139 / 1000At timestep:     186 / 1000At timestep:     236 / 1000At timestep:     273 / 1000At timestep:     295 / 1000At timestep:     322 / 1000At timestep:     345 / 1000At timestep:     395 / 1000At timestep:     419 / 1000At timestep:     469 / 1000At timestep:     499 / 1000At timestep:     533 / 1000At timestep:     580 / 1000At timestep:     630 / 1000At timestep:     676 / 1000At timestep:     726 / 1000At timestep:     760 / 1000At timestep:     789 / 1000At timestep:     839 / 1000At timestep:     889 / 1000At timestep:     905 / 1000At timestep:     955 / 1000At timestep:     974 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -17.59000015258789
Train_BestReturn : -17.59000015258789
TimeSinceStart : 265.07488679885864
Exploration Critic Loss : 5357141.0
Exploitation Critic Loss : 0.17714375257492065
Exploration Model Loss : 24.530441284179688
Actor Loss : 0.5552605390548706
Eval_AverageReturn : -38.0
Eval_StdReturn : 12.051173210144043
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 38.61538461538461
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -18.430000
best mean reward -17.590000
running time 302.714279
At timestep:     45 / 1000At timestep:     72 / 1000At timestep:     88 / 1000At timestep:     138 / 1000At timestep:     160 / 1000At timestep:     203 / 1000At timestep:     219 / 1000At timestep:     241 / 1000At timestep:     268 / 1000At timestep:     318 / 1000At timestep:     342 / 1000At timestep:     385 / 1000At timestep:     423 / 1000At timestep:     464 / 1000At timestep:     512 / 1000At timestep:     540 / 1000At timestep:     563 / 1000At timestep:     577 / 1000At timestep:     611 / 1000At timestep:     659 / 1000At timestep:     695 / 1000At timestep:     743 / 1000At timestep:     770 / 1000At timestep:     790 / 1000At timestep:     815 / 1000At timestep:     855 / 1000At timestep:     905 / 1000At timestep:     924 / 1000At timestep:     943 / 1000At timestep:     987 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -18.43000030517578
Train_BestReturn : -17.59000015258789
TimeSinceStart : 302.7142791748047
Exploration Critic Loss : 4932812.0
Exploitation Critic Loss : 0.15787233412265778
Exploration Model Loss : 33.305908203125
Actor Loss : 0.6497544050216675
Eval_AverageReturn : -31.838708877563477
Eval_StdReturn : 11.919343948364258
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 32.74193548387097
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -17.889999
best mean reward -17.590000
running time 341.882328
At timestep:     17 / 1000At timestep:     37 / 1000At timestep:     56 / 1000At timestep:     90 / 1000At timestep:     121 / 1000At timestep:     138 / 1000At timestep:     158 / 1000At timestep:     176 / 1000At timestep:     194 / 1000At timestep:     244 / 1000At timestep:     275 / 1000At timestep:     292 / 1000At timestep:     337 / 1000At timestep:     355 / 1000At timestep:     388 / 1000At timestep:     403 / 1000At timestep:     422 / 1000At timestep:     454 / 1000At timestep:     485 / 1000At timestep:     532 / 1000At timestep:     566 / 1000At timestep:     591 / 1000At timestep:     617 / 1000At timestep:     635 / 1000At timestep:     679 / 1000At timestep:     697 / 1000At timestep:     742 / 1000At timestep:     766 / 1000At timestep:     816 / 1000At timestep:     853 / 1000At timestep:     874 / 1000At timestep:     899 / 1000At timestep:     915 / 1000At timestep:     934 / 1000At timestep:     963 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -17.889999389648438
Train_BestReturn : -17.59000015258789
TimeSinceStart : 341.8823275566101
Exploration Critic Loss : 5088257.0
Exploitation Critic Loss : 0.1286935806274414
Exploration Model Loss : 23.028900146484375
Actor Loss : 0.6141304969787598
Eval_AverageReturn : -26.91666603088379
Eval_StdReturn : 10.868851661682129
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 27.86111111111111
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -17.549999
best mean reward -17.549999
running time 383.695116
At timestep:     26 / 1000At timestep:     65 / 1000At timestep:     88 / 1000At timestep:     123 / 1000At timestep:     172 / 1000At timestep:     193 / 1000At timestep:     221 / 1000At timestep:     243 / 1000At timestep:     273 / 1000At timestep:     293 / 1000At timestep:     314 / 1000At timestep:     336 / 1000At timestep:     360 / 1000At timestep:     383 / 1000At timestep:     411 / 1000At timestep:     443 / 1000At timestep:     459 / 1000At timestep:     490 / 1000At timestep:     537 / 1000At timestep:     568 / 1000At timestep:     592 / 1000At timestep:     620 / 1000At timestep:     641 / 1000At timestep:     658 / 1000At timestep:     684 / 1000At timestep:     709 / 1000At timestep:     738 / 1000At timestep:     764 / 1000At timestep:     778 / 1000At timestep:     800 / 1000At timestep:     832 / 1000At timestep:     857 / 1000At timestep:     877 / 1000At timestep:     910 / 1000At timestep:     933 / 1000At timestep:     951 / 1000At timestep:     984 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -17.549999237060547
Train_BestReturn : -17.549999237060547
TimeSinceStart : 383.6951160430908
Exploration Critic Loss : 3934881.25
Exploitation Critic Loss : 0.1609271913766861
Exploration Model Loss : 21.107513427734375
Actor Loss : 0.6249641180038452
Eval_AverageReturn : -25.3157901763916
Eval_StdReturn : 7.598840713500977
Eval_MaxReturn : -13.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 26.31578947368421
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -17.780001
best mean reward -17.549999
running time 426.693341
At timestep:     23 / 1000At timestep:     65 / 1000At timestep:     82 / 1000At timestep:     106 / 1000At timestep:     134 / 1000At timestep:     177 / 1000At timestep:     223 / 1000At timestep:     250 / 1000At timestep:     273 / 1000At timestep:     315 / 1000At timestep:     357 / 1000At timestep:     386 / 1000At timestep:     413 / 1000At timestep:     438 / 1000At timestep:     466 / 1000At timestep:     481 / 1000At timestep:     520 / 1000At timestep:     537 / 1000At timestep:     587 / 1000At timestep:     619 / 1000At timestep:     648 / 1000At timestep:     681 / 1000At timestep:     731 / 1000At timestep:     766 / 1000At timestep:     786 / 1000At timestep:     808 / 1000At timestep:     834 / 1000At timestep:     855 / 1000At timestep:     905 / 1000At timestep:     936 / 1000At timestep:     983 / 1000At timestep:     999 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -17.780000686645508
Train_BestReturn : -17.549999237060547
TimeSinceStart : 426.69334077835083
Exploration Critic Loss : 3219849.5
Exploitation Critic Loss : 0.2044670283794403
Exploration Model Loss : 20.513885498046875
Actor Loss : 0.6371448040008545
Eval_AverageReturn : -30.060606002807617
Eval_StdReturn : 10.741985321044922
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 30.96969696969697
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -17.139999
best mean reward -17.139999
running time 466.519074
At timestep:     22 / 1000At timestep:     46 / 1000At timestep:     92 / 1000At timestep:     115 / 1000At timestep:     132 / 1000At timestep:     182 / 1000At timestep:     209 / 1000At timestep:     231 / 1000At timestep:     253 / 1000At timestep:     279 / 1000At timestep:     305 / 1000At timestep:     328 / 1000At timestep:     353 / 1000At timestep:     403 / 1000At timestep:     422 / 1000At timestep:     436 / 1000At timestep:     455 / 1000At timestep:     477 / 1000At timestep:     506 / 1000At timestep:     529 / 1000At timestep:     551 / 1000At timestep:     573 / 1000At timestep:     589 / 1000At timestep:     605 / 1000At timestep:     617 / 1000At timestep:     667 / 1000At timestep:     696 / 1000At timestep:     730 / 1000At timestep:     751 / 1000At timestep:     763 / 1000At timestep:     792 / 1000At timestep:     818 / 1000At timestep:     862 / 1000At timestep:     884 / 1000At timestep:     908 / 1000At timestep:     937 / 1000At timestep:     962 / 1000At timestep:     986 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -17.139999389648438
Train_BestReturn : -17.139999389648438
TimeSinceStart : 466.51907420158386
Exploration Critic Loss : 2986298.25
Exploitation Critic Loss : 0.11864209175109863
Exploration Model Loss : 20.3082275390625
Actor Loss : 0.5745388269424438
Eval_AverageReturn : -25.076923370361328
Eval_StdReturn : 9.866765975952148
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 26.0
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -17.700001
best mean reward -17.139999
running time 506.541089
At timestep:     49 / 1000At timestep:     75 / 1000At timestep:     113 / 1000At timestep:     142 / 1000At timestep:     179 / 1000At timestep:     205 / 1000At timestep:     234 / 1000At timestep:     260 / 1000At timestep:     287 / 1000At timestep:     302 / 1000At timestep:     331 / 1000At timestep:     366 / 1000At timestep:     384 / 1000At timestep:     428 / 1000At timestep:     444 / 1000At timestep:     468 / 1000At timestep:     518 / 1000At timestep:     541 / 1000At timestep:     562 / 1000At timestep:     586 / 1000At timestep:     613 / 1000At timestep:     642 / 1000At timestep:     661 / 1000At timestep:     698 / 1000At timestep:     724 / 1000At timestep:     774 / 1000At timestep:     812 / 1000At timestep:     837 / 1000At timestep:     865 / 1000At timestep:     883 / 1000At timestep:     897 / 1000At timestep:     928 / 1000At timestep:     966 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -17.700000762939453
Train_BestReturn : -17.139999389648438
TimeSinceStart : 506.5410888195038
Exploration Critic Loss : 2421131.0
Exploitation Critic Loss : 0.14988520741462708
Exploration Model Loss : 20.23443603515625
Actor Loss : 0.6520418524742126
Eval_AverageReturn : -28.617647171020508
Eval_StdReturn : 9.719642639160156
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 29.58823529411765
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -17.389999
best mean reward -17.139999
running time 547.002843
At timestep:     34 / 1000At timestep:     49 / 1000At timestep:     69 / 1000At timestep:     94 / 1000At timestep:     115 / 1000At timestep:     151 / 1000At timestep:     169 / 1000At timestep:     208 / 1000At timestep:     226 / 1000At timestep:     275 / 1000At timestep:     293 / 1000At timestep:     315 / 1000At timestep:     337 / 1000At timestep:     378 / 1000At timestep:     424 / 1000At timestep:     453 / 1000At timestep:     468 / 1000At timestep:     507 / 1000At timestep:     532 / 1000At timestep:     552 / 1000At timestep:     568 / 1000At timestep:     596 / 1000At timestep:     623 / 1000At timestep:     653 / 1000At timestep:     678 / 1000At timestep:     698 / 1000At timestep:     723 / 1000At timestep:     764 / 1000At timestep:     796 / 1000At timestep:     846 / 1000At timestep:     871 / 1000At timestep:     895 / 1000At timestep:     926 / 1000At timestep:     941 / 1000At timestep:     959 / 1000At timestep:     977 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -17.389999389648438
Train_BestReturn : -17.139999389648438
TimeSinceStart : 547.0028433799744
Exploration Critic Loss : 1967463.625
Exploitation Critic Loss : 0.16470330953598022
Exploration Model Loss : 20.206954956054688
Actor Loss : 0.5968118906021118
Eval_AverageReturn : -26.486486434936523
Eval_StdReturn : 9.920280456542969
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 27.45945945945946
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -17.959999
best mean reward -17.139999
running time 587.410539
At timestep:     21 / 1000At timestep:     39 / 1000At timestep:     89 / 1000At timestep:     103 / 1000At timestep:     135 / 1000At timestep:     166 / 1000At timestep:     186 / 1000At timestep:     236 / 1000At timestep:     272 / 1000At timestep:     322 / 1000At timestep:     341 / 1000At timestep:     355 / 1000At timestep:     386 / 1000At timestep:     399 / 1000At timestep:     424 / 1000At timestep:     443 / 1000At timestep:     460 / 1000At timestep:     485 / 1000At timestep:     529 / 1000At timestep:     554 / 1000At timestep:     585 / 1000At timestep:     605 / 1000At timestep:     626 / 1000At timestep:     648 / 1000At timestep:     682 / 1000At timestep:     711 / 1000At timestep:     737 / 1000At timestep:     761 / 1000At timestep:     779 / 1000At timestep:     794 / 1000At timestep:     832 / 1000At timestep:     852 / 1000At timestep:     871 / 1000At timestep:     913 / 1000At timestep:     940 / 1000At timestep:     956 / 1000At timestep:     990 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -17.959999084472656
Train_BestReturn : -17.139999389648438
TimeSinceStart : 587.4105389118195
Exploration Critic Loss : 3448767.0
Exploitation Critic Loss : 0.09572009742259979
Exploration Model Loss : 20.197235107421875
Actor Loss : 0.6029050946235657
Eval_AverageReturn : -25.578947067260742
Eval_StdReturn : 10.469583511352539
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 26.526315789473685
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -18.559999
best mean reward -17.139999
running time 627.473845
At timestep:     17 / 1000At timestep:     52 / 1000At timestep:     75 / 1000At timestep:     102 / 1000At timestep:     127 / 1000At timestep:     164 / 1000At timestep:     181 / 1000At timestep:     200 / 1000At timestep:     221 / 1000At timestep:     244 / 1000At timestep:     267 / 1000At timestep:     293 / 1000At timestep:     321 / 1000At timestep:     351 / 1000At timestep:     375 / 1000At timestep:     398 / 1000At timestep:     418 / 1000At timestep:     437 / 1000At timestep:     478 / 1000At timestep:     495 / 1000At timestep:     526 / 1000At timestep:     542 / 1000At timestep:     563 / 1000At timestep:     610 / 1000At timestep:     639 / 1000At timestep:     666 / 1000At timestep:     696 / 1000At timestep:     707 / 1000At timestep:     750 / 1000At timestep:     764 / 1000At timestep:     801 / 1000At timestep:     839 / 1000At timestep:     856 / 1000At timestep:     881 / 1000At timestep:     894 / 1000At timestep:     918 / 1000At timestep:     946 / 1000At timestep:     962 / 1000At timestep:     978 / 1000At timestep:     991 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -18.559999465942383
Train_BestReturn : -17.139999389648438
TimeSinceStart : 627.4738450050354
Exploration Critic Loss : 1459500.0
Exploitation Critic Loss : 0.19593413174152374
Exploration Model Loss : 20.193313598632812
Actor Loss : 0.6270118355751038
Eval_AverageReturn : -23.560976028442383
Eval_StdReturn : 8.65898323059082
Eval_MaxReturn : -10.0
Eval_MinReturn : -46.0
Eval_AverageEpLen : 24.5609756097561
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.600000
best mean reward -17.139999
running time 667.988499
At timestep:     15 / 1000At timestep:     62 / 1000At timestep:     84 / 1000At timestep:     113 / 1000At timestep:     137 / 1000At timestep:     160 / 1000At timestep:     179 / 1000At timestep:     199 / 1000At timestep:     218 / 1000At timestep:     242 / 1000At timestep:     265 / 1000At timestep:     315 / 1000At timestep:     337 / 1000At timestep:     357 / 1000At timestep:     384 / 1000At timestep:     411 / 1000At timestep:     433 / 1000At timestep:     463 / 1000At timestep:     478 / 1000At timestep:     496 / 1000At timestep:     512 / 1000At timestep:     541 / 1000At timestep:     561 / 1000At timestep:     593 / 1000At timestep:     617 / 1000At timestep:     643 / 1000At timestep:     664 / 1000At timestep:     705 / 1000At timestep:     725 / 1000At timestep:     747 / 1000At timestep:     771 / 1000At timestep:     788 / 1000At timestep:     811 / 1000At timestep:     835 / 1000At timestep:     854 / 1000At timestep:     877 / 1000At timestep:     893 / 1000At timestep:     912 / 1000At timestep:     927 / 1000At timestep:     977 / 1000At timestep:     991 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.600000381469727
Train_BestReturn : -17.139999389648438
TimeSinceStart : 667.9884986877441
Exploration Critic Loss : 856170.125
Exploitation Critic Loss : 0.058491505682468414
Exploration Model Loss : 20.191757202148438
Actor Loss : 0.5463511943817139
Eval_AverageReturn : -23.309524536132812
Eval_StdReturn : 8.680816650390625
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 24.285714285714285
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.219999
best mean reward -17.139999
running time 708.830500
At timestep:     20 / 1000At timestep:     39 / 1000At timestep:     86 / 1000At timestep:     115 / 1000At timestep:     142 / 1000At timestep:     163 / 1000At timestep:     185 / 1000At timestep:     199 / 1000At timestep:     220 / 1000At timestep:     248 / 1000At timestep:     283 / 1000At timestep:     315 / 1000At timestep:     342 / 1000At timestep:     357 / 1000At timestep:     407 / 1000At timestep:     427 / 1000At timestep:     449 / 1000At timestep:     467 / 1000At timestep:     499 / 1000At timestep:     514 / 1000At timestep:     532 / 1000At timestep:     548 / 1000At timestep:     561 / 1000At timestep:     583 / 1000At timestep:     599 / 1000At timestep:     649 / 1000At timestep:     669 / 1000At timestep:     694 / 1000At timestep:     709 / 1000At timestep:     736 / 1000At timestep:     751 / 1000At timestep:     776 / 1000At timestep:     799 / 1000At timestep:     814 / 1000At timestep:     843 / 1000At timestep:     858 / 1000At timestep:     890 / 1000At timestep:     912 / 1000At timestep:     933 / 1000At timestep:     955 / 1000At timestep:     987 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.219999313354492
Train_BestReturn : -17.139999389648438
TimeSinceStart : 708.8304996490479
Exploration Critic Loss : 1053421.25
Exploitation Critic Loss : 0.10626459121704102
Exploration Model Loss : 20.191238403320312
Actor Loss : 0.5492222309112549
Eval_AverageReturn : -23.14285659790039
Eval_StdReturn : 9.111927032470703
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 24.095238095238095
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.299999
best mean reward -17.139999
running time 750.580147
At timestep:     24 / 1000At timestep:     51 / 1000At timestep:     101 / 1000At timestep:     112 / 1000At timestep:     143 / 1000At timestep:     164 / 1000At timestep:     184 / 1000At timestep:     210 / 1000At timestep:     229 / 1000At timestep:     251 / 1000At timestep:     275 / 1000At timestep:     300 / 1000At timestep:     324 / 1000At timestep:     353 / 1000At timestep:     369 / 1000At timestep:     399 / 1000At timestep:     418 / 1000At timestep:     433 / 1000At timestep:     453 / 1000At timestep:     470 / 1000At timestep:     486 / 1000At timestep:     501 / 1000At timestep:     517 / 1000At timestep:     562 / 1000At timestep:     593 / 1000At timestep:     615 / 1000At timestep:     630 / 1000At timestep:     659 / 1000At timestep:     709 / 1000At timestep:     722 / 1000At timestep:     758 / 1000At timestep:     779 / 1000At timestep:     797 / 1000At timestep:     817 / 1000At timestep:     832 / 1000At timestep:     855 / 1000At timestep:     880 / 1000At timestep:     907 / 1000At timestep:     934 / 1000At timestep:     948 / 1000At timestep:     968 / 1000At timestep:     984 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.299999237060547
Train_BestReturn : -17.139999389648438
TimeSinceStart : 750.5801465511322
Exploration Critic Loss : 896001.0
Exploitation Critic Loss : 0.12474532425403595
Exploration Model Loss : 20.190902709960938
Actor Loss : 0.547076940536499
Eval_AverageReturn : -22.372093200683594
Eval_StdReturn : 8.975479125976562
Eval_MaxReturn : -10.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 23.325581395348838
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.250000
best mean reward -17.139999
running time 791.698774
At timestep:     16 / 1000At timestep:     59 / 1000At timestep:     82 / 1000At timestep:     102 / 1000At timestep:     126 / 1000At timestep:     167 / 1000At timestep:     182 / 1000At timestep:     211 / 1000At timestep:     230 / 1000At timestep:     247 / 1000At timestep:     272 / 1000At timestep:     289 / 1000At timestep:     322 / 1000At timestep:     355 / 1000At timestep:     383 / 1000At timestep:     408 / 1000At timestep:     426 / 1000At timestep:     456 / 1000At timestep:     481 / 1000At timestep:     493 / 1000At timestep:     510 / 1000At timestep:     548 / 1000At timestep:     581 / 1000At timestep:     602 / 1000At timestep:     642 / 1000At timestep:     670 / 1000At timestep:     695 / 1000At timestep:     712 / 1000At timestep:     731 / 1000At timestep:     776 / 1000At timestep:     804 / 1000At timestep:     816 / 1000At timestep:     835 / 1000At timestep:     854 / 1000At timestep:     884 / 1000At timestep:     907 / 1000At timestep:     925 / 1000At timestep:     944 / 1000At timestep:     974 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.25
Train_BestReturn : -17.139999389648438
TimeSinceStart : 791.6987738609314
Exploration Critic Loss : 1205808.25
Exploitation Critic Loss : 0.11044614017009735
Exploration Model Loss : 20.190902709960938
Actor Loss : 0.5357121825218201
Eval_AverageReturn : -24.049999237060547
Eval_StdReturn : 8.360472679138184
Eval_MaxReturn : -11.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 25.05
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -17.129999
best mean reward -17.129999
running time 832.891063
At timestep:     17 / 1000At timestep:     33 / 1000At timestep:     52 / 1000At timestep:     76 / 1000At timestep:     106 / 1000At timestep:     123 / 1000At timestep:     142 / 1000At timestep:     178 / 1000At timestep:     201 / 1000At timestep:     220 / 1000At timestep:     237 / 1000At timestep:     257 / 1000At timestep:     275 / 1000At timestep:     289 / 1000At timestep:     310 / 1000At timestep:     333 / 1000At timestep:     356 / 1000At timestep:     373 / 1000At timestep:     386 / 1000At timestep:     403 / 1000At timestep:     453 / 1000At timestep:     482 / 1000At timestep:     504 / 1000At timestep:     532 / 1000At timestep:     575 / 1000At timestep:     597 / 1000At timestep:     618 / 1000At timestep:     640 / 1000At timestep:     660 / 1000At timestep:     675 / 1000At timestep:     691 / 1000At timestep:     719 / 1000At timestep:     744 / 1000At timestep:     764 / 1000At timestep:     785 / 1000At timestep:     809 / 1000At timestep:     830 / 1000At timestep:     852 / 1000At timestep:     878 / 1000At timestep:     889 / 1000At timestep:     930 / 1000At timestep:     948 / 1000At timestep:     972 / 1000At timestep:     995 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -17.1299991607666
Train_BestReturn : -17.1299991607666
TimeSinceStart : 832.891063451767
Exploration Critic Loss : 1063114.5
Exploitation Critic Loss : 0.12571436166763306
Exploration Model Loss : 20.19061279296875
Actor Loss : 0.5167804956436157
Eval_AverageReturn : -21.600000381469727
Eval_StdReturn : 7.654773235321045
Eval_MaxReturn : -10.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 22.57777777777778
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -17.719999
best mean reward -17.129999
running time 873.965961
At timestep:     20 / 1000At timestep:     44 / 1000At timestep:     64 / 1000At timestep:     79 / 1000At timestep:     95 / 1000At timestep:     121 / 1000At timestep:     138 / 1000At timestep:     159 / 1000At timestep:     183 / 1000At timestep:     203 / 1000At timestep:     221 / 1000At timestep:     249 / 1000At timestep:     271 / 1000At timestep:     297 / 1000At timestep:     328 / 1000At timestep:     362 / 1000At timestep:     377 / 1000At timestep:     395 / 1000At timestep:     415 / 1000At timestep:     449 / 1000At timestep:     467 / 1000At timestep:     490 / 1000At timestep:     520 / 1000At timestep:     536 / 1000At timestep:     571 / 1000At timestep:     585 / 1000At timestep:     615 / 1000At timestep:     630 / 1000At timestep:     643 / 1000At timestep:     657 / 1000At timestep:     676 / 1000At timestep:     710 / 1000At timestep:     729 / 1000At timestep:     748 / 1000At timestep:     761 / 1000At timestep:     778 / 1000At timestep:     828 / 1000At timestep:     848 / 1000At timestep:     866 / 1000At timestep:     890 / 1000At timestep:     903 / 1000At timestep:     922 / 1000At timestep:     941 / 1000At timestep:     957 / 1000At timestep:     974 / 1000At timestep:     994 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -17.719999313354492
Train_BestReturn : -17.1299991607666
TimeSinceStart : 873.9659609794617
Exploration Critic Loss : 868517.875
Exploitation Critic Loss : 0.0756697729229927
Exploration Model Loss : 20.19061279296875
Actor Loss : 0.601182222366333
Eval_AverageReturn : -20.553192138671875
Eval_StdReturn : 7.3998517990112305
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 21.53191489361702
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -17.770000
best mean reward -17.129999
running time 915.151192
At timestep:     17 / 1000At timestep:     49 / 1000At timestep:     78 / 1000At timestep:     97 / 1000At timestep:     116 / 1000At timestep:     134 / 1000At timestep:     156 / 1000At timestep:     177 / 1000At timestep:     199 / 1000At timestep:     222 / 1000At timestep:     246 / 1000At timestep:     261 / 1000At timestep:     284 / 1000At timestep:     310 / 1000At timestep:     324 / 1000At timestep:     354 / 1000At timestep:     376 / 1000At timestep:     413 / 1000At timestep:     428 / 1000At timestep:     456 / 1000At timestep:     481 / 1000At timestep:     502 / 1000At timestep:     520 / 1000At timestep:     537 / 1000At timestep:     562 / 1000At timestep:     583 / 1000At timestep:     609 / 1000At timestep:     642 / 1000At timestep:     672 / 1000At timestep:     684 / 1000At timestep:     718 / 1000At timestep:     739 / 1000At timestep:     766 / 1000At timestep:     786 / 1000At timestep:     800 / 1000At timestep:     819 / 1000At timestep:     845 / 1000At timestep:     859 / 1000At timestep:     873 / 1000At timestep:     884 / 1000At timestep:     903 / 1000At timestep:     918 / 1000At timestep:     939 / 1000At timestep:     969 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -17.770000457763672
Train_BestReturn : -17.1299991607666
TimeSinceStart : 915.1511921882629
Exploration Critic Loss : 914220.0
Exploitation Critic Loss : 0.14626920223236084
Exploration Model Loss : 20.190170288085938
Actor Loss : 0.5435459613800049
Eval_AverageReturn : -21.53333282470703
Eval_StdReturn : 7.041464328765869
Eval_MaxReturn : -10.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 22.533333333333335
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -17.889999
best mean reward -17.129999
running time 956.040233
At timestep:     20 / 1000At timestep:     40 / 1000At timestep:     57 / 1000At timestep:     77 / 1000At timestep:     98 / 1000At timestep:     120 / 1000At timestep:     167 / 1000At timestep:     194 / 1000At timestep:     212 / 1000At timestep:     244 / 1000At timestep:     263 / 1000At timestep:     278 / 1000At timestep:     297 / 1000At timestep:     310 / 1000At timestep:     333 / 1000At timestep:     357 / 1000At timestep:     375 / 1000At timestep:     389 / 1000At timestep:     408 / 1000At timestep:     426 / 1000At timestep:     438 / 1000At timestep:     473 / 1000At timestep:     503 / 1000At timestep:     551 / 1000At timestep:     583 / 1000At timestep:     601 / 1000At timestep:     636 / 1000At timestep:     659 / 1000At timestep:     676 / 1000At timestep:     691 / 1000At timestep:     715 / 1000At timestep:     751 / 1000At timestep:     768 / 1000At timestep:     789 / 1000At timestep:     801 / 1000At timestep:     825 / 1000At timestep:     849 / 1000At timestep:     865 / 1000At timestep:     900 / 1000At timestep:     916 / 1000At timestep:     934 / 1000At timestep:     948 / 1000At timestep:     969 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -17.889999389648438
Train_BestReturn : -17.1299991607666
TimeSinceStart : 956.0402331352234
Exploration Critic Loss : 1015560.875
Exploitation Critic Loss : 0.11022307723760605
Exploration Model Loss : 20.189956665039062
Actor Loss : 0.47944486141204834
Eval_AverageReturn : -21.93181800842285
Eval_StdReturn : 8.75525188446045
Eval_MaxReturn : -11.0
Eval_MinReturn : -47.0
Eval_AverageEpLen : 22.931818181818183
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -17.400000
best mean reward -17.129999
running time 997.398522
At timestep:     33 / 1000At timestep:     58 / 1000At timestep:     96 / 1000At timestep:     122 / 1000At timestep:     138 / 1000At timestep:     163 / 1000At timestep:     187 / 1000At timestep:     207 / 1000At timestep:     226 / 1000At timestep:     240 / 1000At timestep:     264 / 1000At timestep:     293 / 1000At timestep:     311 / 1000At timestep:     341 / 1000At timestep:     362 / 1000At timestep:     380 / 1000At timestep:     430 / 1000At timestep:     446 / 1000At timestep:     461 / 1000At timestep:     498 / 1000At timestep:     522 / 1000At timestep:     543 / 1000At timestep:     557 / 1000At timestep:     574 / 1000At timestep:     588 / 1000At timestep:     606 / 1000At timestep:     623 / 1000At timestep:     643 / 1000At timestep:     662 / 1000At timestep:     677 / 1000At timestep:     698 / 1000At timestep:     726 / 1000At timestep:     741 / 1000At timestep:     765 / 1000At timestep:     790 / 1000At timestep:     809 / 1000At timestep:     829 / 1000At timestep:     846 / 1000At timestep:     864 / 1000At timestep:     883 / 1000At timestep:     901 / 1000At timestep:     921 / 1000At timestep:     944 / 1000At timestep:     963 / 1000At timestep:     985 / 1000At timestep:     999 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -17.399999618530273
Train_BestReturn : -17.1299991607666
TimeSinceStart : 997.398521900177
Exploration Critic Loss : 971323.625
Exploitation Critic Loss : 0.10942904651165009
Exploration Model Loss : 20.189590454101562
Actor Loss : 0.4741758704185486
Eval_AverageReturn : -20.63829803466797
Eval_StdReturn : 7.093376159667969
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 21.617021276595743
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -16.870001
best mean reward -16.870001
running time 1039.388415
At timestep:     27 / 1000At timestep:     75 / 1000At timestep:     96 / 1000At timestep:     111 / 1000At timestep:     146 / 1000At timestep:     165 / 1000At timestep:     186 / 1000At timestep:     226 / 1000At timestep:     243 / 1000At timestep:     261 / 1000At timestep:     282 / 1000At timestep:     296 / 1000At timestep:     313 / 1000At timestep:     337 / 1000At timestep:     363 / 1000At timestep:     378 / 1000At timestep:     397 / 1000At timestep:     428 / 1000At timestep:     451 / 1000At timestep:     471 / 1000At timestep:     521 / 1000At timestep:     534 / 1000At timestep:     560 / 1000At timestep:     584 / 1000At timestep:     603 / 1000At timestep:     633 / 1000At timestep:     650 / 1000At timestep:     673 / 1000At timestep:     701 / 1000At timestep:     718 / 1000At timestep:     736 / 1000At timestep:     757 / 1000At timestep:     773 / 1000At timestep:     795 / 1000At timestep:     820 / 1000At timestep:     842 / 1000At timestep:     859 / 1000At timestep:     880 / 1000At timestep:     904 / 1000At timestep:     938 / 1000At timestep:     976 / 1000At timestep:     991 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -16.8700008392334
Train_BestReturn : -16.8700008392334
TimeSinceStart : 1039.3884148597717
Exploration Critic Loss : 1051370.0
Exploitation Critic Loss : 0.07103101909160614
Exploration Model Loss : 20.188980102539062
Actor Loss : 0.5292792320251465
Eval_AverageReturn : -22.418603897094727
Eval_StdReturn : 8.565024375915527
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 23.3953488372093
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -16.889999
best mean reward -16.870001
running time 1080.389566
At timestep:     20 / 1000At timestep:     38 / 1000At timestep:     54 / 1000At timestep:     73 / 1000At timestep:     95 / 1000At timestep:     111 / 1000At timestep:     128 / 1000At timestep:     144 / 1000At timestep:     161 / 1000At timestep:     181 / 1000At timestep:     194 / 1000At timestep:     218 / 1000At timestep:     242 / 1000At timestep:     265 / 1000At timestep:     287 / 1000At timestep:     304 / 1000At timestep:     329 / 1000At timestep:     352 / 1000At timestep:     370 / 1000At timestep:     403 / 1000At timestep:     417 / 1000At timestep:     436 / 1000At timestep:     453 / 1000At timestep:     466 / 1000At timestep:     478 / 1000At timestep:     507 / 1000At timestep:     526 / 1000At timestep:     549 / 1000At timestep:     565 / 1000At timestep:     587 / 1000At timestep:     602 / 1000At timestep:     624 / 1000At timestep:     643 / 1000At timestep:     661 / 1000At timestep:     675 / 1000At timestep:     691 / 1000At timestep:     718 / 1000At timestep:     736 / 1000At timestep:     752 / 1000At timestep:     774 / 1000At timestep:     789 / 1000At timestep:     805 / 1000At timestep:     829 / 1000At timestep:     847 / 1000At timestep:     869 / 1000At timestep:     887 / 1000At timestep:     913 / 1000At timestep:     929 / 1000At timestep:     944 / 1000At timestep:     977 / 1000At timestep:     996 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -16.889999389648438
Train_BestReturn : -16.8700008392334
TimeSinceStart : 1080.3895659446716
Exploration Critic Loss : 763273.25
Exploitation Critic Loss : 0.07286860048770905
Exploration Model Loss : 20.188400268554688
Actor Loss : 0.5200619101524353
Eval_AverageReturn : -18.653846740722656
Eval_StdReturn : 4.72263240814209
Eval_MaxReturn : -11.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 19.653846153846153
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -17.420000
best mean reward -16.870001
running time 1123.293581
At timestep:     19 / 1000At timestep:     35 / 1000At timestep:     47 / 1000At timestep:     59 / 1000At timestep:     92 / 1000At timestep:     106 / 1000At timestep:     133 / 1000At timestep:     156 / 1000At timestep:     172 / 1000At timestep:     187 / 1000At timestep:     203 / 1000At timestep:     222 / 1000At timestep:     239 / 1000At timestep:     258 / 1000At timestep:     283 / 1000At timestep:     298 / 1000At timestep:     322 / 1000At timestep:     347 / 1000At timestep:     367 / 1000At timestep:     379 / 1000At timestep:     403 / 1000At timestep:     424 / 1000At timestep:     446 / 1000At timestep:     469 / 1000At timestep:     482 / 1000At timestep:     498 / 1000At timestep:     531 / 1000At timestep:     552 / 1000At timestep:     569 / 1000At timestep:     584 / 1000At timestep:     603 / 1000At timestep:     621 / 1000At timestep:     658 / 1000At timestep:     680 / 1000At timestep:     694 / 1000At timestep:     711 / 1000At timestep:     742 / 1000At timestep:     761 / 1000At timestep:     774 / 1000At timestep:     793 / 1000At timestep:     830 / 1000At timestep:     844 / 1000At timestep:     866 / 1000At timestep:     893 / 1000At timestep:     911 / 1000At timestep:     948 / 1000At timestep:     967 / 1000At timestep:     981 / 1000At timestep:     996 / 1000At timestep:     1036 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -17.420000076293945
Train_BestReturn : -16.8700008392334
TimeSinceStart : 1123.293580532074
Exploration Critic Loss : 958332.375
Exploitation Critic Loss : 0.06975823640823364
Exploration Model Loss : 20.18841552734375
Actor Loss : 0.5452191829681396
Eval_AverageReturn : -19.719999313354492
Eval_StdReturn : 7.144340515136719
Eval_MaxReturn : -11.0
Eval_MinReturn : -39.0
Eval_AverageEpLen : 20.72
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -17.080000
best mean reward -16.870001
running time 1165.869496
At timestep:     22 / 1000At timestep:     38 / 1000At timestep:     60 / 1000At timestep:     78 / 1000At timestep:     97 / 1000At timestep:     114 / 1000At timestep:     133 / 1000At timestep:     151 / 1000At timestep:     188 / 1000At timestep:     206 / 1000At timestep:     243 / 1000At timestep:     262 / 1000At timestep:     288 / 1000At timestep:     319 / 1000At timestep:     340 / 1000At timestep:     368 / 1000At timestep:     404 / 1000At timestep:     424 / 1000At timestep:     443 / 1000At timestep:     471 / 1000At timestep:     500 / 1000At timestep:     514 / 1000At timestep:     555 / 1000At timestep:     568 / 1000At timestep:     583 / 1000At timestep:     615 / 1000At timestep:     627 / 1000At timestep:     646 / 1000At timestep:     696 / 1000At timestep:     709 / 1000At timestep:     729 / 1000At timestep:     746 / 1000At timestep:     763 / 1000At timestep:     790 / 1000At timestep:     807 / 1000At timestep:     832 / 1000At timestep:     849 / 1000At timestep:     874 / 1000At timestep:     891 / 1000At timestep:     936 / 1000At timestep:     956 / 1000At timestep:     978 / 1000At timestep:     999 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -17.079999923706055
Train_BestReturn : -16.8700008392334
TimeSinceStart : 1165.8694961071014
Exploration Critic Loss : 596347.75
Exploitation Critic Loss : 0.12608936429023743
Exploration Model Loss : 20.188430786132812
Actor Loss : 0.4577330946922302
Eval_AverageReturn : -22.340909957885742
Eval_StdReturn : 8.728072166442871
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 23.318181818181817
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -17.180000
best mean reward -16.870001
running time 1207.161150
At timestep:     23 / 1000At timestep:     36 / 1000At timestep:     66 / 1000At timestep:     97 / 1000At timestep:     113 / 1000At timestep:     130 / 1000At timestep:     146 / 1000At timestep:     163 / 1000At timestep:     178 / 1000At timestep:     190 / 1000At timestep:     208 / 1000At timestep:     232 / 1000At timestep:     247 / 1000At timestep:     263 / 1000At timestep:     285 / 1000At timestep:     306 / 1000At timestep:     322 / 1000At timestep:     342 / 1000At timestep:     372 / 1000At timestep:     395 / 1000At timestep:     421 / 1000At timestep:     435 / 1000At timestep:     451 / 1000At timestep:     464 / 1000At timestep:     486 / 1000At timestep:     508 / 1000At timestep:     523 / 1000At timestep:     554 / 1000At timestep:     597 / 1000At timestep:     611 / 1000At timestep:     631 / 1000At timestep:     662 / 1000At timestep:     690 / 1000At timestep:     722 / 1000At timestep:     766 / 1000At timestep:     779 / 1000At timestep:     809 / 1000At timestep:     825 / 1000At timestep:     850 / 1000At timestep:     866 / 1000At timestep:     882 / 1000At timestep:     894 / 1000At timestep:     921 / 1000At timestep:     945 / 1000At timestep:     968 / 1000At timestep:     995 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -17.18000030517578
Train_BestReturn : -16.8700008392334
TimeSinceStart : 1207.1611495018005
Exploration Critic Loss : 945928.9375
Exploitation Critic Loss : 0.11596149951219559
Exploration Model Loss : 20.18841552734375
Actor Loss : 0.5794435143470764
Eval_AverageReturn : -20.744680404663086
Eval_StdReturn : 7.558194637298584
Eval_MaxReturn : -11.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 21.74468085106383
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -17.340000
best mean reward -16.870001
running time 1250.163576
At timestep:     38 / 1000At timestep:     70 / 1000At timestep:     86 / 1000At timestep:     109 / 1000At timestep:     128 / 1000At timestep:     146 / 1000At timestep:     160 / 1000At timestep:     179 / 1000At timestep:     229 / 1000At timestep:     253 / 1000At timestep:     276 / 1000At timestep:     295 / 1000At timestep:     327 / 1000At timestep:     346 / 1000At timestep:     367 / 1000At timestep:     380 / 1000At timestep:     394 / 1000At timestep:     410 / 1000At timestep:     440 / 1000At timestep:     470 / 1000At timestep:     489 / 1000At timestep:     506 / 1000At timestep:     520 / 1000At timestep:     558 / 1000At timestep:     569 / 1000At timestep:     588 / 1000At timestep:     604 / 1000At timestep:     619 / 1000At timestep:     631 / 1000At timestep:     665 / 1000At timestep:     685 / 1000At timestep:     700 / 1000At timestep:     723 / 1000At timestep:     737 / 1000At timestep:     760 / 1000At timestep:     796 / 1000At timestep:     813 / 1000At timestep:     834 / 1000At timestep:     849 / 1000At timestep:     874 / 1000At timestep:     897 / 1000At timestep:     912 / 1000At timestep:     929 / 1000At timestep:     952 / 1000At timestep:     966 / 1000At timestep:     990 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -17.34000015258789
Train_BestReturn : -16.8700008392334
TimeSinceStart : 1250.16357588768
Exploration Critic Loss : 778251.5
Exploitation Critic Loss : 0.1411689817905426
Exploration Model Loss : 20.187911987304688
Actor Loss : 0.5036571025848389
Eval_AverageReturn : -20.489360809326172
Eval_StdReturn : 8.14462947845459
Eval_MaxReturn : -10.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 21.46808510638298
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -16.840000
best mean reward -16.840000
running time 1292.283857
At timestep:     25 / 1000At timestep:     42 / 1000At timestep:     68 / 1000At timestep:     80 / 1000At timestep:     94 / 1000At timestep:     112 / 1000At timestep:     133 / 1000At timestep:     164 / 1000At timestep:     179 / 1000At timestep:     198 / 1000At timestep:     216 / 1000At timestep:     226 / 1000At timestep:     238 / 1000At timestep:     253 / 1000At timestep:     279 / 1000At timestep:     294 / 1000At timestep:     313 / 1000At timestep:     330 / 1000At timestep:     352 / 1000At timestep:     387 / 1000At timestep:     415 / 1000At timestep:     444 / 1000At timestep:     468 / 1000At timestep:     482 / 1000At timestep:     504 / 1000At timestep:     520 / 1000At timestep:     541 / 1000At timestep:     565 / 1000At timestep:     585 / 1000At timestep:     605 / 1000At timestep:     624 / 1000At timestep:     639 / 1000At timestep:     657 / 1000At timestep:     670 / 1000At timestep:     695 / 1000At timestep:     712 / 1000At timestep:     745 / 1000At timestep:     762 / 1000At timestep:     781 / 1000At timestep:     797 / 1000At timestep:     813 / 1000At timestep:     836 / 1000At timestep:     881 / 1000At timestep:     902 / 1000At timestep:     922 / 1000At timestep:     943 / 1000At timestep:     961 / 1000At timestep:     974 / 1000At timestep:     995 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -16.84000015258789
Train_BestReturn : -16.84000015258789
TimeSinceStart : 1292.2838566303253
Exploration Critic Loss : 637116.5
Exploitation Critic Loss : 0.08732491731643677
Exploration Model Loss : 20.187301635742188
Actor Loss : 0.49890944361686707
Eval_AverageReturn : -19.280000686645508
Eval_StdReturn : 6.418847560882568
Eval_MaxReturn : -9.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 20.28
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -17.910000
best mean reward -16.840000
running time 1334.790624
At timestep:     22 / 1000At timestep:     47 / 1000At timestep:     64 / 1000At timestep:     78 / 1000At timestep:     94 / 1000At timestep:     112 / 1000At timestep:     127 / 1000At timestep:     141 / 1000At timestep:     168 / 1000At timestep:     187 / 1000At timestep:     201 / 1000At timestep:     228 / 1000At timestep:     246 / 1000At timestep:     264 / 1000At timestep:     283 / 1000At timestep:     311 / 1000At timestep:     330 / 1000At timestep:     350 / 1000At timestep:     375 / 1000At timestep:     403 / 1000At timestep:     440 / 1000At timestep:     458 / 1000At timestep:     477 / 1000At timestep:     506 / 1000At timestep:     519 / 1000At timestep:     537 / 1000At timestep:     558 / 1000At timestep:     573 / 1000At timestep:     595 / 1000At timestep:     618 / 1000At timestep:     653 / 1000At timestep:     670 / 1000At timestep:     701 / 1000At timestep:     721 / 1000At timestep:     734 / 1000At timestep:     750 / 1000At timestep:     767 / 1000At timestep:     781 / 1000At timestep:     804 / 1000At timestep:     823 / 1000At timestep:     841 / 1000At timestep:     867 / 1000At timestep:     883 / 1000At timestep:     901 / 1000At timestep:     918 / 1000At timestep:     945 / 1000At timestep:     974 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -17.90999984741211
Train_BestReturn : -16.84000015258789
TimeSinceStart : 1334.790623664856
Exploration Critic Loss : 533668.6875
Exploitation Critic Loss : 0.07497541606426239
Exploration Model Loss : 20.186904907226562
Actor Loss : 0.5397922992706299
Eval_AverageReturn : -19.95833396911621
Eval_StdReturn : 5.940532684326172
Eval_MaxReturn : -12.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 20.958333333333332
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -17.870001
best mean reward -16.840000
running time 1386.968707
At timestep:     18 / 1000At timestep:     39 / 1000At timestep:     60 / 1000At timestep:     74 / 1000At timestep:     96 / 1000At timestep:     106 / 1000At timestep:     124 / 1000At timestep:     142 / 1000At timestep:     165 / 1000At timestep:     181 / 1000At timestep:     195 / 1000At timestep:     214 / 1000At timestep:     239 / 1000At timestep:     275 / 1000At timestep:     296 / 1000At timestep:     320 / 1000At timestep:     342 / 1000At timestep:     364 / 1000At timestep:     390 / 1000At timestep:     431 / 1000At timestep:     450 / 1000At timestep:     469 / 1000At timestep:     490 / 1000At timestep:     514 / 1000At timestep:     532 / 1000At timestep:     551 / 1000At timestep:     573 / 1000At timestep:     601 / 1000At timestep:     621 / 1000At timestep:     634 / 1000At timestep:     650 / 1000At timestep:     679 / 1000At timestep:     699 / 1000At timestep:     718 / 1000At timestep:     736 / 1000At timestep:     758 / 1000At timestep:     803 / 1000At timestep:     820 / 1000At timestep:     836 / 1000At timestep:     851 / 1000At timestep:     874 / 1000At timestep:     890 / 1000At timestep:     908 / 1000At timestep:     926 / 1000At timestep:     944 / 1000At timestep:     959 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -17.8700008392334
Train_BestReturn : -16.84000015258789
TimeSinceStart : 1386.9687070846558
Exploration Critic Loss : 913452.375
Exploitation Critic Loss : 0.133297860622406
Exploration Model Loss : 20.186386108398438
Actor Loss : 0.548262357711792
Eval_AverageReturn : -20.382978439331055
Eval_StdReturn : 7.393364429473877
Eval_MaxReturn : -9.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 21.382978723404257
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -17.830000
best mean reward -16.840000
running time 1438.823604
At timestep:     26 / 1000At timestep:     52 / 1000At timestep:     70 / 1000At timestep:     85 / 1000At timestep:     100 / 1000At timestep:     119 / 1000At timestep:     151 / 1000At timestep:     171 / 1000At timestep:     187 / 1000At timestep:     201 / 1000At timestep:     216 / 1000At timestep:     233 / 1000At timestep:     255 / 1000At timestep:     282 / 1000At timestep:     296 / 1000At timestep:     309 / 1000At timestep:     327 / 1000At timestep:     351 / 1000At timestep:     367 / 1000At timestep:     391 / 1000At timestep:     408 / 1000At timestep:     432 / 1000At timestep:     460 / 1000At timestep:     475 / 1000At timestep:     497 / 1000At timestep:     517 / 1000At timestep:     546 / 1000At timestep:     575 / 1000At timestep:     593 / 1000At timestep:     623 / 1000At timestep:     648 / 1000At timestep:     663 / 1000At timestep:     678 / 1000At timestep:     698 / 1000At timestep:     716 / 1000At timestep:     734 / 1000At timestep:     761 / 1000At timestep:     774 / 1000At timestep:     793 / 1000At timestep:     813 / 1000At timestep:     831 / 1000At timestep:     849 / 1000At timestep:     866 / 1000At timestep:     887 / 1000At timestep:     916 / 1000At timestep:     937 / 1000At timestep:     953 / 1000At timestep:     965 / 1000At timestep:     989 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -17.829999923706055
Train_BestReturn : -16.84000015258789
TimeSinceStart : 1438.8236043453217
Exploration Critic Loss : 881063.8125
Exploitation Critic Loss : 0.15867897868156433
Exploration Model Loss : 20.185791015625
Actor Loss : 0.5016474723815918
Eval_AverageReturn : -19.18000030517578
Eval_StdReturn : 5.1134724617004395
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 20.18
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -18.139999
best mean reward -16.840000
running time 1492.580812
At timestep:     14 / 1000At timestep:     27 / 1000At timestep:     44 / 1000At timestep:     55 / 1000At timestep:     68 / 1000At timestep:     87 / 1000At timestep:     105 / 1000At timestep:     130 / 1000At timestep:     150 / 1000At timestep:     170 / 1000At timestep:     186 / 1000At timestep:     203 / 1000At timestep:     216 / 1000At timestep:     232 / 1000At timestep:     257 / 1000At timestep:     276 / 1000At timestep:     291 / 1000At timestep:     306 / 1000At timestep:     324 / 1000At timestep:     341 / 1000At timestep:     370 / 1000At timestep:     420 / 1000At timestep:     443 / 1000At timestep:     458 / 1000At timestep:     479 / 1000At timestep:     505 / 1000At timestep:     521 / 1000At timestep:     537 / 1000At timestep:     552 / 1000At timestep:     572 / 1000At timestep:     592 / 1000At timestep:     606 / 1000At timestep:     621 / 1000At timestep:     648 / 1000At timestep:     674 / 1000At timestep:     689 / 1000At timestep:     714 / 1000At timestep:     744 / 1000At timestep:     768 / 1000At timestep:     794 / 1000At timestep:     816 / 1000At timestep:     841 / 1000At timestep:     871 / 1000At timestep:     899 / 1000At timestep:     916 / 1000At timestep:     952 / 1000At timestep:     970 / 1000At timestep:     997 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -18.139999389648438
Train_BestReturn : -16.84000015258789
TimeSinceStart : 1492.580812215805
Exploration Critic Loss : 679453.6875
Exploitation Critic Loss : 0.07325518131256104
Exploration Model Loss : 20.185256958007812
Actor Loss : 0.510940670967102
Eval_AverageReturn : -19.714284896850586
Eval_StdReturn : 7.065292835235596
Eval_MaxReturn : -10.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 20.693877551020407
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -17.440001
best mean reward -16.840000
running time 1555.540454
At timestep:     21 / 1000At timestep:     35 / 1000At timestep:     51 / 1000At timestep:     71 / 1000At timestep:     89 / 1000At timestep:     119 / 1000At timestep:     138 / 1000At timestep:     165 / 1000At timestep:     189 / 1000At timestep:     201 / 1000At timestep:     223 / 1000At timestep:     253 / 1000At timestep:     266 / 1000At timestep:     284 / 1000At timestep:     296 / 1000At timestep:     332 / 1000At timestep:     353 / 1000At timestep:     370 / 1000At timestep:     389 / 1000At timestep:     409 / 1000At timestep:     433 / 1000At timestep:     449 / 1000At timestep:     492 / 1000At timestep:     511 / 1000At timestep:     523 / 1000At timestep:     543 / 1000At timestep:     560 / 1000At timestep:     586 / 1000At timestep:     612 / 1000At timestep:     626 / 1000At timestep:     649 / 1000At timestep:     662 / 1000At timestep:     691 / 1000At timestep:     716 / 1000At timestep:     730 / 1000At timestep:     753 / 1000At timestep:     768 / 1000At timestep:     783 / 1000At timestep:     800 / 1000At timestep:     816 / 1000At timestep:     839 / 1000At timestep:     855 / 1000At timestep:     868 / 1000At timestep:     886 / 1000At timestep:     907 / 1000At timestep:     923 / 1000At timestep:     944 / 1000At timestep:     962 / 1000At timestep:     978 / 1000At timestep:     998 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -17.440000534057617
Train_BestReturn : -16.84000015258789
TimeSinceStart : 1555.5404541492462
Exploration Critic Loss : 467709.5
Exploitation Critic Loss : 0.08353964984416962
Exploration Model Loss : 20.184829711914062
Actor Loss : 0.5196279287338257
Eval_AverageReturn : -18.921567916870117
Eval_StdReturn : 6.138413906097412
Eval_MaxReturn : -11.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 19.92156862745098
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -16.780001
best mean reward -16.780001
running time 1620.499361
At timestep:     18 / 1000At timestep:     31 / 1000At timestep:     53 / 1000At timestep:     70 / 1000At timestep:     96 / 1000At timestep:     110 / 1000At timestep:     122 / 1000At timestep:     138 / 1000At timestep:     161 / 1000At timestep:     180 / 1000At timestep:     196 / 1000At timestep:     214 / 1000At timestep:     248 / 1000At timestep:     263 / 1000At timestep:     281 / 1000At timestep:     304 / 1000At timestep:     321 / 1000At timestep:     338 / 1000At timestep:     366 / 1000At timestep:     383 / 1000At timestep:     400 / 1000At timestep:     419 / 1000At timestep:     434 / 1000At timestep:     451 / 1000At timestep:     466 / 1000At timestep:     484 / 1000At timestep:     507 / 1000At timestep:     523 / 1000At timestep:     535 / 1000At timestep:     556 / 1000At timestep:     571 / 1000At timestep:     583 / 1000At timestep:     601 / 1000At timestep:     627 / 1000At timestep:     646 / 1000At timestep:     665 / 1000At timestep:     684 / 1000At timestep:     706 / 1000At timestep:     725 / 1000At timestep:     749 / 1000At timestep:     775 / 1000At timestep:     788 / 1000At timestep:     816 / 1000At timestep:     832 / 1000At timestep:     867 / 1000At timestep:     879 / 1000At timestep:     898 / 1000At timestep:     924 / 1000At timestep:     944 / 1000At timestep:     958 / 1000At timestep:     977 / 1000At timestep:     997 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -16.780000686645508
Train_BestReturn : -16.780000686645508
TimeSinceStart : 1620.4993605613708
Exploration Critic Loss : 425683.6875
Exploitation Critic Loss : 0.10960615426301956
Exploration Model Loss : 20.184371948242188
Actor Loss : 0.4289650321006775
Eval_AverageReturn : -18.226415634155273
Eval_StdReturn : 5.119852066040039
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.22641509433962
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -17.230000
best mean reward -16.780001
running time 1684.020508
At timestep:     21 / 1000At timestep:     42 / 1000At timestep:     64 / 1000At timestep:     78 / 1000At timestep:     94 / 1000At timestep:     112 / 1000At timestep:     130 / 1000At timestep:     153 / 1000At timestep:     175 / 1000At timestep:     190 / 1000At timestep:     208 / 1000At timestep:     230 / 1000At timestep:     246 / 1000At timestep:     268 / 1000At timestep:     286 / 1000At timestep:     317 / 1000At timestep:     333 / 1000At timestep:     354 / 1000At timestep:     377 / 1000At timestep:     403 / 1000At timestep:     421 / 1000At timestep:     448 / 1000At timestep:     465 / 1000At timestep:     487 / 1000At timestep:     514 / 1000At timestep:     531 / 1000At timestep:     547 / 1000At timestep:     560 / 1000At timestep:     581 / 1000At timestep:     596 / 1000At timestep:     614 / 1000At timestep:     635 / 1000At timestep:     678 / 1000At timestep:     701 / 1000At timestep:     720 / 1000At timestep:     741 / 1000At timestep:     754 / 1000At timestep:     771 / 1000At timestep:     798 / 1000At timestep:     816 / 1000At timestep:     840 / 1000At timestep:     860 / 1000At timestep:     881 / 1000At timestep:     897 / 1000At timestep:     913 / 1000At timestep:     947 / 1000At timestep:     964 / 1000At timestep:     981 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -17.229999542236328
Train_BestReturn : -16.780000686645508
TimeSinceStart : 1684.0205080509186
Exploration Critic Loss : 548779.9375
Exploitation Critic Loss : 0.08351632952690125
Exploration Model Loss : 20.184097290039062
Actor Loss : 0.4957875907421112
Eval_AverageReturn : -19.4489803314209
Eval_StdReturn : 5.428801536560059
Eval_MaxReturn : -12.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 20.448979591836736
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -17.600000
best mean reward -16.780001
running time 1747.807048
At timestep:     19 / 1000At timestep:     36 / 1000At timestep:     58 / 1000At timestep:     79 / 1000At timestep:     91 / 1000At timestep:     106 / 1000At timestep:     126 / 1000At timestep:     150 / 1000At timestep:     174 / 1000At timestep:     189 / 1000At timestep:     211 / 1000At timestep:     225 / 1000At timestep:     246 / 1000At timestep:     261 / 1000At timestep:     284 / 1000At timestep:     304 / 1000At timestep:     324 / 1000At timestep:     354 / 1000At timestep:     379 / 1000At timestep:     394 / 1000At timestep:     415 / 1000At timestep:     430 / 1000At timestep:     446 / 1000At timestep:     468 / 1000At timestep:     484 / 1000At timestep:     518 / 1000At timestep:     543 / 1000At timestep:     558 / 1000At timestep:     584 / 1000At timestep:     603 / 1000At timestep:     622 / 1000At timestep:     635 / 1000At timestep:     652 / 1000At timestep:     670 / 1000At timestep:     708 / 1000At timestep:     723 / 1000At timestep:     746 / 1000At timestep:     778 / 1000At timestep:     801 / 1000At timestep:     829 / 1000At timestep:     858 / 1000At timestep:     875 / 1000At timestep:     895 / 1000At timestep:     915 / 1000At timestep:     930 / 1000At timestep:     957 / 1000At timestep:     978 / 1000At timestep:     995 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -17.600000381469727
Train_BestReturn : -16.780000686645508
TimeSinceStart : 1747.8070483207703
Exploration Critic Loss : 501218.4375
Exploitation Critic Loss : 0.10083289444446564
Exploration Model Loss : 20.183975219726562
Actor Loss : 0.5175660848617554
Eval_AverageReturn : -19.836734771728516
Eval_StdReturn : 5.614656448364258
Eval_MaxReturn : -11.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 20.836734693877553
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -16.809999
best mean reward -16.780001
running time 1812.489557
At timestep:     14 / 1000At timestep:     31 / 1000At timestep:     52 / 1000At timestep:     69 / 1000At timestep:     86 / 1000At timestep:     102 / 1000At timestep:     117 / 1000At timestep:     132 / 1000At timestep:     148 / 1000At timestep:     161 / 1000At timestep:     178 / 1000At timestep:     195 / 1000At timestep:     222 / 1000At timestep:     247 / 1000At timestep:     266 / 1000At timestep:     287 / 1000At timestep:     303 / 1000At timestep:     318 / 1000At timestep:     332 / 1000At timestep:     359 / 1000At timestep:     397 / 1000At timestep:     422 / 1000At timestep:     442 / 1000At timestep:     466 / 1000At timestep:     484 / 1000At timestep:     514 / 1000At timestep:     538 / 1000At timestep:     576 / 1000At timestep:     595 / 1000At timestep:     609 / 1000At timestep:     628 / 1000At timestep:     648 / 1000At timestep:     670 / 1000At timestep:     687 / 1000At timestep:     706 / 1000At timestep:     727 / 1000At timestep:     743 / 1000At timestep:     755 / 1000At timestep:     786 / 1000At timestep:     806 / 1000At timestep:     821 / 1000At timestep:     845 / 1000At timestep:     861 / 1000At timestep:     875 / 1000At timestep:     904 / 1000At timestep:     925 / 1000At timestep:     942 / 1000At timestep:     956 / 1000At timestep:     976 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -16.809999465942383
Train_BestReturn : -16.780000686645508
TimeSinceStart : 1812.4895570278168
Exploration Critic Loss : 566143.5
Exploitation Critic Loss : 0.0997140109539032
Exploration Model Loss : 20.184005737304688
Actor Loss : 0.47366660833358765
Eval_AverageReturn : -19.079999923706055
Eval_StdReturn : 5.935789585113525
Eval_MaxReturn : -11.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 20.08
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -16.860001
best mean reward -16.780001
running time 1876.441056
At timestep:     29 / 1000At timestep:     51 / 1000At timestep:     83 / 1000At timestep:     96 / 1000At timestep:     110 / 1000At timestep:     131 / 1000At timestep:     149 / 1000At timestep:     166 / 1000At timestep:     184 / 1000At timestep:     206 / 1000At timestep:     223 / 1000At timestep:     239 / 1000At timestep:     264 / 1000At timestep:     283 / 1000At timestep:     311 / 1000At timestep:     327 / 1000At timestep:     347 / 1000At timestep:     361 / 1000At timestep:     378 / 1000At timestep:     405 / 1000At timestep:     422 / 1000At timestep:     435 / 1000At timestep:     458 / 1000At timestep:     473 / 1000At timestep:     511 / 1000At timestep:     534 / 1000At timestep:     552 / 1000At timestep:     575 / 1000At timestep:     593 / 1000At timestep:     615 / 1000At timestep:     640 / 1000At timestep:     661 / 1000At timestep:     676 / 1000At timestep:     694 / 1000At timestep:     714 / 1000At timestep:     745 / 1000At timestep:     762 / 1000At timestep:     780 / 1000At timestep:     796 / 1000At timestep:     811 / 1000At timestep:     825 / 1000At timestep:     841 / 1000At timestep:     853 / 1000At timestep:     872 / 1000At timestep:     896 / 1000At timestep:     916 / 1000At timestep:     929 / 1000At timestep:     947 / 1000At timestep:     969 / 1000At timestep:     986 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -16.860000610351562
Train_BestReturn : -16.780000686645508
TimeSinceStart : 1876.4410557746887
Exploration Critic Loss : 506823.0625
Exploitation Critic Loss : 0.11898700147867203
Exploration Model Loss : 20.183990478515625
Actor Loss : 0.5047009587287903
Eval_AverageReturn : -18.705883026123047
Eval_StdReturn : 5.299996376037598
Eval_MaxReturn : -11.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 19.705882352941178
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -17.420000
best mean reward -16.780001
running time 1939.671647
At timestep:     12 / 1000At timestep:     29 / 1000At timestep:     44 / 1000At timestep:     61 / 1000At timestep:     77 / 1000At timestep:     91 / 1000At timestep:     116 / 1000At timestep:     140 / 1000At timestep:     158 / 1000At timestep:     173 / 1000At timestep:     185 / 1000At timestep:     216 / 1000At timestep:     235 / 1000At timestep:     248 / 1000At timestep:     264 / 1000At timestep:     285 / 1000At timestep:     317 / 1000At timestep:     343 / 1000At timestep:     362 / 1000At timestep:     376 / 1000At timestep:     403 / 1000At timestep:     418 / 1000At timestep:     433 / 1000At timestep:     467 / 1000At timestep:     486 / 1000At timestep:     526 / 1000At timestep:     553 / 1000At timestep:     570 / 1000At timestep:     589 / 1000At timestep:     614 / 1000At timestep:     641 / 1000At timestep:     659 / 1000At timestep:     678 / 1000At timestep:     694 / 1000At timestep:     716 / 1000At timestep:     744 / 1000At timestep:     769 / 1000At timestep:     784 / 1000At timestep:     806 / 1000At timestep:     831 / 1000At timestep:     849 / 1000At timestep:     869 / 1000At timestep:     896 / 1000At timestep:     913 / 1000At timestep:     927 / 1000At timestep:     938 / 1000At timestep:     957 / 1000At timestep:     976 / 1000At timestep:     993 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -17.420000076293945
Train_BestReturn : -16.780000686645508
TimeSinceStart : 1939.6716468334198
Exploration Critic Loss : 464264.03125
Exploitation Critic Loss : 0.07761680334806442
Exploration Model Loss : 20.184005737304688
Actor Loss : 0.43837207555770874
Eval_AverageReturn : -19.299999237060547
Eval_StdReturn : 6.139217853546143
Eval_MaxReturn : -10.0
Eval_MinReturn : -39.0
Eval_AverageEpLen : 20.3
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -17.790001
best mean reward -16.780001
running time 2005.065616
At timestep:     18 / 1000At timestep:     37 / 1000At timestep:     71 / 1000At timestep:     86 / 1000At timestep:     104 / 1000At timestep:     131 / 1000At timestep:     150 / 1000At timestep:     184 / 1000At timestep:     200 / 1000At timestep:     223 / 1000At timestep:     248 / 1000At timestep:     267 / 1000At timestep:     291 / 1000At timestep:     311 / 1000At timestep:     338 / 1000At timestep:     352 / 1000At timestep:     372 / 1000At timestep:     401 / 1000At timestep:     430 / 1000At timestep:     444 / 1000At timestep:     464 / 1000At timestep:     489 / 1000At timestep:     503 / 1000At timestep:     519 / 1000At timestep:     545 / 1000At timestep:     564 / 1000At timestep:     589 / 1000At timestep:     601 / 1000At timestep:     624 / 1000At timestep:     638 / 1000At timestep:     684 / 1000At timestep:     698 / 1000At timestep:     719 / 1000At timestep:     737 / 1000At timestep:     752 / 1000At timestep:     780 / 1000At timestep:     807 / 1000At timestep:     831 / 1000At timestep:     854 / 1000At timestep:     879 / 1000At timestep:     897 / 1000At timestep:     913 / 1000At timestep:     932 / 1000At timestep:     958 / 1000At timestep:     984 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -17.790000915527344
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2005.0656161308289
Exploration Critic Loss : 402131.8125
Exploitation Critic Loss : 0.09014663100242615
Exploration Model Loss : 20.183975219726562
Actor Loss : 0.5173213481903076
Eval_AverageReturn : -20.7608699798584
Eval_StdReturn : 6.491415023803711
Eval_MaxReturn : -11.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 21.76086956521739
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -17.240000
best mean reward -16.780001
running time 2068.936057
At timestep:     12 / 1000At timestep:     29 / 1000At timestep:     52 / 1000At timestep:     73 / 1000At timestep:     97 / 1000At timestep:     122 / 1000At timestep:     135 / 1000At timestep:     149 / 1000At timestep:     165 / 1000At timestep:     188 / 1000At timestep:     204 / 1000At timestep:     221 / 1000At timestep:     244 / 1000At timestep:     255 / 1000At timestep:     275 / 1000At timestep:     291 / 1000At timestep:     307 / 1000At timestep:     341 / 1000At timestep:     365 / 1000At timestep:     376 / 1000At timestep:     400 / 1000At timestep:     416 / 1000At timestep:     443 / 1000At timestep:     456 / 1000At timestep:     468 / 1000At timestep:     499 / 1000At timestep:     527 / 1000At timestep:     539 / 1000At timestep:     553 / 1000At timestep:     575 / 1000At timestep:     595 / 1000At timestep:     611 / 1000At timestep:     635 / 1000At timestep:     652 / 1000At timestep:     671 / 1000At timestep:     701 / 1000At timestep:     714 / 1000At timestep:     738 / 1000At timestep:     756 / 1000At timestep:     775 / 1000At timestep:     790 / 1000At timestep:     815 / 1000At timestep:     832 / 1000At timestep:     856 / 1000At timestep:     899 / 1000At timestep:     921 / 1000At timestep:     953 / 1000At timestep:     969 / 1000At timestep:     985 / 1000At timestep:     999 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -17.239999771118164
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2068.936057329178
Exploration Critic Loss : 296304.5625
Exploitation Critic Loss : 0.11105126142501831
Exploration Model Loss : 20.183990478515625
Actor Loss : 0.49178630113601685
Eval_AverageReturn : -18.86274528503418
Eval_StdReturn : 6.603682518005371
Eval_MaxReturn : -10.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 19.862745098039216
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -17.389999
best mean reward -16.780001
running time 2133.131989
At timestep:     17 / 1000At timestep:     36 / 1000At timestep:     47 / 1000At timestep:     65 / 1000At timestep:     86 / 1000At timestep:     105 / 1000At timestep:     122 / 1000At timestep:     136 / 1000At timestep:     148 / 1000At timestep:     161 / 1000At timestep:     178 / 1000At timestep:     195 / 1000At timestep:     221 / 1000At timestep:     238 / 1000At timestep:     260 / 1000At timestep:     284 / 1000At timestep:     301 / 1000At timestep:     319 / 1000At timestep:     335 / 1000At timestep:     354 / 1000At timestep:     377 / 1000At timestep:     396 / 1000At timestep:     418 / 1000At timestep:     438 / 1000At timestep:     456 / 1000At timestep:     480 / 1000At timestep:     505 / 1000At timestep:     516 / 1000At timestep:     526 / 1000At timestep:     547 / 1000At timestep:     569 / 1000At timestep:     592 / 1000At timestep:     612 / 1000At timestep:     651 / 1000At timestep:     667 / 1000At timestep:     688 / 1000At timestep:     706 / 1000At timestep:     722 / 1000At timestep:     740 / 1000At timestep:     756 / 1000At timestep:     772 / 1000At timestep:     791 / 1000At timestep:     808 / 1000At timestep:     828 / 1000At timestep:     842 / 1000At timestep:     858 / 1000At timestep:     878 / 1000At timestep:     898 / 1000At timestep:     922 / 1000At timestep:     940 / 1000At timestep:     959 / 1000At timestep:     979 / 1000At timestep:     995 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -17.389999389648438
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2133.1319892406464
Exploration Critic Loss : 397467.375
Exploitation Critic Loss : 0.12002536654472351
Exploration Model Loss : 20.183990478515625
Actor Loss : 0.4353076219558716
Eval_AverageReturn : -17.759260177612305
Eval_StdReturn : 4.4635772705078125
Eval_MaxReturn : -9.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 18.75925925925926
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -18.190001
best mean reward -16.780001
running time 2196.323851
At timestep:     23 / 1000At timestep:     46 / 1000At timestep:     63 / 1000At timestep:     82 / 1000At timestep:     104 / 1000At timestep:     126 / 1000At timestep:     145 / 1000At timestep:     160 / 1000At timestep:     188 / 1000At timestep:     209 / 1000At timestep:     227 / 1000At timestep:     243 / 1000At timestep:     269 / 1000At timestep:     285 / 1000At timestep:     301 / 1000At timestep:     325 / 1000At timestep:     350 / 1000At timestep:     377 / 1000At timestep:     395 / 1000At timestep:     420 / 1000At timestep:     433 / 1000At timestep:     457 / 1000At timestep:     471 / 1000At timestep:     488 / 1000At timestep:     514 / 1000At timestep:     529 / 1000At timestep:     550 / 1000At timestep:     564 / 1000At timestep:     603 / 1000At timestep:     619 / 1000At timestep:     638 / 1000At timestep:     659 / 1000At timestep:     678 / 1000At timestep:     708 / 1000At timestep:     726 / 1000At timestep:     746 / 1000At timestep:     765 / 1000At timestep:     801 / 1000At timestep:     817 / 1000At timestep:     855 / 1000At timestep:     868 / 1000At timestep:     899 / 1000At timestep:     914 / 1000At timestep:     937 / 1000At timestep:     956 / 1000At timestep:     968 / 1000At timestep:     988 / 1000At timestep:     1004 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_easy_supervised_lam1_PointmassEasy-v0_22-11-2022_20-59-28/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -18.190000534057617
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2196.323851108551
Exploration Critic Loss : 279881.0625
Exploitation Critic Loss : 0.15298357605934143
Exploration Model Loss : 20.183990478515625
Actor Loss : 0.4710473120212555
Eval_AverageReturn : -19.91666603088379
Eval_StdReturn : 6.237766265869141
Eval_MaxReturn : -11.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 20.916666666666668
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_supervised_lam2_PointmassEasy-v0_22-11-2022_21-37-12 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_supervised_lam2_PointmassEasy-v0_22-11-2022_21-37-12
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002502
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0025022029876708984
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 18.079789
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -50.0
TimeSinceStart : 18.07978868484497
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 34.971484
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -50.0
TimeSinceStart : 34.97148418426514
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -49.983334
best mean reward -inf
running time 81.525555
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -49.983333587646484
TimeSinceStart : 81.525554895401
Exploration Critic Loss : 666198.375
Exploitation Critic Loss : 0.050532031804323196
Exploration Model Loss : 46.440555572509766
Actor Loss : 1.1902735233306885
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -46.988094
best mean reward -inf
running time 129.593183
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     475 / 1000At timestep:     525 / 1000At timestep:     575 / 1000At timestep:     599 / 1000At timestep:     649 / 1000At timestep:     699 / 1000At timestep:     749 / 1000At timestep:     795 / 1000At timestep:     845 / 1000At timestep:     895 / 1000At timestep:     945 / 1000At timestep:     995 / 1000At timestep:     1045 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -46.988094329833984
TimeSinceStart : 129.5931830406189
Exploration Critic Loss : 3294330.75
Exploitation Critic Loss : 0.08536933362483978
Exploration Model Loss : 31.501882553100586
Actor Loss : 0.9609923958778381
Eval_AverageReturn : -47.272727966308594
Eval_StdReturn : 7.592950344085693
Eval_MaxReturn : -23.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 47.5
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -39.880001
best mean reward -39.880001
running time 181.112398
At timestep:     24 / 1000At timestep:     74 / 1000At timestep:     96 / 1000At timestep:     127 / 1000At timestep:     177 / 1000At timestep:     227 / 1000At timestep:     277 / 1000At timestep:     327 / 1000At timestep:     349 / 1000At timestep:     399 / 1000At timestep:     449 / 1000At timestep:     499 / 1000At timestep:     549 / 1000At timestep:     599 / 1000At timestep:     649 / 1000At timestep:     689 / 1000At timestep:     739 / 1000At timestep:     789 / 1000At timestep:     839 / 1000At timestep:     889 / 1000At timestep:     924 / 1000At timestep:     974 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -39.880001068115234
Train_BestReturn : -39.880001068115234
TimeSinceStart : 181.112398147583
Exploration Critic Loss : 7089323.0
Exploitation Critic Loss : 0.10765489190816879
Exploration Model Loss : 30.053741455078125
Actor Loss : 0.769631028175354
Eval_AverageReturn : -44.21739196777344
Eval_StdReturn : 10.257368087768555
Eval_MaxReturn : -21.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 44.52173913043478
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -31.190001
best mean reward -31.190001
running time 234.931733
At timestep:     31 / 1000At timestep:     81 / 1000At timestep:     111 / 1000At timestep:     161 / 1000At timestep:     181 / 1000At timestep:     231 / 1000At timestep:     281 / 1000At timestep:     331 / 1000At timestep:     381 / 1000At timestep:     431 / 1000At timestep:     458 / 1000At timestep:     508 / 1000At timestep:     530 / 1000At timestep:     580 / 1000At timestep:     609 / 1000At timestep:     659 / 1000At timestep:     709 / 1000At timestep:     759 / 1000At timestep:     804 / 1000At timestep:     843 / 1000At timestep:     868 / 1000At timestep:     918 / 1000At timestep:     959 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -31.190000534057617
Train_BestReturn : -31.190000534057617
TimeSinceStart : 234.9317331314087
Exploration Critic Loss : 8752916.0
Exploitation Critic Loss : 0.11685921996831894
Exploration Model Loss : 37.63725280761719
Actor Loss : 0.6258925199508667
Eval_AverageReturn : -41.625
Eval_StdReturn : 11.13482666015625
Eval_MaxReturn : -19.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 42.041666666666664
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -21.280001
best mean reward -21.280001
running time 292.891978
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     141 / 1000At timestep:     191 / 1000At timestep:     232 / 1000At timestep:     282 / 1000At timestep:     322 / 1000At timestep:     372 / 1000At timestep:     422 / 1000At timestep:     438 / 1000At timestep:     488 / 1000At timestep:     518 / 1000At timestep:     536 / 1000At timestep:     553 / 1000At timestep:     603 / 1000At timestep:     631 / 1000At timestep:     656 / 1000At timestep:     677 / 1000At timestep:     727 / 1000At timestep:     752 / 1000At timestep:     791 / 1000At timestep:     819 / 1000At timestep:     862 / 1000At timestep:     892 / 1000At timestep:     942 / 1000At timestep:     992 / 1000At timestep:     1042 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -21.280000686645508
Train_BestReturn : -21.280000686645508
TimeSinceStart : 292.8919777870178
Exploration Critic Loss : 8720531.0
Exploitation Critic Loss : 0.09890953451395035
Exploration Model Loss : 31.529891967773438
Actor Loss : 0.5890322923660278
Eval_AverageReturn : -38.03703689575195
Eval_StdReturn : 12.659299850463867
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 38.592592592592595
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -17.690001
best mean reward -17.690001
running time 350.643083
At timestep:     45 / 1000At timestep:     84 / 1000At timestep:     131 / 1000At timestep:     152 / 1000At timestep:     188 / 1000At timestep:     233 / 1000At timestep:     258 / 1000At timestep:     293 / 1000At timestep:     323 / 1000At timestep:     373 / 1000At timestep:     414 / 1000At timestep:     448 / 1000At timestep:     496 / 1000At timestep:     531 / 1000At timestep:     557 / 1000At timestep:     585 / 1000At timestep:     605 / 1000At timestep:     623 / 1000At timestep:     643 / 1000At timestep:     690 / 1000At timestep:     712 / 1000At timestep:     733 / 1000At timestep:     783 / 1000At timestep:     826 / 1000At timestep:     868 / 1000At timestep:     891 / 1000At timestep:     920 / 1000At timestep:     970 / 1000At timestep:     998 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -17.690000534057617
Train_BestReturn : -17.690000534057617
TimeSinceStart : 350.6430826187134
Exploration Critic Loss : 5869436.0
Exploitation Critic Loss : 0.15145498514175415
Exploration Model Loss : 24.720077514648438
Actor Loss : 0.6244332194328308
Eval_AverageReturn : -33.5
Eval_StdReturn : 10.597955703735352
Eval_MaxReturn : -17.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 34.4
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -17.510000
best mean reward -17.510000
running time 410.761137
At timestep:     50 / 1000At timestep:     82 / 1000At timestep:     111 / 1000At timestep:     134 / 1000At timestep:     159 / 1000At timestep:     196 / 1000At timestep:     246 / 1000At timestep:     268 / 1000At timestep:     318 / 1000At timestep:     359 / 1000At timestep:     409 / 1000At timestep:     459 / 1000At timestep:     509 / 1000At timestep:     559 / 1000At timestep:     587 / 1000At timestep:     637 / 1000At timestep:     687 / 1000At timestep:     731 / 1000At timestep:     756 / 1000At timestep:     783 / 1000At timestep:     812 / 1000At timestep:     862 / 1000At timestep:     902 / 1000At timestep:     936 / 1000At timestep:     966 / 1000At timestep:     994 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -17.510000228881836
Train_BestReturn : -17.510000228881836
TimeSinceStart : 410.76113724708557
Exploration Critic Loss : 9681948.0
Exploitation Critic Loss : 0.11285509169101715
Exploration Model Loss : 21.49481201171875
Actor Loss : 0.6442721486091614
Eval_AverageReturn : -37.33333206176758
Eval_StdReturn : 10.893423080444336
Eval_MaxReturn : -21.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 37.96296296296296
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -17.809999
best mean reward -17.510000
running time 467.252042
At timestep:     23 / 1000At timestep:     47 / 1000At timestep:     68 / 1000At timestep:     118 / 1000At timestep:     134 / 1000At timestep:     178 / 1000At timestep:     195 / 1000At timestep:     224 / 1000At timestep:     244 / 1000At timestep:     266 / 1000At timestep:     292 / 1000At timestep:     319 / 1000At timestep:     342 / 1000At timestep:     361 / 1000At timestep:     401 / 1000At timestep:     426 / 1000At timestep:     476 / 1000At timestep:     500 / 1000At timestep:     530 / 1000At timestep:     569 / 1000At timestep:     598 / 1000At timestep:     646 / 1000At timestep:     672 / 1000At timestep:     690 / 1000At timestep:     714 / 1000At timestep:     744 / 1000At timestep:     763 / 1000At timestep:     781 / 1000At timestep:     818 / 1000At timestep:     867 / 1000At timestep:     885 / 1000At timestep:     904 / 1000At timestep:     948 / 1000At timestep:     975 / 1000At timestep:     997 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -17.809999465942383
Train_BestReturn : -17.510000228881836
TimeSinceStart : 467.2520418167114
Exploration Critic Loss : 7595430.5
Exploitation Critic Loss : 0.22909697890281677
Exploration Model Loss : 20.572601318359375
Actor Loss : 0.6719729900360107
Eval_AverageReturn : -27.27777862548828
Eval_StdReturn : 10.481759071350098
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 28.22222222222222
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -17.719999
best mean reward -17.510000
running time 528.571190
At timestep:     17 / 1000At timestep:     36 / 1000At timestep:     66 / 1000At timestep:     97 / 1000At timestep:     116 / 1000At timestep:     140 / 1000At timestep:     154 / 1000At timestep:     180 / 1000At timestep:     206 / 1000At timestep:     243 / 1000At timestep:     266 / 1000At timestep:     286 / 1000At timestep:     311 / 1000At timestep:     328 / 1000At timestep:     366 / 1000At timestep:     384 / 1000At timestep:     402 / 1000At timestep:     452 / 1000At timestep:     475 / 1000At timestep:     504 / 1000At timestep:     531 / 1000At timestep:     567 / 1000At timestep:     591 / 1000At timestep:     622 / 1000At timestep:     652 / 1000At timestep:     702 / 1000At timestep:     746 / 1000At timestep:     769 / 1000At timestep:     792 / 1000At timestep:     819 / 1000At timestep:     857 / 1000At timestep:     874 / 1000At timestep:     894 / 1000At timestep:     921 / 1000At timestep:     951 / 1000At timestep:     971 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -17.719999313354492
Train_BestReturn : -17.510000228881836
TimeSinceStart : 528.5711896419525
Exploration Critic Loss : 6440410.0
Exploitation Critic Loss : 0.09837532043457031
Exploration Model Loss : 20.260604858398438
Actor Loss : 0.5612976551055908
Eval_AverageReturn : -26.135135650634766
Eval_StdReturn : 8.908430099487305
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 27.08108108108108
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -17.760000
best mean reward -17.510000
running time 587.595854
At timestep:     38 / 1000At timestep:     61 / 1000At timestep:     109 / 1000At timestep:     155 / 1000At timestep:     174 / 1000At timestep:     201 / 1000At timestep:     242 / 1000At timestep:     261 / 1000At timestep:     278 / 1000At timestep:     301 / 1000At timestep:     335 / 1000At timestep:     356 / 1000At timestep:     372 / 1000At timestep:     418 / 1000At timestep:     430 / 1000At timestep:     451 / 1000At timestep:     476 / 1000At timestep:     507 / 1000At timestep:     526 / 1000At timestep:     563 / 1000At timestep:     602 / 1000At timestep:     642 / 1000At timestep:     661 / 1000At timestep:     687 / 1000At timestep:     716 / 1000At timestep:     766 / 1000At timestep:     805 / 1000At timestep:     834 / 1000At timestep:     851 / 1000At timestep:     886 / 1000At timestep:     928 / 1000At timestep:     978 / 1000At timestep:     999 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -17.760000228881836
Train_BestReturn : -17.510000228881836
TimeSinceStart : 587.595853805542
Exploration Critic Loss : 5416955.0
Exploitation Critic Loss : 0.160244882106781
Exploration Model Loss : 20.148727416992188
Actor Loss : 0.6216563582420349
Eval_AverageReturn : -29.41176414489746
Eval_StdReturn : 11.032352447509766
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 30.352941176470587
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -17.639999
best mean reward -17.510000
running time 648.927937
At timestep:     28 / 1000At timestep:     63 / 1000At timestep:     82 / 1000At timestep:     112 / 1000At timestep:     128 / 1000At timestep:     146 / 1000At timestep:     172 / 1000At timestep:     191 / 1000At timestep:     210 / 1000At timestep:     230 / 1000At timestep:     280 / 1000At timestep:     307 / 1000At timestep:     335 / 1000At timestep:     351 / 1000At timestep:     372 / 1000At timestep:     386 / 1000At timestep:     410 / 1000At timestep:     425 / 1000At timestep:     460 / 1000At timestep:     481 / 1000At timestep:     509 / 1000At timestep:     536 / 1000At timestep:     566 / 1000At timestep:     592 / 1000At timestep:     630 / 1000At timestep:     650 / 1000At timestep:     673 / 1000At timestep:     705 / 1000At timestep:     726 / 1000At timestep:     753 / 1000At timestep:     779 / 1000At timestep:     805 / 1000At timestep:     826 / 1000At timestep:     862 / 1000At timestep:     908 / 1000At timestep:     928 / 1000At timestep:     950 / 1000At timestep:     986 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -17.639999389648438
Train_BestReturn : -17.510000228881836
TimeSinceStart : 648.9279365539551
Exploration Critic Loss : 5054282.5
Exploitation Critic Loss : 0.1190401166677475
Exploration Model Loss : 20.108169555664062
Actor Loss : 0.5891250967979431
Eval_AverageReturn : -25.153846740722656
Eval_StdReturn : 8.125738143920898
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 26.128205128205128
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -17.240000
best mean reward -17.240000
running time 710.384645
At timestep:     17 / 1000At timestep:     36 / 1000At timestep:     59 / 1000At timestep:     78 / 1000At timestep:     120 / 1000At timestep:     154 / 1000At timestep:     169 / 1000At timestep:     192 / 1000At timestep:     215 / 1000At timestep:     242 / 1000At timestep:     260 / 1000At timestep:     280 / 1000At timestep:     299 / 1000At timestep:     341 / 1000At timestep:     361 / 1000At timestep:     376 / 1000At timestep:     391 / 1000At timestep:     411 / 1000At timestep:     431 / 1000At timestep:     452 / 1000At timestep:     475 / 1000At timestep:     493 / 1000At timestep:     537 / 1000At timestep:     555 / 1000At timestep:     605 / 1000At timestep:     637 / 1000At timestep:     652 / 1000At timestep:     678 / 1000At timestep:     699 / 1000At timestep:     719 / 1000At timestep:     745 / 1000At timestep:     768 / 1000At timestep:     802 / 1000At timestep:     828 / 1000At timestep:     845 / 1000At timestep:     862 / 1000At timestep:     912 / 1000At timestep:     943 / 1000At timestep:     964 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -17.239999771118164
Train_BestReturn : -17.239999771118164
TimeSinceStart : 710.3846452236176
Exploration Critic Loss : 2600704.75
Exploitation Critic Loss : 0.08041805028915405
Exploration Model Loss : 20.091934204101562
Actor Loss : 0.5125938057899475
Eval_AverageReturn : -24.125
Eval_StdReturn : 9.69326400756836
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 25.1
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -18.160000
best mean reward -17.240000
running time 771.178101
At timestep:     18 / 1000At timestep:     52 / 1000At timestep:     87 / 1000At timestep:     103 / 1000At timestep:     134 / 1000At timestep:     160 / 1000At timestep:     178 / 1000At timestep:     210 / 1000At timestep:     224 / 1000At timestep:     243 / 1000At timestep:     258 / 1000At timestep:     287 / 1000At timestep:     322 / 1000At timestep:     340 / 1000At timestep:     382 / 1000At timestep:     400 / 1000At timestep:     421 / 1000At timestep:     471 / 1000At timestep:     493 / 1000At timestep:     518 / 1000At timestep:     549 / 1000At timestep:     572 / 1000At timestep:     591 / 1000At timestep:     623 / 1000At timestep:     650 / 1000At timestep:     672 / 1000At timestep:     703 / 1000At timestep:     728 / 1000At timestep:     749 / 1000At timestep:     769 / 1000At timestep:     795 / 1000At timestep:     820 / 1000At timestep:     838 / 1000At timestep:     853 / 1000At timestep:     877 / 1000At timestep:     897 / 1000At timestep:     937 / 1000At timestep:     963 / 1000At timestep:     989 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -18.15999984741211
Train_BestReturn : -17.239999771118164
TimeSinceStart : 771.1781005859375
Exploration Critic Loss : 3643143.75
Exploitation Critic Loss : 0.10207466781139374
Exploration Model Loss : 20.085708618164062
Actor Loss : 0.5532636642456055
Eval_AverageReturn : -24.299999237060547
Eval_StdReturn : 7.953615188598633
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 25.275
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -17.660000
best mean reward -17.240000
running time 832.909647
At timestep:     16 / 1000At timestep:     41 / 1000At timestep:     68 / 1000At timestep:     93 / 1000At timestep:     113 / 1000At timestep:     129 / 1000At timestep:     162 / 1000At timestep:     177 / 1000At timestep:     213 / 1000At timestep:     234 / 1000At timestep:     255 / 1000At timestep:     272 / 1000At timestep:     308 / 1000At timestep:     340 / 1000At timestep:     372 / 1000At timestep:     404 / 1000At timestep:     423 / 1000At timestep:     473 / 1000At timestep:     497 / 1000At timestep:     519 / 1000At timestep:     537 / 1000At timestep:     554 / 1000At timestep:     580 / 1000At timestep:     628 / 1000At timestep:     644 / 1000At timestep:     661 / 1000At timestep:     679 / 1000At timestep:     697 / 1000At timestep:     716 / 1000At timestep:     755 / 1000At timestep:     793 / 1000At timestep:     818 / 1000At timestep:     849 / 1000At timestep:     878 / 1000At timestep:     896 / 1000At timestep:     918 / 1000At timestep:     941 / 1000At timestep:     985 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -17.65999984741211
Train_BestReturn : -17.239999771118164
TimeSinceStart : 832.9096465110779
Exploration Critic Loss : 8780506.0
Exploitation Critic Loss : 0.07864302396774292
Exploration Model Loss : 20.083709716796875
Actor Loss : 0.5762423276901245
Eval_AverageReturn : -24.69230842590332
Eval_StdReturn : 9.31817626953125
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 25.666666666666668
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -17.830000
best mean reward -17.240000
running time 892.935076
At timestep:     49 / 1000At timestep:     83 / 1000At timestep:     107 / 1000At timestep:     134 / 1000At timestep:     160 / 1000At timestep:     182 / 1000At timestep:     232 / 1000At timestep:     255 / 1000At timestep:     272 / 1000At timestep:     299 / 1000At timestep:     316 / 1000At timestep:     344 / 1000At timestep:     356 / 1000At timestep:     374 / 1000At timestep:     404 / 1000At timestep:     421 / 1000At timestep:     443 / 1000At timestep:     466 / 1000At timestep:     493 / 1000At timestep:     514 / 1000At timestep:     545 / 1000At timestep:     595 / 1000At timestep:     627 / 1000At timestep:     645 / 1000At timestep:     661 / 1000At timestep:     679 / 1000At timestep:     712 / 1000At timestep:     746 / 1000At timestep:     766 / 1000At timestep:     796 / 1000At timestep:     820 / 1000At timestep:     835 / 1000At timestep:     876 / 1000At timestep:     906 / 1000At timestep:     920 / 1000At timestep:     959 / 1000At timestep:     979 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -17.829999923706055
Train_BestReturn : -17.239999771118164
TimeSinceStart : 892.9350764751434
Exploration Critic Loss : 2188471.0
Exploitation Critic Loss : 0.11025986820459366
Exploration Model Loss : 20.0828857421875
Actor Loss : 0.5928226709365845
Eval_AverageReturn : -25.342105865478516
Eval_StdReturn : 9.71779727935791
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 26.31578947368421
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -17.820000
best mean reward -17.240000
running time 952.403942
At timestep:     44 / 1000At timestep:     89 / 1000At timestep:     114 / 1000At timestep:     136 / 1000At timestep:     171 / 1000At timestep:     197 / 1000At timestep:     213 / 1000At timestep:     241 / 1000At timestep:     261 / 1000At timestep:     290 / 1000At timestep:     308 / 1000At timestep:     331 / 1000At timestep:     351 / 1000At timestep:     373 / 1000At timestep:     414 / 1000At timestep:     436 / 1000At timestep:     463 / 1000At timestep:     484 / 1000At timestep:     505 / 1000At timestep:     523 / 1000At timestep:     540 / 1000At timestep:     566 / 1000At timestep:     584 / 1000At timestep:     602 / 1000At timestep:     623 / 1000At timestep:     646 / 1000At timestep:     678 / 1000At timestep:     713 / 1000At timestep:     743 / 1000At timestep:     759 / 1000At timestep:     781 / 1000At timestep:     813 / 1000At timestep:     828 / 1000At timestep:     861 / 1000At timestep:     879 / 1000At timestep:     898 / 1000At timestep:     921 / 1000At timestep:     938 / 1000At timestep:     957 / 1000At timestep:     979 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -17.81999969482422
Train_BestReturn : -17.239999771118164
TimeSinceStart : 952.4039423465729
Exploration Critic Loss : 2286530.5
Exploitation Critic Loss : 0.08766734600067139
Exploration Model Loss : 20.08258056640625
Actor Loss : 0.5483424663543701
Eval_AverageReturn : -23.414634704589844
Eval_StdReturn : 7.47671365737915
Eval_MaxReturn : -14.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 24.414634146341463
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.510000
best mean reward -17.240000
running time 1014.024231
At timestep:     21 / 1000At timestep:     40 / 1000At timestep:     61 / 1000At timestep:     77 / 1000At timestep:     107 / 1000At timestep:     133 / 1000At timestep:     146 / 1000At timestep:     166 / 1000At timestep:     183 / 1000At timestep:     198 / 1000At timestep:     222 / 1000At timestep:     239 / 1000At timestep:     289 / 1000At timestep:     320 / 1000At timestep:     345 / 1000At timestep:     388 / 1000At timestep:     411 / 1000At timestep:     434 / 1000At timestep:     453 / 1000At timestep:     467 / 1000At timestep:     484 / 1000At timestep:     498 / 1000At timestep:     521 / 1000At timestep:     543 / 1000At timestep:     570 / 1000At timestep:     598 / 1000At timestep:     615 / 1000At timestep:     632 / 1000At timestep:     660 / 1000At timestep:     685 / 1000At timestep:     696 / 1000At timestep:     718 / 1000At timestep:     734 / 1000At timestep:     757 / 1000At timestep:     777 / 1000At timestep:     799 / 1000At timestep:     820 / 1000At timestep:     841 / 1000At timestep:     863 / 1000At timestep:     881 / 1000At timestep:     902 / 1000At timestep:     940 / 1000At timestep:     971 / 1000At timestep:     999 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.510000228881836
Train_BestReturn : -17.239999771118164
TimeSinceStart : 1014.0242314338684
Exploration Critic Loss : 1381571.625
Exploitation Critic Loss : 0.06486555933952332
Exploration Model Loss : 20.082412719726562
Actor Loss : 0.5157325267791748
Eval_AverageReturn : -21.622222900390625
Eval_StdReturn : 7.543323516845703
Eval_MaxReturn : -10.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 22.6
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.360001
best mean reward -17.240000
running time 1076.584862
At timestep:     22 / 1000At timestep:     53 / 1000At timestep:     79 / 1000At timestep:     112 / 1000At timestep:     138 / 1000At timestep:     174 / 1000At timestep:     193 / 1000At timestep:     221 / 1000At timestep:     243 / 1000At timestep:     262 / 1000At timestep:     278 / 1000At timestep:     303 / 1000At timestep:     323 / 1000At timestep:     338 / 1000At timestep:     352 / 1000At timestep:     362 / 1000At timestep:     389 / 1000At timestep:     412 / 1000At timestep:     435 / 1000At timestep:     457 / 1000At timestep:     480 / 1000At timestep:     496 / 1000At timestep:     512 / 1000At timestep:     528 / 1000At timestep:     565 / 1000At timestep:     584 / 1000At timestep:     609 / 1000At timestep:     628 / 1000At timestep:     666 / 1000At timestep:     693 / 1000At timestep:     718 / 1000At timestep:     733 / 1000At timestep:     762 / 1000At timestep:     780 / 1000At timestep:     796 / 1000At timestep:     820 / 1000At timestep:     837 / 1000At timestep:     851 / 1000At timestep:     874 / 1000At timestep:     896 / 1000At timestep:     913 / 1000At timestep:     944 / 1000At timestep:     972 / 1000At timestep:     999 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.360000610351562
Train_BestReturn : -17.239999771118164
TimeSinceStart : 1076.5848622322083
Exploration Critic Loss : 941011.375
Exploitation Critic Loss : 0.0565740130841732
Exploration Model Loss : 20.082244873046875
Actor Loss : 0.5230177044868469
Eval_AverageReturn : -21.64444351196289
Eval_StdReturn : 6.43654727935791
Eval_MaxReturn : -9.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 22.644444444444446
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.219999
best mean reward -17.219999
running time 1138.728947
At timestep:     25 / 1000At timestep:     51 / 1000At timestep:     78 / 1000At timestep:     92 / 1000At timestep:     123 / 1000At timestep:     145 / 1000At timestep:     163 / 1000At timestep:     187 / 1000At timestep:     210 / 1000At timestep:     227 / 1000At timestep:     250 / 1000At timestep:     273 / 1000At timestep:     296 / 1000At timestep:     321 / 1000At timestep:     338 / 1000At timestep:     353 / 1000At timestep:     373 / 1000At timestep:     396 / 1000At timestep:     414 / 1000At timestep:     441 / 1000At timestep:     455 / 1000At timestep:     473 / 1000At timestep:     487 / 1000At timestep:     506 / 1000At timestep:     532 / 1000At timestep:     572 / 1000At timestep:     593 / 1000At timestep:     607 / 1000At timestep:     629 / 1000At timestep:     649 / 1000At timestep:     665 / 1000At timestep:     678 / 1000At timestep:     691 / 1000At timestep:     705 / 1000At timestep:     721 / 1000At timestep:     752 / 1000At timestep:     770 / 1000At timestep:     794 / 1000At timestep:     810 / 1000At timestep:     827 / 1000At timestep:     840 / 1000At timestep:     857 / 1000At timestep:     906 / 1000At timestep:     923 / 1000At timestep:     947 / 1000At timestep:     963 / 1000At timestep:     980 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.219999313354492
Train_BestReturn : -17.219999313354492
TimeSinceStart : 1138.7289469242096
Exploration Critic Loss : 1118968.0
Exploitation Critic Loss : 0.0714903473854065
Exploration Model Loss : 20.082061767578125
Actor Loss : 0.5270122289657593
Eval_AverageReturn : -19.95833396911621
Eval_StdReturn : 6.897941589355469
Eval_MaxReturn : -12.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 20.958333333333332
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.450001
best mean reward -17.219999
running time 1201.828878
At timestep:     18 / 1000At timestep:     31 / 1000At timestep:     61 / 1000At timestep:     81 / 1000At timestep:     96 / 1000At timestep:     111 / 1000At timestep:     144 / 1000At timestep:     180 / 1000At timestep:     194 / 1000At timestep:     210 / 1000At timestep:     231 / 1000At timestep:     263 / 1000At timestep:     300 / 1000At timestep:     314 / 1000At timestep:     339 / 1000At timestep:     359 / 1000At timestep:     383 / 1000At timestep:     402 / 1000At timestep:     426 / 1000At timestep:     457 / 1000At timestep:     474 / 1000At timestep:     488 / 1000At timestep:     504 / 1000At timestep:     531 / 1000At timestep:     563 / 1000At timestep:     586 / 1000At timestep:     610 / 1000At timestep:     630 / 1000At timestep:     658 / 1000At timestep:     683 / 1000At timestep:     702 / 1000At timestep:     751 / 1000At timestep:     767 / 1000At timestep:     783 / 1000At timestep:     796 / 1000At timestep:     814 / 1000At timestep:     835 / 1000At timestep:     865 / 1000At timestep:     886 / 1000At timestep:     902 / 1000At timestep:     930 / 1000At timestep:     949 / 1000At timestep:     978 / 1000At timestep:     989 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.450000762939453
Train_BestReturn : -17.219999313354492
TimeSinceStart : 1201.8288776874542
Exploration Critic Loss : 1243893.25
Exploitation Critic Loss : 0.06894505769014359
Exploration Model Loss : 20.08184814453125
Actor Loss : 0.4735959768295288
Eval_AverageReturn : -21.53333282470703
Eval_StdReturn : 7.753422260284424
Eval_MaxReturn : -10.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 22.533333333333335
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -17.530001
best mean reward -17.219999
running time 1264.415500
At timestep:     21 / 1000At timestep:     53 / 1000At timestep:     73 / 1000At timestep:     100 / 1000At timestep:     121 / 1000At timestep:     145 / 1000At timestep:     169 / 1000At timestep:     185 / 1000At timestep:     206 / 1000At timestep:     233 / 1000At timestep:     252 / 1000At timestep:     267 / 1000At timestep:     306 / 1000At timestep:     326 / 1000At timestep:     340 / 1000At timestep:     354 / 1000At timestep:     374 / 1000At timestep:     396 / 1000At timestep:     421 / 1000At timestep:     437 / 1000At timestep:     450 / 1000At timestep:     475 / 1000At timestep:     521 / 1000At timestep:     541 / 1000At timestep:     561 / 1000At timestep:     609 / 1000At timestep:     627 / 1000At timestep:     640 / 1000At timestep:     656 / 1000At timestep:     685 / 1000At timestep:     712 / 1000At timestep:     732 / 1000At timestep:     750 / 1000At timestep:     770 / 1000At timestep:     790 / 1000At timestep:     802 / 1000At timestep:     837 / 1000At timestep:     853 / 1000At timestep:     882 / 1000At timestep:     911 / 1000At timestep:     928 / 1000At timestep:     941 / 1000At timestep:     959 / 1000At timestep:     981 / 1000At timestep:     993 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -17.530000686645508
Train_BestReturn : -17.219999313354492
TimeSinceStart : 1264.4155004024506
Exploration Critic Loss : 1025736.0625
Exploitation Critic Loss : 0.10260464251041412
Exploration Model Loss : 20.0816650390625
Actor Loss : 0.5454986095428467
Eval_AverageReturn : -21.065217971801758
Eval_StdReturn : 7.992937088012695
Eval_MaxReturn : -11.0
Eval_MinReturn : -47.0
Eval_AverageEpLen : 22.065217391304348
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -18.020000
best mean reward -17.219999
running time 1327.616666
At timestep:     17 / 1000At timestep:     31 / 1000At timestep:     46 / 1000At timestep:     74 / 1000At timestep:     92 / 1000At timestep:     142 / 1000At timestep:     159 / 1000At timestep:     172 / 1000At timestep:     188 / 1000At timestep:     211 / 1000At timestep:     229 / 1000At timestep:     248 / 1000At timestep:     264 / 1000At timestep:     303 / 1000At timestep:     326 / 1000At timestep:     344 / 1000At timestep:     368 / 1000At timestep:     391 / 1000At timestep:     421 / 1000At timestep:     436 / 1000At timestep:     457 / 1000At timestep:     473 / 1000At timestep:     494 / 1000At timestep:     514 / 1000At timestep:     534 / 1000At timestep:     552 / 1000At timestep:     578 / 1000At timestep:     596 / 1000At timestep:     608 / 1000At timestep:     623 / 1000At timestep:     668 / 1000At timestep:     690 / 1000At timestep:     711 / 1000At timestep:     729 / 1000At timestep:     747 / 1000At timestep:     789 / 1000At timestep:     806 / 1000At timestep:     823 / 1000At timestep:     860 / 1000At timestep:     875 / 1000At timestep:     903 / 1000At timestep:     923 / 1000At timestep:     944 / 1000At timestep:     962 / 1000At timestep:     976 / 1000At timestep:     993 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -18.020000457763672
Train_BestReturn : -17.219999313354492
TimeSinceStart : 1327.616665840149
Exploration Critic Loss : 941593.25
Exploitation Critic Loss : 0.07449543476104736
Exploration Model Loss : 20.081512451171875
Actor Loss : 0.5870208740234375
Eval_AverageReturn : -20.787233352661133
Eval_StdReturn : 8.500152587890625
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 21.76595744680851
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -18.049999
best mean reward -17.219999
running time 1390.510660
At timestep:     29 / 1000At timestep:     49 / 1000At timestep:     67 / 1000At timestep:     87 / 1000At timestep:     104 / 1000At timestep:     125 / 1000At timestep:     155 / 1000At timestep:     167 / 1000At timestep:     191 / 1000At timestep:     208 / 1000At timestep:     235 / 1000At timestep:     276 / 1000At timestep:     298 / 1000At timestep:     319 / 1000At timestep:     365 / 1000At timestep:     379 / 1000At timestep:     399 / 1000At timestep:     418 / 1000At timestep:     443 / 1000At timestep:     458 / 1000At timestep:     482 / 1000At timestep:     500 / 1000At timestep:     516 / 1000At timestep:     533 / 1000At timestep:     554 / 1000At timestep:     587 / 1000At timestep:     603 / 1000At timestep:     633 / 1000At timestep:     653 / 1000At timestep:     669 / 1000At timestep:     694 / 1000At timestep:     715 / 1000At timestep:     734 / 1000At timestep:     751 / 1000At timestep:     773 / 1000At timestep:     803 / 1000At timestep:     821 / 1000At timestep:     838 / 1000At timestep:     855 / 1000At timestep:     873 / 1000At timestep:     892 / 1000At timestep:     923 / 1000At timestep:     951 / 1000At timestep:     973 / 1000At timestep:     994 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -18.049999237060547
Train_BestReturn : -17.219999313354492
TimeSinceStart : 1390.5106601715088
Exploration Critic Loss : 1612261.5
Exploitation Critic Loss : 0.1229938417673111
Exploration Model Loss : 20.081329345703125
Actor Loss : 0.506421685218811
Eval_AverageReturn : -21.369565963745117
Eval_StdReturn : 6.935600280761719
Eval_MaxReturn : -11.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 22.369565217391305
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -17.770000
best mean reward -17.219999
running time 1454.501859
At timestep:     27 / 1000At timestep:     43 / 1000At timestep:     66 / 1000At timestep:     82 / 1000At timestep:     104 / 1000At timestep:     120 / 1000At timestep:     138 / 1000At timestep:     154 / 1000At timestep:     174 / 1000At timestep:     192 / 1000At timestep:     210 / 1000At timestep:     231 / 1000At timestep:     245 / 1000At timestep:     263 / 1000At timestep:     289 / 1000At timestep:     308 / 1000At timestep:     325 / 1000At timestep:     336 / 1000At timestep:     357 / 1000At timestep:     373 / 1000At timestep:     399 / 1000At timestep:     417 / 1000At timestep:     449 / 1000At timestep:     473 / 1000At timestep:     493 / 1000At timestep:     521 / 1000At timestep:     540 / 1000At timestep:     565 / 1000At timestep:     604 / 1000At timestep:     617 / 1000At timestep:     633 / 1000At timestep:     649 / 1000At timestep:     674 / 1000At timestep:     695 / 1000At timestep:     715 / 1000At timestep:     735 / 1000At timestep:     760 / 1000At timestep:     774 / 1000At timestep:     797 / 1000At timestep:     817 / 1000At timestep:     850 / 1000At timestep:     862 / 1000At timestep:     879 / 1000At timestep:     906 / 1000At timestep:     938 / 1000At timestep:     953 / 1000At timestep:     978 / 1000At timestep:     999 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -17.770000457763672
Train_BestReturn : -17.219999313354492
TimeSinceStart : 1454.5018589496613
Exploration Critic Loss : 1046051.75
Exploitation Critic Loss : 0.07607616484165192
Exploration Model Loss : 20.08111572265625
Actor Loss : 0.500360906124115
Eval_AverageReturn : -19.714284896850586
Eval_StdReturn : 5.764069080352783
Eval_MaxReturn : -10.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 20.714285714285715
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -17.280001
best mean reward -17.219999
running time 1518.864844
At timestep:     21 / 1000At timestep:     40 / 1000At timestep:     54 / 1000At timestep:     85 / 1000At timestep:     102 / 1000At timestep:     125 / 1000At timestep:     141 / 1000At timestep:     177 / 1000At timestep:     201 / 1000At timestep:     235 / 1000At timestep:     253 / 1000At timestep:     278 / 1000At timestep:     296 / 1000At timestep:     317 / 1000At timestep:     331 / 1000At timestep:     364 / 1000At timestep:     380 / 1000At timestep:     398 / 1000At timestep:     447 / 1000At timestep:     484 / 1000At timestep:     499 / 1000At timestep:     515 / 1000At timestep:     536 / 1000At timestep:     556 / 1000At timestep:     570 / 1000At timestep:     585 / 1000At timestep:     607 / 1000At timestep:     624 / 1000At timestep:     642 / 1000At timestep:     673 / 1000At timestep:     708 / 1000At timestep:     730 / 1000At timestep:     756 / 1000At timestep:     775 / 1000At timestep:     804 / 1000At timestep:     821 / 1000At timestep:     851 / 1000At timestep:     864 / 1000At timestep:     889 / 1000At timestep:     907 / 1000At timestep:     921 / 1000At timestep:     935 / 1000At timestep:     959 / 1000At timestep:     993 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -17.280000686645508
Train_BestReturn : -17.219999313354492
TimeSinceStart : 1518.8648436069489
Exploration Critic Loss : 1072893.75
Exploitation Critic Loss : 0.10998887568712234
Exploration Model Loss : 20.0809326171875
Actor Loss : 0.5934001207351685
Eval_AverageReturn : -21.44444465637207
Eval_StdReturn : 7.956982612609863
Eval_MaxReturn : -12.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 22.444444444444443
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -17.010000
best mean reward -17.010000
running time 1581.713050
At timestep:     25 / 1000At timestep:     39 / 1000At timestep:     58 / 1000At timestep:     76 / 1000At timestep:     103 / 1000At timestep:     125 / 1000At timestep:     141 / 1000At timestep:     153 / 1000At timestep:     178 / 1000At timestep:     195 / 1000At timestep:     218 / 1000At timestep:     231 / 1000At timestep:     248 / 1000At timestep:     275 / 1000At timestep:     302 / 1000At timestep:     323 / 1000At timestep:     337 / 1000At timestep:     372 / 1000At timestep:     391 / 1000At timestep:     407 / 1000At timestep:     435 / 1000At timestep:     460 / 1000At timestep:     477 / 1000At timestep:     502 / 1000At timestep:     523 / 1000At timestep:     537 / 1000At timestep:     560 / 1000At timestep:     580 / 1000At timestep:     596 / 1000At timestep:     620 / 1000At timestep:     653 / 1000At timestep:     672 / 1000At timestep:     689 / 1000At timestep:     707 / 1000At timestep:     733 / 1000At timestep:     753 / 1000At timestep:     779 / 1000At timestep:     798 / 1000At timestep:     822 / 1000At timestep:     842 / 1000At timestep:     857 / 1000At timestep:     878 / 1000At timestep:     899 / 1000At timestep:     915 / 1000At timestep:     936 / 1000At timestep:     951 / 1000At timestep:     977 / 1000At timestep:     999 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -17.010000228881836
Train_BestReturn : -17.010000228881836
TimeSinceStart : 1581.7130498886108
Exploration Critic Loss : 1412342.125
Exploitation Critic Loss : 0.0719592422246933
Exploration Model Loss : 20.080581665039062
Actor Loss : 0.5198111534118652
Eval_AverageReturn : -19.67346954345703
Eval_StdReturn : 5.096568584442139
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 20.6734693877551
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -17.180000
best mean reward -17.010000
running time 1646.607779
At timestep:     39 / 1000At timestep:     51 / 1000At timestep:     67 / 1000At timestep:     83 / 1000At timestep:     109 / 1000At timestep:     123 / 1000At timestep:     147 / 1000At timestep:     169 / 1000At timestep:     194 / 1000At timestep:     217 / 1000At timestep:     233 / 1000At timestep:     259 / 1000At timestep:     281 / 1000At timestep:     301 / 1000At timestep:     324 / 1000At timestep:     341 / 1000At timestep:     356 / 1000At timestep:     387 / 1000At timestep:     399 / 1000At timestep:     412 / 1000At timestep:     460 / 1000At timestep:     480 / 1000At timestep:     500 / 1000At timestep:     524 / 1000At timestep:     542 / 1000At timestep:     576 / 1000At timestep:     592 / 1000At timestep:     611 / 1000At timestep:     625 / 1000At timestep:     650 / 1000At timestep:     674 / 1000At timestep:     691 / 1000At timestep:     709 / 1000At timestep:     737 / 1000At timestep:     754 / 1000At timestep:     789 / 1000At timestep:     811 / 1000At timestep:     834 / 1000At timestep:     850 / 1000At timestep:     862 / 1000At timestep:     881 / 1000At timestep:     906 / 1000At timestep:     925 / 1000At timestep:     948 / 1000At timestep:     966 / 1000At timestep:     978 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -17.18000030517578
Train_BestReturn : -17.010000228881836
TimeSinceStart : 1646.6077785491943
Exploration Critic Loss : 906192.875
Exploitation Critic Loss : 0.17761528491973877
Exploration Model Loss : 20.080291748046875
Actor Loss : 0.44199809432029724
Eval_AverageReturn : -20.276596069335938
Eval_StdReturn : 7.213173866271973
Eval_MaxReturn : -11.0
Eval_MinReturn : -47.0
Eval_AverageEpLen : 21.27659574468085
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -17.610001
best mean reward -17.010000
running time 1709.313021
At timestep:     13 / 1000At timestep:     42 / 1000At timestep:     57 / 1000At timestep:     86 / 1000At timestep:     112 / 1000At timestep:     138 / 1000At timestep:     153 / 1000At timestep:     168 / 1000At timestep:     188 / 1000At timestep:     219 / 1000At timestep:     236 / 1000At timestep:     270 / 1000At timestep:     305 / 1000At timestep:     324 / 1000At timestep:     339 / 1000At timestep:     358 / 1000At timestep:     381 / 1000At timestep:     401 / 1000At timestep:     417 / 1000At timestep:     434 / 1000At timestep:     456 / 1000At timestep:     474 / 1000At timestep:     492 / 1000At timestep:     510 / 1000At timestep:     525 / 1000At timestep:     545 / 1000At timestep:     566 / 1000At timestep:     584 / 1000At timestep:     605 / 1000At timestep:     624 / 1000At timestep:     643 / 1000At timestep:     659 / 1000At timestep:     680 / 1000At timestep:     700 / 1000At timestep:     721 / 1000At timestep:     740 / 1000At timestep:     756 / 1000At timestep:     771 / 1000At timestep:     792 / 1000At timestep:     819 / 1000At timestep:     845 / 1000At timestep:     863 / 1000At timestep:     877 / 1000At timestep:     894 / 1000At timestep:     911 / 1000At timestep:     925 / 1000At timestep:     945 / 1000At timestep:     964 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -17.610000610351562
Train_BestReturn : -17.010000228881836
TimeSinceStart : 1709.313021183014
Exploration Critic Loss : 921854.75
Exploitation Critic Loss : 0.06943698227405548
Exploration Model Loss : 20.079940795898438
Actor Loss : 0.5410244464874268
Eval_AverageReturn : -19.612245559692383
Eval_StdReturn : 6.255825996398926
Eval_MaxReturn : -12.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 20.612244897959183
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -17.260000
best mean reward -17.010000
running time 1774.580206
At timestep:     17 / 1000At timestep:     41 / 1000At timestep:     58 / 1000At timestep:     74 / 1000At timestep:     93 / 1000At timestep:     105 / 1000At timestep:     127 / 1000At timestep:     154 / 1000At timestep:     172 / 1000At timestep:     187 / 1000At timestep:     206 / 1000At timestep:     226 / 1000At timestep:     244 / 1000At timestep:     268 / 1000At timestep:     290 / 1000At timestep:     317 / 1000At timestep:     346 / 1000At timestep:     366 / 1000At timestep:     382 / 1000At timestep:     403 / 1000At timestep:     415 / 1000At timestep:     441 / 1000At timestep:     466 / 1000At timestep:     490 / 1000At timestep:     519 / 1000At timestep:     533 / 1000At timestep:     548 / 1000At timestep:     576 / 1000At timestep:     601 / 1000At timestep:     620 / 1000At timestep:     645 / 1000At timestep:     663 / 1000At timestep:     680 / 1000At timestep:     693 / 1000At timestep:     713 / 1000At timestep:     736 / 1000At timestep:     758 / 1000At timestep:     784 / 1000At timestep:     817 / 1000At timestep:     836 / 1000At timestep:     856 / 1000At timestep:     876 / 1000At timestep:     914 / 1000At timestep:     934 / 1000At timestep:     954 / 1000At timestep:     969 / 1000At timestep:     995 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -17.260000228881836
Train_BestReturn : -17.010000228881836
TimeSinceStart : 1774.5802059173584
Exploration Critic Loss : 1144150.125
Exploitation Critic Loss : 0.13415956497192383
Exploration Model Loss : 20.079620361328125
Actor Loss : 0.4340035319328308
Eval_AverageReturn : -20.02083396911621
Eval_StdReturn : 5.437100887298584
Eval_MaxReturn : -11.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 21.020833333333332
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -16.920000
best mean reward -16.920000
running time 1839.304228
At timestep:     19 / 1000At timestep:     33 / 1000At timestep:     57 / 1000At timestep:     72 / 1000At timestep:     92 / 1000At timestep:     107 / 1000At timestep:     122 / 1000At timestep:     143 / 1000At timestep:     159 / 1000At timestep:     175 / 1000At timestep:     193 / 1000At timestep:     224 / 1000At timestep:     243 / 1000At timestep:     273 / 1000At timestep:     300 / 1000At timestep:     317 / 1000At timestep:     339 / 1000At timestep:     366 / 1000At timestep:     389 / 1000At timestep:     413 / 1000At timestep:     426 / 1000At timestep:     440 / 1000At timestep:     477 / 1000At timestep:     489 / 1000At timestep:     507 / 1000At timestep:     520 / 1000At timestep:     546 / 1000At timestep:     592 / 1000At timestep:     606 / 1000At timestep:     623 / 1000At timestep:     637 / 1000At timestep:     659 / 1000At timestep:     693 / 1000At timestep:     713 / 1000At timestep:     739 / 1000At timestep:     766 / 1000At timestep:     778 / 1000At timestep:     803 / 1000At timestep:     823 / 1000At timestep:     842 / 1000At timestep:     862 / 1000At timestep:     876 / 1000At timestep:     891 / 1000At timestep:     918 / 1000At timestep:     941 / 1000At timestep:     962 / 1000At timestep:     987 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -16.920000076293945
Train_BestReturn : -16.920000076293945
TimeSinceStart : 1839.3042283058167
Exploration Critic Loss : 1083572.375
Exploitation Critic Loss : 0.08603083342313766
Exploration Model Loss : 20.079254150390625
Actor Loss : 0.4729929566383362
Eval_AverageReturn : -20.0
Eval_StdReturn : 6.892024517059326
Eval_MaxReturn : -11.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 21.0
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -17.059999
best mean reward -16.920000
running time 1902.069077
At timestep:     13 / 1000At timestep:     33 / 1000At timestep:     69 / 1000At timestep:     86 / 1000At timestep:     106 / 1000At timestep:     129 / 1000At timestep:     151 / 1000At timestep:     167 / 1000At timestep:     184 / 1000At timestep:     201 / 1000At timestep:     228 / 1000At timestep:     246 / 1000At timestep:     271 / 1000At timestep:     292 / 1000At timestep:     310 / 1000At timestep:     334 / 1000At timestep:     353 / 1000At timestep:     391 / 1000At timestep:     404 / 1000At timestep:     421 / 1000At timestep:     435 / 1000At timestep:     467 / 1000At timestep:     481 / 1000At timestep:     501 / 1000At timestep:     519 / 1000At timestep:     536 / 1000At timestep:     567 / 1000At timestep:     586 / 1000At timestep:     603 / 1000At timestep:     626 / 1000At timestep:     643 / 1000At timestep:     667 / 1000At timestep:     683 / 1000At timestep:     698 / 1000At timestep:     709 / 1000At timestep:     731 / 1000At timestep:     745 / 1000At timestep:     758 / 1000At timestep:     779 / 1000At timestep:     806 / 1000At timestep:     837 / 1000At timestep:     857 / 1000At timestep:     867 / 1000At timestep:     889 / 1000At timestep:     919 / 1000At timestep:     942 / 1000At timestep:     960 / 1000At timestep:     979 / 1000At timestep:     996 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -17.059999465942383
Train_BestReturn : -16.920000076293945
TimeSinceStart : 1902.0690774917603
Exploration Critic Loss : 1392532.125
Exploitation Critic Loss : 0.12855201959609985
Exploration Model Loss : 20.078948974609375
Actor Loss : 0.4817855954170227
Eval_AverageReturn : -19.31999969482422
Eval_StdReturn : 6.067750453948975
Eval_MaxReturn : -9.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 20.32
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -16.780001
best mean reward -16.780001
running time 1966.597602
At timestep:     27 / 1000At timestep:     47 / 1000At timestep:     67 / 1000At timestep:     82 / 1000At timestep:     95 / 1000At timestep:     106 / 1000At timestep:     128 / 1000At timestep:     145 / 1000At timestep:     164 / 1000At timestep:     192 / 1000At timestep:     206 / 1000At timestep:     228 / 1000At timestep:     246 / 1000At timestep:     263 / 1000At timestep:     292 / 1000At timestep:     313 / 1000At timestep:     327 / 1000At timestep:     356 / 1000At timestep:     379 / 1000At timestep:     411 / 1000At timestep:     426 / 1000At timestep:     443 / 1000At timestep:     460 / 1000At timestep:     474 / 1000At timestep:     490 / 1000At timestep:     521 / 1000At timestep:     543 / 1000At timestep:     562 / 1000At timestep:     581 / 1000At timestep:     597 / 1000At timestep:     617 / 1000At timestep:     636 / 1000At timestep:     661 / 1000At timestep:     679 / 1000At timestep:     696 / 1000At timestep:     708 / 1000At timestep:     734 / 1000At timestep:     745 / 1000At timestep:     759 / 1000At timestep:     776 / 1000At timestep:     800 / 1000At timestep:     819 / 1000At timestep:     839 / 1000At timestep:     859 / 1000At timestep:     895 / 1000At timestep:     915 / 1000At timestep:     935 / 1000At timestep:     950 / 1000At timestep:     971 / 1000At timestep:     990 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -16.780000686645508
Train_BestReturn : -16.780000686645508
TimeSinceStart : 1966.5976016521454
Exploration Critic Loss : 1028064.625
Exploitation Critic Loss : 0.096169114112854
Exploration Model Loss : 20.07855224609375
Actor Loss : 0.46909016370773315
Eval_AverageReturn : -18.843137741088867
Eval_StdReturn : 5.410130500793457
Eval_MaxReturn : -10.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 19.84313725490196
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -17.389999
best mean reward -16.780001
running time 2030.904739
At timestep:     11 / 1000At timestep:     33 / 1000At timestep:     53 / 1000At timestep:     71 / 1000At timestep:     84 / 1000At timestep:     102 / 1000At timestep:     122 / 1000At timestep:     137 / 1000At timestep:     147 / 1000At timestep:     171 / 1000At timestep:     192 / 1000At timestep:     209 / 1000At timestep:     237 / 1000At timestep:     253 / 1000At timestep:     271 / 1000At timestep:     296 / 1000At timestep:     324 / 1000At timestep:     357 / 1000At timestep:     379 / 1000At timestep:     397 / 1000At timestep:     447 / 1000At timestep:     482 / 1000At timestep:     501 / 1000At timestep:     520 / 1000At timestep:     533 / 1000At timestep:     570 / 1000At timestep:     585 / 1000At timestep:     608 / 1000At timestep:     627 / 1000At timestep:     659 / 1000At timestep:     674 / 1000At timestep:     698 / 1000At timestep:     713 / 1000At timestep:     734 / 1000At timestep:     754 / 1000At timestep:     770 / 1000At timestep:     786 / 1000At timestep:     804 / 1000At timestep:     824 / 1000At timestep:     852 / 1000At timestep:     873 / 1000At timestep:     889 / 1000At timestep:     913 / 1000At timestep:     931 / 1000At timestep:     953 / 1000At timestep:     975 / 1000At timestep:     993 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -17.389999389648438
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2030.904738664627
Exploration Critic Loss : 967490.8125
Exploitation Critic Loss : 0.075023353099823
Exploration Model Loss : 20.0784912109375
Actor Loss : 0.47210952639579773
Eval_AverageReturn : -20.1875
Eval_StdReturn : 7.224542140960693
Eval_MaxReturn : -9.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 21.166666666666668
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -17.150000
best mean reward -16.780001
running time 2094.691700
At timestep:     16 / 1000At timestep:     37 / 1000At timestep:     61 / 1000At timestep:     74 / 1000At timestep:     96 / 1000At timestep:     112 / 1000At timestep:     128 / 1000At timestep:     143 / 1000At timestep:     161 / 1000At timestep:     174 / 1000At timestep:     188 / 1000At timestep:     214 / 1000At timestep:     228 / 1000At timestep:     248 / 1000At timestep:     276 / 1000At timestep:     295 / 1000At timestep:     315 / 1000At timestep:     337 / 1000At timestep:     359 / 1000At timestep:     383 / 1000At timestep:     406 / 1000At timestep:     426 / 1000At timestep:     445 / 1000At timestep:     461 / 1000At timestep:     480 / 1000At timestep:     494 / 1000At timestep:     514 / 1000At timestep:     530 / 1000At timestep:     545 / 1000At timestep:     572 / 1000At timestep:     592 / 1000At timestep:     609 / 1000At timestep:     625 / 1000At timestep:     640 / 1000At timestep:     670 / 1000At timestep:     684 / 1000At timestep:     704 / 1000At timestep:     720 / 1000At timestep:     742 / 1000At timestep:     758 / 1000At timestep:     785 / 1000At timestep:     803 / 1000At timestep:     825 / 1000At timestep:     838 / 1000At timestep:     859 / 1000At timestep:     876 / 1000At timestep:     894 / 1000At timestep:     912 / 1000At timestep:     928 / 1000At timestep:     941 / 1000At timestep:     958 / 1000At timestep:     985 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -17.149999618530273
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2094.6916999816895
Exploration Critic Loss : 1044226.0625
Exploitation Critic Loss : 0.09535962343215942
Exploration Model Loss : 20.078460693359375
Actor Loss : 0.5129989981651306
Eval_AverageReturn : -18.01886749267578
Eval_StdReturn : 4.284639358520508
Eval_MaxReturn : -12.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 19.0188679245283
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -17.860001
best mean reward -16.780001
running time 2158.178776
At timestep:     19 / 1000At timestep:     40 / 1000At timestep:     56 / 1000At timestep:     76 / 1000At timestep:     92 / 1000At timestep:     108 / 1000At timestep:     124 / 1000At timestep:     154 / 1000At timestep:     177 / 1000At timestep:     189 / 1000At timestep:     203 / 1000At timestep:     218 / 1000At timestep:     240 / 1000At timestep:     257 / 1000At timestep:     282 / 1000At timestep:     299 / 1000At timestep:     315 / 1000At timestep:     339 / 1000At timestep:     360 / 1000At timestep:     381 / 1000At timestep:     398 / 1000At timestep:     430 / 1000At timestep:     451 / 1000At timestep:     467 / 1000At timestep:     485 / 1000At timestep:     505 / 1000At timestep:     522 / 1000At timestep:     542 / 1000At timestep:     566 / 1000At timestep:     579 / 1000At timestep:     600 / 1000At timestep:     621 / 1000At timestep:     641 / 1000At timestep:     663 / 1000At timestep:     687 / 1000At timestep:     710 / 1000At timestep:     728 / 1000At timestep:     750 / 1000At timestep:     766 / 1000At timestep:     783 / 1000At timestep:     806 / 1000At timestep:     826 / 1000At timestep:     844 / 1000At timestep:     858 / 1000At timestep:     877 / 1000At timestep:     899 / 1000At timestep:     921 / 1000At timestep:     939 / 1000At timestep:     956 / 1000At timestep:     975 / 1000At timestep:     988 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -17.860000610351562
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2158.178776025772
Exploration Critic Loss : 1037499.25
Exploitation Critic Loss : 0.07234185934066772
Exploration Model Loss : 20.078216552734375
Actor Loss : 0.5242151021957397
Eval_AverageReturn : -18.44230842590332
Eval_StdReturn : 3.939023017883301
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.442307692307693
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -18.370001
best mean reward -16.780001
running time 2222.491551
At timestep:     20 / 1000At timestep:     39 / 1000At timestep:     50 / 1000At timestep:     63 / 1000At timestep:     83 / 1000At timestep:     100 / 1000At timestep:     120 / 1000At timestep:     135 / 1000At timestep:     152 / 1000At timestep:     174 / 1000At timestep:     191 / 1000At timestep:     206 / 1000At timestep:     220 / 1000At timestep:     235 / 1000At timestep:     256 / 1000At timestep:     275 / 1000At timestep:     288 / 1000At timestep:     309 / 1000At timestep:     328 / 1000At timestep:     341 / 1000At timestep:     357 / 1000At timestep:     371 / 1000At timestep:     387 / 1000At timestep:     416 / 1000At timestep:     431 / 1000At timestep:     443 / 1000At timestep:     462 / 1000At timestep:     490 / 1000At timestep:     509 / 1000At timestep:     527 / 1000At timestep:     543 / 1000At timestep:     561 / 1000At timestep:     584 / 1000At timestep:     597 / 1000At timestep:     607 / 1000At timestep:     623 / 1000At timestep:     641 / 1000At timestep:     666 / 1000At timestep:     682 / 1000At timestep:     719 / 1000At timestep:     735 / 1000At timestep:     759 / 1000At timestep:     786 / 1000At timestep:     807 / 1000At timestep:     826 / 1000At timestep:     845 / 1000At timestep:     864 / 1000At timestep:     878 / 1000At timestep:     899 / 1000At timestep:     931 / 1000At timestep:     955 / 1000At timestep:     985 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -18.3700008392334
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2222.491550922394
Exploration Critic Loss : 888505.8125
Exploitation Critic Loss : 0.06075205281376839
Exploration Model Loss : 20.077911376953125
Actor Loss : 0.39650747179985046
Eval_AverageReturn : -17.88679313659668
Eval_StdReturn : 5.39622688293457
Eval_MaxReturn : -9.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 18.88679245283019
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -17.459999
best mean reward -16.780001
running time 2287.602852
At timestep:     18 / 1000At timestep:     40 / 1000At timestep:     52 / 1000At timestep:     68 / 1000At timestep:     85 / 1000At timestep:     104 / 1000At timestep:     129 / 1000At timestep:     149 / 1000At timestep:     169 / 1000At timestep:     183 / 1000At timestep:     201 / 1000At timestep:     217 / 1000At timestep:     240 / 1000At timestep:     257 / 1000At timestep:     276 / 1000At timestep:     295 / 1000At timestep:     314 / 1000At timestep:     338 / 1000At timestep:     360 / 1000At timestep:     372 / 1000At timestep:     391 / 1000At timestep:     424 / 1000At timestep:     448 / 1000At timestep:     469 / 1000At timestep:     489 / 1000At timestep:     505 / 1000At timestep:     523 / 1000At timestep:     534 / 1000At timestep:     550 / 1000At timestep:     575 / 1000At timestep:     593 / 1000At timestep:     613 / 1000At timestep:     633 / 1000At timestep:     650 / 1000At timestep:     667 / 1000At timestep:     689 / 1000At timestep:     706 / 1000At timestep:     725 / 1000At timestep:     743 / 1000At timestep:     762 / 1000At timestep:     785 / 1000At timestep:     808 / 1000At timestep:     840 / 1000At timestep:     860 / 1000At timestep:     876 / 1000At timestep:     891 / 1000At timestep:     916 / 1000At timestep:     936 / 1000At timestep:     955 / 1000At timestep:     969 / 1000At timestep:     986 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -17.459999084472656
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2287.602851629257
Exploration Critic Loss : 695059.375
Exploitation Critic Loss : 0.0769549161195755
Exploration Model Loss : 20.077606201171875
Actor Loss : 0.43296027183532715
Eval_AverageReturn : -18.423076629638672
Eval_StdReturn : 4.221493244171143
Eval_MaxReturn : -10.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 19.423076923076923
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -17.190001
best mean reward -16.780001
running time 2353.872784
At timestep:     20 / 1000At timestep:     36 / 1000At timestep:     51 / 1000At timestep:     66 / 1000At timestep:     87 / 1000At timestep:     109 / 1000At timestep:     134 / 1000At timestep:     154 / 1000At timestep:     171 / 1000At timestep:     188 / 1000At timestep:     202 / 1000At timestep:     224 / 1000At timestep:     241 / 1000At timestep:     262 / 1000At timestep:     278 / 1000At timestep:     299 / 1000At timestep:     313 / 1000At timestep:     332 / 1000At timestep:     353 / 1000At timestep:     367 / 1000At timestep:     386 / 1000At timestep:     416 / 1000At timestep:     437 / 1000At timestep:     451 / 1000At timestep:     462 / 1000At timestep:     474 / 1000At timestep:     491 / 1000At timestep:     511 / 1000At timestep:     527 / 1000At timestep:     543 / 1000At timestep:     555 / 1000At timestep:     577 / 1000At timestep:     588 / 1000At timestep:     603 / 1000At timestep:     626 / 1000At timestep:     645 / 1000At timestep:     675 / 1000At timestep:     697 / 1000At timestep:     730 / 1000At timestep:     750 / 1000At timestep:     772 / 1000At timestep:     793 / 1000At timestep:     814 / 1000At timestep:     830 / 1000At timestep:     863 / 1000At timestep:     878 / 1000At timestep:     896 / 1000At timestep:     929 / 1000At timestep:     946 / 1000At timestep:     970 / 1000At timestep:     993 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -17.190000534057617
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2353.8727843761444
Exploration Critic Loss : 838912.5
Exploitation Critic Loss : 0.09103024005889893
Exploration Model Loss : 20.077316284179688
Actor Loss : 0.5078399777412415
Eval_AverageReturn : -18.326923370361328
Eval_StdReturn : 5.369863033294678
Eval_MaxReturn : -10.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 19.326923076923077
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -17.420000
best mean reward -16.780001
running time 2417.086416
At timestep:     22 / 1000At timestep:     49 / 1000At timestep:     71 / 1000At timestep:     92 / 1000At timestep:     112 / 1000At timestep:     128 / 1000At timestep:     148 / 1000At timestep:     163 / 1000At timestep:     190 / 1000At timestep:     205 / 1000At timestep:     219 / 1000At timestep:     234 / 1000At timestep:     254 / 1000At timestep:     277 / 1000At timestep:     298 / 1000At timestep:     315 / 1000At timestep:     337 / 1000At timestep:     356 / 1000At timestep:     388 / 1000At timestep:     433 / 1000At timestep:     454 / 1000At timestep:     466 / 1000At timestep:     481 / 1000At timestep:     503 / 1000At timestep:     530 / 1000At timestep:     549 / 1000At timestep:     569 / 1000At timestep:     584 / 1000At timestep:     610 / 1000At timestep:     627 / 1000At timestep:     646 / 1000At timestep:     663 / 1000At timestep:     692 / 1000At timestep:     712 / 1000At timestep:     727 / 1000At timestep:     758 / 1000At timestep:     774 / 1000At timestep:     793 / 1000At timestep:     810 / 1000At timestep:     829 / 1000At timestep:     851 / 1000At timestep:     868 / 1000At timestep:     892 / 1000At timestep:     933 / 1000At timestep:     966 / 1000At timestep:     979 / 1000At timestep:     995 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -17.420000076293945
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2417.086416244507
Exploration Critic Loss : 403509.3125
Exploitation Critic Loss : 0.08919194340705872
Exploration Model Loss : 20.0770263671875
Actor Loss : 0.5373002886772156
Eval_AverageReturn : -20.10416603088379
Eval_StdReturn : 6.699625492095947
Eval_MaxReturn : -11.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 21.104166666666668
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -17.440001
best mean reward -16.780001
running time 2480.742857
At timestep:     11 / 1000At timestep:     25 / 1000At timestep:     45 / 1000At timestep:     64 / 1000At timestep:     87 / 1000At timestep:     105 / 1000At timestep:     126 / 1000At timestep:     155 / 1000At timestep:     177 / 1000At timestep:     196 / 1000At timestep:     216 / 1000At timestep:     231 / 1000At timestep:     254 / 1000At timestep:     272 / 1000At timestep:     288 / 1000At timestep:     304 / 1000At timestep:     348 / 1000At timestep:     378 / 1000At timestep:     398 / 1000At timestep:     419 / 1000At timestep:     435 / 1000At timestep:     452 / 1000At timestep:     471 / 1000At timestep:     497 / 1000At timestep:     519 / 1000At timestep:     539 / 1000At timestep:     553 / 1000At timestep:     568 / 1000At timestep:     588 / 1000At timestep:     618 / 1000At timestep:     644 / 1000At timestep:     663 / 1000At timestep:     680 / 1000At timestep:     699 / 1000At timestep:     721 / 1000At timestep:     732 / 1000At timestep:     752 / 1000At timestep:     770 / 1000At timestep:     796 / 1000At timestep:     813 / 1000At timestep:     833 / 1000At timestep:     863 / 1000At timestep:     882 / 1000At timestep:     908 / 1000At timestep:     919 / 1000At timestep:     936 / 1000At timestep:     952 / 1000At timestep:     987 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -17.440000534057617
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2480.7428574562073
Exploration Critic Loss : 966559.875
Exploitation Critic Loss : 0.06977327167987823
Exploration Model Loss : 20.076675415039062
Actor Loss : 0.4286099970340729
Eval_AverageReturn : -19.53061294555664
Eval_StdReturn : 6.108214855194092
Eval_MaxReturn : -10.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 20.53061224489796
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -17.059999
best mean reward -16.780001
running time 2541.535630
At timestep:     29 / 1000At timestep:     44 / 1000At timestep:     67 / 1000At timestep:     83 / 1000At timestep:     99 / 1000At timestep:     115 / 1000At timestep:     130 / 1000At timestep:     145 / 1000At timestep:     161 / 1000At timestep:     178 / 1000At timestep:     192 / 1000At timestep:     206 / 1000At timestep:     219 / 1000At timestep:     233 / 1000At timestep:     249 / 1000At timestep:     270 / 1000At timestep:     290 / 1000At timestep:     310 / 1000At timestep:     324 / 1000At timestep:     335 / 1000At timestep:     352 / 1000At timestep:     370 / 1000At timestep:     390 / 1000At timestep:     408 / 1000At timestep:     432 / 1000At timestep:     457 / 1000At timestep:     483 / 1000At timestep:     497 / 1000At timestep:     529 / 1000At timestep:     554 / 1000At timestep:     579 / 1000At timestep:     605 / 1000At timestep:     626 / 1000At timestep:     644 / 1000At timestep:     664 / 1000At timestep:     681 / 1000At timestep:     697 / 1000At timestep:     709 / 1000At timestep:     725 / 1000At timestep:     744 / 1000At timestep:     758 / 1000At timestep:     779 / 1000At timestep:     809 / 1000At timestep:     828 / 1000At timestep:     847 / 1000At timestep:     867 / 1000At timestep:     883 / 1000At timestep:     895 / 1000At timestep:     911 / 1000At timestep:     938 / 1000At timestep:     967 / 1000At timestep:     980 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -17.059999465942383
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2541.5356302261353
Exploration Critic Loss : 717129.3125
Exploitation Critic Loss : 0.08778254687786102
Exploration Model Loss : 20.076492309570312
Actor Loss : 0.424021452665329
Eval_AverageReturn : -17.88679313659668
Eval_StdReturn : 5.045679569244385
Eval_MaxReturn : -10.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 18.88679245283019
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -17.110001
best mean reward -16.780001
running time 2606.020032
At timestep:     18 / 1000At timestep:     33 / 1000At timestep:     57 / 1000At timestep:     73 / 1000At timestep:     98 / 1000At timestep:     118 / 1000At timestep:     135 / 1000At timestep:     151 / 1000At timestep:     172 / 1000At timestep:     193 / 1000At timestep:     209 / 1000At timestep:     229 / 1000At timestep:     250 / 1000At timestep:     266 / 1000At timestep:     283 / 1000At timestep:     295 / 1000At timestep:     313 / 1000At timestep:     330 / 1000At timestep:     355 / 1000At timestep:     373 / 1000At timestep:     396 / 1000At timestep:     419 / 1000At timestep:     443 / 1000At timestep:     461 / 1000At timestep:     482 / 1000At timestep:     504 / 1000At timestep:     522 / 1000At timestep:     563 / 1000At timestep:     587 / 1000At timestep:     606 / 1000At timestep:     630 / 1000At timestep:     644 / 1000At timestep:     669 / 1000At timestep:     689 / 1000At timestep:     714 / 1000At timestep:     731 / 1000At timestep:     750 / 1000At timestep:     771 / 1000At timestep:     790 / 1000At timestep:     809 / 1000At timestep:     828 / 1000At timestep:     841 / 1000At timestep:     856 / 1000At timestep:     874 / 1000At timestep:     889 / 1000At timestep:     907 / 1000At timestep:     925 / 1000At timestep:     959 / 1000At timestep:     974 / 1000At timestep:     992 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -17.110000610351562
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2606.020031929016
Exploration Critic Loss : 683836.0
Exploitation Critic Loss : 0.07834027707576752
Exploration Model Loss : 20.076202392578125
Actor Loss : 0.4709343910217285
Eval_AverageReturn : -18.80392074584961
Eval_StdReturn : 4.89104700088501
Eval_MaxReturn : -11.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 19.80392156862745
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -17.840000
best mean reward -16.780001
running time 2669.199703
At timestep:     16 / 1000At timestep:     37 / 1000At timestep:     59 / 1000At timestep:     74 / 1000At timestep:     89 / 1000At timestep:     107 / 1000At timestep:     124 / 1000At timestep:     140 / 1000At timestep:     162 / 1000At timestep:     190 / 1000At timestep:     205 / 1000At timestep:     220 / 1000At timestep:     234 / 1000At timestep:     262 / 1000At timestep:     290 / 1000At timestep:     312 / 1000At timestep:     332 / 1000At timestep:     353 / 1000At timestep:     370 / 1000At timestep:     381 / 1000At timestep:     399 / 1000At timestep:     416 / 1000At timestep:     433 / 1000At timestep:     449 / 1000At timestep:     462 / 1000At timestep:     480 / 1000At timestep:     500 / 1000At timestep:     524 / 1000At timestep:     538 / 1000At timestep:     554 / 1000At timestep:     568 / 1000At timestep:     588 / 1000At timestep:     616 / 1000At timestep:     639 / 1000At timestep:     664 / 1000At timestep:     679 / 1000At timestep:     698 / 1000At timestep:     719 / 1000At timestep:     732 / 1000At timestep:     758 / 1000At timestep:     781 / 1000At timestep:     801 / 1000At timestep:     833 / 1000At timestep:     851 / 1000At timestep:     864 / 1000At timestep:     881 / 1000At timestep:     896 / 1000At timestep:     916 / 1000At timestep:     942 / 1000At timestep:     958 / 1000At timestep:     973 / 1000At timestep:     988 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -17.84000015258789
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2669.199702501297
Exploration Critic Loss : 644163.625
Exploitation Critic Loss : 0.11718243360519409
Exploration Model Loss : 20.075927734375
Actor Loss : 0.448208749294281
Eval_AverageReturn : -18.056604385375977
Eval_StdReturn : 4.720149517059326
Eval_MaxReturn : -10.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.056603773584907
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -18.000000
best mean reward -16.780001
running time 2733.619702
At timestep:     12 / 1000At timestep:     27 / 1000At timestep:     42 / 1000At timestep:     57 / 1000At timestep:     86 / 1000At timestep:     104 / 1000At timestep:     121 / 1000At timestep:     137 / 1000At timestep:     169 / 1000At timestep:     185 / 1000At timestep:     203 / 1000At timestep:     228 / 1000At timestep:     240 / 1000At timestep:     252 / 1000At timestep:     269 / 1000At timestep:     290 / 1000At timestep:     312 / 1000At timestep:     338 / 1000At timestep:     359 / 1000At timestep:     382 / 1000At timestep:     406 / 1000At timestep:     427 / 1000At timestep:     447 / 1000At timestep:     461 / 1000At timestep:     481 / 1000At timestep:     496 / 1000At timestep:     515 / 1000At timestep:     534 / 1000At timestep:     553 / 1000At timestep:     568 / 1000At timestep:     585 / 1000At timestep:     604 / 1000At timestep:     623 / 1000At timestep:     643 / 1000At timestep:     655 / 1000At timestep:     688 / 1000At timestep:     709 / 1000At timestep:     743 / 1000At timestep:     769 / 1000At timestep:     787 / 1000At timestep:     800 / 1000At timestep:     823 / 1000At timestep:     843 / 1000At timestep:     861 / 1000At timestep:     888 / 1000At timestep:     901 / 1000At timestep:     931 / 1000At timestep:     948 / 1000At timestep:     964 / 1000At timestep:     983 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -18.0
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2733.619702100754
Exploration Critic Loss : 681076.0
Exploitation Critic Loss : 0.09366373717784882
Exploration Model Loss : 20.075607299804688
Actor Loss : 0.43792515993118286
Eval_AverageReturn : -18.843137741088867
Eval_StdReturn : 5.5637922286987305
Eval_MaxReturn : -11.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 19.84313725490196
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -17.299999
best mean reward -16.780001
running time 2798.031553
At timestep:     11 / 1000At timestep:     32 / 1000At timestep:     43 / 1000At timestep:     61 / 1000At timestep:     82 / 1000At timestep:     104 / 1000At timestep:     122 / 1000At timestep:     144 / 1000At timestep:     162 / 1000At timestep:     176 / 1000At timestep:     197 / 1000At timestep:     213 / 1000At timestep:     232 / 1000At timestep:     249 / 1000At timestep:     272 / 1000At timestep:     288 / 1000At timestep:     304 / 1000At timestep:     320 / 1000At timestep:     341 / 1000At timestep:     366 / 1000At timestep:     388 / 1000At timestep:     406 / 1000At timestep:     427 / 1000At timestep:     442 / 1000At timestep:     469 / 1000At timestep:     484 / 1000At timestep:     500 / 1000At timestep:     528 / 1000At timestep:     559 / 1000At timestep:     583 / 1000At timestep:     606 / 1000At timestep:     619 / 1000At timestep:     635 / 1000At timestep:     655 / 1000At timestep:     678 / 1000At timestep:     705 / 1000At timestep:     730 / 1000At timestep:     748 / 1000At timestep:     771 / 1000At timestep:     792 / 1000At timestep:     810 / 1000At timestep:     837 / 1000At timestep:     850 / 1000At timestep:     867 / 1000At timestep:     889 / 1000At timestep:     907 / 1000At timestep:     924 / 1000At timestep:     949 / 1000At timestep:     966 / 1000At timestep:     986 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -17.299999237060547
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2798.031552553177
Exploration Critic Loss : 509444.21875
Exploitation Critic Loss : 0.07044734060764313
Exploration Model Loss : 20.0753173828125
Actor Loss : 0.520851194858551
Eval_AverageReturn : -18.647058486938477
Eval_StdReturn : 4.422861576080322
Eval_MaxReturn : -10.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 19.647058823529413
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -17.410000
best mean reward -16.780001
running time 2861.903244
At timestep:     14 / 1000At timestep:     32 / 1000At timestep:     50 / 1000At timestep:     67 / 1000At timestep:     92 / 1000At timestep:     108 / 1000At timestep:     122 / 1000At timestep:     139 / 1000At timestep:     158 / 1000At timestep:     174 / 1000At timestep:     193 / 1000At timestep:     208 / 1000At timestep:     221 / 1000At timestep:     239 / 1000At timestep:     253 / 1000At timestep:     269 / 1000At timestep:     284 / 1000At timestep:     302 / 1000At timestep:     325 / 1000At timestep:     341 / 1000At timestep:     367 / 1000At timestep:     387 / 1000At timestep:     402 / 1000At timestep:     423 / 1000At timestep:     444 / 1000At timestep:     469 / 1000At timestep:     497 / 1000At timestep:     514 / 1000At timestep:     539 / 1000At timestep:     557 / 1000At timestep:     573 / 1000At timestep:     595 / 1000At timestep:     619 / 1000At timestep:     651 / 1000At timestep:     670 / 1000At timestep:     687 / 1000At timestep:     707 / 1000At timestep:     724 / 1000At timestep:     755 / 1000At timestep:     770 / 1000At timestep:     786 / 1000At timestep:     804 / 1000At timestep:     821 / 1000At timestep:     839 / 1000At timestep:     858 / 1000At timestep:     881 / 1000At timestep:     900 / 1000At timestep:     922 / 1000At timestep:     942 / 1000At timestep:     954 / 1000At timestep:     970 / 1000At timestep:     992 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -17.40999984741211
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2861.9032442569733
Exploration Critic Loss : 506837.375
Exploitation Critic Loss : 0.06261927634477615
Exploration Model Loss : 20.075042724609375
Actor Loss : 0.48100602626800537
Eval_AverageReturn : -18.09433937072754
Eval_StdReturn : 4.265987396240234
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.09433962264151
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -17.840000
best mean reward -16.780001
running time 2925.597836
At timestep:     24 / 1000At timestep:     43 / 1000At timestep:     67 / 1000At timestep:     85 / 1000At timestep:     100 / 1000At timestep:     115 / 1000At timestep:     131 / 1000At timestep:     157 / 1000At timestep:     172 / 1000At timestep:     194 / 1000At timestep:     212 / 1000At timestep:     239 / 1000At timestep:     260 / 1000At timestep:     278 / 1000At timestep:     305 / 1000At timestep:     326 / 1000At timestep:     343 / 1000At timestep:     366 / 1000At timestep:     380 / 1000At timestep:     396 / 1000At timestep:     426 / 1000At timestep:     448 / 1000At timestep:     471 / 1000At timestep:     491 / 1000At timestep:     506 / 1000At timestep:     524 / 1000At timestep:     540 / 1000At timestep:     554 / 1000At timestep:     587 / 1000At timestep:     610 / 1000At timestep:     630 / 1000At timestep:     651 / 1000At timestep:     679 / 1000At timestep:     700 / 1000At timestep:     728 / 1000At timestep:     753 / 1000At timestep:     776 / 1000At timestep:     791 / 1000At timestep:     813 / 1000At timestep:     830 / 1000At timestep:     846 / 1000At timestep:     866 / 1000At timestep:     883 / 1000At timestep:     916 / 1000At timestep:     943 / 1000At timestep:     964 / 1000At timestep:     982 / 1000At timestep:     998 / 1000At timestep:     1014 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_easy_supervised_lam2_PointmassEasy-v0_22-11-2022_21-37-12/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -17.84000015258789
Train_BestReturn : -16.780000686645508
TimeSinceStart : 2925.59783577919
Exploration Critic Loss : 681569.625
Exploitation Critic Loss : 0.078541100025177
Exploration Model Loss : 20.074722290039062
Actor Loss : 0.45572853088378906
Eval_AverageReturn : -19.693878173828125
Eval_StdReturn : 4.899829864501953
Eval_MaxReturn : -13.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 20.693877551020407
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_supervised_lam10_PointmassEasy-v0_22-11-2022_22-27-04 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_supervised_lam10_PointmassEasy-v0_22-11-2022_22-27-04
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002385
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.002384662628173828
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -49.650002
best mean reward -inf
running time 16.759895
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -49.650001525878906
TimeSinceStart : 16.75989532470703
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -49.599998
best mean reward -inf
running time 34.141511
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -49.599998474121094
TimeSinceStart : 34.1415114402771
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -46.609375
best mean reward -inf
running time 81.512831
At timestep:     50 / 1000At timestep:     95 / 1000At timestep:     145 / 1000At timestep:     195 / 1000At timestep:     245 / 1000At timestep:     287 / 1000At timestep:     331 / 1000At timestep:     381 / 1000At timestep:     431 / 1000At timestep:     481 / 1000At timestep:     531 / 1000At timestep:     581 / 1000At timestep:     631 / 1000At timestep:     681 / 1000At timestep:     731 / 1000At timestep:     781 / 1000At timestep:     831 / 1000At timestep:     881 / 1000At timestep:     931 / 1000At timestep:     981 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -46.609375
TimeSinceStart : 81.51283144950867
Exploration Critic Loss : 768491.3125
Exploitation Critic Loss : 0.07408905029296875
Exploration Model Loss : 53.11866760253906
Actor Loss : 1.1886558532714844
Eval_AverageReturn : -48.095237731933594
Eval_StdReturn : 4.439055442810059
Eval_MaxReturn : -32.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 48.285714285714285
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -40.701031
best mean reward -inf
running time 131.552242
At timestep:     50 / 1000At timestep:     95 / 1000At timestep:     145 / 1000At timestep:     195 / 1000At timestep:     241 / 1000At timestep:     272 / 1000At timestep:     322 / 1000At timestep:     372 / 1000At timestep:     410 / 1000At timestep:     455 / 1000At timestep:     505 / 1000At timestep:     555 / 1000At timestep:     585 / 1000At timestep:     622 / 1000At timestep:     672 / 1000At timestep:     722 / 1000At timestep:     772 / 1000At timestep:     822 / 1000At timestep:     872 / 1000At timestep:     922 / 1000At timestep:     963 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -40.70103073120117
TimeSinceStart : 131.55224180221558
Exploration Critic Loss : 7679201.0
Exploitation Critic Loss : 0.11290709674358368
Exploration Model Loss : 79.78553771972656
Actor Loss : 0.9746887683868408
Eval_AverageReturn : -45.681819915771484
Eval_StdReturn : 6.717591285705566
Eval_MaxReturn : -29.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 46.04545454545455
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -30.790001
best mean reward -30.790001
running time 183.094916
At timestep:     44 / 1000At timestep:     94 / 1000At timestep:     114 / 1000At timestep:     144 / 1000At timestep:     194 / 1000At timestep:     233 / 1000At timestep:     283 / 1000At timestep:     333 / 1000At timestep:     383 / 1000At timestep:     433 / 1000At timestep:     483 / 1000At timestep:     533 / 1000At timestep:     583 / 1000At timestep:     633 / 1000At timestep:     654 / 1000At timestep:     704 / 1000At timestep:     754 / 1000At timestep:     797 / 1000At timestep:     841 / 1000At timestep:     891 / 1000At timestep:     927 / 1000At timestep:     977 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -30.790000915527344
Train_BestReturn : -30.790000915527344
TimeSinceStart : 183.09491562843323
Exploration Critic Loss : 5281328.5
Exploitation Critic Loss : 0.07559919357299805
Exploration Model Loss : 60.84966278076172
Actor Loss : 0.8014199733734131
Eval_AverageReturn : -44.260868072509766
Eval_StdReturn : 9.501566886901855
Eval_MaxReturn : -19.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 44.65217391304348
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -21.910000
best mean reward -21.910000
running time 238.579122
At timestep:     46 / 1000At timestep:     79 / 1000At timestep:     110 / 1000At timestep:     138 / 1000At timestep:     160 / 1000At timestep:     185 / 1000At timestep:     230 / 1000At timestep:     280 / 1000At timestep:     315 / 1000At timestep:     352 / 1000At timestep:     402 / 1000At timestep:     446 / 1000At timestep:     474 / 1000At timestep:     524 / 1000At timestep:     559 / 1000At timestep:     597 / 1000At timestep:     622 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     821 / 1000At timestep:     850 / 1000At timestep:     894 / 1000At timestep:     944 / 1000At timestep:     994 / 1000At timestep:     1044 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -21.90999984741211
Train_BestReturn : -21.90999984741211
TimeSinceStart : 238.5791220664978
Exploration Critic Loss : 5103367.5
Exploitation Critic Loss : 0.09741795063018799
Exploration Model Loss : 43.51290512084961
Actor Loss : 0.6801677942276001
Eval_AverageReturn : -38.0
Eval_StdReturn : 10.603981971740723
Eval_MaxReturn : -20.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 38.666666666666664
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -17.650000
best mean reward -17.650000
running time 295.926476
At timestep:     28 / 1000At timestep:     51 / 1000At timestep:     70 / 1000At timestep:     95 / 1000At timestep:     121 / 1000At timestep:     145 / 1000At timestep:     195 / 1000At timestep:     221 / 1000At timestep:     262 / 1000At timestep:     295 / 1000At timestep:     319 / 1000At timestep:     359 / 1000At timestep:     401 / 1000At timestep:     417 / 1000At timestep:     443 / 1000At timestep:     472 / 1000At timestep:     502 / 1000At timestep:     519 / 1000At timestep:     537 / 1000At timestep:     580 / 1000At timestep:     602 / 1000At timestep:     634 / 1000At timestep:     654 / 1000At timestep:     679 / 1000At timestep:     700 / 1000At timestep:     738 / 1000At timestep:     764 / 1000At timestep:     787 / 1000At timestep:     837 / 1000At timestep:     876 / 1000At timestep:     926 / 1000At timestep:     954 / 1000At timestep:     983 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -17.649999618530273
Train_BestReturn : -17.649999618530273
TimeSinceStart : 295.92647647857666
Exploration Critic Loss : 5764155.0
Exploitation Critic Loss : 0.19107641279697418
Exploration Model Loss : 36.20185852050781
Actor Loss : 0.6319320201873779
Eval_AverageReturn : -28.794116973876953
Eval_StdReturn : 9.688620567321777
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 29.705882352941178
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -16.879999
best mean reward -16.879999
running time 355.289953
At timestep:     41 / 1000At timestep:     68 / 1000At timestep:     101 / 1000At timestep:     139 / 1000At timestep:     168 / 1000At timestep:     218 / 1000At timestep:     249 / 1000At timestep:     270 / 1000At timestep:     305 / 1000At timestep:     341 / 1000At timestep:     380 / 1000At timestep:     402 / 1000At timestep:     439 / 1000At timestep:     463 / 1000At timestep:     513 / 1000At timestep:     536 / 1000At timestep:     559 / 1000At timestep:     598 / 1000At timestep:     612 / 1000At timestep:     654 / 1000At timestep:     700 / 1000At timestep:     721 / 1000At timestep:     746 / 1000At timestep:     782 / 1000At timestep:     821 / 1000At timestep:     871 / 1000At timestep:     905 / 1000At timestep:     951 / 1000At timestep:     973 / 1000At timestep:     995 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -16.8799991607666
Train_BestReturn : -16.8799991607666
TimeSinceStart : 355.2899534702301
Exploration Critic Loss : 4694789.0
Exploitation Critic Loss : 0.11303915083408356
Exploration Model Loss : 34.898681640625
Actor Loss : 0.639828085899353
Eval_AverageReturn : -32.0
Eval_StdReturn : 10.006449699401855
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 32.903225806451616
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -16.639999
best mean reward -16.639999
running time 414.946286
At timestep:     25 / 1000At timestep:     55 / 1000At timestep:     94 / 1000At timestep:     114 / 1000At timestep:     139 / 1000At timestep:     159 / 1000At timestep:     185 / 1000At timestep:     222 / 1000At timestep:     247 / 1000At timestep:     278 / 1000At timestep:     303 / 1000At timestep:     344 / 1000At timestep:     382 / 1000At timestep:     419 / 1000At timestep:     436 / 1000At timestep:     486 / 1000At timestep:     509 / 1000At timestep:     541 / 1000At timestep:     567 / 1000At timestep:     592 / 1000At timestep:     611 / 1000At timestep:     656 / 1000At timestep:     678 / 1000At timestep:     712 / 1000At timestep:     757 / 1000At timestep:     779 / 1000At timestep:     809 / 1000At timestep:     837 / 1000At timestep:     887 / 1000At timestep:     908 / 1000At timestep:     926 / 1000At timestep:     976 / 1000At timestep:     988 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -16.639999389648438
Train_BestReturn : -16.639999389648438
TimeSinceStart : 414.9462857246399
Exploration Critic Loss : 4182993.75
Exploitation Critic Loss : 0.12935137748718262
Exploration Model Loss : 60.438941955566406
Actor Loss : 0.6605098843574524
Eval_AverageReturn : -28.676469802856445
Eval_StdReturn : 10.306252479553223
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 29.61764705882353
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -17.400000
best mean reward -16.639999
running time 472.638374
At timestep:     17 / 1000At timestep:     42 / 1000At timestep:     72 / 1000At timestep:     98 / 1000At timestep:     120 / 1000At timestep:     170 / 1000At timestep:     184 / 1000At timestep:     217 / 1000At timestep:     238 / 1000At timestep:     260 / 1000At timestep:     278 / 1000At timestep:     303 / 1000At timestep:     322 / 1000At timestep:     349 / 1000At timestep:     364 / 1000At timestep:     383 / 1000At timestep:     408 / 1000At timestep:     428 / 1000At timestep:     471 / 1000At timestep:     509 / 1000At timestep:     546 / 1000At timestep:     580 / 1000At timestep:     601 / 1000At timestep:     621 / 1000At timestep:     646 / 1000At timestep:     687 / 1000At timestep:     708 / 1000At timestep:     723 / 1000At timestep:     740 / 1000At timestep:     768 / 1000At timestep:     793 / 1000At timestep:     813 / 1000At timestep:     848 / 1000At timestep:     861 / 1000At timestep:     878 / 1000At timestep:     927 / 1000At timestep:     945 / 1000At timestep:     957 / 1000At timestep:     994 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -17.399999618530273
Train_BestReturn : -16.639999389648438
TimeSinceStart : 472.63837361335754
Exploration Critic Loss : 3187084.0
Exploitation Critic Loss : 0.10818677395582199
Exploration Model Loss : 42.10862731933594
Actor Loss : 0.6365724802017212
Eval_AverageReturn : -24.25
Eval_StdReturn : 9.756407737731934
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 25.225
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -17.290001
best mean reward -16.639999
running time 534.526654
At timestep:     30 / 1000At timestep:     47 / 1000At timestep:     66 / 1000At timestep:     78 / 1000At timestep:     115 / 1000At timestep:     133 / 1000At timestep:     151 / 1000At timestep:     168 / 1000At timestep:     185 / 1000At timestep:     209 / 1000At timestep:     230 / 1000At timestep:     256 / 1000At timestep:     303 / 1000At timestep:     323 / 1000At timestep:     343 / 1000At timestep:     366 / 1000At timestep:     380 / 1000At timestep:     395 / 1000At timestep:     425 / 1000At timestep:     445 / 1000At timestep:     476 / 1000At timestep:     509 / 1000At timestep:     556 / 1000At timestep:     577 / 1000At timestep:     606 / 1000At timestep:     626 / 1000At timestep:     646 / 1000At timestep:     673 / 1000At timestep:     701 / 1000At timestep:     721 / 1000At timestep:     738 / 1000At timestep:     765 / 1000At timestep:     792 / 1000At timestep:     815 / 1000At timestep:     850 / 1000At timestep:     874 / 1000At timestep:     887 / 1000At timestep:     925 / 1000At timestep:     943 / 1000At timestep:     968 / 1000At timestep:     985 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -17.290000915527344
Train_BestReturn : -16.639999389648438
TimeSinceStart : 534.5266540050507
Exploration Critic Loss : 2685423.5
Exploitation Critic Loss : 0.09644003212451935
Exploration Model Loss : 38.35423278808594
Actor Loss : 0.518877387046814
Eval_AverageReturn : -22.904762268066406
Eval_StdReturn : 8.141046524047852
Eval_MaxReturn : -11.0
Eval_MinReturn : -46.0
Eval_AverageEpLen : 23.904761904761905
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -17.410000
best mean reward -16.639999
running time 596.014347
At timestep:     23 / 1000At timestep:     55 / 1000At timestep:     97 / 1000At timestep:     121 / 1000At timestep:     149 / 1000At timestep:     175 / 1000At timestep:     192 / 1000At timestep:     220 / 1000At timestep:     243 / 1000At timestep:     271 / 1000At timestep:     295 / 1000At timestep:     319 / 1000At timestep:     336 / 1000At timestep:     360 / 1000At timestep:     386 / 1000At timestep:     400 / 1000At timestep:     421 / 1000At timestep:     444 / 1000At timestep:     466 / 1000At timestep:     480 / 1000At timestep:     497 / 1000At timestep:     519 / 1000At timestep:     566 / 1000At timestep:     585 / 1000At timestep:     607 / 1000At timestep:     621 / 1000At timestep:     641 / 1000At timestep:     660 / 1000At timestep:     685 / 1000At timestep:     703 / 1000At timestep:     719 / 1000At timestep:     744 / 1000At timestep:     774 / 1000At timestep:     808 / 1000At timestep:     829 / 1000At timestep:     859 / 1000At timestep:     875 / 1000At timestep:     891 / 1000At timestep:     904 / 1000At timestep:     926 / 1000At timestep:     947 / 1000At timestep:     967 / 1000At timestep:     998 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -17.40999984741211
Train_BestReturn : -16.639999389648438
TimeSinceStart : 596.0143465995789
Exploration Critic Loss : 4009341.75
Exploitation Critic Loss : 0.06981121003627777
Exploration Model Loss : 58.696014404296875
Actor Loss : 0.6459709405899048
Eval_AverageReturn : -22.204545974731445
Eval_StdReturn : 6.897230625152588
Eval_MaxReturn : -12.0
Eval_MinReturn : -46.0
Eval_AverageEpLen : 23.204545454545453
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -17.580000
best mean reward -16.639999
running time 658.774244
At timestep:     18 / 1000At timestep:     53 / 1000At timestep:     80 / 1000At timestep:     116 / 1000At timestep:     145 / 1000At timestep:     188 / 1000At timestep:     212 / 1000At timestep:     237 / 1000At timestep:     252 / 1000At timestep:     274 / 1000At timestep:     310 / 1000At timestep:     336 / 1000At timestep:     368 / 1000At timestep:     393 / 1000At timestep:     418 / 1000At timestep:     441 / 1000At timestep:     461 / 1000At timestep:     489 / 1000At timestep:     523 / 1000At timestep:     541 / 1000At timestep:     558 / 1000At timestep:     581 / 1000At timestep:     618 / 1000At timestep:     647 / 1000At timestep:     676 / 1000At timestep:     691 / 1000At timestep:     726 / 1000At timestep:     750 / 1000At timestep:     779 / 1000At timestep:     793 / 1000At timestep:     832 / 1000At timestep:     861 / 1000At timestep:     885 / 1000At timestep:     911 / 1000At timestep:     944 / 1000At timestep:     986 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -17.579999923706055
Train_BestReturn : -16.639999389648438
TimeSinceStart : 658.7742435932159
Exploration Critic Loss : 1618359.75
Exploitation Critic Loss : 0.12067847698926926
Exploration Model Loss : 23.943878173828125
Actor Loss : 0.5492563247680664
Eval_AverageReturn : -26.027027130126953
Eval_StdReturn : 7.677578449249268
Eval_MaxReturn : -13.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 27.027027027027028
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -17.150000
best mean reward -16.639999
running time 718.474996
At timestep:     17 / 1000At timestep:     32 / 1000At timestep:     47 / 1000At timestep:     71 / 1000At timestep:     87 / 1000At timestep:     104 / 1000At timestep:     123 / 1000At timestep:     135 / 1000At timestep:     154 / 1000At timestep:     187 / 1000At timestep:     224 / 1000At timestep:     249 / 1000At timestep:     262 / 1000At timestep:     283 / 1000At timestep:     325 / 1000At timestep:     347 / 1000At timestep:     359 / 1000At timestep:     371 / 1000At timestep:     392 / 1000At timestep:     414 / 1000At timestep:     439 / 1000At timestep:     461 / 1000At timestep:     474 / 1000At timestep:     489 / 1000At timestep:     505 / 1000At timestep:     536 / 1000At timestep:     552 / 1000At timestep:     586 / 1000At timestep:     612 / 1000At timestep:     625 / 1000At timestep:     640 / 1000At timestep:     654 / 1000At timestep:     704 / 1000At timestep:     720 / 1000At timestep:     753 / 1000At timestep:     773 / 1000At timestep:     792 / 1000At timestep:     810 / 1000At timestep:     860 / 1000At timestep:     885 / 1000At timestep:     911 / 1000At timestep:     925 / 1000At timestep:     941 / 1000At timestep:     957 / 1000At timestep:     990 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -17.149999618530273
Train_BestReturn : -16.639999389648438
TimeSinceStart : 718.4749958515167
Exploration Critic Loss : 1836775.75
Exploitation Critic Loss : 0.10592970252037048
Exploration Model Loss : 32.487548828125
Actor Loss : 0.5397509336471558
Eval_AverageReturn : -21.086956024169922
Eval_StdReturn : 9.491016387939453
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 22.043478260869566
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -17.459999
best mean reward -16.639999
running time 781.000298
At timestep:     33 / 1000At timestep:     55 / 1000At timestep:     76 / 1000At timestep:     92 / 1000At timestep:     114 / 1000At timestep:     137 / 1000At timestep:     155 / 1000At timestep:     170 / 1000At timestep:     192 / 1000At timestep:     221 / 1000At timestep:     245 / 1000At timestep:     261 / 1000At timestep:     286 / 1000At timestep:     297 / 1000At timestep:     309 / 1000At timestep:     332 / 1000At timestep:     350 / 1000At timestep:     369 / 1000At timestep:     395 / 1000At timestep:     425 / 1000At timestep:     458 / 1000At timestep:     472 / 1000At timestep:     483 / 1000At timestep:     504 / 1000At timestep:     520 / 1000At timestep:     554 / 1000At timestep:     575 / 1000At timestep:     592 / 1000At timestep:     616 / 1000At timestep:     629 / 1000At timestep:     660 / 1000At timestep:     689 / 1000At timestep:     723 / 1000At timestep:     738 / 1000At timestep:     759 / 1000At timestep:     787 / 1000At timestep:     812 / 1000At timestep:     832 / 1000At timestep:     872 / 1000At timestep:     891 / 1000At timestep:     911 / 1000At timestep:     930 / 1000At timestep:     950 / 1000At timestep:     968 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -17.459999084472656
Train_BestReturn : -16.639999389648438
TimeSinceStart : 781.0002982616425
Exploration Critic Loss : 2580646.0
Exploitation Critic Loss : 0.06272079795598984
Exploration Model Loss : 56.01780700683594
Actor Loss : 0.5514026880264282
Eval_AverageReturn : -21.64444351196289
Eval_StdReturn : 7.899804592132568
Eval_MaxReturn : -10.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 22.622222222222224
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -17.610001
best mean reward -16.639999
running time 842.063461
At timestep:     31 / 1000At timestep:     55 / 1000At timestep:     70 / 1000At timestep:     82 / 1000At timestep:     104 / 1000At timestep:     130 / 1000At timestep:     155 / 1000At timestep:     172 / 1000At timestep:     192 / 1000At timestep:     225 / 1000At timestep:     239 / 1000At timestep:     257 / 1000At timestep:     275 / 1000At timestep:     290 / 1000At timestep:     304 / 1000At timestep:     321 / 1000At timestep:     339 / 1000At timestep:     369 / 1000At timestep:     389 / 1000At timestep:     429 / 1000At timestep:     446 / 1000At timestep:     480 / 1000At timestep:     497 / 1000At timestep:     534 / 1000At timestep:     562 / 1000At timestep:     581 / 1000At timestep:     599 / 1000At timestep:     617 / 1000At timestep:     631 / 1000At timestep:     654 / 1000At timestep:     670 / 1000At timestep:     694 / 1000At timestep:     717 / 1000At timestep:     730 / 1000At timestep:     753 / 1000At timestep:     775 / 1000At timestep:     790 / 1000At timestep:     821 / 1000At timestep:     845 / 1000At timestep:     871 / 1000At timestep:     907 / 1000At timestep:     924 / 1000At timestep:     945 / 1000At timestep:     964 / 1000At timestep:     980 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -17.610000610351562
Train_BestReturn : -16.639999389648438
TimeSinceStart : 842.0634605884552
Exploration Critic Loss : 964832.125
Exploitation Critic Loss : 0.15333200991153717
Exploration Model Loss : 23.451370239257812
Actor Loss : 0.5485559701919556
Eval_AverageReturn : -21.021739959716797
Eval_StdReturn : 7.029407501220703
Eval_MaxReturn : -11.0
Eval_MinReturn : -39.0
Eval_AverageEpLen : 22.02173913043478
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -17.430000
best mean reward -16.639999
running time 903.604931
At timestep:     21 / 1000At timestep:     33 / 1000At timestep:     55 / 1000At timestep:     74 / 1000At timestep:     105 / 1000At timestep:     122 / 1000At timestep:     143 / 1000At timestep:     161 / 1000At timestep:     199 / 1000At timestep:     216 / 1000At timestep:     245 / 1000At timestep:     268 / 1000At timestep:     290 / 1000At timestep:     308 / 1000At timestep:     339 / 1000At timestep:     357 / 1000At timestep:     378 / 1000At timestep:     408 / 1000At timestep:     424 / 1000At timestep:     447 / 1000At timestep:     462 / 1000At timestep:     481 / 1000At timestep:     499 / 1000At timestep:     526 / 1000At timestep:     550 / 1000At timestep:     569 / 1000At timestep:     590 / 1000At timestep:     609 / 1000At timestep:     643 / 1000At timestep:     659 / 1000At timestep:     684 / 1000At timestep:     700 / 1000At timestep:     718 / 1000At timestep:     737 / 1000At timestep:     761 / 1000At timestep:     790 / 1000At timestep:     805 / 1000At timestep:     826 / 1000At timestep:     840 / 1000At timestep:     856 / 1000At timestep:     884 / 1000At timestep:     908 / 1000At timestep:     936 / 1000At timestep:     961 / 1000At timestep:     981 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -17.43000030517578
Train_BestReturn : -16.639999389648438
TimeSinceStart : 903.6049313545227
Exploration Critic Loss : 964785.5
Exploitation Critic Loss : 0.07392355054616928
Exploration Model Loss : 32.24931335449219
Actor Loss : 0.511683464050293
Eval_AverageReturn : -20.934782028198242
Eval_StdReturn : 5.669913291931152
Eval_MaxReturn : -11.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 21.934782608695652
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -17.440001
best mean reward -16.639999
running time 965.138107
At timestep:     15 / 1000At timestep:     30 / 1000At timestep:     52 / 1000At timestep:     70 / 1000At timestep:     97 / 1000At timestep:     120 / 1000At timestep:     148 / 1000At timestep:     164 / 1000At timestep:     183 / 1000At timestep:     196 / 1000At timestep:     218 / 1000At timestep:     241 / 1000At timestep:     260 / 1000At timestep:     282 / 1000At timestep:     308 / 1000At timestep:     336 / 1000At timestep:     356 / 1000At timestep:     381 / 1000At timestep:     409 / 1000At timestep:     425 / 1000At timestep:     474 / 1000At timestep:     503 / 1000At timestep:     527 / 1000At timestep:     556 / 1000At timestep:     573 / 1000At timestep:     598 / 1000At timestep:     620 / 1000At timestep:     640 / 1000At timestep:     655 / 1000At timestep:     668 / 1000At timestep:     692 / 1000At timestep:     710 / 1000At timestep:     734 / 1000At timestep:     746 / 1000At timestep:     794 / 1000At timestep:     821 / 1000At timestep:     851 / 1000At timestep:     870 / 1000At timestep:     885 / 1000At timestep:     914 / 1000At timestep:     931 / 1000At timestep:     948 / 1000At timestep:     967 / 1000At timestep:     985 / 1000At timestep:     998 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -17.440000534057617
Train_BestReturn : -16.639999389648438
TimeSinceStart : 965.138106584549
Exploration Critic Loss : 1500790.625
Exploitation Critic Loss : 0.11492866277694702
Exploration Model Loss : 55.9039306640625
Actor Loss : 0.551038384437561
Eval_AverageReturn : -21.086956024169922
Eval_StdReturn : 7.540689945220947
Eval_MaxReturn : -11.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 22.08695652173913
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.410000
best mean reward -16.639999
running time 1027.697517
At timestep:     25 / 1000At timestep:     42 / 1000At timestep:     57 / 1000At timestep:     74 / 1000At timestep:     103 / 1000At timestep:     153 / 1000At timestep:     189 / 1000At timestep:     227 / 1000At timestep:     259 / 1000At timestep:     272 / 1000At timestep:     289 / 1000At timestep:     316 / 1000At timestep:     338 / 1000At timestep:     365 / 1000At timestep:     415 / 1000At timestep:     434 / 1000At timestep:     454 / 1000At timestep:     478 / 1000At timestep:     492 / 1000At timestep:     510 / 1000At timestep:     533 / 1000At timestep:     551 / 1000At timestep:     569 / 1000At timestep:     593 / 1000At timestep:     611 / 1000At timestep:     630 / 1000At timestep:     649 / 1000At timestep:     665 / 1000At timestep:     690 / 1000At timestep:     709 / 1000At timestep:     726 / 1000At timestep:     755 / 1000At timestep:     775 / 1000At timestep:     793 / 1000At timestep:     821 / 1000At timestep:     842 / 1000At timestep:     857 / 1000At timestep:     871 / 1000At timestep:     898 / 1000At timestep:     913 / 1000At timestep:     944 / 1000At timestep:     958 / 1000At timestep:     977 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.40999984741211
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1027.6975173950195
Exploration Critic Loss : 640831.75
Exploitation Critic Loss : 0.0689554512500763
Exploration Model Loss : 23.422073364257812
Actor Loss : 0.5426859855651855
Eval_AverageReturn : -21.886363983154297
Eval_StdReturn : 8.584383010864258
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 22.84090909090909
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.400000
best mean reward -16.639999
running time 1088.483437
At timestep:     36 / 1000At timestep:     56 / 1000At timestep:     79 / 1000At timestep:     97 / 1000At timestep:     123 / 1000At timestep:     143 / 1000At timestep:     167 / 1000At timestep:     197 / 1000At timestep:     221 / 1000At timestep:     233 / 1000At timestep:     255 / 1000At timestep:     267 / 1000At timestep:     290 / 1000At timestep:     308 / 1000At timestep:     333 / 1000At timestep:     352 / 1000At timestep:     368 / 1000At timestep:     385 / 1000At timestep:     398 / 1000At timestep:     417 / 1000At timestep:     440 / 1000At timestep:     460 / 1000At timestep:     473 / 1000At timestep:     502 / 1000At timestep:     528 / 1000At timestep:     546 / 1000At timestep:     567 / 1000At timestep:     579 / 1000At timestep:     595 / 1000At timestep:     618 / 1000At timestep:     644 / 1000At timestep:     664 / 1000At timestep:     686 / 1000At timestep:     719 / 1000At timestep:     750 / 1000At timestep:     767 / 1000At timestep:     788 / 1000At timestep:     822 / 1000At timestep:     843 / 1000At timestep:     863 / 1000At timestep:     877 / 1000At timestep:     897 / 1000At timestep:     916 / 1000At timestep:     932 / 1000At timestep:     954 / 1000At timestep:     966 / 1000At timestep:     977 / 1000At timestep:     994 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.399999618530273
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1088.4834372997284
Exploration Critic Loss : 938152.0
Exploitation Critic Loss : 0.08628533780574799
Exploration Model Loss : 32.23455810546875
Actor Loss : 0.4619870185852051
Eval_AverageReturn : -19.918367385864258
Eval_StdReturn : 6.026596546173096
Eval_MaxReturn : -10.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 20.918367346938776
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.510000
best mean reward -16.639999
running time 1150.542346
At timestep:     35 / 1000At timestep:     58 / 1000At timestep:     81 / 1000At timestep:     119 / 1000At timestep:     144 / 1000At timestep:     169 / 1000At timestep:     190 / 1000At timestep:     214 / 1000At timestep:     235 / 1000At timestep:     257 / 1000At timestep:     277 / 1000At timestep:     289 / 1000At timestep:     315 / 1000At timestep:     341 / 1000At timestep:     358 / 1000At timestep:     379 / 1000At timestep:     398 / 1000At timestep:     428 / 1000At timestep:     445 / 1000At timestep:     477 / 1000At timestep:     498 / 1000At timestep:     513 / 1000At timestep:     529 / 1000At timestep:     546 / 1000At timestep:     581 / 1000At timestep:     606 / 1000At timestep:     626 / 1000At timestep:     657 / 1000At timestep:     675 / 1000At timestep:     692 / 1000At timestep:     720 / 1000At timestep:     732 / 1000At timestep:     750 / 1000At timestep:     774 / 1000At timestep:     800 / 1000At timestep:     818 / 1000At timestep:     829 / 1000At timestep:     850 / 1000At timestep:     868 / 1000At timestep:     885 / 1000At timestep:     906 / 1000At timestep:     934 / 1000At timestep:     953 / 1000At timestep:     972 / 1000At timestep:     997 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.510000228881836
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1150.5423460006714
Exploration Critic Loss : 1169291.5
Exploitation Critic Loss : 0.11942071467638016
Exploration Model Loss : 55.89869689941406
Actor Loss : 0.5186237692832947
Eval_AverageReturn : -20.95652198791504
Eval_StdReturn : 6.118242263793945
Eval_MaxReturn : -10.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 21.956521739130434
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.600000
best mean reward -16.639999
running time 1211.954915
At timestep:     24 / 1000At timestep:     36 / 1000At timestep:     55 / 1000At timestep:     73 / 1000At timestep:     90 / 1000At timestep:     115 / 1000At timestep:     132 / 1000At timestep:     148 / 1000At timestep:     177 / 1000At timestep:     209 / 1000At timestep:     227 / 1000At timestep:     240 / 1000At timestep:     255 / 1000At timestep:     295 / 1000At timestep:     312 / 1000At timestep:     352 / 1000At timestep:     370 / 1000At timestep:     389 / 1000At timestep:     407 / 1000At timestep:     425 / 1000At timestep:     451 / 1000At timestep:     473 / 1000At timestep:     489 / 1000At timestep:     507 / 1000At timestep:     518 / 1000At timestep:     544 / 1000At timestep:     566 / 1000At timestep:     593 / 1000At timestep:     611 / 1000At timestep:     630 / 1000At timestep:     650 / 1000At timestep:     669 / 1000At timestep:     711 / 1000At timestep:     735 / 1000At timestep:     759 / 1000At timestep:     783 / 1000At timestep:     801 / 1000At timestep:     813 / 1000At timestep:     835 / 1000At timestep:     858 / 1000At timestep:     880 / 1000At timestep:     904 / 1000At timestep:     926 / 1000At timestep:     943 / 1000At timestep:     964 / 1000At timestep:     986 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.600000381469727
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1211.9549152851105
Exploration Critic Loss : 1262232.375
Exploitation Critic Loss : 0.07918570935726166
Exploration Model Loss : 23.421173095703125
Actor Loss : 0.5018160343170166
Eval_AverageReturn : -20.510639190673828
Eval_StdReturn : 6.642456531524658
Eval_MaxReturn : -10.0
Eval_MinReturn : -41.0
Eval_AverageEpLen : 21.51063829787234
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -17.520000
best mean reward -16.639999
running time 1276.132424
At timestep:     15 / 1000At timestep:     30 / 1000At timestep:     49 / 1000At timestep:     70 / 1000At timestep:     89 / 1000At timestep:     109 / 1000At timestep:     132 / 1000At timestep:     156 / 1000At timestep:     189 / 1000At timestep:     210 / 1000At timestep:     225 / 1000At timestep:     238 / 1000At timestep:     254 / 1000At timestep:     289 / 1000At timestep:     302 / 1000At timestep:     325 / 1000At timestep:     347 / 1000At timestep:     369 / 1000At timestep:     385 / 1000At timestep:     398 / 1000At timestep:     417 / 1000At timestep:     445 / 1000At timestep:     465 / 1000At timestep:     478 / 1000At timestep:     493 / 1000At timestep:     521 / 1000At timestep:     552 / 1000At timestep:     585 / 1000At timestep:     609 / 1000At timestep:     631 / 1000At timestep:     651 / 1000At timestep:     673 / 1000At timestep:     690 / 1000At timestep:     703 / 1000At timestep:     731 / 1000At timestep:     754 / 1000At timestep:     770 / 1000At timestep:     791 / 1000At timestep:     808 / 1000At timestep:     827 / 1000At timestep:     843 / 1000At timestep:     864 / 1000At timestep:     897 / 1000At timestep:     909 / 1000At timestep:     926 / 1000At timestep:     949 / 1000At timestep:     967 / 1000At timestep:     984 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -17.520000457763672
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1276.1324243545532
Exploration Critic Loss : 1104006.25
Exploitation Critic Loss : 0.12762151658535004
Exploration Model Loss : 32.23439025878906
Actor Loss : 0.524855375289917
Eval_AverageReturn : -19.428571701049805
Eval_StdReturn : 5.802884578704834
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 20.428571428571427
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -17.830000
best mean reward -16.639999
running time 1339.584790
At timestep:     24 / 1000At timestep:     59 / 1000At timestep:     78 / 1000At timestep:     98 / 1000At timestep:     119 / 1000At timestep:     142 / 1000At timestep:     158 / 1000At timestep:     199 / 1000At timestep:     220 / 1000At timestep:     241 / 1000At timestep:     255 / 1000At timestep:     277 / 1000At timestep:     291 / 1000At timestep:     306 / 1000At timestep:     327 / 1000At timestep:     366 / 1000At timestep:     381 / 1000At timestep:     396 / 1000At timestep:     421 / 1000At timestep:     449 / 1000At timestep:     462 / 1000At timestep:     477 / 1000At timestep:     505 / 1000At timestep:     521 / 1000At timestep:     536 / 1000At timestep:     560 / 1000At timestep:     579 / 1000At timestep:     597 / 1000At timestep:     622 / 1000At timestep:     635 / 1000At timestep:     650 / 1000At timestep:     663 / 1000At timestep:     676 / 1000At timestep:     701 / 1000At timestep:     721 / 1000At timestep:     736 / 1000At timestep:     756 / 1000At timestep:     772 / 1000At timestep:     790 / 1000At timestep:     805 / 1000At timestep:     821 / 1000At timestep:     839 / 1000At timestep:     857 / 1000At timestep:     881 / 1000At timestep:     896 / 1000At timestep:     922 / 1000At timestep:     934 / 1000At timestep:     949 / 1000At timestep:     969 / 1000At timestep:     993 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -17.829999923706055
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1339.5847902297974
Exploration Critic Loss : 816642.25
Exploitation Critic Loss : 0.09644126147031784
Exploration Model Loss : 55.89886474609375
Actor Loss : 0.5198870897293091
Eval_AverageReturn : -18.784313201904297
Eval_StdReturn : 6.300680637359619
Eval_MaxReturn : -11.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 19.784313725490197
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -18.299999
best mean reward -16.639999
running time 1402.974324
At timestep:     15 / 1000At timestep:     32 / 1000At timestep:     51 / 1000At timestep:     80 / 1000At timestep:     97 / 1000At timestep:     118 / 1000At timestep:     135 / 1000At timestep:     152 / 1000At timestep:     175 / 1000At timestep:     195 / 1000At timestep:     215 / 1000At timestep:     234 / 1000At timestep:     252 / 1000At timestep:     271 / 1000At timestep:     294 / 1000At timestep:     315 / 1000At timestep:     334 / 1000At timestep:     353 / 1000At timestep:     369 / 1000At timestep:     389 / 1000At timestep:     407 / 1000At timestep:     434 / 1000At timestep:     452 / 1000At timestep:     467 / 1000At timestep:     485 / 1000At timestep:     501 / 1000At timestep:     519 / 1000At timestep:     542 / 1000At timestep:     569 / 1000At timestep:     587 / 1000At timestep:     609 / 1000At timestep:     632 / 1000At timestep:     648 / 1000At timestep:     680 / 1000At timestep:     702 / 1000At timestep:     720 / 1000At timestep:     740 / 1000At timestep:     768 / 1000At timestep:     792 / 1000At timestep:     807 / 1000At timestep:     833 / 1000At timestep:     852 / 1000At timestep:     874 / 1000At timestep:     891 / 1000At timestep:     908 / 1000At timestep:     920 / 1000At timestep:     939 / 1000At timestep:     962 / 1000At timestep:     981 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -18.299999237060547
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1402.974324464798
Exploration Critic Loss : 1062178.875
Exploitation Critic Loss : 0.09776775538921356
Exploration Model Loss : 23.420974731445312
Actor Loss : 0.5698486566543579
Eval_AverageReturn : -19.079999923706055
Eval_StdReturn : 3.9589898586273193
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 20.08
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -18.110001
best mean reward -16.639999
running time 1462.787458
At timestep:     21 / 1000At timestep:     41 / 1000At timestep:     58 / 1000At timestep:     74 / 1000At timestep:     95 / 1000At timestep:     118 / 1000At timestep:     142 / 1000At timestep:     162 / 1000At timestep:     188 / 1000At timestep:     202 / 1000At timestep:     225 / 1000At timestep:     244 / 1000At timestep:     269 / 1000At timestep:     284 / 1000At timestep:     305 / 1000At timestep:     317 / 1000At timestep:     338 / 1000At timestep:     360 / 1000At timestep:     382 / 1000At timestep:     400 / 1000At timestep:     415 / 1000At timestep:     429 / 1000At timestep:     445 / 1000At timestep:     469 / 1000At timestep:     485 / 1000At timestep:     505 / 1000At timestep:     527 / 1000At timestep:     550 / 1000At timestep:     575 / 1000At timestep:     620 / 1000At timestep:     643 / 1000At timestep:     663 / 1000At timestep:     688 / 1000At timestep:     706 / 1000At timestep:     717 / 1000At timestep:     737 / 1000At timestep:     753 / 1000At timestep:     771 / 1000At timestep:     792 / 1000At timestep:     819 / 1000At timestep:     837 / 1000At timestep:     861 / 1000At timestep:     879 / 1000At timestep:     899 / 1000At timestep:     921 / 1000At timestep:     938 / 1000At timestep:     956 / 1000At timestep:     971 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -18.110000610351562
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1462.7874584197998
Exploration Critic Loss : 1095178.5
Exploitation Critic Loss : 0.14900998771190643
Exploration Model Loss : 32.23420715332031
Actor Loss : 0.583350419998169
Eval_AverageReturn : -19.40816307067871
Eval_StdReturn : 5.271867752075195
Eval_MaxReturn : -10.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 20.408163265306122
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -17.420000
best mean reward -16.639999
running time 1524.969305
At timestep:     19 / 1000At timestep:     31 / 1000At timestep:     48 / 1000At timestep:     67 / 1000At timestep:     96 / 1000At timestep:     113 / 1000At timestep:     135 / 1000At timestep:     153 / 1000At timestep:     170 / 1000At timestep:     195 / 1000At timestep:     216 / 1000At timestep:     240 / 1000At timestep:     257 / 1000At timestep:     277 / 1000At timestep:     293 / 1000At timestep:     320 / 1000At timestep:     340 / 1000At timestep:     361 / 1000At timestep:     386 / 1000At timestep:     401 / 1000At timestep:     417 / 1000At timestep:     444 / 1000At timestep:     481 / 1000At timestep:     502 / 1000At timestep:     532 / 1000At timestep:     554 / 1000At timestep:     574 / 1000At timestep:     590 / 1000At timestep:     605 / 1000At timestep:     621 / 1000At timestep:     635 / 1000At timestep:     658 / 1000At timestep:     674 / 1000At timestep:     690 / 1000At timestep:     708 / 1000At timestep:     725 / 1000At timestep:     755 / 1000At timestep:     770 / 1000At timestep:     790 / 1000At timestep:     819 / 1000At timestep:     845 / 1000At timestep:     863 / 1000At timestep:     893 / 1000At timestep:     914 / 1000At timestep:     934 / 1000At timestep:     956 / 1000At timestep:     987 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -17.420000076293945
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1524.969304561615
Exploration Critic Loss : 837375.125
Exploitation Critic Loss : 0.06870172917842865
Exploration Model Loss : 55.89903259277344
Actor Loss : 0.4921282231807709
Eval_AverageReturn : -19.875
Eval_StdReturn : 5.4299516677856445
Eval_MaxReturn : -11.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 20.875
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -16.910000
best mean reward -16.639999
running time 1588.394331
At timestep:     16 / 1000At timestep:     34 / 1000At timestep:     59 / 1000At timestep:     88 / 1000At timestep:     108 / 1000At timestep:     136 / 1000At timestep:     155 / 1000At timestep:     170 / 1000At timestep:     192 / 1000At timestep:     209 / 1000At timestep:     226 / 1000At timestep:     246 / 1000At timestep:     263 / 1000At timestep:     280 / 1000At timestep:     303 / 1000At timestep:     322 / 1000At timestep:     344 / 1000At timestep:     361 / 1000At timestep:     376 / 1000At timestep:     398 / 1000At timestep:     425 / 1000At timestep:     442 / 1000At timestep:     458 / 1000At timestep:     495 / 1000At timestep:     513 / 1000At timestep:     530 / 1000At timestep:     552 / 1000At timestep:     574 / 1000At timestep:     591 / 1000At timestep:     608 / 1000At timestep:     631 / 1000At timestep:     655 / 1000At timestep:     670 / 1000At timestep:     702 / 1000At timestep:     717 / 1000At timestep:     741 / 1000At timestep:     760 / 1000At timestep:     776 / 1000At timestep:     797 / 1000At timestep:     811 / 1000At timestep:     825 / 1000At timestep:     839 / 1000At timestep:     858 / 1000At timestep:     878 / 1000At timestep:     903 / 1000At timestep:     920 / 1000At timestep:     947 / 1000At timestep:     965 / 1000At timestep:     982 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -16.90999984741211
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1588.3943309783936
Exploration Critic Loss : 1376274.5
Exploitation Critic Loss : 0.06915087252855301
Exploration Model Loss : 23.420791625976562
Actor Loss : 0.4233006238937378
Eval_AverageReturn : -19.059999465942383
Eval_StdReturn : 4.834914684295654
Eval_MaxReturn : -13.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 20.06
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -17.400000
best mean reward -16.639999
running time 1649.994382
At timestep:     13 / 1000At timestep:     37 / 1000At timestep:     55 / 1000At timestep:     69 / 1000At timestep:     99 / 1000At timestep:     120 / 1000At timestep:     143 / 1000At timestep:     160 / 1000At timestep:     178 / 1000At timestep:     190 / 1000At timestep:     206 / 1000At timestep:     222 / 1000At timestep:     238 / 1000At timestep:     256 / 1000At timestep:     272 / 1000At timestep:     295 / 1000At timestep:     308 / 1000At timestep:     326 / 1000At timestep:     340 / 1000At timestep:     362 / 1000At timestep:     380 / 1000At timestep:     402 / 1000At timestep:     420 / 1000At timestep:     457 / 1000At timestep:     475 / 1000At timestep:     489 / 1000At timestep:     512 / 1000At timestep:     530 / 1000At timestep:     548 / 1000At timestep:     569 / 1000At timestep:     591 / 1000At timestep:     611 / 1000At timestep:     623 / 1000At timestep:     645 / 1000At timestep:     672 / 1000At timestep:     694 / 1000At timestep:     714 / 1000At timestep:     735 / 1000At timestep:     756 / 1000At timestep:     780 / 1000At timestep:     800 / 1000At timestep:     823 / 1000At timestep:     851 / 1000At timestep:     866 / 1000At timestep:     890 / 1000At timestep:     927 / 1000At timestep:     956 / 1000At timestep:     974 / 1000At timestep:     988 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -17.399999618530273
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1649.9943821430206
Exploration Critic Loss : 1396942.25
Exploitation Critic Loss : 0.10766083747148514
Exploration Model Loss : 32.23402404785156
Actor Loss : 0.4923262596130371
Eval_AverageReturn : -19.18000030517578
Eval_StdReturn : 5.454135894775391
Eval_MaxReturn : -11.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 20.18
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -17.500000
best mean reward -16.639999
running time 1713.517691
At timestep:     23 / 1000At timestep:     40 / 1000At timestep:     53 / 1000At timestep:     79 / 1000At timestep:     95 / 1000At timestep:     114 / 1000At timestep:     128 / 1000At timestep:     146 / 1000At timestep:     171 / 1000At timestep:     188 / 1000At timestep:     214 / 1000At timestep:     246 / 1000At timestep:     260 / 1000At timestep:     277 / 1000At timestep:     291 / 1000At timestep:     307 / 1000At timestep:     335 / 1000At timestep:     353 / 1000At timestep:     369 / 1000At timestep:     384 / 1000At timestep:     396 / 1000At timestep:     412 / 1000At timestep:     431 / 1000At timestep:     453 / 1000At timestep:     465 / 1000At timestep:     492 / 1000At timestep:     508 / 1000At timestep:     534 / 1000At timestep:     549 / 1000At timestep:     565 / 1000At timestep:     581 / 1000At timestep:     599 / 1000At timestep:     617 / 1000At timestep:     643 / 1000At timestep:     662 / 1000At timestep:     673 / 1000At timestep:     695 / 1000At timestep:     717 / 1000At timestep:     741 / 1000At timestep:     760 / 1000At timestep:     782 / 1000At timestep:     798 / 1000At timestep:     817 / 1000At timestep:     838 / 1000At timestep:     855 / 1000At timestep:     882 / 1000At timestep:     905 / 1000At timestep:     931 / 1000At timestep:     947 / 1000At timestep:     970 / 1000At timestep:     989 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -17.5
Train_BestReturn : -16.639999389648438
TimeSinceStart : 1713.517690896988
Exploration Critic Loss : 997559.375
Exploitation Critic Loss : 0.0817592591047287
Exploration Model Loss : 55.90025329589844
Actor Loss : 0.43286609649658203
Eval_AverageReturn : -18.326923370361328
Eval_StdReturn : 4.766554355621338
Eval_MaxReturn : -10.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.326923076923077
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -16.610001
best mean reward -16.610001
running time 1777.918488
At timestep:     16 / 1000At timestep:     30 / 1000At timestep:     44 / 1000At timestep:     62 / 1000At timestep:     87 / 1000At timestep:     105 / 1000At timestep:     122 / 1000At timestep:     153 / 1000At timestep:     173 / 1000At timestep:     201 / 1000At timestep:     224 / 1000At timestep:     238 / 1000At timestep:     265 / 1000At timestep:     292 / 1000At timestep:     313 / 1000At timestep:     329 / 1000At timestep:     353 / 1000At timestep:     378 / 1000At timestep:     395 / 1000At timestep:     409 / 1000At timestep:     447 / 1000At timestep:     473 / 1000At timestep:     489 / 1000At timestep:     507 / 1000At timestep:     521 / 1000At timestep:     537 / 1000At timestep:     561 / 1000At timestep:     588 / 1000At timestep:     609 / 1000At timestep:     625 / 1000At timestep:     647 / 1000At timestep:     661 / 1000At timestep:     683 / 1000At timestep:     715 / 1000At timestep:     729 / 1000At timestep:     774 / 1000At timestep:     789 / 1000At timestep:     806 / 1000At timestep:     820 / 1000At timestep:     844 / 1000At timestep:     864 / 1000At timestep:     890 / 1000At timestep:     910 / 1000At timestep:     934 / 1000At timestep:     950 / 1000At timestep:     966 / 1000At timestep:     989 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -16.610000610351562
Train_BestReturn : -16.610000610351562
TimeSinceStart : 1777.9184882640839
Exploration Critic Loss : 678182.4375
Exploitation Critic Loss : 0.06948460638523102
Exploration Model Loss : 23.419570922851562
Actor Loss : 0.4647153615951538
Eval_AverageReturn : -20.04166603088379
Eval_StdReturn : 6.538215637207031
Eval_MaxReturn : -13.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 21.041666666666668
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -16.459999
best mean reward -16.459999
running time 1841.310304
At timestep:     14 / 1000At timestep:     34 / 1000At timestep:     48 / 1000At timestep:     68 / 1000At timestep:     85 / 1000At timestep:     103 / 1000At timestep:     125 / 1000At timestep:     152 / 1000At timestep:     170 / 1000At timestep:     191 / 1000At timestep:     214 / 1000At timestep:     236 / 1000At timestep:     250 / 1000At timestep:     271 / 1000At timestep:     288 / 1000At timestep:     306 / 1000At timestep:     320 / 1000At timestep:     352 / 1000At timestep:     378 / 1000At timestep:     405 / 1000At timestep:     424 / 1000At timestep:     444 / 1000At timestep:     461 / 1000At timestep:     487 / 1000At timestep:     502 / 1000At timestep:     518 / 1000At timestep:     544 / 1000At timestep:     560 / 1000At timestep:     577 / 1000At timestep:     594 / 1000At timestep:     614 / 1000At timestep:     631 / 1000At timestep:     653 / 1000At timestep:     692 / 1000At timestep:     715 / 1000At timestep:     738 / 1000At timestep:     753 / 1000At timestep:     769 / 1000At timestep:     793 / 1000At timestep:     820 / 1000At timestep:     849 / 1000At timestep:     873 / 1000At timestep:     893 / 1000At timestep:     917 / 1000At timestep:     943 / 1000At timestep:     959 / 1000At timestep:     976 / 1000At timestep:     991 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -16.459999084472656
Train_BestReturn : -16.459999084472656
TimeSinceStart : 1841.310304403305
Exploration Critic Loss : 805834.9375
Exploitation Critic Loss : 0.1292792409658432
Exploration Model Loss : 32.23280334472656
Actor Loss : 0.559381902217865
Eval_AverageReturn : -19.5510196685791
Eval_StdReturn : 5.202280521392822
Eval_MaxReturn : -13.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 20.551020408163264
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -16.889999
best mean reward -16.459999
running time 1905.653419
At timestep:     21 / 1000At timestep:     39 / 1000At timestep:     56 / 1000At timestep:     82 / 1000At timestep:     93 / 1000At timestep:     117 / 1000At timestep:     136 / 1000At timestep:     157 / 1000At timestep:     185 / 1000At timestep:     202 / 1000At timestep:     230 / 1000At timestep:     258 / 1000At timestep:     276 / 1000At timestep:     296 / 1000At timestep:     310 / 1000At timestep:     330 / 1000At timestep:     357 / 1000At timestep:     382 / 1000At timestep:     406 / 1000At timestep:     423 / 1000At timestep:     448 / 1000At timestep:     464 / 1000At timestep:     484 / 1000At timestep:     502 / 1000At timestep:     517 / 1000At timestep:     532 / 1000At timestep:     550 / 1000At timestep:     574 / 1000At timestep:     594 / 1000At timestep:     610 / 1000At timestep:     629 / 1000At timestep:     652 / 1000At timestep:     671 / 1000At timestep:     692 / 1000At timestep:     715 / 1000At timestep:     733 / 1000At timestep:     753 / 1000At timestep:     771 / 1000At timestep:     796 / 1000At timestep:     820 / 1000At timestep:     846 / 1000At timestep:     862 / 1000At timestep:     875 / 1000At timestep:     898 / 1000At timestep:     910 / 1000At timestep:     929 / 1000At timestep:     947 / 1000At timestep:     962 / 1000At timestep:     990 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -16.889999389648438
Train_BestReturn : -16.459999084472656
TimeSinceStart : 1905.6534192562103
Exploration Critic Loss : 562545.875
Exploitation Critic Loss : 0.08534625172615051
Exploration Model Loss : 55.90104675292969
Actor Loss : 0.569558322429657
Eval_AverageReturn : -19.280000686645508
Eval_StdReturn : 4.404724597930908
Eval_MaxReturn : -10.0
Eval_MinReturn : -27.0
Eval_AverageEpLen : 20.28
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -17.090000
best mean reward -16.459999
running time 1969.230392
At timestep:     18 / 1000At timestep:     46 / 1000At timestep:     60 / 1000At timestep:     78 / 1000At timestep:     99 / 1000At timestep:     114 / 1000At timestep:     138 / 1000At timestep:     158 / 1000At timestep:     177 / 1000At timestep:     195 / 1000At timestep:     210 / 1000At timestep:     228 / 1000At timestep:     249 / 1000At timestep:     266 / 1000At timestep:     284 / 1000At timestep:     300 / 1000At timestep:     323 / 1000At timestep:     344 / 1000At timestep:     356 / 1000At timestep:     373 / 1000At timestep:     392 / 1000At timestep:     436 / 1000At timestep:     453 / 1000At timestep:     471 / 1000At timestep:     492 / 1000At timestep:     542 / 1000At timestep:     565 / 1000At timestep:     586 / 1000At timestep:     601 / 1000At timestep:     617 / 1000At timestep:     645 / 1000At timestep:     659 / 1000At timestep:     679 / 1000At timestep:     698 / 1000At timestep:     730 / 1000At timestep:     742 / 1000At timestep:     761 / 1000At timestep:     776 / 1000At timestep:     805 / 1000At timestep:     820 / 1000At timestep:     834 / 1000At timestep:     859 / 1000At timestep:     875 / 1000At timestep:     891 / 1000At timestep:     908 / 1000At timestep:     924 / 1000At timestep:     955 / 1000At timestep:     975 / 1000At timestep:     999 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -17.09000015258789
Train_BestReturn : -16.459999084472656
TimeSinceStart : 1969.2303924560547
Exploration Critic Loss : 671423.9375
Exploitation Critic Loss : 0.07266375422477722
Exploration Model Loss : 23.418655395507812
Actor Loss : 0.45404449105262756
Eval_AverageReturn : -19.459999084472656
Eval_StdReturn : 7.203360557556152
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 20.44
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -17.600000
best mean reward -16.459999
running time 2032.073876
At timestep:     15 / 1000At timestep:     33 / 1000At timestep:     49 / 1000At timestep:     66 / 1000At timestep:     94 / 1000At timestep:     118 / 1000At timestep:     132 / 1000At timestep:     155 / 1000At timestep:     172 / 1000At timestep:     194 / 1000At timestep:     225 / 1000At timestep:     247 / 1000At timestep:     268 / 1000At timestep:     293 / 1000At timestep:     307 / 1000At timestep:     327 / 1000At timestep:     352 / 1000At timestep:     369 / 1000At timestep:     386 / 1000At timestep:     410 / 1000At timestep:     430 / 1000At timestep:     445 / 1000At timestep:     463 / 1000At timestep:     485 / 1000At timestep:     505 / 1000At timestep:     538 / 1000At timestep:     560 / 1000At timestep:     579 / 1000At timestep:     596 / 1000At timestep:     627 / 1000At timestep:     646 / 1000At timestep:     674 / 1000At timestep:     687 / 1000At timestep:     704 / 1000At timestep:     726 / 1000At timestep:     758 / 1000At timestep:     778 / 1000At timestep:     796 / 1000At timestep:     813 / 1000At timestep:     833 / 1000At timestep:     851 / 1000At timestep:     873 / 1000At timestep:     891 / 1000At timestep:     903 / 1000At timestep:     934 / 1000At timestep:     955 / 1000At timestep:     988 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -17.600000381469727
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2032.0738756656647
Exploration Critic Loss : 747847.125
Exploitation Critic Loss : 0.08942513167858124
Exploration Model Loss : 32.231903076171875
Actor Loss : 0.5742151737213135
Eval_AverageReturn : -19.91666603088379
Eval_StdReturn : 5.4038310050964355
Eval_MaxReturn : -11.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 20.916666666666668
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -17.209999
best mean reward -16.459999
running time 2094.902276
At timestep:     32 / 1000At timestep:     49 / 1000At timestep:     61 / 1000At timestep:     90 / 1000At timestep:     114 / 1000At timestep:     133 / 1000At timestep:     149 / 1000At timestep:     174 / 1000At timestep:     187 / 1000At timestep:     203 / 1000At timestep:     216 / 1000At timestep:     231 / 1000At timestep:     265 / 1000At timestep:     280 / 1000At timestep:     302 / 1000At timestep:     327 / 1000At timestep:     351 / 1000At timestep:     369 / 1000At timestep:     390 / 1000At timestep:     422 / 1000At timestep:     441 / 1000At timestep:     456 / 1000At timestep:     473 / 1000At timestep:     504 / 1000At timestep:     532 / 1000At timestep:     557 / 1000At timestep:     572 / 1000At timestep:     593 / 1000At timestep:     616 / 1000At timestep:     642 / 1000At timestep:     658 / 1000At timestep:     677 / 1000At timestep:     699 / 1000At timestep:     720 / 1000At timestep:     737 / 1000At timestep:     753 / 1000At timestep:     764 / 1000At timestep:     779 / 1000At timestep:     801 / 1000At timestep:     822 / 1000At timestep:     842 / 1000At timestep:     863 / 1000At timestep:     882 / 1000At timestep:     905 / 1000At timestep:     926 / 1000At timestep:     946 / 1000At timestep:     963 / 1000At timestep:     982 / 1000At timestep:     995 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -17.209999084472656
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2094.9022760391235
Exploration Critic Loss : 958220.5
Exploitation Critic Loss : 0.08557797968387604
Exploration Model Loss : 55.90135192871094
Actor Loss : 0.5670109391212463
Eval_AverageReturn : -19.18000030517578
Eval_StdReturn : 5.457801342010498
Eval_MaxReturn : -10.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 20.18
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -17.750000
best mean reward -16.459999
running time 2157.096975
At timestep:     24 / 1000At timestep:     47 / 1000At timestep:     61 / 1000At timestep:     80 / 1000At timestep:     105 / 1000At timestep:     121 / 1000At timestep:     143 / 1000At timestep:     162 / 1000At timestep:     183 / 1000At timestep:     203 / 1000At timestep:     215 / 1000At timestep:     236 / 1000At timestep:     256 / 1000At timestep:     276 / 1000At timestep:     291 / 1000At timestep:     306 / 1000At timestep:     318 / 1000At timestep:     333 / 1000At timestep:     347 / 1000At timestep:     365 / 1000At timestep:     387 / 1000At timestep:     409 / 1000At timestep:     429 / 1000At timestep:     444 / 1000At timestep:     463 / 1000At timestep:     492 / 1000At timestep:     510 / 1000At timestep:     533 / 1000At timestep:     554 / 1000At timestep:     569 / 1000At timestep:     588 / 1000At timestep:     604 / 1000At timestep:     618 / 1000At timestep:     640 / 1000At timestep:     657 / 1000At timestep:     674 / 1000At timestep:     700 / 1000At timestep:     714 / 1000At timestep:     739 / 1000At timestep:     756 / 1000At timestep:     770 / 1000At timestep:     796 / 1000At timestep:     810 / 1000At timestep:     823 / 1000At timestep:     844 / 1000At timestep:     863 / 1000At timestep:     884 / 1000At timestep:     897 / 1000At timestep:     914 / 1000At timestep:     939 / 1000At timestep:     955 / 1000At timestep:     982 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -17.75
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2157.096974849701
Exploration Critic Loss : 1013294.5
Exploitation Critic Loss : 0.0838608592748642
Exploration Model Loss : 23.418472290039062
Actor Loss : 0.4955442249774933
Eval_AverageReturn : -18.075471878051758
Eval_StdReturn : 4.412029266357422
Eval_MaxReturn : -11.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 19.07547169811321
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -17.830000
best mean reward -16.459999
running time 2219.063692
At timestep:     19 / 1000At timestep:     38 / 1000At timestep:     57 / 1000At timestep:     74 / 1000At timestep:     91 / 1000At timestep:     108 / 1000At timestep:     118 / 1000At timestep:     140 / 1000At timestep:     155 / 1000At timestep:     170 / 1000At timestep:     183 / 1000At timestep:     196 / 1000At timestep:     216 / 1000At timestep:     231 / 1000At timestep:     249 / 1000At timestep:     270 / 1000At timestep:     291 / 1000At timestep:     310 / 1000At timestep:     325 / 1000At timestep:     341 / 1000At timestep:     363 / 1000At timestep:     388 / 1000At timestep:     405 / 1000At timestep:     423 / 1000At timestep:     440 / 1000At timestep:     456 / 1000At timestep:     474 / 1000At timestep:     505 / 1000At timestep:     542 / 1000At timestep:     559 / 1000At timestep:     582 / 1000At timestep:     608 / 1000At timestep:     629 / 1000At timestep:     646 / 1000At timestep:     667 / 1000At timestep:     683 / 1000At timestep:     699 / 1000At timestep:     717 / 1000At timestep:     731 / 1000At timestep:     742 / 1000At timestep:     758 / 1000At timestep:     779 / 1000At timestep:     803 / 1000At timestep:     818 / 1000At timestep:     850 / 1000At timestep:     868 / 1000At timestep:     892 / 1000At timestep:     910 / 1000At timestep:     926 / 1000At timestep:     944 / 1000At timestep:     988 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -17.829999923706055
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2219.063691854477
Exploration Critic Loss : 675714.75
Exploitation Critic Loss : 0.07746042311191559
Exploration Model Loss : 32.231719970703125
Actor Loss : 0.4245506823062897
Eval_AverageReturn : -18.25
Eval_StdReturn : 6.037940979003906
Eval_MaxReturn : -9.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 19.25
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -17.180000
best mean reward -16.459999
running time 2284.329446
At timestep:     23 / 1000At timestep:     39 / 1000At timestep:     61 / 1000At timestep:     83 / 1000At timestep:     104 / 1000At timestep:     123 / 1000At timestep:     137 / 1000At timestep:     156 / 1000At timestep:     177 / 1000At timestep:     190 / 1000At timestep:     205 / 1000At timestep:     223 / 1000At timestep:     241 / 1000At timestep:     270 / 1000At timestep:     284 / 1000At timestep:     311 / 1000At timestep:     334 / 1000At timestep:     351 / 1000At timestep:     385 / 1000At timestep:     401 / 1000At timestep:     418 / 1000At timestep:     439 / 1000At timestep:     457 / 1000At timestep:     472 / 1000At timestep:     496 / 1000At timestep:     508 / 1000At timestep:     526 / 1000At timestep:     542 / 1000At timestep:     560 / 1000At timestep:     582 / 1000At timestep:     600 / 1000At timestep:     621 / 1000At timestep:     642 / 1000At timestep:     660 / 1000At timestep:     672 / 1000At timestep:     695 / 1000At timestep:     716 / 1000At timestep:     730 / 1000At timestep:     752 / 1000At timestep:     769 / 1000At timestep:     785 / 1000At timestep:     807 / 1000At timestep:     820 / 1000At timestep:     839 / 1000At timestep:     864 / 1000At timestep:     876 / 1000At timestep:     904 / 1000At timestep:     919 / 1000At timestep:     935 / 1000At timestep:     954 / 1000At timestep:     974 / 1000At timestep:     992 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -17.18000030517578
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2284.329445838928
Exploration Critic Loss : 901224.25
Exploitation Critic Loss : 0.1922573745250702
Exploration Model Loss : 55.90156555175781
Actor Loss : 0.4390433430671692
Eval_AverageReturn : -17.98113250732422
Eval_StdReturn : 4.516179084777832
Eval_MaxReturn : -11.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 18.9811320754717
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -17.430000
best mean reward -16.459999
running time 2348.935600
At timestep:     16 / 1000At timestep:     31 / 1000At timestep:     60 / 1000At timestep:     77 / 1000At timestep:     97 / 1000At timestep:     115 / 1000At timestep:     138 / 1000At timestep:     156 / 1000At timestep:     177 / 1000At timestep:     191 / 1000At timestep:     215 / 1000At timestep:     230 / 1000At timestep:     254 / 1000At timestep:     272 / 1000At timestep:     292 / 1000At timestep:     315 / 1000At timestep:     329 / 1000At timestep:     345 / 1000At timestep:     361 / 1000At timestep:     396 / 1000At timestep:     411 / 1000At timestep:     431 / 1000At timestep:     461 / 1000At timestep:     479 / 1000At timestep:     495 / 1000At timestep:     524 / 1000At timestep:     541 / 1000At timestep:     555 / 1000At timestep:     573 / 1000At timestep:     589 / 1000At timestep:     606 / 1000At timestep:     624 / 1000At timestep:     637 / 1000At timestep:     669 / 1000At timestep:     688 / 1000At timestep:     704 / 1000At timestep:     723 / 1000At timestep:     744 / 1000At timestep:     756 / 1000At timestep:     781 / 1000At timestep:     804 / 1000At timestep:     838 / 1000At timestep:     856 / 1000At timestep:     884 / 1000At timestep:     902 / 1000At timestep:     920 / 1000At timestep:     939 / 1000At timestep:     957 / 1000At timestep:     999 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -17.43000030517578
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2348.935600042343
Exploration Critic Loss : 791888.25
Exploitation Critic Loss : 0.07661522924900055
Exploration Model Loss : 23.418258666992188
Actor Loss : 0.5175468921661377
Eval_AverageReturn : -19.459999084472656
Eval_StdReturn : 6.210346221923828
Eval_MaxReturn : -11.0
Eval_MinReturn : -41.0
Eval_AverageEpLen : 20.46
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -17.629999
best mean reward -16.459999
running time 2412.429574
At timestep:     24 / 1000At timestep:     43 / 1000At timestep:     64 / 1000At timestep:     80 / 1000At timestep:     97 / 1000At timestep:     115 / 1000At timestep:     138 / 1000At timestep:     161 / 1000At timestep:     180 / 1000At timestep:     200 / 1000At timestep:     218 / 1000At timestep:     234 / 1000At timestep:     252 / 1000At timestep:     268 / 1000At timestep:     294 / 1000At timestep:     306 / 1000At timestep:     320 / 1000At timestep:     334 / 1000At timestep:     348 / 1000At timestep:     362 / 1000At timestep:     381 / 1000At timestep:     405 / 1000At timestep:     435 / 1000At timestep:     451 / 1000At timestep:     475 / 1000At timestep:     497 / 1000At timestep:     518 / 1000At timestep:     532 / 1000At timestep:     556 / 1000At timestep:     572 / 1000At timestep:     584 / 1000At timestep:     601 / 1000At timestep:     618 / 1000At timestep:     652 / 1000At timestep:     674 / 1000At timestep:     687 / 1000At timestep:     723 / 1000At timestep:     743 / 1000At timestep:     759 / 1000At timestep:     780 / 1000At timestep:     795 / 1000At timestep:     817 / 1000At timestep:     833 / 1000At timestep:     849 / 1000At timestep:     867 / 1000At timestep:     881 / 1000At timestep:     899 / 1000At timestep:     921 / 1000At timestep:     943 / 1000At timestep:     955 / 1000At timestep:     970 / 1000At timestep:     988 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -17.6299991607666
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2412.4295740127563
Exploration Critic Loss : 1004298.5
Exploitation Critic Loss : 0.14930808544158936
Exploration Model Loss : 32.23152160644531
Actor Loss : 0.49126511812210083
Eval_AverageReturn : -17.924528121948242
Eval_StdReturn : 5.0389018058776855
Eval_MaxReturn : -11.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 18.92452830188679
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -17.320000
best mean reward -16.459999
running time 2480.779923
At timestep:     13 / 1000At timestep:     37 / 1000At timestep:     51 / 1000At timestep:     73 / 1000At timestep:     92 / 1000At timestep:     114 / 1000At timestep:     134 / 1000At timestep:     149 / 1000At timestep:     164 / 1000At timestep:     179 / 1000At timestep:     201 / 1000At timestep:     217 / 1000At timestep:     231 / 1000At timestep:     248 / 1000At timestep:     270 / 1000At timestep:     293 / 1000At timestep:     311 / 1000At timestep:     330 / 1000At timestep:     344 / 1000At timestep:     373 / 1000At timestep:     408 / 1000At timestep:     437 / 1000At timestep:     463 / 1000At timestep:     487 / 1000At timestep:     507 / 1000At timestep:     525 / 1000At timestep:     544 / 1000At timestep:     568 / 1000At timestep:     586 / 1000At timestep:     607 / 1000At timestep:     625 / 1000At timestep:     640 / 1000At timestep:     662 / 1000At timestep:     679 / 1000At timestep:     697 / 1000At timestep:     725 / 1000At timestep:     754 / 1000At timestep:     770 / 1000At timestep:     785 / 1000At timestep:     807 / 1000At timestep:     831 / 1000At timestep:     847 / 1000At timestep:     868 / 1000At timestep:     892 / 1000At timestep:     912 / 1000At timestep:     928 / 1000At timestep:     953 / 1000At timestep:     974 / 1000At timestep:     998 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -17.31999969482422
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2480.779923439026
Exploration Critic Loss : 869544.375
Exploitation Critic Loss : 0.07785466313362122
Exploration Model Loss : 55.9017333984375
Actor Loss : 0.4748610258102417
Eval_AverageReturn : -19.3799991607666
Eval_StdReturn : 4.702722549438477
Eval_MaxReturn : -12.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 20.38
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -16.620001
best mean reward -16.459999
running time 2552.367757
At timestep:     16 / 1000At timestep:     51 / 1000At timestep:     75 / 1000At timestep:     88 / 1000At timestep:     106 / 1000At timestep:     127 / 1000At timestep:     139 / 1000At timestep:     163 / 1000At timestep:     179 / 1000At timestep:     195 / 1000At timestep:     221 / 1000At timestep:     239 / 1000At timestep:     256 / 1000At timestep:     273 / 1000At timestep:     286 / 1000At timestep:     300 / 1000At timestep:     323 / 1000At timestep:     335 / 1000At timestep:     348 / 1000At timestep:     364 / 1000At timestep:     386 / 1000At timestep:     402 / 1000At timestep:     424 / 1000At timestep:     437 / 1000At timestep:     464 / 1000At timestep:     485 / 1000At timestep:     501 / 1000At timestep:     529 / 1000At timestep:     551 / 1000At timestep:     568 / 1000At timestep:     594 / 1000At timestep:     629 / 1000At timestep:     657 / 1000At timestep:     676 / 1000At timestep:     694 / 1000At timestep:     711 / 1000At timestep:     731 / 1000At timestep:     749 / 1000At timestep:     772 / 1000At timestep:     791 / 1000At timestep:     807 / 1000At timestep:     820 / 1000At timestep:     836 / 1000At timestep:     858 / 1000At timestep:     872 / 1000At timestep:     884 / 1000At timestep:     908 / 1000At timestep:     925 / 1000At timestep:     944 / 1000At timestep:     965 / 1000At timestep:     981 / 1000At timestep:     995 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -16.6200008392334
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2552.367756843567
Exploration Critic Loss : 583581.5
Exploitation Critic Loss : 0.067543625831604
Exploration Model Loss : 23.4180908203125
Actor Loss : 0.5139769911766052
Eval_AverageReturn : -18.169811248779297
Eval_StdReturn : 5.322560787200928
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.169811320754718
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -17.010000
best mean reward -16.459999
running time 2622.775361
At timestep:     30 / 1000At timestep:     46 / 1000At timestep:     68 / 1000At timestep:     90 / 1000At timestep:     116 / 1000At timestep:     131 / 1000At timestep:     158 / 1000At timestep:     191 / 1000At timestep:     207 / 1000At timestep:     225 / 1000At timestep:     244 / 1000At timestep:     265 / 1000At timestep:     287 / 1000At timestep:     303 / 1000At timestep:     323 / 1000At timestep:     347 / 1000At timestep:     364 / 1000At timestep:     380 / 1000At timestep:     402 / 1000At timestep:     422 / 1000At timestep:     443 / 1000At timestep:     464 / 1000At timestep:     487 / 1000At timestep:     503 / 1000At timestep:     520 / 1000At timestep:     538 / 1000At timestep:     558 / 1000At timestep:     585 / 1000At timestep:     607 / 1000At timestep:     630 / 1000At timestep:     659 / 1000At timestep:     685 / 1000At timestep:     717 / 1000At timestep:     738 / 1000At timestep:     756 / 1000At timestep:     775 / 1000At timestep:     789 / 1000At timestep:     802 / 1000At timestep:     814 / 1000At timestep:     833 / 1000At timestep:     850 / 1000At timestep:     869 / 1000At timestep:     893 / 1000At timestep:     913 / 1000At timestep:     933 / 1000At timestep:     950 / 1000At timestep:     965 / 1000At timestep:     983 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -17.010000228881836
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2622.7753608226776
Exploration Critic Loss : 600278.0
Exploitation Critic Loss : 0.07675236463546753
Exploration Model Loss : 32.23133850097656
Actor Loss : 0.4483962655067444
Eval_AverageReturn : -19.46938705444336
Eval_StdReturn : 4.655924320220947
Eval_MaxReturn : -11.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 20.46938775510204
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -17.950001
best mean reward -16.459999
running time 2687.847395
At timestep:     23 / 1000At timestep:     36 / 1000At timestep:     56 / 1000At timestep:     85 / 1000At timestep:     99 / 1000At timestep:     117 / 1000At timestep:     138 / 1000At timestep:     158 / 1000At timestep:     176 / 1000At timestep:     192 / 1000At timestep:     209 / 1000At timestep:     228 / 1000At timestep:     247 / 1000At timestep:     269 / 1000At timestep:     285 / 1000At timestep:     302 / 1000At timestep:     325 / 1000At timestep:     342 / 1000At timestep:     358 / 1000At timestep:     374 / 1000At timestep:     406 / 1000At timestep:     421 / 1000At timestep:     445 / 1000At timestep:     461 / 1000At timestep:     476 / 1000At timestep:     490 / 1000At timestep:     512 / 1000At timestep:     528 / 1000At timestep:     545 / 1000At timestep:     562 / 1000At timestep:     575 / 1000At timestep:     595 / 1000At timestep:     625 / 1000At timestep:     641 / 1000At timestep:     655 / 1000At timestep:     684 / 1000At timestep:     706 / 1000At timestep:     726 / 1000At timestep:     754 / 1000At timestep:     779 / 1000At timestep:     791 / 1000At timestep:     812 / 1000At timestep:     828 / 1000At timestep:     849 / 1000At timestep:     880 / 1000At timestep:     912 / 1000At timestep:     938 / 1000At timestep:     956 / 1000At timestep:     972 / 1000At timestep:     992 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -17.950000762939453
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2687.8473947048187
Exploration Critic Loss : 504938.25
Exploitation Critic Loss : 0.11876586079597473
Exploration Model Loss : 55.90191650390625
Actor Loss : 0.5182623863220215
Eval_AverageReturn : -18.80392074584961
Eval_StdReturn : 5.183002471923828
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.80392156862745
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -17.930000
best mean reward -16.459999
running time 2754.710874
At timestep:     15 / 1000At timestep:     31 / 1000At timestep:     48 / 1000At timestep:     64 / 1000At timestep:     82 / 1000At timestep:     100 / 1000At timestep:     120 / 1000At timestep:     136 / 1000At timestep:     148 / 1000At timestep:     163 / 1000At timestep:     182 / 1000At timestep:     203 / 1000At timestep:     224 / 1000At timestep:     242 / 1000At timestep:     262 / 1000At timestep:     284 / 1000At timestep:     312 / 1000At timestep:     333 / 1000At timestep:     344 / 1000At timestep:     364 / 1000At timestep:     383 / 1000At timestep:     405 / 1000At timestep:     428 / 1000At timestep:     443 / 1000At timestep:     455 / 1000At timestep:     470 / 1000At timestep:     500 / 1000At timestep:     525 / 1000At timestep:     538 / 1000At timestep:     564 / 1000At timestep:     581 / 1000At timestep:     603 / 1000At timestep:     622 / 1000At timestep:     641 / 1000At timestep:     659 / 1000At timestep:     674 / 1000At timestep:     700 / 1000At timestep:     717 / 1000At timestep:     738 / 1000At timestep:     756 / 1000At timestep:     774 / 1000At timestep:     791 / 1000At timestep:     813 / 1000At timestep:     835 / 1000At timestep:     862 / 1000At timestep:     881 / 1000At timestep:     896 / 1000At timestep:     910 / 1000At timestep:     936 / 1000At timestep:     968 / 1000At timestep:     991 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -17.93000030517578
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2754.710873603821
Exploration Critic Loss : 579667.75
Exploitation Critic Loss : 0.10805210471153259
Exploration Model Loss : 23.417922973632812
Actor Loss : 0.5308352112770081
Eval_AverageReturn : -18.44230842590332
Eval_StdReturn : 4.542167663574219
Eval_MaxReturn : -10.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.442307692307693
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -16.950001
best mean reward -16.459999
running time 2829.886461
At timestep:     21 / 1000At timestep:     39 / 1000At timestep:     54 / 1000At timestep:     75 / 1000At timestep:     101 / 1000At timestep:     122 / 1000At timestep:     142 / 1000At timestep:     163 / 1000At timestep:     182 / 1000At timestep:     200 / 1000At timestep:     221 / 1000At timestep:     244 / 1000At timestep:     258 / 1000At timestep:     277 / 1000At timestep:     291 / 1000At timestep:     310 / 1000At timestep:     327 / 1000At timestep:     347 / 1000At timestep:     362 / 1000At timestep:     380 / 1000At timestep:     398 / 1000At timestep:     414 / 1000At timestep:     437 / 1000At timestep:     473 / 1000At timestep:     491 / 1000At timestep:     517 / 1000At timestep:     538 / 1000At timestep:     558 / 1000At timestep:     576 / 1000At timestep:     597 / 1000At timestep:     618 / 1000At timestep:     635 / 1000At timestep:     654 / 1000At timestep:     670 / 1000At timestep:     693 / 1000At timestep:     717 / 1000At timestep:     738 / 1000At timestep:     757 / 1000At timestep:     778 / 1000At timestep:     797 / 1000At timestep:     819 / 1000At timestep:     844 / 1000At timestep:     859 / 1000At timestep:     876 / 1000At timestep:     896 / 1000At timestep:     921 / 1000At timestep:     943 / 1000At timestep:     962 / 1000At timestep:     978 / 1000At timestep:     994 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -16.950000762939453
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2829.8864607810974
Exploration Critic Loss : 358034.1875
Exploitation Critic Loss : 0.1301674097776413
Exploration Model Loss : 32.23115539550781
Actor Loss : 0.5510218143463135
Eval_AverageReturn : -19.156862258911133
Eval_StdReturn : 4.207241535186768
Eval_MaxReturn : -13.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 20.15686274509804
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -17.070000
best mean reward -16.459999
running time 2904.182400
At timestep:     20 / 1000At timestep:     46 / 1000At timestep:     65 / 1000At timestep:     79 / 1000At timestep:     101 / 1000At timestep:     119 / 1000At timestep:     137 / 1000At timestep:     154 / 1000At timestep:     179 / 1000At timestep:     192 / 1000At timestep:     205 / 1000At timestep:     225 / 1000At timestep:     243 / 1000At timestep:     262 / 1000At timestep:     276 / 1000At timestep:     296 / 1000At timestep:     315 / 1000At timestep:     334 / 1000At timestep:     350 / 1000At timestep:     373 / 1000At timestep:     390 / 1000At timestep:     407 / 1000At timestep:     428 / 1000At timestep:     446 / 1000At timestep:     465 / 1000At timestep:     483 / 1000At timestep:     504 / 1000At timestep:     532 / 1000At timestep:     565 / 1000At timestep:     580 / 1000At timestep:     610 / 1000At timestep:     627 / 1000At timestep:     645 / 1000At timestep:     661 / 1000At timestep:     694 / 1000At timestep:     714 / 1000At timestep:     733 / 1000At timestep:     751 / 1000At timestep:     769 / 1000At timestep:     781 / 1000At timestep:     795 / 1000At timestep:     813 / 1000At timestep:     835 / 1000At timestep:     849 / 1000At timestep:     863 / 1000At timestep:     889 / 1000At timestep:     904 / 1000At timestep:     924 / 1000At timestep:     945 / 1000At timestep:     967 / 1000At timestep:     985 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -17.06999969482422
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2904.1824004650116
Exploration Critic Loss : 574777.5625
Exploitation Critic Loss : 0.06246592104434967
Exploration Model Loss : 55.90208435058594
Actor Loss : 0.5247510671615601
Eval_AverageReturn : -18.230770111083984
Eval_StdReturn : 4.647643089294434
Eval_MaxReturn : -11.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 19.23076923076923
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -17.860001
best mean reward -16.459999
running time 2978.514627
At timestep:     24 / 1000At timestep:     51 / 1000At timestep:     70 / 1000At timestep:     85 / 1000At timestep:     105 / 1000At timestep:     126 / 1000At timestep:     143 / 1000At timestep:     165 / 1000At timestep:     183 / 1000At timestep:     203 / 1000At timestep:     217 / 1000At timestep:     238 / 1000At timestep:     253 / 1000At timestep:     276 / 1000At timestep:     296 / 1000At timestep:     312 / 1000At timestep:     335 / 1000At timestep:     350 / 1000At timestep:     373 / 1000At timestep:     390 / 1000At timestep:     412 / 1000At timestep:     429 / 1000At timestep:     444 / 1000At timestep:     460 / 1000At timestep:     482 / 1000At timestep:     496 / 1000At timestep:     510 / 1000At timestep:     531 / 1000At timestep:     551 / 1000At timestep:     564 / 1000At timestep:     579 / 1000At timestep:     595 / 1000At timestep:     616 / 1000At timestep:     635 / 1000At timestep:     654 / 1000At timestep:     688 / 1000At timestep:     707 / 1000At timestep:     722 / 1000At timestep:     743 / 1000At timestep:     765 / 1000At timestep:     783 / 1000At timestep:     805 / 1000At timestep:     827 / 1000At timestep:     844 / 1000At timestep:     858 / 1000At timestep:     881 / 1000At timestep:     898 / 1000At timestep:     916 / 1000At timestep:     933 / 1000At timestep:     948 / 1000At timestep:     967 / 1000At timestep:     1001 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_easy_supervised_lam10_PointmassEasy-v0_22-11-2022_22-27-04/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -17.860000610351562
Train_BestReturn : -16.459999084472656
TimeSinceStart : 2978.5146272182465
Exploration Critic Loss : 542470.3125
Exploitation Critic Loss : 0.11616884917020798
Exploration Model Loss : 23.41766357421875
Actor Loss : 0.42414146661758423
Eval_AverageReturn : -18.25
Eval_StdReturn : 4.340662002563477
Eval_MaxReturn : -12.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 19.25
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_supervised_lam20_PointmassEasy-v0_22-11-2022_23-18-01 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_supervised_lam20_PointmassEasy-v0_22-11-2022_23-18-01
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.004645
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.004644632339477539
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -49.950001
best mean reward -inf
running time 19.675709
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -49.95000076293945
TimeSinceStart : 19.67570924758911
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -49.825001
best mean reward -inf
running time 38.657408
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -49.82500076293945
TimeSinceStart : 38.657407999038696
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -43.955223
best mean reward -inf
running time 94.963037
At timestep:     50 / 1000At timestep:     99 / 1000At timestep:     149 / 1000At timestep:     199 / 1000At timestep:     249 / 1000At timestep:     299 / 1000At timestep:     331 / 1000At timestep:     381 / 1000At timestep:     431 / 1000At timestep:     481 / 1000At timestep:     531 / 1000At timestep:     581 / 1000At timestep:     626 / 1000At timestep:     676 / 1000At timestep:     726 / 1000At timestep:     776 / 1000At timestep:     815 / 1000At timestep:     865 / 1000At timestep:     915 / 1000At timestep:     965 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -43.955223083496094
TimeSinceStart : 94.96303677558899
Exploration Critic Loss : 478801.5
Exploitation Critic Loss : 0.028309613466262817
Exploration Model Loss : 31.514860153198242
Actor Loss : 1.1457446813583374
Eval_AverageReturn : -48.14285659790039
Eval_StdReturn : 4.7437744140625
Eval_MaxReturn : -31.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 48.333333333333336
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -35.250000
best mean reward -35.250000
running time 154.647172
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     222 / 1000At timestep:     243 / 1000At timestep:     277 / 1000At timestep:     327 / 1000At timestep:     377 / 1000At timestep:     411 / 1000At timestep:     439 / 1000At timestep:     489 / 1000At timestep:     508 / 1000At timestep:     558 / 1000At timestep:     583 / 1000At timestep:     613 / 1000At timestep:     639 / 1000At timestep:     675 / 1000At timestep:     725 / 1000At timestep:     770 / 1000At timestep:     820 / 1000At timestep:     861 / 1000At timestep:     887 / 1000At timestep:     937 / 1000At timestep:     971 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -35.25
Train_BestReturn : -35.25
TimeSinceStart : 154.6471722126007
Exploration Critic Loss : 5490015.5
Exploitation Critic Loss : 0.05819230526685715
Exploration Model Loss : 32.074607849121094
Actor Loss : 0.8244507312774658
Eval_AverageReturn : -38.11538314819336
Eval_StdReturn : 11.556971549987793
Eval_MaxReturn : -18.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 38.69230769230769
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -23.120001
best mean reward -23.120001
running time 217.781302
At timestep:     47 / 1000At timestep:     97 / 1000At timestep:     119 / 1000At timestep:     139 / 1000At timestep:     176 / 1000At timestep:     226 / 1000At timestep:     242 / 1000At timestep:     285 / 1000At timestep:     318 / 1000At timestep:     368 / 1000At timestep:     418 / 1000At timestep:     451 / 1000At timestep:     501 / 1000At timestep:     529 / 1000At timestep:     551 / 1000At timestep:     593 / 1000At timestep:     643 / 1000At timestep:     693 / 1000At timestep:     718 / 1000At timestep:     760 / 1000At timestep:     810 / 1000At timestep:     847 / 1000At timestep:     897 / 1000At timestep:     947 / 1000At timestep:     997 / 1000At timestep:     1043 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -23.1200008392334
Train_BestReturn : -23.1200008392334
TimeSinceStart : 217.7813024520874
Exploration Critic Loss : 26534950.0
Exploitation Critic Loss : 0.07475147396326065
Exploration Model Loss : 191.12930297851562
Actor Loss : 0.7475341558456421
Eval_AverageReturn : -39.53845977783203
Eval_StdReturn : 11.659366607666016
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 40.11538461538461
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -19.110001
best mean reward -19.110001
running time 283.835658
At timestep:     37 / 1000At timestep:     62 / 1000At timestep:     99 / 1000At timestep:     128 / 1000At timestep:     153 / 1000At timestep:     175 / 1000At timestep:     224 / 1000At timestep:     274 / 1000At timestep:     304 / 1000At timestep:     338 / 1000At timestep:     375 / 1000At timestep:     425 / 1000At timestep:     465 / 1000At timestep:     511 / 1000At timestep:     538 / 1000At timestep:     587 / 1000At timestep:     610 / 1000At timestep:     634 / 1000At timestep:     668 / 1000At timestep:     696 / 1000At timestep:     725 / 1000At timestep:     775 / 1000At timestep:     807 / 1000At timestep:     839 / 1000At timestep:     884 / 1000At timestep:     924 / 1000At timestep:     946 / 1000At timestep:     966 / 1000At timestep:     991 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -19.110000610351562
Train_BestReturn : -19.110000610351562
TimeSinceStart : 283.8356580734253
Exploration Critic Loss : 9222428.0
Exploitation Critic Loss : 0.09093999117612839
Exploration Model Loss : 89.22359466552734
Actor Loss : 0.66736900806427
Eval_AverageReturn : -33.16666793823242
Eval_StdReturn : 9.58847713470459
Eval_MaxReturn : -19.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 34.1
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -17.740000
best mean reward -17.740000
running time 350.258024
At timestep:     39 / 1000At timestep:     78 / 1000At timestep:     107 / 1000At timestep:     134 / 1000At timestep:     156 / 1000At timestep:     185 / 1000At timestep:     218 / 1000At timestep:     249 / 1000At timestep:     282 / 1000At timestep:     305 / 1000At timestep:     319 / 1000At timestep:     356 / 1000At timestep:     406 / 1000At timestep:     433 / 1000At timestep:     451 / 1000At timestep:     471 / 1000At timestep:     506 / 1000At timestep:     524 / 1000At timestep:     541 / 1000At timestep:     575 / 1000At timestep:     614 / 1000At timestep:     646 / 1000At timestep:     668 / 1000At timestep:     692 / 1000At timestep:     713 / 1000At timestep:     733 / 1000At timestep:     754 / 1000At timestep:     785 / 1000At timestep:     806 / 1000At timestep:     838 / 1000At timestep:     864 / 1000At timestep:     893 / 1000At timestep:     924 / 1000At timestep:     946 / 1000At timestep:     978 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -17.739999771118164
Train_BestReturn : -17.739999771118164
TimeSinceStart : 350.2580235004425
Exploration Critic Loss : 4636575.5
Exploitation Critic Loss : 0.15957555174827576
Exploration Model Loss : 40.220794677734375
Actor Loss : 0.620468020439148
Eval_AverageReturn : -27.13888931274414
Eval_StdReturn : 7.760844707489014
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 28.11111111111111
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -17.780001
best mean reward -17.740000
running time 419.756836
At timestep:     29 / 1000At timestep:     49 / 1000At timestep:     70 / 1000At timestep:     98 / 1000At timestep:     130 / 1000At timestep:     173 / 1000At timestep:     203 / 1000At timestep:     230 / 1000At timestep:     255 / 1000At timestep:     292 / 1000At timestep:     315 / 1000At timestep:     361 / 1000At timestep:     404 / 1000At timestep:     443 / 1000At timestep:     465 / 1000At timestep:     503 / 1000At timestep:     521 / 1000At timestep:     549 / 1000At timestep:     581 / 1000At timestep:     603 / 1000At timestep:     622 / 1000At timestep:     652 / 1000At timestep:     688 / 1000At timestep:     719 / 1000At timestep:     749 / 1000At timestep:     776 / 1000At timestep:     795 / 1000At timestep:     845 / 1000At timestep:     878 / 1000At timestep:     904 / 1000At timestep:     934 / 1000At timestep:     961 / 1000At timestep:     987 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -17.780000686645508
Train_BestReturn : -17.739999771118164
TimeSinceStart : 419.75683641433716
Exploration Critic Loss : 10064169.0
Exploitation Critic Loss : 0.09388670325279236
Exploration Model Loss : 87.51956939697266
Actor Loss : 0.6078261137008667
Eval_AverageReturn : -28.558822631835938
Eval_StdReturn : 8.193222999572754
Eval_MaxReturn : -16.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 29.529411764705884
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -17.299999
best mean reward -17.299999
running time 488.927046
At timestep:     21 / 1000At timestep:     50 / 1000At timestep:     79 / 1000At timestep:     107 / 1000At timestep:     146 / 1000At timestep:     163 / 1000At timestep:     179 / 1000At timestep:     210 / 1000At timestep:     246 / 1000At timestep:     282 / 1000At timestep:     317 / 1000At timestep:     332 / 1000At timestep:     368 / 1000At timestep:     388 / 1000At timestep:     417 / 1000At timestep:     443 / 1000At timestep:     460 / 1000At timestep:     500 / 1000At timestep:     525 / 1000At timestep:     550 / 1000At timestep:     572 / 1000At timestep:     597 / 1000At timestep:     623 / 1000At timestep:     647 / 1000At timestep:     680 / 1000At timestep:     693 / 1000At timestep:     738 / 1000At timestep:     775 / 1000At timestep:     804 / 1000At timestep:     830 / 1000At timestep:     849 / 1000At timestep:     885 / 1000At timestep:     917 / 1000At timestep:     946 / 1000At timestep:     982 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -17.299999237060547
Train_BestReturn : -17.299999237060547
TimeSinceStart : 488.9270462989807
Exploration Critic Loss : 3799297.5
Exploitation Critic Loss : 0.09483209252357483
Exploration Model Loss : 33.37861633300781
Actor Loss : 0.5636229515075684
Eval_AverageReturn : -26.91666603088379
Eval_StdReturn : 7.725768566131592
Eval_MaxReturn : -12.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 27.916666666666668
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -17.820000
best mean reward -17.299999
running time 557.512532
At timestep:     15 / 1000At timestep:     33 / 1000At timestep:     66 / 1000At timestep:     84 / 1000At timestep:     97 / 1000At timestep:     116 / 1000At timestep:     133 / 1000At timestep:     160 / 1000At timestep:     183 / 1000At timestep:     206 / 1000At timestep:     227 / 1000At timestep:     244 / 1000At timestep:     259 / 1000At timestep:     281 / 1000At timestep:     301 / 1000At timestep:     328 / 1000At timestep:     349 / 1000At timestep:     372 / 1000At timestep:     387 / 1000At timestep:     411 / 1000At timestep:     428 / 1000At timestep:     470 / 1000At timestep:     498 / 1000At timestep:     543 / 1000At timestep:     569 / 1000At timestep:     595 / 1000At timestep:     621 / 1000At timestep:     663 / 1000At timestep:     691 / 1000At timestep:     715 / 1000At timestep:     738 / 1000At timestep:     763 / 1000At timestep:     783 / 1000At timestep:     813 / 1000At timestep:     834 / 1000At timestep:     877 / 1000At timestep:     891 / 1000At timestep:     935 / 1000At timestep:     954 / 1000At timestep:     980 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -17.81999969482422
Train_BestReturn : -17.299999237060547
TimeSinceStart : 557.5125317573547
Exploration Critic Loss : 6047893.0
Exploitation Critic Loss : 0.12489601969718933
Exploration Model Loss : 55.40775680541992
Actor Loss : 0.6474212408065796
Eval_AverageReturn : -23.80487823486328
Eval_StdReturn : 8.520332336425781
Eval_MaxReturn : -12.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 24.804878048780488
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -18.299999
best mean reward -17.299999
running time 627.956778
At timestep:     21 / 1000At timestep:     47 / 1000At timestep:     65 / 1000At timestep:     84 / 1000At timestep:     101 / 1000At timestep:     131 / 1000At timestep:     148 / 1000At timestep:     167 / 1000At timestep:     185 / 1000At timestep:     219 / 1000At timestep:     250 / 1000At timestep:     274 / 1000At timestep:     310 / 1000At timestep:     331 / 1000At timestep:     345 / 1000At timestep:     360 / 1000At timestep:     381 / 1000At timestep:     396 / 1000At timestep:     413 / 1000At timestep:     436 / 1000At timestep:     461 / 1000At timestep:     483 / 1000At timestep:     504 / 1000At timestep:     529 / 1000At timestep:     547 / 1000At timestep:     574 / 1000At timestep:     598 / 1000At timestep:     633 / 1000At timestep:     660 / 1000At timestep:     679 / 1000At timestep:     708 / 1000At timestep:     729 / 1000At timestep:     743 / 1000At timestep:     777 / 1000At timestep:     809 / 1000At timestep:     839 / 1000At timestep:     864 / 1000At timestep:     894 / 1000At timestep:     915 / 1000At timestep:     947 / 1000At timestep:     967 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -18.299999237060547
Train_BestReturn : -17.299999237060547
TimeSinceStart : 627.9567775726318
Exploration Critic Loss : 3372452.0
Exploitation Critic Loss : 0.08284920454025269
Exploration Model Loss : 21.904098510742188
Actor Loss : 0.6182940006256104
Eval_AverageReturn : -23.0
Eval_StdReturn : 6.647591590881348
Eval_MaxReturn : -13.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 24.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -17.950001
best mean reward -17.299999
running time 697.521540
At timestep:     35 / 1000At timestep:     48 / 1000At timestep:     69 / 1000At timestep:     99 / 1000At timestep:     118 / 1000At timestep:     139 / 1000At timestep:     170 / 1000At timestep:     203 / 1000At timestep:     237 / 1000At timestep:     260 / 1000At timestep:     280 / 1000At timestep:     303 / 1000At timestep:     328 / 1000At timestep:     344 / 1000At timestep:     361 / 1000At timestep:     383 / 1000At timestep:     405 / 1000At timestep:     425 / 1000At timestep:     449 / 1000At timestep:     480 / 1000At timestep:     494 / 1000At timestep:     523 / 1000At timestep:     545 / 1000At timestep:     578 / 1000At timestep:     594 / 1000At timestep:     615 / 1000At timestep:     630 / 1000At timestep:     654 / 1000At timestep:     672 / 1000At timestep:     687 / 1000At timestep:     699 / 1000At timestep:     717 / 1000At timestep:     733 / 1000At timestep:     755 / 1000At timestep:     788 / 1000At timestep:     817 / 1000At timestep:     841 / 1000At timestep:     862 / 1000At timestep:     879 / 1000At timestep:     896 / 1000At timestep:     913 / 1000At timestep:     932 / 1000At timestep:     957 / 1000At timestep:     974 / 1000At timestep:     996 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -17.950000762939453
Train_BestReturn : -17.299999237060547
TimeSinceStart : 697.5215396881104
Exploration Critic Loss : 4934364.5
Exploitation Critic Loss : 0.12958845496177673
Exploration Model Loss : 18.850006103515625
Actor Loss : 0.5193973183631897
Eval_AverageReturn : -21.19565200805664
Eval_StdReturn : 6.03835391998291
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 22.195652173913043
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -17.830000
best mean reward -17.299999
running time 769.007983
At timestep:     20 / 1000At timestep:     49 / 1000At timestep:     78 / 1000At timestep:     109 / 1000At timestep:     126 / 1000At timestep:     155 / 1000At timestep:     186 / 1000At timestep:     205 / 1000At timestep:     252 / 1000At timestep:     266 / 1000At timestep:     283 / 1000At timestep:     333 / 1000At timestep:     365 / 1000At timestep:     384 / 1000At timestep:     405 / 1000At timestep:     420 / 1000At timestep:     434 / 1000At timestep:     456 / 1000At timestep:     474 / 1000At timestep:     499 / 1000At timestep:     540 / 1000At timestep:     561 / 1000At timestep:     601 / 1000At timestep:     635 / 1000At timestep:     655 / 1000At timestep:     695 / 1000At timestep:     731 / 1000At timestep:     756 / 1000At timestep:     782 / 1000At timestep:     797 / 1000At timestep:     812 / 1000At timestep:     838 / 1000At timestep:     865 / 1000At timestep:     899 / 1000At timestep:     927 / 1000At timestep:     953 / 1000At timestep:     983 / 1000At timestep:     998 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -17.829999923706055
Train_BestReturn : -17.299999237060547
TimeSinceStart : 769.0079834461212
Exploration Critic Loss : 3383819.0
Exploitation Critic Loss : 0.1001402735710144
Exploration Model Loss : 17.985626220703125
Actor Loss : 0.554613471031189
Eval_AverageReturn : -24.9743595123291
Eval_StdReturn : 9.371224403381348
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 25.94871794871795
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -17.379999
best mean reward -17.299999
running time 838.149446
At timestep:     17 / 1000At timestep:     33 / 1000At timestep:     47 / 1000At timestep:     70 / 1000At timestep:     101 / 1000At timestep:     115 / 1000At timestep:     131 / 1000At timestep:     153 / 1000At timestep:     186 / 1000At timestep:     208 / 1000At timestep:     241 / 1000At timestep:     255 / 1000At timestep:     286 / 1000At timestep:     316 / 1000At timestep:     341 / 1000At timestep:     354 / 1000At timestep:     371 / 1000At timestep:     394 / 1000At timestep:     417 / 1000At timestep:     443 / 1000At timestep:     457 / 1000At timestep:     474 / 1000At timestep:     490 / 1000At timestep:     506 / 1000At timestep:     529 / 1000At timestep:     545 / 1000At timestep:     569 / 1000At timestep:     589 / 1000At timestep:     616 / 1000At timestep:     633 / 1000At timestep:     648 / 1000At timestep:     669 / 1000At timestep:     693 / 1000At timestep:     711 / 1000At timestep:     726 / 1000At timestep:     752 / 1000At timestep:     774 / 1000At timestep:     790 / 1000At timestep:     810 / 1000At timestep:     829 / 1000At timestep:     854 / 1000At timestep:     873 / 1000At timestep:     894 / 1000At timestep:     912 / 1000At timestep:     928 / 1000At timestep:     946 / 1000At timestep:     974 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -17.3799991607666
Train_BestReturn : -17.299999237060547
TimeSinceStart : 838.149445772171
Exploration Critic Loss : 3329956.0
Exploitation Critic Loss : 0.07802247256040573
Exploration Model Loss : 17.692733764648438
Actor Loss : 0.5722253322601318
Eval_AverageReturn : -20.08333396911621
Eval_StdReturn : 5.897857189178467
Eval_MaxReturn : -12.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 21.083333333333332
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -17.360001
best mean reward -17.299999
running time 908.428083
At timestep:     35 / 1000At timestep:     57 / 1000At timestep:     89 / 1000At timestep:     108 / 1000At timestep:     139 / 1000At timestep:     173 / 1000At timestep:     192 / 1000At timestep:     220 / 1000At timestep:     240 / 1000At timestep:     256 / 1000At timestep:     285 / 1000At timestep:     301 / 1000At timestep:     313 / 1000At timestep:     338 / 1000At timestep:     353 / 1000At timestep:     386 / 1000At timestep:     408 / 1000At timestep:     427 / 1000At timestep:     462 / 1000At timestep:     485 / 1000At timestep:     512 / 1000At timestep:     530 / 1000At timestep:     561 / 1000At timestep:     588 / 1000At timestep:     607 / 1000At timestep:     634 / 1000At timestep:     661 / 1000At timestep:     681 / 1000At timestep:     705 / 1000At timestep:     740 / 1000At timestep:     762 / 1000At timestep:     792 / 1000At timestep:     817 / 1000At timestep:     841 / 1000At timestep:     865 / 1000At timestep:     896 / 1000At timestep:     921 / 1000At timestep:     933 / 1000At timestep:     953 / 1000At timestep:     969 / 1000At timestep:     992 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -17.360000610351562
Train_BestReturn : -17.299999237060547
TimeSinceStart : 908.4280831813812
Exploration Critic Loss : 2682575.25
Exploitation Critic Loss : 0.07868465036153793
Exploration Model Loss : 17.588531494140625
Actor Loss : 0.5913378000259399
Eval_AverageReturn : -23.071428298950195
Eval_StdReturn : 6.276914596557617
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 24.071428571428573
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -17.490000
best mean reward -17.299999
running time 977.658404
At timestep:     17 / 1000At timestep:     47 / 1000At timestep:     66 / 1000At timestep:     87 / 1000At timestep:     107 / 1000At timestep:     124 / 1000At timestep:     150 / 1000At timestep:     168 / 1000At timestep:     201 / 1000At timestep:     225 / 1000At timestep:     247 / 1000At timestep:     264 / 1000At timestep:     283 / 1000At timestep:     299 / 1000At timestep:     318 / 1000At timestep:     336 / 1000At timestep:     356 / 1000At timestep:     387 / 1000At timestep:     411 / 1000At timestep:     426 / 1000At timestep:     448 / 1000At timestep:     476 / 1000At timestep:     495 / 1000At timestep:     512 / 1000At timestep:     547 / 1000At timestep:     568 / 1000At timestep:     587 / 1000At timestep:     609 / 1000At timestep:     629 / 1000At timestep:     648 / 1000At timestep:     669 / 1000At timestep:     703 / 1000At timestep:     724 / 1000At timestep:     738 / 1000At timestep:     760 / 1000At timestep:     777 / 1000At timestep:     798 / 1000At timestep:     820 / 1000At timestep:     851 / 1000At timestep:     876 / 1000At timestep:     900 / 1000At timestep:     920 / 1000At timestep:     935 / 1000At timestep:     957 / 1000At timestep:     977 / 1000At timestep:     995 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -17.489999771118164
Train_BestReturn : -17.299999237060547
TimeSinceStart : 977.6584038734436
Exploration Critic Loss : 3048097.5
Exploitation Critic Loss : 0.13763511180877686
Exploration Model Loss : 17.550125122070312
Actor Loss : 0.4392126202583313
Eval_AverageReturn : -20.76595687866211
Eval_StdReturn : 5.087376117706299
Eval_MaxReturn : -13.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 21.76595744680851
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -18.020000
best mean reward -17.299999
running time 1049.288200
At timestep:     23 / 1000At timestep:     43 / 1000At timestep:     63 / 1000At timestep:     103 / 1000At timestep:     123 / 1000At timestep:     144 / 1000At timestep:     165 / 1000At timestep:     188 / 1000At timestep:     201 / 1000At timestep:     221 / 1000At timestep:     251 / 1000At timestep:     278 / 1000At timestep:     300 / 1000At timestep:     333 / 1000At timestep:     350 / 1000At timestep:     366 / 1000At timestep:     381 / 1000At timestep:     402 / 1000At timestep:     414 / 1000At timestep:     431 / 1000At timestep:     458 / 1000At timestep:     478 / 1000At timestep:     497 / 1000At timestep:     513 / 1000At timestep:     550 / 1000At timestep:     578 / 1000At timestep:     603 / 1000At timestep:     631 / 1000At timestep:     647 / 1000At timestep:     677 / 1000At timestep:     703 / 1000At timestep:     720 / 1000At timestep:     735 / 1000At timestep:     760 / 1000At timestep:     780 / 1000At timestep:     799 / 1000At timestep:     821 / 1000At timestep:     831 / 1000At timestep:     853 / 1000At timestep:     880 / 1000At timestep:     901 / 1000At timestep:     942 / 1000At timestep:     959 / 1000At timestep:     973 / 1000At timestep:     992 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -18.020000457763672
Train_BestReturn : -17.299999237060547
TimeSinceStart : 1049.2882001399994
Exploration Critic Loss : 1319585.5
Exploitation Critic Loss : 0.1333974152803421
Exploration Model Loss : 17.536041259765625
Actor Loss : 0.5962634086608887
Eval_AverageReturn : -20.934782028198242
Eval_StdReturn : 6.783618927001953
Eval_MaxReturn : -9.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 21.934782608695652
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -18.410000
best mean reward -17.299999
running time 1120.120166
At timestep:     18 / 1000At timestep:     38 / 1000At timestep:     73 / 1000At timestep:     103 / 1000At timestep:     128 / 1000At timestep:     155 / 1000At timestep:     180 / 1000At timestep:     194 / 1000At timestep:     214 / 1000At timestep:     237 / 1000At timestep:     259 / 1000At timestep:     283 / 1000At timestep:     307 / 1000At timestep:     338 / 1000At timestep:     357 / 1000At timestep:     395 / 1000At timestep:     413 / 1000At timestep:     431 / 1000At timestep:     455 / 1000At timestep:     472 / 1000At timestep:     502 / 1000At timestep:     526 / 1000At timestep:     560 / 1000At timestep:     577 / 1000At timestep:     597 / 1000At timestep:     622 / 1000At timestep:     651 / 1000At timestep:     666 / 1000At timestep:     686 / 1000At timestep:     712 / 1000At timestep:     736 / 1000At timestep:     757 / 1000At timestep:     789 / 1000At timestep:     817 / 1000At timestep:     838 / 1000At timestep:     867 / 1000At timestep:     882 / 1000At timestep:     903 / 1000At timestep:     923 / 1000At timestep:     946 / 1000At timestep:     968 / 1000At timestep:     991 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -18.40999984741211
Train_BestReturn : -17.299999237060547
TimeSinceStart : 1120.1201655864716
Exploration Critic Loss : 1626586.125
Exploitation Critic Loss : 0.1001993864774704
Exploration Model Loss : 17.530166625976562
Actor Loss : 0.513658881187439
Eval_AverageReturn : -22.55813980102539
Eval_StdReturn : 5.478508949279785
Eval_MaxReturn : -13.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 23.558139534883722
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.850000
best mean reward -17.299999
running time 1189.607764
At timestep:     16 / 1000At timestep:     34 / 1000At timestep:     54 / 1000At timestep:     68 / 1000At timestep:     96 / 1000At timestep:     120 / 1000At timestep:     132 / 1000At timestep:     151 / 1000At timestep:     189 / 1000At timestep:     215 / 1000At timestep:     234 / 1000At timestep:     260 / 1000At timestep:     273 / 1000At timestep:     292 / 1000At timestep:     324 / 1000At timestep:     347 / 1000At timestep:     370 / 1000At timestep:     380 / 1000At timestep:     426 / 1000At timestep:     444 / 1000At timestep:     460 / 1000At timestep:     485 / 1000At timestep:     498 / 1000At timestep:     513 / 1000At timestep:     537 / 1000At timestep:     555 / 1000At timestep:     572 / 1000At timestep:     594 / 1000At timestep:     615 / 1000At timestep:     632 / 1000At timestep:     653 / 1000At timestep:     670 / 1000At timestep:     697 / 1000At timestep:     716 / 1000At timestep:     736 / 1000At timestep:     756 / 1000At timestep:     781 / 1000At timestep:     798 / 1000At timestep:     828 / 1000At timestep:     846 / 1000At timestep:     864 / 1000At timestep:     883 / 1000At timestep:     903 / 1000At timestep:     917 / 1000At timestep:     936 / 1000At timestep:     954 / 1000At timestep:     979 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.850000381469727
Train_BestReturn : -17.299999237060547
TimeSinceStart : 1189.6077642440796
Exploration Critic Loss : 955003.125
Exploitation Critic Loss : 0.07306931912899017
Exploration Model Loss : 17.528350830078125
Actor Loss : 0.5565085411071777
Eval_AverageReturn : -20.02083396911621
Eval_StdReturn : 6.562169075012207
Eval_MaxReturn : -9.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 21.020833333333332
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.639999
best mean reward -17.299999
running time 1260.979203
At timestep:     27 / 1000At timestep:     46 / 1000At timestep:     74 / 1000At timestep:     92 / 1000At timestep:     116 / 1000At timestep:     139 / 1000At timestep:     165 / 1000At timestep:     190 / 1000At timestep:     219 / 1000At timestep:     236 / 1000At timestep:     258 / 1000At timestep:     285 / 1000At timestep:     297 / 1000At timestep:     311 / 1000At timestep:     333 / 1000At timestep:     348 / 1000At timestep:     363 / 1000At timestep:     389 / 1000At timestep:     400 / 1000At timestep:     417 / 1000At timestep:     446 / 1000At timestep:     465 / 1000At timestep:     481 / 1000At timestep:     506 / 1000At timestep:     524 / 1000At timestep:     538 / 1000At timestep:     551 / 1000At timestep:     569 / 1000At timestep:     580 / 1000At timestep:     601 / 1000At timestep:     619 / 1000At timestep:     655 / 1000At timestep:     674 / 1000At timestep:     695 / 1000At timestep:     725 / 1000At timestep:     748 / 1000At timestep:     775 / 1000At timestep:     794 / 1000At timestep:     821 / 1000At timestep:     841 / 1000At timestep:     853 / 1000At timestep:     868 / 1000At timestep:     884 / 1000At timestep:     901 / 1000At timestep:     923 / 1000At timestep:     948 / 1000At timestep:     961 / 1000At timestep:     975 / 1000At timestep:     996 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.639999389648438
Train_BestReturn : -17.299999237060547
TimeSinceStart : 1260.9792029857635
Exploration Critic Loss : 891885.6875
Exploitation Critic Loss : 0.08467045426368713
Exploration Model Loss : 17.5274658203125
Actor Loss : 0.5028929710388184
Eval_AverageReturn : -19.479999542236328
Eval_StdReturn : 5.790474891662598
Eval_MaxReturn : -10.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 20.48
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.770000
best mean reward -17.299999
running time 1333.568584
At timestep:     31 / 1000At timestep:     52 / 1000At timestep:     76 / 1000At timestep:     101 / 1000At timestep:     115 / 1000At timestep:     132 / 1000At timestep:     152 / 1000At timestep:     190 / 1000At timestep:     208 / 1000At timestep:     229 / 1000At timestep:     261 / 1000At timestep:     277 / 1000At timestep:     303 / 1000At timestep:     330 / 1000At timestep:     353 / 1000At timestep:     376 / 1000At timestep:     395 / 1000At timestep:     415 / 1000At timestep:     446 / 1000At timestep:     483 / 1000At timestep:     500 / 1000At timestep:     513 / 1000At timestep:     529 / 1000At timestep:     544 / 1000At timestep:     562 / 1000At timestep:     586 / 1000At timestep:     612 / 1000At timestep:     628 / 1000At timestep:     655 / 1000At timestep:     673 / 1000At timestep:     694 / 1000At timestep:     722 / 1000At timestep:     734 / 1000At timestep:     762 / 1000At timestep:     783 / 1000At timestep:     802 / 1000At timestep:     823 / 1000At timestep:     838 / 1000At timestep:     857 / 1000At timestep:     871 / 1000At timestep:     894 / 1000At timestep:     916 / 1000At timestep:     951 / 1000At timestep:     970 / 1000At timestep:     990 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.770000457763672
Train_BestReturn : -17.299999237060547
TimeSinceStart : 1333.568583726883
Exploration Critic Loss : 1055600.25
Exploitation Critic Loss : 0.08434517681598663
Exploration Model Loss : 17.527969360351562
Actor Loss : 0.47402817010879517
Eval_AverageReturn : -20.84782600402832
Eval_StdReturn : 6.255319118499756
Eval_MaxReturn : -11.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 21.847826086956523
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.360001
best mean reward -17.299999
running time 1404.670188
At timestep:     14 / 1000At timestep:     35 / 1000At timestep:     53 / 1000At timestep:     70 / 1000At timestep:     88 / 1000At timestep:     112 / 1000At timestep:     130 / 1000At timestep:     146 / 1000At timestep:     182 / 1000At timestep:     204 / 1000At timestep:     227 / 1000At timestep:     242 / 1000At timestep:     257 / 1000At timestep:     273 / 1000At timestep:     302 / 1000At timestep:     328 / 1000At timestep:     356 / 1000At timestep:     375 / 1000At timestep:     391 / 1000At timestep:     412 / 1000At timestep:     430 / 1000At timestep:     449 / 1000At timestep:     480 / 1000At timestep:     501 / 1000At timestep:     521 / 1000At timestep:     537 / 1000At timestep:     564 / 1000At timestep:     586 / 1000At timestep:     614 / 1000At timestep:     627 / 1000At timestep:     652 / 1000At timestep:     686 / 1000At timestep:     702 / 1000At timestep:     716 / 1000At timestep:     742 / 1000At timestep:     774 / 1000At timestep:     801 / 1000At timestep:     815 / 1000At timestep:     830 / 1000At timestep:     858 / 1000At timestep:     875 / 1000At timestep:     897 / 1000At timestep:     913 / 1000At timestep:     933 / 1000At timestep:     948 / 1000At timestep:     966 / 1000At timestep:     988 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.360000610351562
Train_BestReturn : -17.299999237060547
TimeSinceStart : 1404.6701884269714
Exploration Critic Loss : 787633.0
Exploitation Critic Loss : 0.08038175106048584
Exploration Model Loss : 17.528335571289062
Actor Loss : 0.574385404586792
Eval_AverageReturn : -20.20833396911621
Eval_StdReturn : 5.866424083709717
Eval_MaxReturn : -12.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 21.208333333333332
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -16.860001
best mean reward -16.860001
running time 1478.853681
At timestep:     14 / 1000At timestep:     31 / 1000At timestep:     44 / 1000At timestep:     64 / 1000At timestep:     93 / 1000At timestep:     110 / 1000At timestep:     128 / 1000At timestep:     155 / 1000At timestep:     176 / 1000At timestep:     195 / 1000At timestep:     213 / 1000At timestep:     228 / 1000At timestep:     240 / 1000At timestep:     265 / 1000At timestep:     282 / 1000At timestep:     302 / 1000At timestep:     324 / 1000At timestep:     342 / 1000At timestep:     373 / 1000At timestep:     387 / 1000At timestep:     406 / 1000At timestep:     441 / 1000At timestep:     458 / 1000At timestep:     475 / 1000At timestep:     490 / 1000At timestep:     513 / 1000At timestep:     542 / 1000At timestep:     559 / 1000At timestep:     574 / 1000At timestep:     609 / 1000At timestep:     632 / 1000At timestep:     650 / 1000At timestep:     669 / 1000At timestep:     688 / 1000At timestep:     723 / 1000At timestep:     753 / 1000At timestep:     769 / 1000At timestep:     785 / 1000At timestep:     800 / 1000At timestep:     824 / 1000At timestep:     839 / 1000At timestep:     862 / 1000At timestep:     889 / 1000At timestep:     908 / 1000At timestep:     925 / 1000At timestep:     939 / 1000At timestep:     957 / 1000At timestep:     970 / 1000At timestep:     988 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -16.860000610351562
Train_BestReturn : -16.860000610351562
TimeSinceStart : 1478.853681087494
Exploration Critic Loss : 1158606.25
Exploitation Critic Loss : 0.08664949238300323
Exploration Model Loss : 17.529083251953125
Actor Loss : 0.48655271530151367
Eval_AverageReturn : -19.239999771118164
Eval_StdReturn : 5.9348464012146
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 20.24
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -17.820000
best mean reward -16.860001
running time 1551.928926
At timestep:     22 / 1000At timestep:     41 / 1000At timestep:     72 / 1000At timestep:     94 / 1000At timestep:     111 / 1000At timestep:     130 / 1000At timestep:     147 / 1000At timestep:     167 / 1000At timestep:     202 / 1000At timestep:     216 / 1000At timestep:     235 / 1000At timestep:     252 / 1000At timestep:     270 / 1000At timestep:     286 / 1000At timestep:     304 / 1000At timestep:     320 / 1000At timestep:     347 / 1000At timestep:     364 / 1000At timestep:     383 / 1000At timestep:     408 / 1000At timestep:     434 / 1000At timestep:     450 / 1000At timestep:     467 / 1000At timestep:     501 / 1000At timestep:     515 / 1000At timestep:     536 / 1000At timestep:     558 / 1000At timestep:     571 / 1000At timestep:     597 / 1000At timestep:     615 / 1000At timestep:     633 / 1000At timestep:     646 / 1000At timestep:     662 / 1000At timestep:     685 / 1000At timestep:     704 / 1000At timestep:     723 / 1000At timestep:     740 / 1000At timestep:     756 / 1000At timestep:     771 / 1000At timestep:     787 / 1000At timestep:     800 / 1000At timestep:     822 / 1000At timestep:     837 / 1000At timestep:     850 / 1000At timestep:     879 / 1000At timestep:     903 / 1000At timestep:     918 / 1000At timestep:     938 / 1000At timestep:     956 / 1000At timestep:     981 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -17.81999969482422
Train_BestReturn : -16.860000610351562
TimeSinceStart : 1551.9289255142212
Exploration Critic Loss : 1452719.0
Exploitation Critic Loss : 0.08978021144866943
Exploration Model Loss : 17.529586791992188
Actor Loss : 0.5304734706878662
Eval_AverageReturn : -18.60784339904785
Eval_StdReturn : 5.141444206237793
Eval_MaxReturn : -12.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.607843137254903
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -17.500000
best mean reward -16.860001
running time 1625.497073
At timestep:     13 / 1000At timestep:     29 / 1000At timestep:     49 / 1000At timestep:     85 / 1000At timestep:     112 / 1000At timestep:     126 / 1000At timestep:     142 / 1000At timestep:     163 / 1000At timestep:     184 / 1000At timestep:     203 / 1000At timestep:     221 / 1000At timestep:     242 / 1000At timestep:     257 / 1000At timestep:     281 / 1000At timestep:     296 / 1000At timestep:     313 / 1000At timestep:     331 / 1000At timestep:     346 / 1000At timestep:     371 / 1000At timestep:     399 / 1000At timestep:     428 / 1000At timestep:     446 / 1000At timestep:     462 / 1000At timestep:     485 / 1000At timestep:     507 / 1000At timestep:     532 / 1000At timestep:     555 / 1000At timestep:     567 / 1000At timestep:     584 / 1000At timestep:     615 / 1000At timestep:     631 / 1000At timestep:     650 / 1000At timestep:     675 / 1000At timestep:     705 / 1000At timestep:     724 / 1000At timestep:     743 / 1000At timestep:     767 / 1000At timestep:     785 / 1000At timestep:     803 / 1000At timestep:     824 / 1000At timestep:     847 / 1000At timestep:     869 / 1000At timestep:     882 / 1000At timestep:     896 / 1000At timestep:     909 / 1000At timestep:     932 / 1000At timestep:     953 / 1000At timestep:     975 / 1000At timestep:     993 / 1000At timestep:     1005 / 1000