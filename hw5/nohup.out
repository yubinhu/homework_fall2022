


LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env1_random_PointmassEasy-v0_21-11-2022_15-37-36 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env1_random_PointmassEasy-v0_21-11-2022_15-37-36
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001532
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     199 / 1000At timestep:     249 / 1000At timestep:     299 / 1000At timestep:     349 / 1000At timestep:     399 / 1000At timestep:     449 / 1000At timestep:     499 / 1000At timestep:     549 / 1000At timestep:     599 / 1000At timestep:     643 / 1000At timestep:     693 / 1000At timestep:     743 / 1000At timestep:     793 / 1000At timestep:     843 / 1000At timestep:     893 / 1000At timestep:     943 / 1000At timestep:     993 / 1000At timestep:     1040 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0015316009521484375
Eval_AverageReturn : -49.380950927734375
Eval_StdReturn : 1.7036707401275635
Eval_MaxReturn : -43.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 49.523809523809526
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 8.384581
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -50.0
TimeSinceStart : 8.384580850601196
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -49.575001
best mean reward -inf
running time 16.601512
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -49.57500076293945
TimeSinceStart : 16.60151171684265
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -49.716667
best mean reward -inf
running time 25.000876
At timestep:     37 / 1000At timestep:     87 / 1000At timestep:     137 / 1000At timestep:     187 / 1000At timestep:     237 / 1000At timestep:     287 / 1000At timestep:     337 / 1000At timestep:     387 / 1000At timestep:     437 / 1000At timestep:     487 / 1000At timestep:     537 / 1000At timestep:     587 / 1000At timestep:     637 / 1000At timestep:     687 / 1000At timestep:     737 / 1000At timestep:     782 / 1000At timestep:     832 / 1000At timestep:     882 / 1000At timestep:     932 / 1000At timestep:     982 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -49.71666717529297
TimeSinceStart : 25.0008761882782
Eval_AverageReturn : -49.0476188659668
Eval_StdReturn : 3.184429168701172
Eval_MaxReturn : -36.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 49.142857142857146
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -49.737499
best mean reward -inf
running time 33.446498
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     585 / 1000At timestep:     635 / 1000At timestep:     685 / 1000At timestep:     735 / 1000At timestep:     785 / 1000At timestep:     835 / 1000At timestep:     885 / 1000At timestep:     935 / 1000At timestep:     985 / 1000At timestep:     1035 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -49.73749923706055
TimeSinceStart : 33.44649815559387
Eval_AverageReturn : -49.238094329833984
Eval_StdReturn : 3.407341957092285
Eval_MaxReturn : -34.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 49.285714285714285
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -49.669998
best mean reward -inf
running time 41.894545
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -49.66999816894531
TimeSinceStart : 41.89454531669617
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -49.669998
best mean reward -49.669998
running time 50.260646
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -49.66999816894531
Train_BestReturn : -49.66999816894531
TimeSinceStart : 50.260645627975464
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -49.840000
best mean reward -49.669998
running time 58.872010
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -49.84000015258789
Train_BestReturn : -49.66999816894531
TimeSinceStart : 58.87200975418091
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -49.840000
best mean reward -49.669998
running time 67.544535
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -49.84000015258789
Train_BestReturn : -49.66999816894531
TimeSinceStart : 67.54453468322754
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -49.880001
best mean reward -49.669998
running time 76.279388
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -49.880001068115234
Train_BestReturn : -49.66999816894531
TimeSinceStart : 76.27938795089722
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -50.000000
best mean reward -49.669998
running time 85.077146
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     744 / 1000At timestep:     794 / 1000At timestep:     844 / 1000At timestep:     894 / 1000At timestep:     944 / 1000At timestep:     989 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -50.0
Train_BestReturn : -49.66999816894531
TimeSinceStart : 85.07714629173279
Eval_AverageReturn : -49.380950927734375
Eval_StdReturn : 1.9142619371414185
Eval_MaxReturn : -43.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 49.476190476190474
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -50.000000
best mean reward -49.669998
running time 99.502764
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -50.0
Train_BestReturn : -49.66999816894531
TimeSinceStart : 99.50276374816895
Exploitation Critic Loss : 0.12359072268009186
Exploration Critic Loss : 665173.875
Exploration Model Loss : 39.25876235961914
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -49.119999
best mean reward -49.119999
running time 114.150636
At timestep:     15 / 1000At timestep:     36 / 1000At timestep:     50 / 1000At timestep:     74 / 1000At timestep:     88 / 1000At timestep:     104 / 1000At timestep:     121 / 1000At timestep:     139 / 1000At timestep:     152 / 1000At timestep:     190 / 1000At timestep:     208 / 1000At timestep:     223 / 1000At timestep:     250 / 1000At timestep:     278 / 1000At timestep:     293 / 1000At timestep:     315 / 1000At timestep:     340 / 1000At timestep:     358 / 1000At timestep:     383 / 1000At timestep:     394 / 1000At timestep:     410 / 1000At timestep:     429 / 1000At timestep:     445 / 1000At timestep:     467 / 1000At timestep:     477 / 1000At timestep:     493 / 1000At timestep:     517 / 1000At timestep:     551 / 1000At timestep:     571 / 1000At timestep:     597 / 1000At timestep:     610 / 1000At timestep:     637 / 1000At timestep:     654 / 1000At timestep:     683 / 1000At timestep:     702 / 1000At timestep:     717 / 1000At timestep:     740 / 1000At timestep:     773 / 1000At timestep:     797 / 1000At timestep:     814 / 1000At timestep:     856 / 1000At timestep:     874 / 1000At timestep:     900 / 1000At timestep:     917 / 1000At timestep:     937 / 1000At timestep:     958 / 1000At timestep:     976 / 1000At timestep:     998 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -49.119998931884766
Train_BestReturn : -49.119998931884766
TimeSinceStart : 114.15063571929932
Exploitation Critic Loss : 0.8299311399459839
Exploration Critic Loss : 9844415.0
Exploration Model Loss : 57.15092849731445
Eval_AverageReturn : -19.755102157592773
Eval_StdReturn : 6.631804943084717
Eval_MaxReturn : -9.0
Eval_MinReturn : -41.0
Eval_AverageEpLen : 20.755102040816325
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -43.330002
best mean reward -43.330002
running time 134.745856
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     121 / 1000At timestep:     171 / 1000At timestep:     211 / 1000At timestep:     261 / 1000At timestep:     311 / 1000At timestep:     361 / 1000At timestep:     411 / 1000At timestep:     461 / 1000At timestep:     498 / 1000At timestep:     548 / 1000At timestep:     598 / 1000At timestep:     648 / 1000At timestep:     698 / 1000At timestep:     748 / 1000At timestep:     798 / 1000At timestep:     848 / 1000At timestep:     891 / 1000At timestep:     941 / 1000At timestep:     991 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -43.33000183105469
Train_BestReturn : -43.33000183105469
TimeSinceStart : 134.74585628509521
Exploitation Critic Loss : 0.31409239768981934
Exploration Critic Loss : 6525215.0
Exploration Model Loss : 38.71107482910156
Eval_AverageReturn : -46.6363639831543
Eval_StdReturn : 7.176639080047607
Eval_MaxReturn : -20.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 46.90909090909091
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -34.480000
best mean reward -34.480000
running time 152.285769
At timestep:     12 / 1000At timestep:     28 / 1000At timestep:     75 / 1000At timestep:     88 / 1000At timestep:     99 / 1000At timestep:     117 / 1000At timestep:     135 / 1000At timestep:     168 / 1000At timestep:     217 / 1000At timestep:     267 / 1000At timestep:     289 / 1000At timestep:     310 / 1000At timestep:     323 / 1000At timestep:     357 / 1000At timestep:     407 / 1000At timestep:     437 / 1000At timestep:     454 / 1000At timestep:     502 / 1000At timestep:     520 / 1000At timestep:     536 / 1000At timestep:     553 / 1000At timestep:     578 / 1000At timestep:     599 / 1000At timestep:     615 / 1000At timestep:     637 / 1000At timestep:     663 / 1000At timestep:     680 / 1000At timestep:     693 / 1000At timestep:     719 / 1000At timestep:     734 / 1000At timestep:     752 / 1000At timestep:     765 / 1000At timestep:     799 / 1000At timestep:     849 / 1000At timestep:     868 / 1000At timestep:     889 / 1000At timestep:     915 / 1000At timestep:     945 / 1000At timestep:     975 / 1000At timestep:     990 / 1000At timestep:     1040 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -34.47999954223633
Train_BestReturn : -34.47999954223633
TimeSinceStart : 152.2857687473297
Exploitation Critic Loss : 1.242255449295044
Exploration Critic Loss : 6897231.0
Exploration Model Loss : 19.398300170898438
Eval_AverageReturn : -24.463415145874023
Eval_StdReturn : 12.554462432861328
Eval_MaxReturn : -10.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 25.365853658536587
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -24.160000
best mean reward -24.160000
running time 174.370581
At timestep:     13 / 1000At timestep:     27 / 1000At timestep:     40 / 1000At timestep:     54 / 1000At timestep:     72 / 1000At timestep:     92 / 1000At timestep:     108 / 1000At timestep:     121 / 1000At timestep:     137 / 1000At timestep:     154 / 1000At timestep:     170 / 1000At timestep:     186 / 1000At timestep:     199 / 1000At timestep:     214 / 1000At timestep:     229 / 1000At timestep:     240 / 1000At timestep:     253 / 1000At timestep:     269 / 1000At timestep:     281 / 1000At timestep:     291 / 1000At timestep:     308 / 1000At timestep:     324 / 1000At timestep:     340 / 1000At timestep:     357 / 1000At timestep:     373 / 1000At timestep:     390 / 1000At timestep:     405 / 1000At timestep:     420 / 1000At timestep:     433 / 1000At timestep:     450 / 1000At timestep:     467 / 1000At timestep:     485 / 1000At timestep:     504 / 1000At timestep:     519 / 1000At timestep:     537 / 1000At timestep:     550 / 1000At timestep:     565 / 1000At timestep:     580 / 1000At timestep:     597 / 1000At timestep:     611 / 1000At timestep:     627 / 1000At timestep:     643 / 1000At timestep:     661 / 1000At timestep:     674 / 1000At timestep:     693 / 1000At timestep:     702 / 1000At timestep:     723 / 1000At timestep:     736 / 1000At timestep:     752 / 1000At timestep:     768 / 1000At timestep:     784 / 1000At timestep:     797 / 1000At timestep:     810 / 1000At timestep:     823 / 1000At timestep:     836 / 1000At timestep:     849 / 1000At timestep:     863 / 1000At timestep:     875 / 1000At timestep:     890 / 1000At timestep:     901 / 1000At timestep:     918 / 1000At timestep:     937 / 1000At timestep:     948 / 1000At timestep:     967 / 1000At timestep:     988 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -24.15999984741211
Train_BestReturn : -24.15999984741211
TimeSinceStart : 174.3705813884735
Exploitation Critic Loss : 0.8571749925613403
Exploration Critic Loss : 13636322.0
Exploration Model Loss : 15.947280883789062
Eval_AverageReturn : -14.181818008422852
Eval_StdReturn : 2.5459957122802734
Eval_MaxReturn : -8.0
Eval_MinReturn : -20.0
Eval_AverageEpLen : 15.181818181818182
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -19.020000
best mean reward -19.020000
running time 201.225946
At timestep:     13 / 1000At timestep:     26 / 1000At timestep:     38 / 1000At timestep:     54 / 1000At timestep:     67 / 1000At timestep:     84 / 1000At timestep:     97 / 1000At timestep:     110 / 1000At timestep:     127 / 1000At timestep:     141 / 1000At timestep:     156 / 1000At timestep:     170 / 1000At timestep:     185 / 1000At timestep:     198 / 1000At timestep:     211 / 1000At timestep:     227 / 1000At timestep:     242 / 1000At timestep:     251 / 1000At timestep:     268 / 1000At timestep:     281 / 1000At timestep:     296 / 1000At timestep:     311 / 1000At timestep:     324 / 1000At timestep:     337 / 1000At timestep:     353 / 1000At timestep:     369 / 1000At timestep:     384 / 1000At timestep:     394 / 1000At timestep:     412 / 1000At timestep:     426 / 1000At timestep:     442 / 1000At timestep:     455 / 1000At timestep:     469 / 1000At timestep:     484 / 1000At timestep:     499 / 1000At timestep:     517 / 1000At timestep:     528 / 1000At timestep:     543 / 1000At timestep:     557 / 1000At timestep:     572 / 1000At timestep:     586 / 1000At timestep:     606 / 1000At timestep:     619 / 1000At timestep:     638 / 1000At timestep:     655 / 1000At timestep:     670 / 1000At timestep:     685 / 1000At timestep:     703 / 1000At timestep:     719 / 1000At timestep:     733 / 1000At timestep:     748 / 1000At timestep:     766 / 1000At timestep:     783 / 1000At timestep:     802 / 1000At timestep:     822 / 1000At timestep:     841 / 1000At timestep:     857 / 1000At timestep:     869 / 1000At timestep:     881 / 1000At timestep:     896 / 1000At timestep:     911 / 1000At timestep:     926 / 1000At timestep:     940 / 1000At timestep:     953 / 1000At timestep:     967 / 1000At timestep:     984 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -19.020000457763672
Train_BestReturn : -19.020000457763672
TimeSinceStart : 201.22594571113586
Exploitation Critic Loss : 1.2248542308807373
Exploration Critic Loss : 17384776.0
Exploration Model Loss : 15.0115966796875
Eval_AverageReturn : -13.940298080444336
Eval_StdReturn : 2.2319297790527344
Eval_MaxReturn : -8.0
Eval_MinReturn : -19.0
Eval_AverageEpLen : 14.940298507462687
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -18.070000
best mean reward -18.070000
running time 228.277803
At timestep:     13 / 1000At timestep:     27 / 1000At timestep:     39 / 1000At timestep:     53 / 1000At timestep:     70 / 1000At timestep:     88 / 1000At timestep:     101 / 1000At timestep:     116 / 1000At timestep:     130 / 1000At timestep:     146 / 1000At timestep:     163 / 1000At timestep:     173 / 1000At timestep:     189 / 1000At timestep:     204 / 1000At timestep:     219 / 1000At timestep:     234 / 1000At timestep:     250 / 1000At timestep:     269 / 1000At timestep:     285 / 1000At timestep:     302 / 1000At timestep:     315 / 1000At timestep:     332 / 1000At timestep:     346 / 1000At timestep:     360 / 1000At timestep:     378 / 1000At timestep:     391 / 1000At timestep:     406 / 1000At timestep:     424 / 1000At timestep:     437 / 1000At timestep:     451 / 1000At timestep:     464 / 1000At timestep:     476 / 1000At timestep:     493 / 1000At timestep:     508 / 1000At timestep:     523 / 1000At timestep:     537 / 1000At timestep:     554 / 1000At timestep:     568 / 1000At timestep:     583 / 1000At timestep:     602 / 1000At timestep:     616 / 1000At timestep:     629 / 1000At timestep:     648 / 1000At timestep:     664 / 1000At timestep:     682 / 1000At timestep:     698 / 1000At timestep:     712 / 1000At timestep:     731 / 1000At timestep:     744 / 1000At timestep:     760 / 1000At timestep:     774 / 1000At timestep:     787 / 1000At timestep:     801 / 1000At timestep:     818 / 1000At timestep:     829 / 1000At timestep:     847 / 1000At timestep:     860 / 1000At timestep:     875 / 1000At timestep:     892 / 1000At timestep:     904 / 1000At timestep:     919 / 1000At timestep:     936 / 1000At timestep:     952 / 1000At timestep:     968 / 1000At timestep:     982 / 1000At timestep:     995 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -18.06999969482422
Train_BestReturn : -18.06999969482422
TimeSinceStart : 228.27780318260193
Exploitation Critic Loss : 0.6409512758255005
Exploration Critic Loss : 14489660.0
Exploration Model Loss : 14.699371337890625
Eval_AverageReturn : -14.074626922607422
Eval_StdReturn : 2.039267063140869
Eval_MaxReturn : -9.0
Eval_MinReturn : -18.0
Eval_AverageEpLen : 15.074626865671641
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -18.170000
best mean reward -18.070000
running time 255.782674
At timestep:     15 / 1000At timestep:     33 / 1000At timestep:     53 / 1000At timestep:     65 / 1000At timestep:     78 / 1000At timestep:     92 / 1000At timestep:     106 / 1000At timestep:     120 / 1000At timestep:     135 / 1000At timestep:     150 / 1000At timestep:     163 / 1000At timestep:     179 / 1000At timestep:     195 / 1000At timestep:     208 / 1000At timestep:     222 / 1000At timestep:     236 / 1000At timestep:     249 / 1000At timestep:     266 / 1000At timestep:     281 / 1000At timestep:     295 / 1000At timestep:     309 / 1000At timestep:     324 / 1000At timestep:     343 / 1000At timestep:     358 / 1000At timestep:     372 / 1000At timestep:     390 / 1000At timestep:     405 / 1000At timestep:     419 / 1000At timestep:     433 / 1000At timestep:     449 / 1000At timestep:     463 / 1000At timestep:     477 / 1000At timestep:     492 / 1000At timestep:     509 / 1000At timestep:     523 / 1000At timestep:     537 / 1000At timestep:     551 / 1000At timestep:     565 / 1000At timestep:     578 / 1000At timestep:     592 / 1000At timestep:     606 / 1000At timestep:     618 / 1000At timestep:     629 / 1000At timestep:     645 / 1000At timestep:     661 / 1000At timestep:     678 / 1000At timestep:     691 / 1000At timestep:     702 / 1000At timestep:     718 / 1000At timestep:     735 / 1000At timestep:     752 / 1000At timestep:     764 / 1000At timestep:     779 / 1000At timestep:     792 / 1000At timestep:     808 / 1000At timestep:     824 / 1000At timestep:     837 / 1000At timestep:     850 / 1000At timestep:     865 / 1000At timestep:     883 / 1000At timestep:     898 / 1000At timestep:     912 / 1000At timestep:     927 / 1000At timestep:     941 / 1000At timestep:     955 / 1000At timestep:     967 / 1000At timestep:     981 / 1000At timestep:     995 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -18.170000076293945
Train_BestReturn : -18.06999969482422
TimeSinceStart : 255.7826738357544
Exploitation Critic Loss : 1.7480781078338623
Exploration Critic Loss : 15499576.0
Exploration Model Loss : 14.5882568359375
Eval_AverageReturn : -13.623188018798828
Eval_StdReturn : 1.761927843093872
Eval_MaxReturn : -10.0
Eval_MinReturn : -19.0
Eval_AverageEpLen : 14.623188405797102
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.440001
best mean reward -17.440001
running time 284.196053
At timestep:     10 / 1000At timestep:     23 / 1000At timestep:     38 / 1000At timestep:     50 / 1000At timestep:     69 / 1000At timestep:     82 / 1000At timestep:     94 / 1000At timestep:     108 / 1000At timestep:     124 / 1000At timestep:     139 / 1000At timestep:     154 / 1000At timestep:     168 / 1000At timestep:     187 / 1000At timestep:     199 / 1000At timestep:     213 / 1000At timestep:     228 / 1000At timestep:     239 / 1000At timestep:     256 / 1000At timestep:     268 / 1000At timestep:     282 / 1000At timestep:     301 / 1000At timestep:     314 / 1000At timestep:     327 / 1000At timestep:     341 / 1000At timestep:     355 / 1000At timestep:     373 / 1000At timestep:     387 / 1000At timestep:     402 / 1000At timestep:     417 / 1000At timestep:     429 / 1000At timestep:     444 / 1000At timestep:     457 / 1000At timestep:     473 / 1000At timestep:     487 / 1000At timestep:     500 / 1000At timestep:     511 / 1000At timestep:     527 / 1000At timestep:     540 / 1000At timestep:     557 / 1000At timestep:     573 / 1000At timestep:     587 / 1000At timestep:     602 / 1000At timestep:     617 / 1000At timestep:     630 / 1000At timestep:     642 / 1000At timestep:     659 / 1000At timestep:     675 / 1000At timestep:     692 / 1000At timestep:     705 / 1000At timestep:     722 / 1000At timestep:     737 / 1000At timestep:     753 / 1000At timestep:     768 / 1000At timestep:     786 / 1000At timestep:     801 / 1000At timestep:     815 / 1000At timestep:     830 / 1000At timestep:     843 / 1000At timestep:     859 / 1000At timestep:     873 / 1000At timestep:     887 / 1000At timestep:     901 / 1000At timestep:     914 / 1000At timestep:     927 / 1000At timestep:     939 / 1000At timestep:     955 / 1000At timestep:     972 / 1000At timestep:     984 / 1000At timestep:     998 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.440000534057617
Train_BestReturn : -17.440000534057617
TimeSinceStart : 284.19605255126953
Exploitation Critic Loss : 0.5719466209411621
Exploration Critic Loss : 20519870.0
Exploration Model Loss : 14.547210693359375
Eval_AverageReturn : -13.471428871154785
Eval_StdReturn : 1.9619337320327759
Eval_MaxReturn : -9.0
Eval_MinReturn : -18.0
Eval_AverageEpLen : 14.471428571428572
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.129999
best mean reward -17.129999
running time 312.905847
At timestep:     13 / 1000At timestep:     26 / 1000At timestep:     40 / 1000At timestep:     56 / 1000At timestep:     74 / 1000At timestep:     89 / 1000At timestep:     107 / 1000At timestep:     121 / 1000At timestep:     134 / 1000At timestep:     152 / 1000At timestep:     174 / 1000At timestep:     187 / 1000At timestep:     199 / 1000At timestep:     213 / 1000At timestep:     226 / 1000At timestep:     243 / 1000At timestep:     256 / 1000At timestep:     269 / 1000At timestep:     284 / 1000At timestep:     298 / 1000At timestep:     311 / 1000At timestep:     323 / 1000At timestep:     339 / 1000At timestep:     355 / 1000At timestep:     366 / 1000At timestep:     378 / 1000At timestep:     395 / 1000At timestep:     411 / 1000At timestep:     423 / 1000At timestep:     435 / 1000At timestep:     453 / 1000At timestep:     468 / 1000At timestep:     484 / 1000At timestep:     496 / 1000At timestep:     512 / 1000At timestep:     529 / 1000At timestep:     540 / 1000At timestep:     555 / 1000At timestep:     572 / 1000At timestep:     586 / 1000At timestep:     598 / 1000At timestep:     617 / 1000At timestep:     631 / 1000At timestep:     647 / 1000At timestep:     663 / 1000At timestep:     678 / 1000At timestep:     693 / 1000At timestep:     707 / 1000At timestep:     718 / 1000At timestep:     732 / 1000At timestep:     746 / 1000At timestep:     760 / 1000At timestep:     779 / 1000At timestep:     794 / 1000At timestep:     807 / 1000At timestep:     818 / 1000At timestep:     833 / 1000At timestep:     848 / 1000At timestep:     861 / 1000At timestep:     879 / 1000At timestep:     896 / 1000At timestep:     910 / 1000At timestep:     921 / 1000At timestep:     935 / 1000At timestep:     948 / 1000At timestep:     964 / 1000At timestep:     980 / 1000At timestep:     994 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.1299991607666
Train_BestReturn : -17.1299991607666
TimeSinceStart : 312.9058473110199
Exploitation Critic Loss : 0.44383326172828674
Exploration Critic Loss : 11360178.0
Exploration Model Loss : 14.53228759765625
Eval_AverageReturn : -13.623188018798828
Eval_StdReturn : 2.2399628162384033
Eval_MaxReturn : -10.0
Eval_MinReturn : -21.0
Eval_AverageEpLen : 14.623188405797102
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.420000
best mean reward -17.129999
running time 341.749284
At timestep:     13 / 1000At timestep:     27 / 1000At timestep:     45 / 1000At timestep:     61 / 1000At timestep:     75 / 1000At timestep:     89 / 1000At timestep:     101 / 1000At timestep:     114 / 1000At timestep:     125 / 1000At timestep:     143 / 1000At timestep:     159 / 1000At timestep:     172 / 1000At timestep:     187 / 1000At timestep:     203 / 1000At timestep:     217 / 1000At timestep:     229 / 1000At timestep:     243 / 1000At timestep:     257 / 1000At timestep:     271 / 1000At timestep:     290 / 1000At timestep:     309 / 1000At timestep:     324 / 1000At timestep:     335 / 1000At timestep:     350 / 1000At timestep:     365 / 1000At timestep:     378 / 1000At timestep:     395 / 1000At timestep:     413 / 1000At timestep:     428 / 1000At timestep:     447 / 1000At timestep:     463 / 1000At timestep:     475 / 1000At timestep:     489 / 1000At timestep:     503 / 1000At timestep:     519 / 1000At timestep:     533 / 1000At timestep:     550 / 1000At timestep:     563 / 1000At timestep:     579 / 1000At timestep:     594 / 1000At timestep:     608 / 1000At timestep:     623 / 1000At timestep:     636 / 1000At timestep:     651 / 1000At timestep:     666 / 1000At timestep:     682 / 1000At timestep:     695 / 1000At timestep:     707 / 1000At timestep:     719 / 1000At timestep:     732 / 1000At timestep:     744 / 1000At timestep:     759 / 1000At timestep:     775 / 1000At timestep:     790 / 1000At timestep:     800 / 1000At timestep:     816 / 1000At timestep:     831 / 1000At timestep:     845 / 1000At timestep:     858 / 1000At timestep:     872 / 1000At timestep:     886 / 1000At timestep:     903 / 1000At timestep:     919 / 1000At timestep:     933 / 1000At timestep:     946 / 1000At timestep:     958 / 1000At timestep:     972 / 1000At timestep:     984 / 1000At timestep:     997 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.420000076293945
Train_BestReturn : -17.1299991607666
TimeSinceStart : 341.7492837905884
Exploitation Critic Loss : 0.21701720356941223
Exploration Critic Loss : 24302308.0
Exploration Model Loss : 14.526275634765625
Eval_AverageReturn : -13.442856788635254
Eval_StdReturn : 1.9466875791549683
Eval_MaxReturn : -9.0
Eval_MinReturn : -18.0
Eval_AverageEpLen : 14.442857142857143
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.480000
best mean reward -17.129999
running time 370.508695
At timestep:     12 / 1000At timestep:     30 / 1000At timestep:     43 / 1000At timestep:     58 / 1000At timestep:     73 / 1000At timestep:     87 / 1000At timestep:     102 / 1000At timestep:     115 / 1000At timestep:     128 / 1000At timestep:     141 / 1000At timestep:     158 / 1000At timestep:     172 / 1000At timestep:     188 / 1000At timestep:     200 / 1000At timestep:     211 / 1000At timestep:     224 / 1000At timestep:     242 / 1000At timestep:     261 / 1000At timestep:     275 / 1000At timestep:     294 / 1000At timestep:     310 / 1000At timestep:     326 / 1000At timestep:     342 / 1000At timestep:     354 / 1000At timestep:     369 / 1000At timestep:     379 / 1000At timestep:     396 / 1000At timestep:     409 / 1000At timestep:     424 / 1000At timestep:     436 / 1000At timestep:     450 / 1000At timestep:     468 / 1000At timestep:     483 / 1000At timestep:     496 / 1000At timestep:     512 / 1000At timestep:     531 / 1000At timestep:     543 / 1000At timestep:     559 / 1000At timestep:     573 / 1000At timestep:     589 / 1000At timestep:     604 / 1000At timestep:     622 / 1000At timestep:     635 / 1000At timestep:     648 / 1000At timestep:     665 / 1000At timestep:     680 / 1000At timestep:     699 / 1000At timestep:     714 / 1000At timestep:     728 / 1000At timestep:     741 / 1000At timestep:     762 / 1000At timestep:     775 / 1000At timestep:     786 / 1000At timestep:     803 / 1000At timestep:     819 / 1000At timestep:     835 / 1000At timestep:     850 / 1000At timestep:     869 / 1000At timestep:     884 / 1000At timestep:     902 / 1000At timestep:     918 / 1000At timestep:     932 / 1000At timestep:     948 / 1000At timestep:     961 / 1000At timestep:     979 / 1000At timestep:     994 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.479999542236328
Train_BestReturn : -17.1299991607666
TimeSinceStart : 370.5086953639984
Exploitation Critic Loss : 0.8437867760658264
Exploration Critic Loss : 31797976.0
Exploration Model Loss : 14.524627685546875
Eval_AverageReturn : -13.985074996948242
Eval_StdReturn : 2.378338575363159
Eval_MaxReturn : -9.0
Eval_MinReturn : -20.0
Eval_AverageEpLen : 14.985074626865671
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -17.770000
best mean reward -17.129999
running time 399.206781
At timestep:     16 / 1000At timestep:     28 / 1000At timestep:     44 / 1000At timestep:     56 / 1000At timestep:     70 / 1000At timestep:     84 / 1000At timestep:     98 / 1000At timestep:     113 / 1000At timestep:     131 / 1000At timestep:     145 / 1000At timestep:     161 / 1000At timestep:     174 / 1000At timestep:     192 / 1000At timestep:     204 / 1000At timestep:     217 / 1000At timestep:     230 / 1000At timestep:     244 / 1000At timestep:     259 / 1000At timestep:     273 / 1000At timestep:     286 / 1000At timestep:     300 / 1000At timestep:     312 / 1000At timestep:     327 / 1000At timestep:     343 / 1000At timestep:     361 / 1000At timestep:     372 / 1000At timestep:     386 / 1000At timestep:     402 / 1000At timestep:     416 / 1000At timestep:     432 / 1000At timestep:     447 / 1000At timestep:     461 / 1000At timestep:     474 / 1000At timestep:     489 / 1000At timestep:     505 / 1000At timestep:     525 / 1000At timestep:     540 / 1000At timestep:     552 / 1000At timestep:     563 / 1000At timestep:     581 / 1000At timestep:     598 / 1000At timestep:     613 / 1000At timestep:     632 / 1000At timestep:     648 / 1000At timestep:     662 / 1000At timestep:     673 / 1000At timestep:     685 / 1000At timestep:     700 / 1000At timestep:     711 / 1000At timestep:     724 / 1000At timestep:     739 / 1000At timestep:     753 / 1000At timestep:     768 / 1000At timestep:     782 / 1000At timestep:     800 / 1000At timestep:     814 / 1000At timestep:     825 / 1000At timestep:     841 / 1000At timestep:     859 / 1000At timestep:     877 / 1000At timestep:     889 / 1000At timestep:     905 / 1000At timestep:     919 / 1000At timestep:     934 / 1000At timestep:     947 / 1000At timestep:     960 / 1000At timestep:     973 / 1000At timestep:     991 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -17.770000457763672
Train_BestReturn : -17.1299991607666
TimeSinceStart : 399.20678067207336
Exploitation Critic Loss : 1.4674675464630127
Exploration Critic Loss : 34326640.0
Exploration Model Loss : 14.523651123046875
Eval_AverageReturn : -13.594202995300293
Eval_StdReturn : 2.1217904090881348
Eval_MaxReturn : -10.0
Eval_MinReturn : -19.0
Eval_AverageEpLen : 14.594202898550725
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -18.290001
best mean reward -17.129999
running time 428.093468
At timestep:     16 / 1000At timestep:     30 / 1000At timestep:     43 / 1000At timestep:     58 / 1000At timestep:     72 / 1000At timestep:     88 / 1000At timestep:     101 / 1000At timestep:     116 / 1000At timestep:     128 / 1000At timestep:     142 / 1000At timestep:     156 / 1000At timestep:     174 / 1000At timestep:     190 / 1000At timestep:     202 / 1000At timestep:     214 / 1000At timestep:     229 / 1000At timestep:     246 / 1000At timestep:     260 / 1000At timestep:     275 / 1000At timestep:     289 / 1000At timestep:     303 / 1000At timestep:     326 / 1000At timestep:     338 / 1000At timestep:     352 / 1000At timestep:     364 / 1000At timestep:     378 / 1000At timestep:     394 / 1000At timestep:     411 / 1000At timestep:     423 / 1000At timestep:     438 / 1000At timestep:     454 / 1000At timestep:     469 / 1000At timestep:     485 / 1000At timestep:     501 / 1000At timestep:     511 / 1000At timestep:     529 / 1000At timestep:     546 / 1000At timestep:     559 / 1000At timestep:     572 / 1000At timestep:     588 / 1000At timestep:     603 / 1000At timestep:     616 / 1000At timestep:     629 / 1000At timestep:     644 / 1000At timestep:     656 / 1000At timestep:     673 / 1000At timestep:     691 / 1000At timestep:     707 / 1000At timestep:     723 / 1000At timestep:     735 / 1000At timestep:     749 / 1000At timestep:     763 / 1000At timestep:     776 / 1000At timestep:     791 / 1000At timestep:     808 / 1000At timestep:     823 / 1000At timestep:     834 / 1000At timestep:     851 / 1000At timestep:     866 / 1000At timestep:     881 / 1000At timestep:     894 / 1000At timestep:     905 / 1000At timestep:     918 / 1000At timestep:     929 / 1000At timestep:     944 / 1000At timestep:     960 / 1000At timestep:     974 / 1000At timestep:     991 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -18.290000915527344
Train_BestReturn : -17.1299991607666
TimeSinceStart : 428.0934681892395
Exploitation Critic Loss : 1.4827802181243896
Exploration Critic Loss : 24126648.0
Exploration Model Loss : 14.523590087890625
Eval_AverageReturn : -13.536231994628906
Eval_StdReturn : 2.1439478397369385
Eval_MaxReturn : -9.0
Eval_MinReturn : -22.0
Eval_AverageEpLen : 14.53623188405797
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -18.370001
best mean reward -17.129999
running time 457.147361
At timestep:     13 / 1000At timestep:     26 / 1000At timestep:     44 / 1000At timestep:     58 / 1000At timestep:     72 / 1000At timestep:     90 / 1000At timestep:     102 / 1000At timestep:     116 / 1000At timestep:     135 / 1000At timestep:     148 / 1000At timestep:     161 / 1000At timestep:     180 / 1000At timestep:     193 / 1000At timestep:     208 / 1000At timestep:     219 / 1000At timestep:     233 / 1000At timestep:     248 / 1000At timestep:     264 / 1000At timestep:     277 / 1000At timestep:     293 / 1000At timestep:     309 / 1000At timestep:     325 / 1000At timestep:     340 / 1000At timestep:     354 / 1000At timestep:     367 / 1000At timestep:     382 / 1000At timestep:     399 / 1000At timestep:     414 / 1000At timestep:     427 / 1000At timestep:     440 / 1000At timestep:     457 / 1000At timestep:     469 / 1000At timestep:     483 / 1000At timestep:     499 / 1000At timestep:     514 / 1000At timestep:     526 / 1000At timestep:     543 / 1000At timestep:     553 / 1000At timestep:     567 / 1000At timestep:     581 / 1000At timestep:     598 / 1000At timestep:     610 / 1000At timestep:     628 / 1000At timestep:     647 / 1000At timestep:     660 / 1000At timestep:     675 / 1000At timestep:     691 / 1000At timestep:     707 / 1000At timestep:     724 / 1000At timestep:     738 / 1000At timestep:     750 / 1000At timestep:     763 / 1000At timestep:     778 / 1000At timestep:     793 / 1000At timestep:     806 / 1000At timestep:     822 / 1000At timestep:     834 / 1000At timestep:     852 / 1000At timestep:     864 / 1000At timestep:     877 / 1000At timestep:     891 / 1000At timestep:     910 / 1000At timestep:     922 / 1000At timestep:     934 / 1000At timestep:     951 / 1000At timestep:     968 / 1000At timestep:     982 / 1000At timestep:     996 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -18.3700008392334
Train_BestReturn : -17.1299991607666
TimeSinceStart : 457.1473605632782
Exploitation Critic Loss : 0.9588526487350464
Exploration Critic Loss : 27468554.0
Exploration Model Loss : 14.523651123046875
Eval_AverageReturn : -13.666666984558105
Eval_StdReturn : 2.1377673149108887
Eval_MaxReturn : -9.0
Eval_MinReturn : -18.0
Eval_AverageEpLen : 14.666666666666666
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -18.160000
best mean reward -17.129999
running time 486.507858
At timestep:     14 / 1000At timestep:     31 / 1000At timestep:     46 / 1000At timestep:     61 / 1000At timestep:     77 / 1000At timestep:     91 / 1000At timestep:     107 / 1000At timestep:     119 / 1000At timestep:     137 / 1000At timestep:     152 / 1000At timestep:     165 / 1000At timestep:     179 / 1000At timestep:     196 / 1000At timestep:     214 / 1000At timestep:     227 / 1000At timestep:     243 / 1000At timestep:     256 / 1000At timestep:     273 / 1000At timestep:     284 / 1000At timestep:     299 / 1000At timestep:     312 / 1000At timestep:     326 / 1000At timestep:     343 / 1000At timestep:     359 / 1000At timestep:     373 / 1000At timestep:     386 / 1000At timestep:     398 / 1000At timestep:     412 / 1000At timestep:     427 / 1000At timestep:     439 / 1000At timestep:     449 / 1000At timestep:     466 / 1000At timestep:     478 / 1000At timestep:     493 / 1000At timestep:     509 / 1000At timestep:     523 / 1000At timestep:     537 / 1000At timestep:     547 / 1000At timestep:     561 / 1000At timestep:     578 / 1000At timestep:     591 / 1000At timestep:     604 / 1000At timestep:     620 / 1000At timestep:     634 / 1000At timestep:     651 / 1000At timestep:     666 / 1000At timestep:     681 / 1000At timestep:     693 / 1000At timestep:     707 / 1000At timestep:     722 / 1000At timestep:     737 / 1000At timestep:     750 / 1000At timestep:     761 / 1000At timestep:     777 / 1000At timestep:     795 / 1000At timestep:     807 / 1000At timestep:     825 / 1000At timestep:     836 / 1000At timestep:     849 / 1000At timestep:     864 / 1000At timestep:     878 / 1000At timestep:     895 / 1000At timestep:     908 / 1000At timestep:     921 / 1000At timestep:     932 / 1000At timestep:     945 / 1000At timestep:     955 / 1000At timestep:     967 / 1000At timestep:     983 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -18.15999984741211
Train_BestReturn : -17.1299991607666
TimeSinceStart : 486.5078582763672
Exploitation Critic Loss : 0.7568945288658142
Exploration Critic Loss : 16918988.0
Exploration Model Loss : 14.523605346679688
Eval_AverageReturn : -13.285714149475098
Eval_StdReturn : 2.0917720794677734
Eval_MaxReturn : -9.0
Eval_MinReturn : -17.0
Eval_AverageEpLen : 14.285714285714286
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -17.910000
best mean reward -17.129999
running time 516.344864
At timestep:     13 / 1000At timestep:     28 / 1000At timestep:     44 / 1000At timestep:     56 / 1000At timestep:     71 / 1000At timestep:     85 / 1000At timestep:     103 / 1000At timestep:     119 / 1000At timestep:     136 / 1000At timestep:     149 / 1000At timestep:     163 / 1000At timestep:     182 / 1000At timestep:     199 / 1000At timestep:     215 / 1000At timestep:     230 / 1000At timestep:     246 / 1000At timestep:     257 / 1000At timestep:     268 / 1000At timestep:     285 / 1000At timestep:     300 / 1000At timestep:     314 / 1000At timestep:     328 / 1000At timestep:     346 / 1000At timestep:     360 / 1000At timestep:     379 / 1000At timestep:     395 / 1000At timestep:     409 / 1000At timestep:     424 / 1000At timestep:     442 / 1000At timestep:     454 / 1000At timestep:     467 / 1000At timestep:     483 / 1000At timestep:     501 / 1000At timestep:     518 / 1000At timestep:     538 / 1000At timestep:     554 / 1000At timestep:     567 / 1000At timestep:     581 / 1000At timestep:     595 / 1000At timestep:     606 / 1000At timestep:     621 / 1000At timestep:     635 / 1000At timestep:     650 / 1000At timestep:     662 / 1000At timestep:     679 / 1000At timestep:     693 / 1000At timestep:     704 / 1000At timestep:     720 / 1000At timestep:     731 / 1000At timestep:     749 / 1000At timestep:     763 / 1000At timestep:     781 / 1000At timestep:     800 / 1000At timestep:     817 / 1000At timestep:     838 / 1000At timestep:     852 / 1000At timestep:     867 / 1000At timestep:     883 / 1000At timestep:     901 / 1000At timestep:     916 / 1000At timestep:     931 / 1000At timestep:     945 / 1000At timestep:     960 / 1000At timestep:     972 / 1000At timestep:     986 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -17.90999984741211
Train_BestReturn : -17.1299991607666
TimeSinceStart : 516.3448641300201
Exploitation Critic Loss : 0.15582390129566193
Exploration Critic Loss : 41140388.0
Exploration Model Loss : 14.523651123046875
Eval_AverageReturn : -14.151515007019043
Eval_StdReturn : 2.3044254779815674
Eval_MaxReturn : -10.0
Eval_MinReturn : -20.0
Eval_AverageEpLen : 15.151515151515152
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -17.180000
best mean reward -17.129999
running time 545.765271
At timestep:     14 / 1000At timestep:     25 / 1000At timestep:     36 / 1000At timestep:     50 / 1000At timestep:     67 / 1000At timestep:     87 / 1000At timestep:     100 / 1000At timestep:     115 / 1000At timestep:     129 / 1000At timestep:     142 / 1000At timestep:     152 / 1000At timestep:     168 / 1000At timestep:     184 / 1000At timestep:     199 / 1000At timestep:     211 / 1000At timestep:     224 / 1000At timestep:     240 / 1000At timestep:     251 / 1000At timestep:     268 / 1000At timestep:     281 / 1000At timestep:     298 / 1000At timestep:     313 / 1000At timestep:     326 / 1000At timestep:     343 / 1000At timestep:     362 / 1000At timestep:     376 / 1000At timestep:     395 / 1000At timestep:     409 / 1000At timestep:     422 / 1000At timestep:     436 / 1000At timestep:     453 / 1000At timestep:     468 / 1000At timestep:     484 / 1000At timestep:     497 / 1000At timestep:     511 / 1000At timestep:     527 / 1000At timestep:     539 / 1000At timestep:     552 / 1000At timestep:     566 / 1000At timestep:     580 / 1000At timestep:     600 / 1000At timestep:     620 / 1000At timestep:     632 / 1000At timestep:     646 / 1000At timestep:     663 / 1000At timestep:     678 / 1000At timestep:     692 / 1000At timestep:     707 / 1000At timestep:     723 / 1000At timestep:     737 / 1000At timestep:     751 / 1000At timestep:     765 / 1000At timestep:     776 / 1000At timestep:     788 / 1000At timestep:     803 / 1000At timestep:     813 / 1000At timestep:     829 / 1000At timestep:     842 / 1000At timestep:     856 / 1000At timestep:     874 / 1000At timestep:     888 / 1000At timestep:     900 / 1000At timestep:     916 / 1000At timestep:     931 / 1000At timestep:     945 / 1000At timestep:     960 / 1000At timestep:     976 / 1000At timestep:     990 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -17.18000030517578
Train_BestReturn : -17.1299991607666
TimeSinceStart : 545.7652707099915
Exploitation Critic Loss : 0.5211296677589417
Exploration Critic Loss : 18752390.0
Exploration Model Loss : 14.523651123046875
Eval_AverageReturn : -13.623188018798828
Eval_StdReturn : 2.310037612915039
Eval_MaxReturn : -9.0
Eval_MinReturn : -19.0
Eval_AverageEpLen : 14.623188405797102
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -16.879999
best mean reward -16.879999
running time 575.377637
At timestep:     16 / 1000At timestep:     35 / 1000At timestep:     49 / 1000At timestep:     65 / 1000At timestep:     82 / 1000At timestep:     96 / 1000At timestep:     110 / 1000At timestep:     127 / 1000At timestep:     141 / 1000At timestep:     155 / 1000At timestep:     171 / 1000At timestep:     185 / 1000At timestep:     200 / 1000At timestep:     217 / 1000At timestep:     229 / 1000At timestep:     249 / 1000At timestep:     263 / 1000At timestep:     276 / 1000At timestep:     289 / 1000At timestep:     305 / 1000At timestep:     319 / 1000At timestep:     333 / 1000At timestep:     347 / 1000At timestep:     363 / 1000At timestep:     378 / 1000At timestep:     392 / 1000At timestep:     410 / 1000At timestep:     428 / 1000At timestep:     443 / 1000At timestep:     457 / 1000At timestep:     471 / 1000At timestep:     487 / 1000At timestep:     499 / 1000At timestep:     513 / 1000At timestep:     526 / 1000At timestep:     544 / 1000At timestep:     559 / 1000At timestep:     573 / 1000At timestep:     586 / 1000At timestep:     600 / 1000At timestep:     613 / 1000At timestep:     628 / 1000At timestep:     643 / 1000At timestep:     663 / 1000At timestep:     675 / 1000At timestep:     687 / 1000At timestep:     703 / 1000At timestep:     716 / 1000At timestep:     726 / 1000At timestep:     739 / 1000At timestep:     754 / 1000At timestep:     769 / 1000At timestep:     784 / 1000At timestep:     797 / 1000At timestep:     815 / 1000At timestep:     831 / 1000At timestep:     846 / 1000At timestep:     860 / 1000At timestep:     877 / 1000At timestep:     890 / 1000At timestep:     902 / 1000At timestep:     919 / 1000At timestep:     934 / 1000At timestep:     948 / 1000At timestep:     958 / 1000At timestep:     970 / 1000At timestep:     984 / 1000At timestep:     998 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -16.8799991607666
Train_BestReturn : -16.8799991607666
TimeSinceStart : 575.3776371479034
Exploitation Critic Loss : 0.465706467628479
Exploration Critic Loss : 30668458.0
Exploration Model Loss : 14.523651123046875
Eval_AverageReturn : -13.724637985229492
Eval_StdReturn : 2.06337308883667
Eval_MaxReturn : -9.0
Eval_MinReturn : -19.0
Eval_AverageEpLen : 14.72463768115942
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -17.270000
best mean reward -16.879999
running time 605.020564
At timestep:     13 / 1000At timestep:     25 / 1000At timestep:     38 / 1000At timestep:     52 / 1000At timestep:     64 / 1000At timestep:     77 / 1000At timestep:     88 / 1000At timestep:     102 / 1000At timestep:     124 / 1000At timestep:     135 / 1000At timestep:     146 / 1000At timestep:     160 / 1000At timestep:     176 / 1000At timestep:     193 / 1000At timestep:     208 / 1000At timestep:     220 / 1000At timestep:     234 / 1000At timestep:     248 / 1000At timestep:     263 / 1000At timestep:     279 / 1000At timestep:     295 / 1000At timestep:     309 / 1000At timestep:     324 / 1000At timestep:     339 / 1000At timestep:     353 / 1000At timestep:     369 / 1000At timestep:     386 / 1000At timestep:     400 / 1000At timestep:     414 / 1000At timestep:     424 / 1000At timestep:     440 / 1000At timestep:     457 / 1000At timestep:     471 / 1000At timestep:     486 / 1000At timestep:     499 / 1000At timestep:     512 / 1000At timestep:     525 / 1000At timestep:     539 / 1000At timestep:     554 / 1000At timestep:     573 / 1000At timestep:     591 / 1000At timestep:     608 / 1000At timestep:     623 / 1000At timestep:     633 / 1000At timestep:     648 / 1000At timestep:     663 / 1000At timestep:     682 / 1000At timestep:     695 / 1000At timestep:     710 / 1000At timestep:     725 / 1000At timestep:     740 / 1000At timestep:     752 / 1000At timestep:     771 / 1000At timestep:     787 / 1000At timestep:     803 / 1000At timestep:     816 / 1000At timestep:     828 / 1000At timestep:     844 / 1000At timestep:     857 / 1000At timestep:     874 / 1000At timestep:     886 / 1000At timestep:     904 / 1000At timestep:     921 / 1000At timestep:     937 / 1000At timestep:     953 / 1000At timestep:     966 / 1000At timestep:     981 / 1000At timestep:     995 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -17.270000457763672
Train_BestReturn : -16.8799991607666
TimeSinceStart : 605.0205636024475
Exploitation Critic Loss : 0.3774366080760956
Exploration Critic Loss : 15221198.0
Exploration Model Loss : 14.52362060546875
Eval_AverageReturn : -13.637681007385254
Eval_StdReturn : 2.2326369285583496
Eval_MaxReturn : -9.0
Eval_MinReturn : -21.0
Eval_AverageEpLen : 14.63768115942029
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -16.719999
best mean reward -16.719999
running time 635.197181
At timestep:     13 / 1000At timestep:     27 / 1000At timestep:     40 / 1000At timestep:     56 / 1000At timestep:     69 / 1000At timestep:     87 / 1000At timestep:     101 / 1000At timestep:     123 / 1000At timestep:     135 / 1000At timestep:     151 / 1000At timestep:     164 / 1000At timestep:     180 / 1000At timestep:     192 / 1000At timestep:     207 / 1000At timestep:     222 / 1000At timestep:     239 / 1000At timestep:     254 / 1000At timestep:     268 / 1000At timestep:     284 / 1000At timestep:     302 / 1000At timestep:     319 / 1000At timestep:     339 / 1000At timestep:     356 / 1000At timestep:     368 / 1000At timestep:     383 / 1000At timestep:     398 / 1000At timestep:     411 / 1000At timestep:     427 / 1000At timestep:     440 / 1000At timestep:     454 / 1000At timestep:     467 / 1000At timestep:     482 / 1000At timestep:     493 / 1000At timestep:     511 / 1000At timestep:     526 / 1000At timestep:     548 / 1000At timestep:     561 / 1000At timestep:     573 / 1000At timestep:     588 / 1000At timestep:     603 / 1000At timestep:     614 / 1000At timestep:     630 / 1000At timestep:     644 / 1000At timestep:     657 / 1000At timestep:     674 / 1000At timestep:     687 / 1000At timestep:     705 / 1000At timestep:     719 / 1000At timestep:     733 / 1000At timestep:     746 / 1000At timestep:     760 / 1000At timestep:     774 / 1000At timestep:     791 / 1000At timestep:     803 / 1000At timestep:     820 / 1000At timestep:     841 / 1000At timestep:     853 / 1000At timestep:     867 / 1000At timestep:     884 / 1000At timestep:     899 / 1000At timestep:     914 / 1000At timestep:     930 / 1000At timestep:     946 / 1000At timestep:     964 / 1000At timestep:     979 / 1000At timestep:     993 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -16.719999313354492
Train_BestReturn : -16.719999313354492
TimeSinceStart : 635.1971807479858
Exploitation Critic Loss : 0.8241825103759766
Exploration Critic Loss : 14981398.0
Exploration Model Loss : 14.523651123046875
Eval_AverageReturn : -14.029850959777832
Eval_StdReturn : 2.3875932693481445
Eval_MaxReturn : -10.0
Eval_MinReturn : -21.0
Eval_AverageEpLen : 15.029850746268657
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -17.020000
best mean reward -16.719999
running time 664.848659
At timestep:     15 / 1000At timestep:     32 / 1000At timestep:     47 / 1000At timestep:     64 / 1000At timestep:     76 / 1000At timestep:     89 / 1000At timestep:     105 / 1000At timestep:     117 / 1000At timestep:     133 / 1000At timestep:     151 / 1000At timestep:     169 / 1000At timestep:     182 / 1000At timestep:     197 / 1000At timestep:     208 / 1000At timestep:     227 / 1000At timestep:     242 / 1000At timestep:     256 / 1000At timestep:     272 / 1000At timestep:     285 / 1000At timestep:     299 / 1000At timestep:     316 / 1000At timestep:     330 / 1000At timestep:     340 / 1000At timestep:     353 / 1000At timestep:     371 / 1000At timestep:     385 / 1000At timestep:     397 / 1000At timestep:     409 / 1000At timestep:     422 / 1000At timestep:     436 / 1000At timestep:     453 / 1000At timestep:     464 / 1000At timestep:     479 / 1000At timestep:     499 / 1000At timestep:     513 / 1000At timestep:     528 / 1000At timestep:     542 / 1000At timestep:     554 / 1000At timestep:     567 / 1000At timestep:     581 / 1000At timestep:     594 / 1000At timestep:     607 / 1000At timestep:     625 / 1000At timestep:     639 / 1000At timestep:     655 / 1000At timestep:     672 / 1000At timestep:     686 / 1000At timestep:     699 / 1000At timestep:     714 / 1000At timestep:     726 / 1000At timestep:     739 / 1000At timestep:     751 / 1000At timestep:     761 / 1000At timestep:     776 / 1000At timestep:     788 / 1000At timestep:     805 / 1000At timestep:     822 / 1000At timestep:     840 / 1000At timestep:     853 / 1000At timestep:     863 / 1000At timestep:     881 / 1000At timestep:     899 / 1000At timestep:     913 / 1000At timestep:     927 / 1000At timestep:     944 / 1000At timestep:     956 / 1000At timestep:     967 / 1000At timestep:     979 / 1000At timestep:     993 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -17.020000457763672
Train_BestReturn : -16.719999313354492
TimeSinceStart : 664.8486585617065
Exploitation Critic Loss : 0.4835260510444641
Exploration Critic Loss : 25515458.0
Exploration Model Loss : 14.52362060546875
Eval_AverageReturn : -13.442856788635254
Eval_StdReturn : 2.3822901248931885
Eval_MaxReturn : -9.0
Eval_MinReturn : -19.0
Eval_AverageEpLen : 14.442857142857143
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -17.129999
best mean reward -16.719999
running time 695.374645
At timestep:     13 / 1000At timestep:     30 / 1000At timestep:     42 / 1000At timestep:     58 / 1000At timestep:     72 / 1000At timestep:     86 / 1000At timestep:     98 / 1000At timestep:     111 / 1000At timestep:     126 / 1000At timestep:     139 / 1000At timestep:     154 / 1000At timestep:     168 / 1000At timestep:     181 / 1000At timestep:     196 / 1000At timestep:     210 / 1000At timestep:     226 / 1000At timestep:     243 / 1000At timestep:     257 / 1000At timestep:     272 / 1000At timestep:     287 / 1000At timestep:     304 / 1000At timestep:     320 / 1000At timestep:     336 / 1000At timestep:     355 / 1000At timestep:     369 / 1000At timestep:     382 / 1000At timestep:     401 / 1000At timestep:     415 / 1000At timestep:     430 / 1000At timestep:     445 / 1000At timestep:     459 / 1000At timestep:     473 / 1000At timestep:     485 / 1000At timestep:     502 / 1000At timestep:     516 / 1000At timestep:     531 / 1000At timestep:     548 / 1000At timestep:     563 / 1000At timestep:     577 / 1000At timestep:     593 / 1000At timestep:     610 / 1000At timestep:     631 / 1000At timestep:     645 / 1000At timestep:     660 / 1000At timestep:     675 / 1000At timestep:     686 / 1000At timestep:     702 / 1000At timestep:     719 / 1000At timestep:     736 / 1000At timestep:     752 / 1000At timestep:     763 / 1000At timestep:     777 / 1000At timestep:     789 / 1000At timestep:     808 / 1000At timestep:     821 / 1000At timestep:     833 / 1000At timestep:     851 / 1000At timestep:     865 / 1000At timestep:     881 / 1000At timestep:     896 / 1000At timestep:     910 / 1000At timestep:     924 / 1000At timestep:     937 / 1000At timestep:     947 / 1000At timestep:     963 / 1000At timestep:     980 / 1000At timestep:     998 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -17.1299991607666
Train_BestReturn : -16.719999313354492
TimeSinceStart : 695.3746447563171
Exploitation Critic Loss : 0.45449313521385193
Exploration Critic Loss : 20845546.0
Exploration Model Loss : 14.523666381835938
Eval_AverageReturn : -13.897058486938477
Eval_StdReturn : 2.073216676712036
Eval_MaxReturn : -9.0
Eval_MinReturn : -20.0
Eval_AverageEpLen : 14.897058823529411
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -16.950001
best mean reward -16.719999
running time 725.747250
At timestep:     12 / 1000At timestep:     27 / 1000At timestep:     43 / 1000At timestep:     57 / 1000At timestep:     74 / 1000At timestep:     87 / 1000At timestep:     101 / 1000At timestep:     112 / 1000At timestep:     125 / 1000At timestep:     141 / 1000At timestep:     156 / 1000At timestep:     170 / 1000At timestep:     182 / 1000At timestep:     200 / 1000At timestep:     215 / 1000At timestep:     228 / 1000At timestep:     243 / 1000At timestep:     258 / 1000At timestep:     270 / 1000At timestep:     284 / 1000At timestep:     298 / 1000At timestep:     308 / 1000At timestep:     319 / 1000At timestep:     332 / 1000At timestep:     345 / 1000At timestep:     360 / 1000At timestep:     372 / 1000At timestep:     385 / 1000At timestep:     399 / 1000At timestep:     414 / 1000At timestep:     427 / 1000At timestep:     442 / 1000At timestep:     455 / 1000At timestep:     474 / 1000At timestep:     487 / 1000At timestep:     500 / 1000At timestep:     517 / 1000At timestep:     534 / 1000At timestep:     548 / 1000At timestep:     561 / 1000At timestep:     577 / 1000At timestep:     592 / 1000At timestep:     604 / 1000At timestep:     620 / 1000At timestep:     633 / 1000At timestep:     648 / 1000At timestep:     664 / 1000At timestep:     677 / 1000At timestep:     691 / 1000At timestep:     703 / 1000At timestep:     716 / 1000At timestep:     730 / 1000At timestep:     741 / 1000At timestep:     754 / 1000At timestep:     769 / 1000At timestep:     782 / 1000At timestep:     795 / 1000At timestep:     817 / 1000At timestep:     828 / 1000At timestep:     841 / 1000At timestep:     853 / 1000At timestep:     869 / 1000At timestep:     883 / 1000At timestep:     898 / 1000At timestep:     911 / 1000At timestep:     926 / 1000At timestep:     939 / 1000At timestep:     957 / 1000At timestep:     970 / 1000At timestep:     985 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -16.950000762939453
Train_BestReturn : -16.719999313354492
TimeSinceStart : 725.7472498416901
Exploitation Critic Loss : 0.3521232604980469
Exploration Critic Loss : 33948784.0
Exploration Model Loss : 14.523895263671875
Eval_AverageReturn : -13.08450698852539
Eval_StdReturn : 2.0122616291046143
Eval_MaxReturn : -9.0
Eval_MinReturn : -21.0
Eval_AverageEpLen : 14.084507042253522
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -17.750000
best mean reward -16.719999
running time 755.867864
At timestep:     15 / 1000At timestep:     25 / 1000At timestep:     42 / 1000At timestep:     59 / 1000At timestep:     74 / 1000At timestep:     91 / 1000At timestep:     105 / 1000At timestep:     120 / 1000At timestep:     135 / 1000At timestep:     152 / 1000At timestep:     168 / 1000At timestep:     185 / 1000At timestep:     202 / 1000At timestep:     218 / 1000At timestep:     231 / 1000At timestep:     245 / 1000At timestep:     262 / 1000At timestep:     275 / 1000At timestep:     290 / 1000At timestep:     302 / 1000At timestep:     319 / 1000At timestep:     334 / 1000At timestep:     344 / 1000At timestep:     359 / 1000At timestep:     377 / 1000At timestep:     392 / 1000At timestep:     405 / 1000At timestep:     420 / 1000At timestep:     434 / 1000At timestep:     453 / 1000At timestep:     467 / 1000At timestep:     481 / 1000At timestep:     502 / 1000At timestep:     522 / 1000At timestep:     541 / 1000At timestep:     557 / 1000At timestep:     571 / 1000At timestep:     585 / 1000At timestep:     600 / 1000At timestep:     615 / 1000At timestep:     634 / 1000At timestep:     646 / 1000At timestep:     657 / 1000At timestep:     671 / 1000At timestep:     687 / 1000At timestep:     704 / 1000At timestep:     721 / 1000At timestep:     739 / 1000At timestep:     750 / 1000At timestep:     763 / 1000At timestep:     778 / 1000At timestep:     790 / 1000At timestep:     804 / 1000At timestep:     818 / 1000At timestep:     837 / 1000At timestep:     851 / 1000At timestep:     864 / 1000At timestep:     878 / 1000At timestep:     891 / 1000At timestep:     910 / 1000At timestep:     926 / 1000At timestep:     942 / 1000At timestep:     957 / 1000At timestep:     970 / 1000At timestep:     986 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -17.75
Train_BestReturn : -16.719999313354492
TimeSinceStart : 755.8678643703461
Exploitation Critic Loss : 0.5988104939460754
Exploration Critic Loss : 23253748.0
Exploration Model Loss : 14.524093627929688
Eval_AverageReturn : -14.151515007019043
Eval_StdReturn : 2.317538022994995
Eval_MaxReturn : -9.0
Eval_MinReturn : -20.0
Eval_AverageEpLen : 15.151515151515152
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -17.469999
best mean reward -16.719999
running time 785.668097
At timestep:     17 / 1000At timestep:     30 / 1000At timestep:     42 / 1000At timestep:     58 / 1000At timestep:     73 / 1000At timestep:     86 / 1000At timestep:     104 / 1000At timestep:     117 / 1000At timestep:     136 / 1000At timestep:     151 / 1000At timestep:     163 / 1000At timestep:     176 / 1000At timestep:     188 / 1000At timestep:     201 / 1000At timestep:     213 / 1000At timestep:     225 / 1000At timestep:     240 / 1000At timestep:     255 / 1000At timestep:     270 / 1000At timestep:     283 / 1000At timestep:     296 / 1000At timestep:     312 / 1000At timestep:     327 / 1000At timestep:     342 / 1000At timestep:     357 / 1000At timestep:     368 / 1000At timestep:     383 / 1000At timestep:     396 / 1000At timestep:     409 / 1000At timestep:     422 / 1000At timestep:     441 / 1000At timestep:     452 / 1000At timestep:     469 / 1000At timestep:     485 / 1000At timestep:     505 / 1000At timestep:     522 / 1000At timestep:     539 / 1000At timestep:     551 / 1000At timestep:     563 / 1000At timestep:     576 / 1000At timestep:     588 / 1000At timestep:     602 / 1000At timestep:     615 / 1000At timestep:     630 / 1000At timestep:     644 / 1000At timestep:     663 / 1000At timestep:     677 / 1000At timestep:     691 / 1000At timestep:     705 / 1000At timestep:     722 / 1000At timestep:     734 / 1000At timestep:     749 / 1000At timestep:     765 / 1000At timestep:     781 / 1000At timestep:     798 / 1000At timestep:     815 / 1000At timestep:     825 / 1000At timestep:     842 / 1000At timestep:     858 / 1000At timestep:     876 / 1000At timestep:     888 / 1000At timestep:     902 / 1000At timestep:     917 / 1000At timestep:     931 / 1000At timestep:     944 / 1000At timestep:     959 / 1000At timestep:     972 / 1000At timestep:     987 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -17.469999313354492
Train_BestReturn : -16.719999313354492
TimeSinceStart : 785.6680974960327
Exploitation Critic Loss : 0.3737592399120331
Exploration Critic Loss : 25023044.0
Exploration Model Loss : 14.524124145507812
Eval_AverageReturn : -13.550724983215332
Eval_StdReturn : 2.183842182159424
Eval_MaxReturn : -9.0
Eval_MinReturn : -19.0
Eval_AverageEpLen : 14.55072463768116
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -17.500000
best mean reward -16.719999
running time 815.324646
At timestep:     30 / 1000At timestep:     70 / 1000At timestep:     84 / 1000At timestep:     132 / 1000At timestep:     151 / 1000At timestep:     166 / 1000At timestep:     193 / 1000At timestep:     208 / 1000At timestep:     242 / 1000At timestep:     259 / 1000At timestep:     290 / 1000At timestep:     304 / 1000At timestep:     319 / 1000At timestep:     334 / 1000At timestep:     354 / 1000At timestep:     379 / 1000At timestep:     395 / 1000At timestep:     409 / 1000At timestep:     424 / 1000At timestep:     443 / 1000At timestep:     492 / 1000At timestep:     506 / 1000At timestep:     532 / 1000At timestep:     549 / 1000At timestep:     569 / 1000At timestep:     588 / 1000At timestep:     606 / 1000At timestep:     642 / 1000At timestep:     672 / 1000At timestep:     684 / 1000At timestep:     734 / 1000At timestep:     749 / 1000At timestep:     771 / 1000At timestep:     783 / 1000At timestep:     807 / 1000At timestep:     824 / 1000At timestep:     841 / 1000At timestep:     870 / 1000At timestep:     882 / 1000At timestep:     898 / 1000At timestep:     925 / 1000At timestep:     942 / 1000At timestep:     957 / 1000At timestep:     970 / 1000At timestep:     987 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -17.5
Train_BestReturn : -16.719999313354492
TimeSinceStart : 815.3246455192566
Exploitation Critic Loss : 0.8449676632881165
Exploration Critic Loss : 26363092.0
Exploration Model Loss : 14.524124145507812
Eval_AverageReturn : -21.04347801208496
Eval_StdReturn : 9.90817928314209
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 22.02173913043478
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -17.750000
best mean reward -16.719999
running time 841.187953
At timestep:     15 / 1000At timestep:     34 / 1000At timestep:     49 / 1000At timestep:     65 / 1000At timestep:     79 / 1000At timestep:     90 / 1000At timestep:     104 / 1000At timestep:     118 / 1000At timestep:     133 / 1000At timestep:     147 / 1000At timestep:     159 / 1000At timestep:     171 / 1000At timestep:     188 / 1000At timestep:     199 / 1000At timestep:     214 / 1000At timestep:     230 / 1000At timestep:     244 / 1000At timestep:     262 / 1000At timestep:     279 / 1000At timestep:     291 / 1000At timestep:     306 / 1000At timestep:     318 / 1000At timestep:     329 / 1000At timestep:     345 / 1000At timestep:     363 / 1000At timestep:     377 / 1000At timestep:     392 / 1000At timestep:     405 / 1000At timestep:     420 / 1000At timestep:     437 / 1000At timestep:     450 / 1000At timestep:     466 / 1000At timestep:     478 / 1000At timestep:     495 / 1000At timestep:     514 / 1000At timestep:     529 / 1000At timestep:     544 / 1000At timestep:     554 / 1000At timestep:     568 / 1000At timestep:     583 / 1000At timestep:     600 / 1000At timestep:     613 / 1000At timestep:     627 / 1000At timestep:     643 / 1000At timestep:     654 / 1000At timestep:     666 / 1000At timestep:     681 / 1000At timestep:     696 / 1000At timestep:     707 / 1000At timestep:     715 / 1000At timestep:     727 / 1000At timestep:     741 / 1000At timestep:     752 / 1000At timestep:     768 / 1000At timestep:     781 / 1000At timestep:     794 / 1000At timestep:     811 / 1000At timestep:     825 / 1000At timestep:     842 / 1000At timestep:     858 / 1000At timestep:     875 / 1000At timestep:     889 / 1000At timestep:     904 / 1000At timestep:     920 / 1000At timestep:     935 / 1000At timestep:     953 / 1000At timestep:     967 / 1000At timestep:     980 / 1000At timestep:     994 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -17.75
Train_BestReturn : -16.719999313354492
TimeSinceStart : 841.1879525184631
Exploitation Critic Loss : 0.14781224727630615
Exploration Critic Loss : 31264132.0
Exploration Model Loss : 14.524124145507812
Eval_AverageReturn : -13.442856788635254
Eval_StdReturn : 2.2401256561279297
Eval_MaxReturn : -7.0
Eval_MinReturn : -18.0
Eval_AverageEpLen : 14.442857142857143
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -17.750000
best mean reward -16.719999
running time 871.459866
At timestep:     17 / 1000At timestep:     32 / 1000At timestep:     48 / 1000At timestep:     64 / 1000At timestep:     79 / 1000At timestep:     95 / 1000At timestep:     110 / 1000At timestep:     126 / 1000At timestep:     141 / 1000At timestep:     153 / 1000At timestep:     165 / 1000At timestep:     180 / 1000At timestep:     196 / 1000At timestep:     209 / 1000At timestep:     229 / 1000At timestep:     241 / 1000At timestep:     254 / 1000At timestep:     268 / 1000At timestep:     282 / 1000At timestep:     301 / 1000At timestep:     315 / 1000At timestep:     336 / 1000At timestep:     351 / 1000At timestep:     363 / 1000At timestep:     376 / 1000At timestep:     390 / 1000At timestep:     409 / 1000At timestep:     423 / 1000At timestep:     438 / 1000At timestep:     454 / 1000At timestep:     468 / 1000At timestep:     481 / 1000At timestep:     494 / 1000At timestep:     508 / 1000At timestep:     523 / 1000At timestep:     542 / 1000At timestep:     554 / 1000At timestep:     568 / 1000At timestep:     586 / 1000At timestep:     602 / 1000At timestep:     615 / 1000At timestep:     629 / 1000At timestep:     639 / 1000At timestep:     654 / 1000At timestep:     674 / 1000At timestep:     686 / 1000At timestep:     698 / 1000At timestep:     710 / 1000At timestep:     721 / 1000At timestep:     735 / 1000At timestep:     749 / 1000At timestep:     766 / 1000At timestep:     776 / 1000At timestep:     789 / 1000At timestep:     805 / 1000At timestep:     821 / 1000At timestep:     833 / 1000At timestep:     845 / 1000At timestep:     865 / 1000At timestep:     881 / 1000At timestep:     892 / 1000At timestep:     905 / 1000At timestep:     920 / 1000At timestep:     936 / 1000At timestep:     952 / 1000At timestep:     966 / 1000At timestep:     980 / 1000At timestep:     994 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -17.75
Train_BestReturn : -16.719999313354492
TimeSinceStart : 871.4598660469055
Exploitation Critic Loss : 1.445418119430542
Exploration Critic Loss : 18707216.0
Exploration Model Loss : 14.52410888671875
Eval_AverageReturn : -13.637681007385254
Eval_StdReturn : 2.401515483856201
Eval_MaxReturn : -9.0
Eval_MinReturn : -20.0
Eval_AverageEpLen : 14.63768115942029
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -17.129999
best mean reward -16.719999
running time 901.256849
At timestep:     15 / 1000At timestep:     30 / 1000At timestep:     48 / 1000At timestep:     62 / 1000At timestep:     77 / 1000At timestep:     91 / 1000At timestep:     107 / 1000At timestep:     123 / 1000At timestep:     137 / 1000At timestep:     148 / 1000At timestep:     163 / 1000At timestep:     176 / 1000At timestep:     189 / 1000At timestep:     204 / 1000At timestep:     217 / 1000At timestep:     231 / 1000At timestep:     247 / 1000At timestep:     261 / 1000At timestep:     277 / 1000At timestep:     290 / 1000At timestep:     303 / 1000At timestep:     315 / 1000At timestep:     328 / 1000At timestep:     346 / 1000At timestep:     359 / 1000At timestep:     371 / 1000At timestep:     385 / 1000At timestep:     396 / 1000At timestep:     409 / 1000At timestep:     424 / 1000At timestep:     439 / 1000At timestep:     452 / 1000At timestep:     464 / 1000At timestep:     476 / 1000At timestep:     489 / 1000At timestep:     502 / 1000At timestep:     516 / 1000At timestep:     531 / 1000At timestep:     545 / 1000At timestep:     557 / 1000At timestep:     570 / 1000At timestep:     583 / 1000At timestep:     596 / 1000At timestep:     612 / 1000At timestep:     623 / 1000At timestep:     637 / 1000At timestep:     648 / 1000At timestep:     665 / 1000At timestep:     682 / 1000At timestep:     696 / 1000At timestep:     710 / 1000At timestep:     721 / 1000At timestep:     738 / 1000At timestep:     754 / 1000At timestep:     770 / 1000At timestep:     785 / 1000At timestep:     799 / 1000At timestep:     814 / 1000At timestep:     828 / 1000At timestep:     842 / 1000At timestep:     855 / 1000At timestep:     870 / 1000At timestep:     885 / 1000At timestep:     900 / 1000At timestep:     917 / 1000At timestep:     930 / 1000At timestep:     948 / 1000At timestep:     964 / 1000At timestep:     977 / 1000At timestep:     991 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -17.1299991607666
Train_BestReturn : -16.719999313354492
TimeSinceStart : 901.2568490505219
Exploitation Critic Loss : 0.6816043257713318
Exploration Critic Loss : 20722482.0
Exploration Model Loss : 14.52410888671875
Eval_AverageReturn : -13.14084529876709
Eval_StdReturn : 1.7222306728363037
Eval_MaxReturn : -10.0
Eval_MinReturn : -17.0
Eval_AverageEpLen : 14.140845070422536
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -17.129999
best mean reward -16.719999
running time 931.545020
At timestep:     15 / 1000At timestep:     28 / 1000At timestep:     44 / 1000At timestep:     58 / 1000At timestep:     74 / 1000At timestep:     87 / 1000At timestep:     100 / 1000At timestep:     116 / 1000At timestep:     131 / 1000At timestep:     146 / 1000At timestep:     162 / 1000At timestep:     180 / 1000At timestep:     195 / 1000At timestep:     209 / 1000At timestep:     222 / 1000At timestep:     239 / 1000At timestep:     254 / 1000At timestep:     271 / 1000At timestep:     285 / 1000At timestep:     298 / 1000At timestep:     311 / 1000At timestep:     324 / 1000At timestep:     337 / 1000At timestep:     355 / 1000At timestep:     370 / 1000At timestep:     386 / 1000At timestep:     401 / 1000At timestep:     415 / 1000At timestep:     430 / 1000At timestep:     448 / 1000At timestep:     461 / 1000At timestep:     478 / 1000At timestep:     498 / 1000At timestep:     511 / 1000At timestep:     527 / 1000At timestep:     544 / 1000At timestep:     557 / 1000At timestep:     572 / 1000At timestep:     584 / 1000At timestep:     597 / 1000At timestep:     615 / 1000At timestep:     635 / 1000At timestep:     653 / 1000At timestep:     666 / 1000At timestep:     680 / 1000At timestep:     693 / 1000At timestep:     708 / 1000At timestep:     722 / 1000At timestep:     735 / 1000At timestep:     752 / 1000At timestep:     767 / 1000At timestep:     780 / 1000At timestep:     792 / 1000At timestep:     807 / 1000At timestep:     822 / 1000At timestep:     835 / 1000At timestep:     848 / 1000At timestep:     864 / 1000At timestep:     880 / 1000At timestep:     893 / 1000At timestep:     906 / 1000At timestep:     922 / 1000At timestep:     935 / 1000At timestep:     948 / 1000At timestep:     964 / 1000At timestep:     980 / 1000At timestep:     997 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -17.1299991607666
Train_BestReturn : -16.719999313354492
TimeSinceStart : 931.5450201034546
Exploitation Critic Loss : 0.4591749906539917
Exploration Critic Loss : 22330786.0
Exploration Model Loss : 14.52410888671875
Eval_AverageReturn : -13.867647171020508
Eval_StdReturn : 1.8857390880584717
Eval_MaxReturn : -11.0
Eval_MinReturn : -19.0
Eval_AverageEpLen : 14.867647058823529
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -17.530001
best mean reward -16.719999
running time 961.171226
At timestep:     16 / 1000At timestep:     32 / 1000At timestep:     46 / 1000At timestep:     60 / 1000At timestep:     76 / 1000At timestep:     92 / 1000At timestep:     105 / 1000At timestep:     118 / 1000At timestep:     133 / 1000At timestep:     148 / 1000At timestep:     165 / 1000At timestep:     184 / 1000At timestep:     199 / 1000At timestep:     212 / 1000At timestep:     230 / 1000At timestep:     247 / 1000At timestep:     265 / 1000At timestep:     280 / 1000At timestep:     294 / 1000At timestep:     308 / 1000At timestep:     322 / 1000At timestep:     336 / 1000At timestep:     350 / 1000At timestep:     364 / 1000At timestep:     376 / 1000At timestep:     395 / 1000At timestep:     412 / 1000At timestep:     427 / 1000At timestep:     440 / 1000At timestep:     456 / 1000At timestep:     469 / 1000At timestep:     486 / 1000At timestep:     497 / 1000At timestep:     510 / 1000At timestep:     525 / 1000At timestep:     538 / 1000At timestep:     552 / 1000At timestep:     568 / 1000At timestep:     581 / 1000At timestep:     596 / 1000At timestep:     609 / 1000At timestep:     625 / 1000At timestep:     639 / 1000At timestep:     653 / 1000At timestep:     666 / 1000At timestep:     683 / 1000At timestep:     700 / 1000At timestep:     715 / 1000At timestep:     733 / 1000At timestep:     743 / 1000At timestep:     755 / 1000At timestep:     770 / 1000At timestep:     783 / 1000At timestep:     803 / 1000At timestep:     816 / 1000At timestep:     833 / 1000At timestep:     852 / 1000At timestep:     869 / 1000At timestep:     886 / 1000At timestep:     902 / 1000At timestep:     916 / 1000At timestep:     938 / 1000At timestep:     954 / 1000At timestep:     970 / 1000At timestep:     985 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -17.530000686645508
Train_BestReturn : -16.719999313354492
TimeSinceStart : 961.1712255477905
Exploitation Critic Loss : 1.24496328830719
Exploration Critic Loss : 24224324.0
Exploration Model Loss : 14.524169921875
Eval_AverageReturn : -14.151515007019043
Eval_StdReturn : 2.175917863845825
Eval_MaxReturn : -9.0
Eval_MinReturn : -21.0
Eval_AverageEpLen : 15.151515151515152
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -17.290001
best mean reward -16.719999
running time 990.635116
At timestep:     14 / 1000At timestep:     28 / 1000At timestep:     50 / 1000At timestep:     65 / 1000At timestep:     78 / 1000At timestep:     93 / 1000At timestep:     107 / 1000At timestep:     124 / 1000At timestep:     144 / 1000At timestep:     158 / 1000At timestep:     171 / 1000At timestep:     185 / 1000At timestep:     201 / 1000At timestep:     218 / 1000At timestep:     233 / 1000At timestep:     249 / 1000At timestep:     262 / 1000At timestep:     273 / 1000At timestep:     285 / 1000At timestep:     299 / 1000At timestep:     314 / 1000At timestep:     325 / 1000At timestep:     337 / 1000At timestep:     351 / 1000At timestep:     365 / 1000At timestep:     378 / 1000At timestep:     392 / 1000At timestep:     413 / 1000At timestep:     425 / 1000At timestep:     438 / 1000At timestep:     454 / 1000At timestep:     467 / 1000At timestep:     483 / 1000At timestep:     499 / 1000At timestep:     517 / 1000At timestep:     534 / 1000At timestep:     551 / 1000At timestep:     566 / 1000At timestep:     581 / 1000At timestep:     593 / 1000At timestep:     613 / 1000At timestep:     629 / 1000At timestep:     649 / 1000At timestep:     665 / 1000At timestep:     678 / 1000At timestep:     694 / 1000At timestep:     709 / 1000At timestep:     723 / 1000At timestep:     739 / 1000At timestep:     753 / 1000At timestep:     768 / 1000At timestep:     781 / 1000At timestep:     796 / 1000At timestep:     807 / 1000At timestep:     821 / 1000At timestep:     833 / 1000At timestep:     847 / 1000At timestep:     861 / 1000At timestep:     874 / 1000At timestep:     889 / 1000At timestep:     903 / 1000At timestep:     914 / 1000At timestep:     930 / 1000At timestep:     943 / 1000At timestep:     957 / 1000At timestep:     973 / 1000At timestep:     985 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -17.290000915527344
Train_BestReturn : -16.719999313354492
TimeSinceStart : 990.6351163387299
Exploitation Critic Loss : 0.6649681329727173
Exploration Critic Loss : 31143448.0
Exploration Model Loss : 14.524169921875
Eval_AverageReturn : -13.720588684082031
Eval_StdReturn : 2.3190109729766846
Eval_MaxReturn : -10.0
Eval_MinReturn : -21.0
Eval_AverageEpLen : 14.720588235294118
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -17.430000
best mean reward -16.719999
running time 1020.366369
At timestep:     15 / 1000At timestep:     35 / 1000At timestep:     47 / 1000At timestep:     61 / 1000At timestep:     75 / 1000At timestep:     93 / 1000At timestep:     108 / 1000At timestep:     122 / 1000At timestep:     134 / 1000At timestep:     150 / 1000At timestep:     167 / 1000At timestep:     180 / 1000At timestep:     194 / 1000At timestep:     210 / 1000At timestep:     226 / 1000At timestep:     237 / 1000At timestep:     250 / 1000At timestep:     266 / 1000At timestep:     282 / 1000At timestep:     295 / 1000At timestep:     309 / 1000At timestep:     325 / 1000At timestep:     347 / 1000At timestep:     362 / 1000At timestep:     377 / 1000At timestep:     388 / 1000At timestep:     405 / 1000At timestep:     421 / 1000At timestep:     437 / 1000At timestep:     453 / 1000At timestep:     468 / 1000At timestep:     487 / 1000At timestep:     500 / 1000At timestep:     511 / 1000At timestep:     524 / 1000At timestep:     542 / 1000At timestep:     559 / 1000At timestep:     574 / 1000At timestep:     595 / 1000At timestep:     608 / 1000At timestep:     622 / 1000At timestep:     637 / 1000At timestep:     655 / 1000At timestep:     668 / 1000At timestep:     684 / 1000At timestep:     700 / 1000At timestep:     716 / 1000At timestep:     733 / 1000At timestep:     744 / 1000At timestep:     759 / 1000At timestep:     776 / 1000At timestep:     789 / 1000At timestep:     803 / 1000At timestep:     816 / 1000At timestep:     834 / 1000At timestep:     849 / 1000At timestep:     863 / 1000At timestep:     876 / 1000At timestep:     891 / 1000At timestep:     904 / 1000At timestep:     917 / 1000At timestep:     932 / 1000At timestep:     947 / 1000At timestep:     960 / 1000At timestep:     973 / 1000At timestep:     985 / 1000At timestep:     997 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -17.43000030517578
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1020.3663692474365
Exploitation Critic Loss : 0.6128531694412231
Exploration Critic Loss : 25614180.0
Exploration Model Loss : 14.524185180664062
Eval_AverageReturn : -13.867647171020508
Eval_StdReturn : 2.293881893157959
Eval_MaxReturn : -10.0
Eval_MinReturn : -21.0
Eval_AverageEpLen : 14.867647058823529
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -17.850000
best mean reward -16.719999
running time 1049.769500
At timestep:     16 / 1000At timestep:     29 / 1000At timestep:     45 / 1000At timestep:     63 / 1000At timestep:     76 / 1000At timestep:     89 / 1000At timestep:     104 / 1000At timestep:     118 / 1000At timestep:     132 / 1000At timestep:     151 / 1000At timestep:     165 / 1000At timestep:     178 / 1000At timestep:     190 / 1000At timestep:     207 / 1000At timestep:     219 / 1000At timestep:     232 / 1000At timestep:     244 / 1000At timestep:     261 / 1000At timestep:     277 / 1000At timestep:     295 / 1000At timestep:     309 / 1000At timestep:     323 / 1000At timestep:     338 / 1000At timestep:     353 / 1000At timestep:     367 / 1000At timestep:     381 / 1000At timestep:     399 / 1000At timestep:     412 / 1000At timestep:     424 / 1000At timestep:     436 / 1000At timestep:     449 / 1000At timestep:     463 / 1000At timestep:     477 / 1000At timestep:     488 / 1000At timestep:     504 / 1000At timestep:     517 / 1000At timestep:     533 / 1000At timestep:     548 / 1000At timestep:     560 / 1000At timestep:     572 / 1000At timestep:     587 / 1000At timestep:     603 / 1000At timestep:     621 / 1000At timestep:     635 / 1000At timestep:     649 / 1000At timestep:     666 / 1000At timestep:     683 / 1000At timestep:     695 / 1000At timestep:     714 / 1000At timestep:     726 / 1000At timestep:     739 / 1000At timestep:     756 / 1000At timestep:     776 / 1000At timestep:     787 / 1000At timestep:     801 / 1000At timestep:     813 / 1000At timestep:     826 / 1000At timestep:     837 / 1000At timestep:     848 / 1000At timestep:     865 / 1000At timestep:     880 / 1000At timestep:     897 / 1000At timestep:     910 / 1000At timestep:     926 / 1000At timestep:     941 / 1000At timestep:     956 / 1000At timestep:     969 / 1000At timestep:     984 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -17.850000381469727
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1049.7695002555847
Exploitation Critic Loss : 0.7692646384239197
Exploration Critic Loss : 17756866.0
Exploration Model Loss : 14.524261474609375
Eval_AverageReturn : -13.492753982543945
Eval_StdReturn : 2.171109199523926
Eval_MaxReturn : -10.0
Eval_MinReturn : -19.0
Eval_AverageEpLen : 14.492753623188406
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -17.730000
best mean reward -16.719999
running time 1079.766580
At timestep:     14 / 1000At timestep:     27 / 1000At timestep:     39 / 1000At timestep:     54 / 1000At timestep:     67 / 1000At timestep:     82 / 1000At timestep:     98 / 1000At timestep:     114 / 1000At timestep:     127 / 1000At timestep:     141 / 1000At timestep:     153 / 1000At timestep:     164 / 1000At timestep:     179 / 1000At timestep:     196 / 1000At timestep:     211 / 1000At timestep:     228 / 1000At timestep:     243 / 1000At timestep:     253 / 1000At timestep:     268 / 1000At timestep:     285 / 1000At timestep:     302 / 1000At timestep:     315 / 1000At timestep:     331 / 1000At timestep:     346 / 1000At timestep:     358 / 1000At timestep:     370 / 1000At timestep:     383 / 1000At timestep:     401 / 1000At timestep:     415 / 1000At timestep:     429 / 1000At timestep:     445 / 1000At timestep:     458 / 1000At timestep:     474 / 1000At timestep:     490 / 1000At timestep:     506 / 1000At timestep:     519 / 1000At timestep:     536 / 1000At timestep:     549 / 1000At timestep:     566 / 1000At timestep:     582 / 1000At timestep:     601 / 1000At timestep:     614 / 1000At timestep:     628 / 1000At timestep:     640 / 1000At timestep:     658 / 1000At timestep:     670 / 1000At timestep:     682 / 1000At timestep:     695 / 1000At timestep:     710 / 1000At timestep:     725 / 1000At timestep:     739 / 1000At timestep:     754 / 1000At timestep:     766 / 1000At timestep:     781 / 1000At timestep:     795 / 1000At timestep:     810 / 1000At timestep:     825 / 1000At timestep:     841 / 1000At timestep:     857 / 1000At timestep:     873 / 1000At timestep:     888 / 1000At timestep:     900 / 1000At timestep:     912 / 1000At timestep:     923 / 1000At timestep:     939 / 1000At timestep:     955 / 1000At timestep:     972 / 1000At timestep:     984 / 1000At timestep:     999 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -17.729999542236328
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1079.7665801048279
Exploitation Critic Loss : 0.16737668216228485
Exploration Critic Loss : 25006132.0
Exploration Model Loss : 14.524383544921875
Eval_AverageReturn : -13.45714282989502
Eval_StdReturn : 1.9323242902755737
Eval_MaxReturn : -9.0
Eval_MinReturn : -18.0
Eval_AverageEpLen : 14.457142857142857
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -17.590000
best mean reward -16.719999
running time 1110.495928
At timestep:     18 / 1000At timestep:     33 / 1000At timestep:     47 / 1000At timestep:     59 / 1000At timestep:     75 / 1000At timestep:     89 / 1000At timestep:     108 / 1000At timestep:     123 / 1000At timestep:     138 / 1000At timestep:     151 / 1000At timestep:     163 / 1000At timestep:     179 / 1000At timestep:     195 / 1000At timestep:     210 / 1000At timestep:     226 / 1000At timestep:     242 / 1000At timestep:     259 / 1000At timestep:     272 / 1000At timestep:     285 / 1000At timestep:     297 / 1000At timestep:     313 / 1000At timestep:     326 / 1000At timestep:     341 / 1000At timestep:     356 / 1000At timestep:     371 / 1000At timestep:     391 / 1000At timestep:     403 / 1000At timestep:     416 / 1000At timestep:     430 / 1000At timestep:     444 / 1000At timestep:     456 / 1000At timestep:     476 / 1000At timestep:     491 / 1000At timestep:     508 / 1000At timestep:     521 / 1000At timestep:     536 / 1000At timestep:     546 / 1000At timestep:     558 / 1000At timestep:     573 / 1000At timestep:     589 / 1000At timestep:     602 / 1000At timestep:     615 / 1000At timestep:     629 / 1000At timestep:     645 / 1000At timestep:     662 / 1000At timestep:     677 / 1000At timestep:     690 / 1000At timestep:     704 / 1000At timestep:     719 / 1000At timestep:     734 / 1000At timestep:     747 / 1000At timestep:     760 / 1000At timestep:     776 / 1000At timestep:     791 / 1000At timestep:     804 / 1000At timestep:     822 / 1000At timestep:     842 / 1000At timestep:     856 / 1000At timestep:     867 / 1000At timestep:     887 / 1000At timestep:     902 / 1000At timestep:     916 / 1000At timestep:     932 / 1000At timestep:     946 / 1000At timestep:     958 / 1000At timestep:     971 / 1000At timestep:     987 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -17.59000015258789
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1110.4959280490875
Exploitation Critic Loss : 0.7395585775375366
Exploration Critic Loss : 22267124.0
Exploration Model Loss : 14.52447509765625
Eval_AverageReturn : -13.75
Eval_StdReturn : 2.1650636196136475
Eval_MaxReturn : -9.0
Eval_MinReturn : -19.0
Eval_AverageEpLen : 14.75
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -17.889999
best mean reward -16.719999
running time 1141.061432
At timestep:     14 / 1000At timestep:     25 / 1000At timestep:     41 / 1000At timestep:     55 / 1000At timestep:     71 / 1000At timestep:     83 / 1000At timestep:     95 / 1000At timestep:     111 / 1000At timestep:     127 / 1000At timestep:     139 / 1000At timestep:     156 / 1000At timestep:     171 / 1000At timestep:     185 / 1000At timestep:     201 / 1000At timestep:     215 / 1000At timestep:     226 / 1000At timestep:     238 / 1000At timestep:     253 / 1000At timestep:     271 / 1000At timestep:     286 / 1000At timestep:     300 / 1000At timestep:     314 / 1000At timestep:     327 / 1000At timestep:     338 / 1000At timestep:     352 / 1000At timestep:     367 / 1000At timestep:     382 / 1000At timestep:     400 / 1000At timestep:     414 / 1000At timestep:     431 / 1000At timestep:     445 / 1000At timestep:     459 / 1000At timestep:     473 / 1000At timestep:     489 / 1000At timestep:     503 / 1000At timestep:     519 / 1000At timestep:     531 / 1000At timestep:     547 / 1000At timestep:     561 / 1000At timestep:     579 / 1000At timestep:     592 / 1000At timestep:     607 / 1000At timestep:     622 / 1000At timestep:     640 / 1000At timestep:     656 / 1000At timestep:     673 / 1000At timestep:     685 / 1000At timestep:     696 / 1000At timestep:     714 / 1000At timestep:     733 / 1000At timestep:     749 / 1000At timestep:     763 / 1000At timestep:     781 / 1000At timestep:     795 / 1000At timestep:     809 / 1000At timestep:     823 / 1000At timestep:     838 / 1000At timestep:     856 / 1000At timestep:     871 / 1000At timestep:     884 / 1000At timestep:     900 / 1000At timestep:     914 / 1000At timestep:     928 / 1000At timestep:     942 / 1000At timestep:     958 / 1000At timestep:     974 / 1000At timestep:     991 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -17.889999389648438
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1141.0614318847656
Exploitation Critic Loss : 0.1574581116437912
Exploration Critic Loss : 29973988.0
Exploration Model Loss : 14.524459838867188
Eval_AverageReturn : -13.764705657958984
Eval_StdReturn : 1.9487375020980835
Eval_MaxReturn : -10.0
Eval_MinReturn : -18.0
Eval_AverageEpLen : 14.764705882352942
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -18.440001
best mean reward -16.719999
running time 1170.803115
At timestep:     18 / 1000At timestep:     34 / 1000At timestep:     53 / 1000At timestep:     68 / 1000At timestep:     79 / 1000At timestep:     95 / 1000At timestep:     107 / 1000At timestep:     120 / 1000At timestep:     133 / 1000At timestep:     147 / 1000At timestep:     162 / 1000At timestep:     178 / 1000At timestep:     195 / 1000At timestep:     209 / 1000At timestep:     224 / 1000At timestep:     241 / 1000At timestep:     255 / 1000At timestep:     268 / 1000At timestep:     281 / 1000At timestep:     297 / 1000At timestep:     311 / 1000At timestep:     326 / 1000At timestep:     340 / 1000At timestep:     353 / 1000At timestep:     372 / 1000At timestep:     392 / 1000At timestep:     405 / 1000At timestep:     419 / 1000At timestep:     440 / 1000At timestep:     454 / 1000At timestep:     468 / 1000At timestep:     481 / 1000At timestep:     495 / 1000At timestep:     505 / 1000At timestep:     520 / 1000At timestep:     531 / 1000At timestep:     549 / 1000At timestep:     564 / 1000At timestep:     581 / 1000At timestep:     595 / 1000At timestep:     609 / 1000At timestep:     620 / 1000At timestep:     634 / 1000At timestep:     645 / 1000At timestep:     665 / 1000At timestep:     676 / 1000At timestep:     689 / 1000At timestep:     704 / 1000At timestep:     719 / 1000At timestep:     735 / 1000At timestep:     745 / 1000At timestep:     761 / 1000At timestep:     782 / 1000At timestep:     797 / 1000At timestep:     811 / 1000At timestep:     828 / 1000At timestep:     844 / 1000At timestep:     861 / 1000At timestep:     876 / 1000At timestep:     890 / 1000At timestep:     904 / 1000At timestep:     918 / 1000At timestep:     931 / 1000At timestep:     946 / 1000At timestep:     963 / 1000At timestep:     979 / 1000At timestep:     996 / 1000At timestep:     1011 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q1_env1_random_PointmassEasy-v0_21-11-2022_15-37-36/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -18.440000534057617
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1170.8031151294708
Exploitation Critic Loss : 0.6778779625892639
Exploration Critic Loss : 24906074.0
Exploration Model Loss : 14.524444580078125
Eval_AverageReturn : -13.867647171020508
Eval_StdReturn : 2.4187042713165283
Eval_MaxReturn : -9.0
Eval_MinReturn : -20.0
Eval_AverageEpLen : 14.867647058823529
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env2_rnd_PointmassMedium-v0_21-11-2022_15-57-41 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env2_rnd_PointmassMedium-v0_21-11-2022_15-57-41
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001691
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0016908645629882812
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 4.790998
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 4.790998458862305
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 9.695940
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -150.0
TimeSinceStart : 9.695939779281616
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 20.475202
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -150.0
TimeSinceStart : 20.475202083587646
Exploitation Critic Loss : 0.05409811809659004
Exploration Critic Loss : 1136731.75
Exploration Model Loss : 66.37904357910156
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 31.260889
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -150.0
TimeSinceStart : 31.260888814926147
Exploitation Critic Loss : 0.35619843006134033
Exploration Critic Loss : 7352265.5
Exploration Model Loss : 31.224838256835938
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 42.087194
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -150.0
TimeSinceStart : 42.08719444274902
Exploitation Critic Loss : 0.48080337047576904
Exploration Critic Loss : 10954875.0
Exploration Model Loss : 37.10884094238281
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 53.186553
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -150.0
TimeSinceStart : 53.18655347824097
Exploitation Critic Loss : 0.21834655106067657
Exploration Critic Loss : 11205502.0
Exploration Model Loss : 30.095626831054688
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 64.376861
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -150.0
TimeSinceStart : 64.37686061859131
Exploitation Critic Loss : 0.5710123777389526
Exploration Critic Loss : 13955292.0
Exploration Model Loss : 25.1453857421875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 75.776801
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1046 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -150.0
TimeSinceStart : 75.77680087089539
Exploitation Critic Loss : 0.8109331130981445
Exploration Critic Loss : 16527588.0
Exploration Model Loss : 23.892181396484375
Eval_AverageReturn : -149.2857208251953
Eval_StdReturn : 1.7496355772018433
Eval_MaxReturn : -145.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 149.42857142857142
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 87.829042
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -150.0
TimeSinceStart : 87.82904171943665
Exploitation Critic Loss : 0.7672563791275024
Exploration Critic Loss : 17051346.0
Exploration Model Loss : 23.481536865234375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 99.008438
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -150.0
TimeSinceStart : 99.00843811035156
Exploitation Critic Loss : 0.29938191175460815
Exploration Critic Loss : 15102546.0
Exploration Model Loss : 23.33587646484375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -146.932434
best mean reward -inf
running time 111.067171
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     719 / 1000At timestep:     869 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -146.93243408203125
TimeSinceStart : 111.0671706199646
Exploitation Critic Loss : 0.09404630959033966
Exploration Critic Loss : 17729150.0
Exploration Model Loss : 23.283477783203125
Eval_AverageReturn : -145.42857360839844
Eval_StdReturn : 11.197667121887207
Eval_MaxReturn : -118.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 145.57142857142858
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -144.445786
best mean reward -inf
running time 122.799754
At timestep:     84 / 1000At timestep:     234 / 1000At timestep:     288 / 1000At timestep:     438 / 1000At timestep:     469 / 1000At timestep:     502 / 1000At timestep:     563 / 1000At timestep:     713 / 1000At timestep:     777 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1054 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -144.44578552246094
TimeSinceStart : 122.79975414276123
Exploitation Critic Loss : 1.4242255687713623
Exploration Critic Loss : 13040064.0
Exploration Model Loss : 23.264328002929688
Eval_AverageReturn : -87.16666412353516
Eval_StdReturn : 47.45846176147461
Eval_MaxReturn : -30.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 87.83333333333333
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -137.244675
best mean reward -inf
running time 134.890026
At timestep:     150 / 1000At timestep:     211 / 1000At timestep:     361 / 1000At timestep:     445 / 1000At timestep:     574 / 1000At timestep:     724 / 1000At timestep:     756 / 1000At timestep:     894 / 1000At timestep:     1044 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -137.2446746826172
TimeSinceStart : 134.89002585411072
Exploitation Critic Loss : 1.6183223724365234
Exploration Critic Loss : 16536064.0
Exploration Model Loss : 23.25714111328125
Eval_AverageReturn : -115.44444274902344
Eval_StdReturn : 43.0196647644043
Eval_MaxReturn : -31.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 116.0
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -127.489998
best mean reward -127.489998
running time 147.242611
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -127.48999786376953
Train_BestReturn : -127.48999786376953
TimeSinceStart : 147.24261140823364
Exploitation Critic Loss : 0.4832586944103241
Exploration Critic Loss : 12177862.0
Exploration Model Loss : 23.254684448242188
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -107.070000
best mean reward -107.070000
running time 159.961798
At timestep:     61 / 1000At timestep:     101 / 1000At timestep:     164 / 1000At timestep:     211 / 1000At timestep:     243 / 1000At timestep:     296 / 1000At timestep:     339 / 1000At timestep:     374 / 1000At timestep:     425 / 1000At timestep:     465 / 1000At timestep:     524 / 1000At timestep:     575 / 1000At timestep:     643 / 1000At timestep:     680 / 1000At timestep:     714 / 1000At timestep:     741 / 1000At timestep:     782 / 1000At timestep:     827 / 1000At timestep:     900 / 1000At timestep:     947 / 1000At timestep:     993 / 1000At timestep:     1043 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -107.06999969482422
Train_BestReturn : -107.06999969482422
TimeSinceStart : 159.9617977142334
Exploitation Critic Loss : 0.8601179718971252
Exploration Critic Loss : 9608859.0
Exploration Model Loss : 23.254104614257812
Eval_AverageReturn : -46.40909194946289
Eval_StdReturn : 11.637162208557129
Eval_MaxReturn : -26.0
Eval_MinReturn : -72.0
Eval_AverageEpLen : 47.40909090909091
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -91.360001
best mean reward -91.360001
running time 173.649045
At timestep:     53 / 1000At timestep:     82 / 1000At timestep:     110 / 1000At timestep:     159 / 1000At timestep:     309 / 1000At timestep:     340 / 1000At timestep:     475 / 1000At timestep:     543 / 1000At timestep:     581 / 1000At timestep:     630 / 1000At timestep:     680 / 1000At timestep:     830 / 1000At timestep:     890 / 1000At timestep:     951 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -91.36000061035156
Train_BestReturn : -91.36000061035156
TimeSinceStart : 173.64904499053955
Exploitation Critic Loss : 0.7973962426185608
Exploration Critic Loss : 7839138.5
Exploration Model Loss : 23.253860473632812
Eval_AverageReturn : -66.0
Eval_StdReturn : 41.05443572998047
Eval_MaxReturn : -27.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 66.86666666666666
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -67.239998
best mean reward -67.239998
running time 187.269037
At timestep:     24 / 1000At timestep:     65 / 1000At timestep:     98 / 1000At timestep:     136 / 1000At timestep:     196 / 1000At timestep:     224 / 1000At timestep:     263 / 1000At timestep:     300 / 1000At timestep:     326 / 1000At timestep:     356 / 1000At timestep:     390 / 1000At timestep:     448 / 1000At timestep:     521 / 1000At timestep:     577 / 1000At timestep:     628 / 1000At timestep:     673 / 1000At timestep:     708 / 1000At timestep:     748 / 1000At timestep:     765 / 1000At timestep:     803 / 1000At timestep:     852 / 1000At timestep:     903 / 1000At timestep:     934 / 1000At timestep:     970 / 1000At timestep:     995 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -67.23999786376953
Train_BestReturn : -67.23999786376953
TimeSinceStart : 187.26903700828552
Exploitation Critic Loss : 1.5769120454788208
Exploration Critic Loss : 11106509.0
Exploration Model Loss : 23.254074096679688
Eval_AverageReturn : -38.19230651855469
Eval_StdReturn : 12.908015251159668
Eval_MaxReturn : -16.0
Eval_MinReturn : -72.0
Eval_AverageEpLen : 39.19230769230769
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -51.049999
best mean reward -51.049999
running time 202.196724
At timestep:     59 / 1000At timestep:     80 / 1000At timestep:     101 / 1000At timestep:     128 / 1000At timestep:     150 / 1000At timestep:     178 / 1000At timestep:     200 / 1000At timestep:     224 / 1000At timestep:     250 / 1000At timestep:     268 / 1000At timestep:     288 / 1000At timestep:     317 / 1000At timestep:     340 / 1000At timestep:     358 / 1000At timestep:     383 / 1000At timestep:     407 / 1000At timestep:     434 / 1000At timestep:     467 / 1000At timestep:     485 / 1000At timestep:     529 / 1000At timestep:     560 / 1000At timestep:     587 / 1000At timestep:     614 / 1000At timestep:     632 / 1000At timestep:     651 / 1000At timestep:     671 / 1000At timestep:     694 / 1000At timestep:     716 / 1000At timestep:     743 / 1000At timestep:     768 / 1000At timestep:     788 / 1000At timestep:     810 / 1000At timestep:     832 / 1000At timestep:     854 / 1000At timestep:     874 / 1000At timestep:     895 / 1000At timestep:     914 / 1000At timestep:     937 / 1000At timestep:     953 / 1000At timestep:     974 / 1000At timestep:     991 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -51.04999923706055
Train_BestReturn : -51.04999923706055
TimeSinceStart : 202.19672417640686
Exploitation Critic Loss : 0.24576103687286377
Exploration Critic Loss : 10961391.0
Exploration Model Loss : 23.254776000976562
Eval_AverageReturn : -23.5
Eval_StdReturn : 7.663301467895508
Eval_MaxReturn : -15.0
Eval_MinReturn : -58.0
Eval_AverageEpLen : 24.5
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -42.660000
best mean reward -42.660000
running time 218.953215
At timestep:     26 / 1000At timestep:     176 / 1000At timestep:     326 / 1000At timestep:     476 / 1000At timestep:     537 / 1000At timestep:     644 / 1000At timestep:     794 / 1000At timestep:     944 / 1000At timestep:     968 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -42.65999984741211
Train_BestReturn : -42.65999984741211
TimeSinceStart : 218.95321536064148
Exploitation Critic Loss : 0.2632955312728882
Exploration Critic Loss : 11798624.0
Exploration Model Loss : 23.255279541015625
Eval_AverageReturn : -100.30000305175781
Eval_StdReturn : 54.231082916259766
Eval_MaxReturn : -23.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 100.8
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -36.509998
best mean reward -36.509998
running time 232.926317
At timestep:     67 / 1000At timestep:     121 / 1000At timestep:     142 / 1000At timestep:     187 / 1000At timestep:     221 / 1000At timestep:     278 / 1000At timestep:     312 / 1000At timestep:     342 / 1000At timestep:     418 / 1000At timestep:     512 / 1000At timestep:     560 / 1000At timestep:     624 / 1000At timestep:     696 / 1000At timestep:     739 / 1000At timestep:     814 / 1000At timestep:     850 / 1000At timestep:     871 / 1000At timestep:     896 / 1000At timestep:     935 / 1000At timestep:     970 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -36.5099983215332
Train_BestReturn : -36.5099983215332
TimeSinceStart : 232.9263174533844
Exploitation Critic Loss : 0.262840211391449
Exploration Critic Loss : 10738178.0
Exploration Model Loss : 23.255813598632812
Eval_AverageReturn : -47.80952453613281
Eval_StdReturn : 19.645959854125977
Eval_MaxReturn : -20.0
Eval_MinReturn : -93.0
Eval_AverageEpLen : 48.80952380952381
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -33.970001
best mean reward -33.970001
running time 248.369473
At timestep:     27 / 1000At timestep:     47 / 1000At timestep:     70 / 1000At timestep:     96 / 1000At timestep:     115 / 1000At timestep:     133 / 1000At timestep:     157 / 1000At timestep:     184 / 1000At timestep:     205 / 1000At timestep:     220 / 1000At timestep:     258 / 1000At timestep:     278 / 1000At timestep:     300 / 1000At timestep:     326 / 1000At timestep:     352 / 1000At timestep:     373 / 1000At timestep:     391 / 1000At timestep:     413 / 1000At timestep:     438 / 1000At timestep:     467 / 1000At timestep:     485 / 1000At timestep:     509 / 1000At timestep:     534 / 1000At timestep:     560 / 1000At timestep:     589 / 1000At timestep:     611 / 1000At timestep:     638 / 1000At timestep:     657 / 1000At timestep:     681 / 1000At timestep:     700 / 1000At timestep:     721 / 1000At timestep:     745 / 1000At timestep:     764 / 1000At timestep:     783 / 1000At timestep:     832 / 1000At timestep:     854 / 1000At timestep:     881 / 1000At timestep:     901 / 1000At timestep:     928 / 1000At timestep:     953 / 1000At timestep:     973 / 1000At timestep:     996 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -33.970001220703125
Train_BestReturn : -33.970001220703125
TimeSinceStart : 248.36947321891785
Exploitation Critic Loss : 0.5490274429321289
Exploration Critic Loss : 10669833.0
Exploration Model Loss : 23.256362915039062
Eval_AverageReturn : -22.697673797607422
Eval_StdReturn : 5.61366605758667
Eval_MaxReturn : -14.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 23.697674418604652
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -30.620001
best mean reward -30.620001
running time 265.749722
At timestep:     24 / 1000At timestep:     49 / 1000At timestep:     74 / 1000At timestep:     89 / 1000At timestep:     112 / 1000At timestep:     133 / 1000At timestep:     160 / 1000At timestep:     178 / 1000At timestep:     204 / 1000At timestep:     226 / 1000At timestep:     254 / 1000At timestep:     277 / 1000At timestep:     296 / 1000At timestep:     321 / 1000At timestep:     345 / 1000At timestep:     364 / 1000At timestep:     389 / 1000At timestep:     409 / 1000At timestep:     434 / 1000At timestep:     457 / 1000At timestep:     486 / 1000At timestep:     509 / 1000At timestep:     532 / 1000At timestep:     558 / 1000At timestep:     581 / 1000At timestep:     602 / 1000At timestep:     628 / 1000At timestep:     643 / 1000At timestep:     663 / 1000At timestep:     687 / 1000At timestep:     713 / 1000At timestep:     728 / 1000At timestep:     748 / 1000At timestep:     773 / 1000At timestep:     791 / 1000At timestep:     823 / 1000At timestep:     842 / 1000At timestep:     863 / 1000At timestep:     887 / 1000At timestep:     910 / 1000At timestep:     938 / 1000At timestep:     964 / 1000At timestep:     986 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -30.6200008392334
Train_BestReturn : -30.6200008392334
TimeSinceStart : 265.74972224235535
Exploitation Critic Loss : 0.7705255746841431
Exploration Critic Loss : 11766839.0
Exploration Model Loss : 23.25689697265625
Eval_AverageReturn : -21.954545974731445
Eval_StdReturn : 3.655348062515259
Eval_MaxReturn : -14.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 22.954545454545453
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -29.209999
best mean reward -29.209999
running time 283.280128
At timestep:     29 / 1000At timestep:     55 / 1000At timestep:     81 / 1000At timestep:     102 / 1000At timestep:     127 / 1000At timestep:     145 / 1000At timestep:     161 / 1000At timestep:     187 / 1000At timestep:     209 / 1000At timestep:     231 / 1000At timestep:     249 / 1000At timestep:     288 / 1000At timestep:     311 / 1000At timestep:     334 / 1000At timestep:     365 / 1000At timestep:     395 / 1000At timestep:     417 / 1000At timestep:     446 / 1000At timestep:     476 / 1000At timestep:     497 / 1000At timestep:     516 / 1000At timestep:     535 / 1000At timestep:     565 / 1000At timestep:     585 / 1000At timestep:     607 / 1000At timestep:     636 / 1000At timestep:     658 / 1000At timestep:     679 / 1000At timestep:     698 / 1000At timestep:     724 / 1000At timestep:     749 / 1000At timestep:     771 / 1000At timestep:     794 / 1000At timestep:     815 / 1000At timestep:     839 / 1000At timestep:     861 / 1000At timestep:     886 / 1000At timestep:     917 / 1000At timestep:     942 / 1000At timestep:     961 / 1000At timestep:     993 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -29.209999084472656
Train_BestReturn : -29.209999084472656
TimeSinceStart : 283.2801275253296
Exploitation Critic Loss : 0.23828908801078796
Exploration Critic Loss : 8494923.0
Exploration Model Loss : 23.257431030273438
Eval_AverageReturn : -23.119047164916992
Eval_StdReturn : 4.691442966461182
Eval_MaxReturn : -15.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 24.11904761904762
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -28.080000
best mean reward -28.080000
running time 300.576313
At timestep:     22 / 1000At timestep:     43 / 1000At timestep:     63 / 1000At timestep:     86 / 1000At timestep:     115 / 1000At timestep:     135 / 1000At timestep:     158 / 1000At timestep:     181 / 1000At timestep:     202 / 1000At timestep:     223 / 1000At timestep:     242 / 1000At timestep:     268 / 1000At timestep:     293 / 1000At timestep:     319 / 1000At timestep:     348 / 1000At timestep:     378 / 1000At timestep:     397 / 1000At timestep:     421 / 1000At timestep:     443 / 1000At timestep:     469 / 1000At timestep:     485 / 1000At timestep:     504 / 1000At timestep:     523 / 1000At timestep:     544 / 1000At timestep:     565 / 1000At timestep:     585 / 1000At timestep:     605 / 1000At timestep:     627 / 1000At timestep:     647 / 1000At timestep:     671 / 1000At timestep:     696 / 1000At timestep:     717 / 1000At timestep:     744 / 1000At timestep:     768 / 1000At timestep:     788 / 1000At timestep:     806 / 1000At timestep:     830 / 1000At timestep:     853 / 1000At timestep:     878 / 1000At timestep:     900 / 1000At timestep:     919 / 1000At timestep:     944 / 1000At timestep:     964 / 1000At timestep:     987 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -28.079999923706055
Train_BestReturn : -28.079999923706055
TimeSinceStart : 300.57631278038025
Exploitation Critic Loss : 0.22070401906967163
Exploration Critic Loss : 11632617.0
Exploration Model Loss : 23.257980346679688
Eval_AverageReturn : -21.46666717529297
Eval_StdReturn : 3.0448501110076904
Eval_MaxReturn : -15.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 22.466666666666665
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -27.590000
best mean reward -27.590000
running time 318.327480
At timestep:     21 / 1000At timestep:     40 / 1000At timestep:     64 / 1000At timestep:     84 / 1000At timestep:     103 / 1000At timestep:     124 / 1000At timestep:     146 / 1000At timestep:     167 / 1000At timestep:     190 / 1000At timestep:     219 / 1000At timestep:     248 / 1000At timestep:     279 / 1000At timestep:     301 / 1000At timestep:     321 / 1000At timestep:     338 / 1000At timestep:     365 / 1000At timestep:     391 / 1000At timestep:     412 / 1000At timestep:     441 / 1000At timestep:     461 / 1000At timestep:     482 / 1000At timestep:     513 / 1000At timestep:     535 / 1000At timestep:     554 / 1000At timestep:     574 / 1000At timestep:     594 / 1000At timestep:     612 / 1000At timestep:     638 / 1000At timestep:     661 / 1000At timestep:     678 / 1000At timestep:     704 / 1000At timestep:     727 / 1000At timestep:     742 / 1000At timestep:     763 / 1000At timestep:     785 / 1000At timestep:     808 / 1000At timestep:     830 / 1000At timestep:     847 / 1000At timestep:     872 / 1000At timestep:     891 / 1000At timestep:     915 / 1000At timestep:     933 / 1000At timestep:     953 / 1000At timestep:     975 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -27.59000015258789
Train_BestReturn : -27.59000015258789
TimeSinceStart : 318.3274803161621
Exploitation Critic Loss : 0.1928691267967224
Exploration Critic Loss : 10535684.0
Exploration Model Loss : 23.258590698242188
Eval_AverageReturn : -21.244443893432617
Eval_StdReturn : 3.7662570476531982
Eval_MaxReturn : -14.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 22.244444444444444
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -27.629999
best mean reward -27.590000
running time 336.093928
At timestep:     21 / 1000At timestep:     50 / 1000At timestep:     69 / 1000At timestep:     89 / 1000At timestep:     112 / 1000At timestep:     140 / 1000At timestep:     161 / 1000At timestep:     182 / 1000At timestep:     207 / 1000At timestep:     234 / 1000At timestep:     255 / 1000At timestep:     280 / 1000At timestep:     305 / 1000At timestep:     321 / 1000At timestep:     342 / 1000At timestep:     367 / 1000At timestep:     393 / 1000At timestep:     414 / 1000At timestep:     433 / 1000At timestep:     456 / 1000At timestep:     479 / 1000At timestep:     500 / 1000At timestep:     526 / 1000At timestep:     551 / 1000At timestep:     572 / 1000At timestep:     598 / 1000At timestep:     626 / 1000At timestep:     654 / 1000At timestep:     677 / 1000At timestep:     695 / 1000At timestep:     721 / 1000At timestep:     738 / 1000At timestep:     760 / 1000At timestep:     789 / 1000At timestep:     814 / 1000At timestep:     830 / 1000At timestep:     857 / 1000At timestep:     880 / 1000At timestep:     904 / 1000At timestep:     927 / 1000At timestep:     945 / 1000At timestep:     968 / 1000At timestep:     988 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -27.6299991607666
Train_BestReturn : -27.59000015258789
TimeSinceStart : 336.09392762184143
Exploitation Critic Loss : 0.33102115988731384
Exploration Critic Loss : 7850984.0
Exploration Model Loss : 23.258804321289062
Eval_AverageReturn : -22.045454025268555
Eval_StdReturn : 3.4242091178894043
Eval_MaxReturn : -15.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 23.045454545454547
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -27.900000
best mean reward -27.590000
running time 353.959420
At timestep:     20 / 1000At timestep:     40 / 1000At timestep:     66 / 1000At timestep:     90 / 1000At timestep:     112 / 1000At timestep:     133 / 1000At timestep:     159 / 1000At timestep:     190 / 1000At timestep:     210 / 1000At timestep:     242 / 1000At timestep:     269 / 1000At timestep:     293 / 1000At timestep:     316 / 1000At timestep:     333 / 1000At timestep:     355 / 1000At timestep:     380 / 1000At timestep:     402 / 1000At timestep:     423 / 1000At timestep:     453 / 1000At timestep:     477 / 1000At timestep:     499 / 1000At timestep:     532 / 1000At timestep:     546 / 1000At timestep:     576 / 1000At timestep:     606 / 1000At timestep:     629 / 1000At timestep:     653 / 1000At timestep:     684 / 1000At timestep:     702 / 1000At timestep:     739 / 1000At timestep:     775 / 1000At timestep:     794 / 1000At timestep:     818 / 1000At timestep:     838 / 1000At timestep:     863 / 1000At timestep:     886 / 1000At timestep:     912 / 1000At timestep:     940 / 1000At timestep:     968 / 1000At timestep:     985 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -27.899999618530273
Train_BestReturn : -27.59000015258789
TimeSinceStart : 353.959419965744
Exploitation Critic Loss : 0.3929685354232788
Exploration Critic Loss : 9964398.0
Exploration Model Loss : 23.259384155273438
Eval_AverageReturn : -23.682926177978516
Eval_StdReturn : 5.105898380279541
Eval_MaxReturn : -13.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 24.682926829268293
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -27.730000
best mean reward -27.590000
running time 371.556730
At timestep:     22 / 1000At timestep:     47 / 1000At timestep:     72 / 1000At timestep:     95 / 1000At timestep:     112 / 1000At timestep:     134 / 1000At timestep:     162 / 1000At timestep:     191 / 1000At timestep:     209 / 1000At timestep:     232 / 1000At timestep:     250 / 1000At timestep:     272 / 1000At timestep:     299 / 1000At timestep:     320 / 1000At timestep:     343 / 1000At timestep:     368 / 1000At timestep:     389 / 1000At timestep:     417 / 1000At timestep:     437 / 1000At timestep:     456 / 1000At timestep:     478 / 1000At timestep:     501 / 1000At timestep:     519 / 1000At timestep:     541 / 1000At timestep:     561 / 1000At timestep:     581 / 1000At timestep:     605 / 1000At timestep:     626 / 1000At timestep:     645 / 1000At timestep:     670 / 1000At timestep:     691 / 1000At timestep:     717 / 1000At timestep:     735 / 1000At timestep:     762 / 1000At timestep:     784 / 1000At timestep:     802 / 1000At timestep:     828 / 1000At timestep:     854 / 1000At timestep:     876 / 1000At timestep:     896 / 1000At timestep:     920 / 1000At timestep:     942 / 1000At timestep:     960 / 1000At timestep:     981 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -27.729999542236328
Train_BestReturn : -27.59000015258789
TimeSinceStart : 371.55672955513
Exploitation Critic Loss : 0.16331708431243896
Exploration Critic Loss : 8812796.0
Exploration Model Loss : 23.259811401367188
Eval_AverageReturn : -21.244443893432617
Eval_StdReturn : 3.085129737854004
Eval_MaxReturn : -16.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 22.244444444444444
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -27.490000
best mean reward -27.490000
running time 389.468632
At timestep:     20 / 1000At timestep:     40 / 1000At timestep:     57 / 1000At timestep:     81 / 1000At timestep:     105 / 1000At timestep:     129 / 1000At timestep:     151 / 1000At timestep:     174 / 1000At timestep:     197 / 1000At timestep:     220 / 1000At timestep:     246 / 1000At timestep:     267 / 1000At timestep:     292 / 1000At timestep:     316 / 1000At timestep:     338 / 1000At timestep:     362 / 1000At timestep:     381 / 1000At timestep:     402 / 1000At timestep:     422 / 1000At timestep:     448 / 1000At timestep:     467 / 1000At timestep:     491 / 1000At timestep:     510 / 1000At timestep:     534 / 1000At timestep:     567 / 1000At timestep:     587 / 1000At timestep:     601 / 1000At timestep:     621 / 1000At timestep:     643 / 1000At timestep:     663 / 1000At timestep:     685 / 1000At timestep:     705 / 1000At timestep:     726 / 1000At timestep:     745 / 1000At timestep:     770 / 1000At timestep:     795 / 1000At timestep:     817 / 1000At timestep:     848 / 1000At timestep:     872 / 1000At timestep:     897 / 1000At timestep:     922 / 1000At timestep:     941 / 1000At timestep:     961 / 1000At timestep:     982 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -27.489999771118164
Train_BestReturn : -27.489999771118164
TimeSinceStart : 389.4686322212219
Exploitation Critic Loss : 0.6655660271644592
Exploration Critic Loss : 7696722.5
Exploration Model Loss : 23.260299682617188
Eval_AverageReturn : -21.399999618530273
Eval_StdReturn : 3.309246301651001
Eval_MaxReturn : -13.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 22.4
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -27.240000
best mean reward -27.240000
running time 407.277636
At timestep:     18 / 1000At timestep:     32 / 1000At timestep:     58 / 1000At timestep:     84 / 1000At timestep:     107 / 1000At timestep:     128 / 1000At timestep:     154 / 1000At timestep:     176 / 1000At timestep:     198 / 1000At timestep:     220 / 1000At timestep:     242 / 1000At timestep:     270 / 1000At timestep:     295 / 1000At timestep:     315 / 1000At timestep:     337 / 1000At timestep:     357 / 1000At timestep:     383 / 1000At timestep:     401 / 1000At timestep:     423 / 1000At timestep:     440 / 1000At timestep:     470 / 1000At timestep:     493 / 1000At timestep:     511 / 1000At timestep:     533 / 1000At timestep:     552 / 1000At timestep:     573 / 1000At timestep:     594 / 1000At timestep:     615 / 1000At timestep:     636 / 1000At timestep:     667 / 1000At timestep:     684 / 1000At timestep:     702 / 1000At timestep:     727 / 1000At timestep:     755 / 1000At timestep:     779 / 1000At timestep:     803 / 1000At timestep:     828 / 1000At timestep:     851 / 1000At timestep:     882 / 1000At timestep:     907 / 1000At timestep:     929 / 1000At timestep:     952 / 1000At timestep:     973 / 1000At timestep:     994 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -27.239999771118164
Train_BestReturn : -27.239999771118164
TimeSinceStart : 407.277636051178
Exploitation Critic Loss : 0.4234466254711151
Exploration Critic Loss : 10101950.0
Exploration Model Loss : 23.260848999023438
Eval_AverageReturn : -21.66666603088379
Eval_StdReturn : 3.675746440887451
Eval_MaxReturn : -13.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 22.666666666666668
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -27.809999
best mean reward -27.240000
running time 425.179136
At timestep:     20 / 1000At timestep:     41 / 1000At timestep:     66 / 1000At timestep:     91 / 1000At timestep:     109 / 1000At timestep:     129 / 1000At timestep:     148 / 1000At timestep:     167 / 1000At timestep:     190 / 1000At timestep:     216 / 1000At timestep:     237 / 1000At timestep:     259 / 1000At timestep:     281 / 1000At timestep:     301 / 1000At timestep:     331 / 1000At timestep:     351 / 1000At timestep:     368 / 1000At timestep:     390 / 1000At timestep:     409 / 1000At timestep:     432 / 1000At timestep:     453 / 1000At timestep:     475 / 1000At timestep:     493 / 1000At timestep:     523 / 1000At timestep:     552 / 1000At timestep:     574 / 1000At timestep:     596 / 1000At timestep:     616 / 1000At timestep:     638 / 1000At timestep:     663 / 1000At timestep:     684 / 1000At timestep:     706 / 1000At timestep:     724 / 1000At timestep:     748 / 1000At timestep:     772 / 1000At timestep:     791 / 1000At timestep:     816 / 1000At timestep:     838 / 1000At timestep:     858 / 1000At timestep:     882 / 1000At timestep:     903 / 1000At timestep:     921 / 1000At timestep:     945 / 1000At timestep:     981 / 1000At timestep:     996 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -27.809999465942383
Train_BestReturn : -27.239999771118164
TimeSinceStart : 425.17913603782654
Exploitation Critic Loss : 0.19777798652648926
Exploration Critic Loss : 8171222.0
Exploration Model Loss : 23.26141357421875
Eval_AverageReturn : -21.15217399597168
Eval_StdReturn : 3.7472736835479736
Eval_MaxReturn : -14.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 22.152173913043477
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -27.700001
best mean reward -27.240000
running time 443.261675
At timestep:     19 / 1000At timestep:     48 / 1000At timestep:     69 / 1000At timestep:     93 / 1000At timestep:     115 / 1000At timestep:     141 / 1000At timestep:     161 / 1000At timestep:     185 / 1000At timestep:     204 / 1000At timestep:     234 / 1000At timestep:     255 / 1000At timestep:     277 / 1000At timestep:     295 / 1000At timestep:     313 / 1000At timestep:     337 / 1000At timestep:     366 / 1000At timestep:     390 / 1000At timestep:     415 / 1000At timestep:     436 / 1000At timestep:     455 / 1000At timestep:     473 / 1000At timestep:     500 / 1000At timestep:     522 / 1000At timestep:     544 / 1000At timestep:     565 / 1000At timestep:     586 / 1000At timestep:     605 / 1000At timestep:     630 / 1000At timestep:     653 / 1000At timestep:     671 / 1000At timestep:     691 / 1000At timestep:     715 / 1000At timestep:     739 / 1000At timestep:     761 / 1000At timestep:     781 / 1000At timestep:     807 / 1000At timestep:     832 / 1000At timestep:     856 / 1000At timestep:     886 / 1000At timestep:     910 / 1000At timestep:     928 / 1000At timestep:     954 / 1000At timestep:     970 / 1000At timestep:     994 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -27.700000762939453
Train_BestReturn : -27.239999771118164
TimeSinceStart : 443.26167488098145
Exploitation Critic Loss : 0.17134660482406616
Exploration Critic Loss : 9189401.0
Exploration Model Loss : 23.261947631835938
Eval_AverageReturn : -21.577777862548828
Eval_StdReturn : 3.389453411102295
Eval_MaxReturn : -15.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 22.57777777777778
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -27.780001
best mean reward -27.240000
running time 461.015157
At timestep:     15 / 1000At timestep:     47 / 1000At timestep:     71 / 1000At timestep:     91 / 1000At timestep:     115 / 1000At timestep:     147 / 1000At timestep:     175 / 1000At timestep:     201 / 1000At timestep:     221 / 1000At timestep:     250 / 1000At timestep:     273 / 1000At timestep:     297 / 1000At timestep:     318 / 1000At timestep:     338 / 1000At timestep:     363 / 1000At timestep:     380 / 1000At timestep:     403 / 1000At timestep:     423 / 1000At timestep:     449 / 1000At timestep:     472 / 1000At timestep:     497 / 1000At timestep:     517 / 1000At timestep:     539 / 1000At timestep:     562 / 1000At timestep:     581 / 1000At timestep:     599 / 1000At timestep:     621 / 1000At timestep:     640 / 1000At timestep:     662 / 1000At timestep:     686 / 1000At timestep:     709 / 1000At timestep:     733 / 1000At timestep:     756 / 1000At timestep:     778 / 1000At timestep:     800 / 1000At timestep:     818 / 1000At timestep:     841 / 1000At timestep:     862 / 1000At timestep:     885 / 1000At timestep:     908 / 1000At timestep:     924 / 1000At timestep:     950 / 1000At timestep:     972 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -27.780000686645508
Train_BestReturn : -27.239999771118164
TimeSinceStart : 461.0151574611664
Exploitation Critic Loss : 0.19482135772705078
Exploration Critic Loss : 10779287.0
Exploration Model Loss : 23.262496948242188
Eval_AverageReturn : -21.727272033691406
Eval_StdReturn : 3.6141364574432373
Eval_MaxReturn : -14.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 22.727272727272727
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -27.340000
best mean reward -27.240000
running time 478.506084
At timestep:     20 / 1000At timestep:     41 / 1000At timestep:     59 / 1000At timestep:     79 / 1000At timestep:     99 / 1000At timestep:     124 / 1000At timestep:     148 / 1000At timestep:     181 / 1000At timestep:     202 / 1000At timestep:     223 / 1000At timestep:     242 / 1000At timestep:     258 / 1000At timestep:     283 / 1000At timestep:     303 / 1000At timestep:     328 / 1000At timestep:     355 / 1000At timestep:     377 / 1000At timestep:     397 / 1000At timestep:     419 / 1000At timestep:     437 / 1000At timestep:     463 / 1000At timestep:     485 / 1000At timestep:     503 / 1000At timestep:     532 / 1000At timestep:     560 / 1000At timestep:     582 / 1000At timestep:     614 / 1000At timestep:     639 / 1000At timestep:     666 / 1000At timestep:     686 / 1000At timestep:     720 / 1000At timestep:     748 / 1000At timestep:     765 / 1000At timestep:     793 / 1000At timestep:     814 / 1000At timestep:     843 / 1000At timestep:     873 / 1000At timestep:     905 / 1000At timestep:     933 / 1000At timestep:     961 / 1000At timestep:     988 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -27.34000015258789
Train_BestReturn : -27.239999771118164
TimeSinceStart : 478.5060839653015
Exploitation Critic Loss : 0.22265759110450745
Exploration Critic Loss : 12104083.0
Exploration Model Loss : 23.262985229492188
Eval_AverageReturn : -23.214284896850586
Eval_StdReturn : 4.69820499420166
Eval_MaxReturn : -15.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 24.214285714285715
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -27.490000
best mean reward -27.240000
running time 495.408740
At timestep:     19 / 1000At timestep:     42 / 1000At timestep:     65 / 1000At timestep:     86 / 1000At timestep:     112 / 1000At timestep:     135 / 1000At timestep:     158 / 1000At timestep:     181 / 1000At timestep:     199 / 1000At timestep:     218 / 1000At timestep:     242 / 1000At timestep:     262 / 1000At timestep:     292 / 1000At timestep:     320 / 1000At timestep:     339 / 1000At timestep:     362 / 1000At timestep:     385 / 1000At timestep:     407 / 1000At timestep:     439 / 1000At timestep:     457 / 1000At timestep:     478 / 1000At timestep:     502 / 1000At timestep:     523 / 1000At timestep:     543 / 1000At timestep:     565 / 1000At timestep:     582 / 1000At timestep:     601 / 1000At timestep:     624 / 1000At timestep:     650 / 1000At timestep:     671 / 1000At timestep:     695 / 1000At timestep:     725 / 1000At timestep:     747 / 1000At timestep:     772 / 1000At timestep:     796 / 1000At timestep:     815 / 1000At timestep:     833 / 1000At timestep:     856 / 1000At timestep:     883 / 1000At timestep:     900 / 1000At timestep:     929 / 1000At timestep:     951 / 1000At timestep:     968 / 1000At timestep:     995 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -27.489999771118164
Train_BestReturn : -27.239999771118164
TimeSinceStart : 495.408739566803
Exploitation Critic Loss : 0.42641371488571167
Exploration Critic Loss : 9674314.0
Exploration Model Loss : 23.263504028320312
Eval_AverageReturn : -21.488889694213867
Eval_StdReturn : 3.7215421199798584
Eval_MaxReturn : -16.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 22.488888888888887
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -27.350000
best mean reward -27.240000
running time 512.822065
At timestep:     22 / 1000At timestep:     46 / 1000At timestep:     64 / 1000At timestep:     92 / 1000At timestep:     118 / 1000At timestep:     139 / 1000At timestep:     161 / 1000At timestep:     180 / 1000At timestep:     202 / 1000At timestep:     224 / 1000At timestep:     247 / 1000At timestep:     279 / 1000At timestep:     304 / 1000At timestep:     334 / 1000At timestep:     361 / 1000At timestep:     382 / 1000At timestep:     404 / 1000At timestep:     439 / 1000At timestep:     456 / 1000At timestep:     481 / 1000At timestep:     500 / 1000At timestep:     514 / 1000At timestep:     537 / 1000At timestep:     555 / 1000At timestep:     573 / 1000At timestep:     596 / 1000At timestep:     616 / 1000At timestep:     641 / 1000At timestep:     669 / 1000At timestep:     691 / 1000At timestep:     712 / 1000At timestep:     746 / 1000At timestep:     777 / 1000At timestep:     800 / 1000At timestep:     823 / 1000At timestep:     854 / 1000At timestep:     877 / 1000At timestep:     904 / 1000At timestep:     930 / 1000At timestep:     963 / 1000At timestep:     987 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -27.350000381469727
Train_BestReturn : -27.239999771118164
TimeSinceStart : 512.8220653533936
Exploitation Critic Loss : 0.1521826535463333
Exploration Critic Loss : 10306545.0
Exploration Model Loss : 23.2640380859375
Eval_AverageReturn : -23.119047164916992
Eval_StdReturn : 4.741922378540039
Eval_MaxReturn : -13.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 24.11904761904762
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -27.879999
best mean reward -27.240000
running time 530.465601
At timestep:     25 / 1000At timestep:     42 / 1000At timestep:     68 / 1000At timestep:     101 / 1000At timestep:     120 / 1000At timestep:     145 / 1000At timestep:     164 / 1000At timestep:     183 / 1000At timestep:     211 / 1000At timestep:     236 / 1000At timestep:     258 / 1000At timestep:     276 / 1000At timestep:     295 / 1000At timestep:     312 / 1000At timestep:     329 / 1000At timestep:     350 / 1000At timestep:     365 / 1000At timestep:     390 / 1000At timestep:     417 / 1000At timestep:     439 / 1000At timestep:     459 / 1000At timestep:     494 / 1000At timestep:     512 / 1000At timestep:     535 / 1000At timestep:     564 / 1000At timestep:     584 / 1000At timestep:     605 / 1000At timestep:     632 / 1000At timestep:     657 / 1000At timestep:     677 / 1000At timestep:     701 / 1000At timestep:     727 / 1000At timestep:     747 / 1000At timestep:     763 / 1000At timestep:     787 / 1000At timestep:     811 / 1000At timestep:     833 / 1000At timestep:     853 / 1000At timestep:     872 / 1000At timestep:     900 / 1000At timestep:     923 / 1000At timestep:     944 / 1000At timestep:     965 / 1000At timestep:     987 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -27.8799991607666
Train_BestReturn : -27.239999771118164
TimeSinceStart : 530.4656009674072
Exploitation Critic Loss : 0.15756607055664062
Exploration Critic Loss : 9538988.0
Exploration Model Loss : 23.264602661132812
Eval_AverageReturn : -21.422222137451172
Eval_StdReturn : 4.229467391967773
Eval_MaxReturn : -14.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 22.42222222222222
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -27.139999
best mean reward -27.139999
running time 548.780288
At timestep:     20 / 1000At timestep:     41 / 1000At timestep:     68 / 1000At timestep:     88 / 1000At timestep:     109 / 1000At timestep:     131 / 1000At timestep:     156 / 1000At timestep:     178 / 1000At timestep:     204 / 1000At timestep:     222 / 1000At timestep:     241 / 1000At timestep:     263 / 1000At timestep:     290 / 1000At timestep:     312 / 1000At timestep:     332 / 1000At timestep:     357 / 1000At timestep:     384 / 1000At timestep:     407 / 1000At timestep:     423 / 1000At timestep:     448 / 1000At timestep:     471 / 1000At timestep:     493 / 1000At timestep:     511 / 1000At timestep:     535 / 1000At timestep:     559 / 1000At timestep:     577 / 1000At timestep:     605 / 1000At timestep:     627 / 1000At timestep:     644 / 1000At timestep:     668 / 1000At timestep:     690 / 1000At timestep:     709 / 1000At timestep:     733 / 1000At timestep:     757 / 1000At timestep:     778 / 1000At timestep:     803 / 1000At timestep:     825 / 1000At timestep:     845 / 1000At timestep:     877 / 1000At timestep:     895 / 1000At timestep:     919 / 1000At timestep:     944 / 1000At timestep:     965 / 1000At timestep:     985 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -27.139999389648438
Train_BestReturn : -27.139999389648438
TimeSinceStart : 548.7802879810333
Exploitation Critic Loss : 0.2236730009317398
Exploration Critic Loss : 9628277.0
Exploration Model Loss : 23.265151977539062
Eval_AverageReturn : -21.377777099609375
Eval_StdReturn : 3.192274570465088
Eval_MaxReturn : -15.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 22.377777777777776
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -26.680000
best mean reward -26.680000
running time 567.018769
At timestep:     24 / 1000At timestep:     44 / 1000At timestep:     65 / 1000At timestep:     86 / 1000At timestep:     107 / 1000At timestep:     130 / 1000At timestep:     155 / 1000At timestep:     172 / 1000At timestep:     198 / 1000At timestep:     225 / 1000At timestep:     246 / 1000At timestep:     265 / 1000At timestep:     290 / 1000At timestep:     312 / 1000At timestep:     331 / 1000At timestep:     349 / 1000At timestep:     371 / 1000At timestep:     398 / 1000At timestep:     424 / 1000At timestep:     447 / 1000At timestep:     476 / 1000At timestep:     498 / 1000At timestep:     522 / 1000At timestep:     539 / 1000At timestep:     554 / 1000At timestep:     574 / 1000At timestep:     592 / 1000At timestep:     617 / 1000At timestep:     646 / 1000At timestep:     677 / 1000At timestep:     700 / 1000At timestep:     720 / 1000At timestep:     743 / 1000At timestep:     761 / 1000At timestep:     785 / 1000At timestep:     817 / 1000At timestep:     841 / 1000At timestep:     873 / 1000At timestep:     898 / 1000At timestep:     921 / 1000At timestep:     943 / 1000At timestep:     964 / 1000At timestep:     985 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -26.68000030517578
Train_BestReturn : -26.68000030517578
TimeSinceStart : 567.0187692642212
Exploitation Critic Loss : 0.5734550356864929
Exploration Critic Loss : 10780700.0
Exploration Model Loss : 23.265731811523438
Eval_AverageReturn : -21.840909957885742
Eval_StdReturn : 3.913515090942383
Eval_MaxReturn : -14.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 22.84090909090909
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -26.139999
best mean reward -26.139999
running time 584.949076
At timestep:     29 / 1000At timestep:     52 / 1000At timestep:     71 / 1000At timestep:     99 / 1000At timestep:     123 / 1000At timestep:     142 / 1000At timestep:     167 / 1000At timestep:     191 / 1000At timestep:     214 / 1000At timestep:     238 / 1000At timestep:     254 / 1000At timestep:     280 / 1000At timestep:     306 / 1000At timestep:     330 / 1000At timestep:     349 / 1000At timestep:     372 / 1000At timestep:     396 / 1000At timestep:     418 / 1000At timestep:     438 / 1000At timestep:     462 / 1000At timestep:     487 / 1000At timestep:     505 / 1000At timestep:     525 / 1000At timestep:     550 / 1000At timestep:     572 / 1000At timestep:     594 / 1000At timestep:     619 / 1000At timestep:     641 / 1000At timestep:     661 / 1000At timestep:     690 / 1000At timestep:     714 / 1000At timestep:     738 / 1000At timestep:     758 / 1000At timestep:     789 / 1000At timestep:     805 / 1000At timestep:     829 / 1000At timestep:     855 / 1000At timestep:     874 / 1000At timestep:     894 / 1000At timestep:     922 / 1000At timestep:     946 / 1000At timestep:     966 / 1000At timestep:     986 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -26.139999389648438
Train_BestReturn : -26.139999389648438
TimeSinceStart : 584.9490761756897
Exploitation Critic Loss : 0.18708068132400513
Exploration Critic Loss : 10799605.0
Exploration Model Loss : 23.266265869140625
Eval_AverageReturn : -21.840909957885742
Eval_StdReturn : 3.3974802494049072
Eval_MaxReturn : -15.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 22.84090909090909
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -26.910000
best mean reward -26.139999
running time 602.244675
At timestep:     25 / 1000At timestep:     45 / 1000At timestep:     72 / 1000At timestep:     92 / 1000At timestep:     112 / 1000At timestep:     133 / 1000At timestep:     156 / 1000At timestep:     179 / 1000At timestep:     202 / 1000At timestep:     226 / 1000At timestep:     244 / 1000At timestep:     267 / 1000At timestep:     296 / 1000At timestep:     321 / 1000At timestep:     343 / 1000At timestep:     364 / 1000At timestep:     386 / 1000At timestep:     411 / 1000At timestep:     433 / 1000At timestep:     453 / 1000At timestep:     478 / 1000At timestep:     496 / 1000At timestep:     518 / 1000At timestep:     536 / 1000At timestep:     561 / 1000At timestep:     586 / 1000At timestep:     614 / 1000At timestep:     638 / 1000At timestep:     661 / 1000At timestep:     689 / 1000At timestep:     720 / 1000At timestep:     743 / 1000At timestep:     762 / 1000At timestep:     781 / 1000At timestep:     813 / 1000At timestep:     836 / 1000At timestep:     857 / 1000At timestep:     873 / 1000At timestep:     895 / 1000At timestep:     918 / 1000At timestep:     932 / 1000At timestep:     954 / 1000At timestep:     976 / 1000At timestep:     995 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -26.90999984741211
Train_BestReturn : -26.139999389648438
TimeSinceStart : 602.2446749210358
Exploitation Critic Loss : 0.17168563604354858
Exploration Critic Loss : 10699617.0
Exploration Model Loss : 23.266769409179688
Eval_AverageReturn : -21.64444351196289
Eval_StdReturn : 3.578674554824829
Eval_MaxReturn : -13.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 22.644444444444446
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -27.040001
best mean reward -26.139999
running time 619.687040
At timestep:     21 / 1000At timestep:     42 / 1000At timestep:     61 / 1000At timestep:     88 / 1000At timestep:     112 / 1000At timestep:     132 / 1000At timestep:     156 / 1000At timestep:     179 / 1000At timestep:     200 / 1000At timestep:     222 / 1000At timestep:     243 / 1000At timestep:     261 / 1000At timestep:     290 / 1000At timestep:     323 / 1000At timestep:     348 / 1000At timestep:     377 / 1000At timestep:     395 / 1000At timestep:     415 / 1000At timestep:     438 / 1000At timestep:     459 / 1000At timestep:     480 / 1000At timestep:     501 / 1000At timestep:     521 / 1000At timestep:     543 / 1000At timestep:     569 / 1000At timestep:     590 / 1000At timestep:     615 / 1000At timestep:     632 / 1000At timestep:     654 / 1000At timestep:     676 / 1000At timestep:     705 / 1000At timestep:     726 / 1000At timestep:     745 / 1000At timestep:     773 / 1000At timestep:     793 / 1000At timestep:     811 / 1000At timestep:     830 / 1000At timestep:     851 / 1000At timestep:     867 / 1000At timestep:     890 / 1000At timestep:     921 / 1000At timestep:     951 / 1000At timestep:     973 / 1000At timestep:     990 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -27.040000915527344
Train_BestReturn : -26.139999389648438
TimeSinceStart : 619.6870396137238
Exploitation Critic Loss : 0.226759135723114
Exploration Critic Loss : 12320833.0
Exploration Model Loss : 23.267257690429688
Eval_AverageReturn : -21.46666717529297
Eval_StdReturn : 3.9361572265625
Eval_MaxReturn : -15.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 22.466666666666665
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -27.680000
best mean reward -26.139999
running time 637.164214
At timestep:     19 / 1000At timestep:     41 / 1000At timestep:     62 / 1000At timestep:     80 / 1000At timestep:     98 / 1000At timestep:     120 / 1000At timestep:     147 / 1000At timestep:     176 / 1000At timestep:     193 / 1000At timestep:     219 / 1000At timestep:     239 / 1000At timestep:     263 / 1000At timestep:     283 / 1000At timestep:     302 / 1000At timestep:     320 / 1000At timestep:     346 / 1000At timestep:     377 / 1000At timestep:     399 / 1000At timestep:     425 / 1000At timestep:     450 / 1000At timestep:     473 / 1000At timestep:     497 / 1000At timestep:     516 / 1000At timestep:     544 / 1000At timestep:     568 / 1000At timestep:     596 / 1000At timestep:     614 / 1000At timestep:     634 / 1000At timestep:     659 / 1000At timestep:     682 / 1000At timestep:     707 / 1000At timestep:     734 / 1000At timestep:     761 / 1000At timestep:     788 / 1000At timestep:     808 / 1000At timestep:     833 / 1000At timestep:     858 / 1000At timestep:     879 / 1000At timestep:     898 / 1000At timestep:     921 / 1000At timestep:     942 / 1000At timestep:     964 / 1000At timestep:     990 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -27.68000030517578
Train_BestReturn : -26.139999389648438
TimeSinceStart : 637.1642141342163
Exploitation Critic Loss : 0.3472411632537842
Exploration Critic Loss : 16052605.0
Exploration Model Loss : 23.267623901367188
Eval_AverageReturn : -21.954545974731445
Eval_StdReturn : 3.483431816101074
Eval_MaxReturn : -16.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 22.954545454545453
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -27.629999
best mean reward -26.139999
running time 654.447884
At timestep:     23 / 1000At timestep:     46 / 1000At timestep:     66 / 1000At timestep:     88 / 1000At timestep:     109 / 1000At timestep:     131 / 1000At timestep:     153 / 1000At timestep:     171 / 1000At timestep:     202 / 1000At timestep:     222 / 1000At timestep:     243 / 1000At timestep:     263 / 1000At timestep:     285 / 1000At timestep:     304 / 1000At timestep:     329 / 1000At timestep:     351 / 1000At timestep:     373 / 1000At timestep:     388 / 1000At timestep:     407 / 1000At timestep:     431 / 1000At timestep:     455 / 1000At timestep:     478 / 1000At timestep:     495 / 1000At timestep:     518 / 1000At timestep:     534 / 1000At timestep:     558 / 1000At timestep:     579 / 1000At timestep:     603 / 1000At timestep:     628 / 1000At timestep:     646 / 1000At timestep:     671 / 1000At timestep:     698 / 1000At timestep:     720 / 1000At timestep:     748 / 1000At timestep:     770 / 1000At timestep:     790 / 1000At timestep:     818 / 1000At timestep:     838 / 1000At timestep:     855 / 1000At timestep:     880 / 1000At timestep:     901 / 1000At timestep:     929 / 1000At timestep:     952 / 1000At timestep:     976 / 1000At timestep:     998 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -27.6299991607666
Train_BestReturn : -26.139999389648438
TimeSinceStart : 654.447883605957
Exploitation Critic Loss : 0.21023151278495789
Exploration Critic Loss : 11581152.0
Exploration Model Loss : 23.26806640625
Eval_AverageReturn : -21.19565200805664
Eval_StdReturn : 3.2411723136901855
Eval_MaxReturn : -14.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 22.195652173913043
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -27.900000
best mean reward -26.139999
running time 671.950549
At timestep:     25 / 1000At timestep:     48 / 1000At timestep:     70 / 1000At timestep:     96 / 1000At timestep:     117 / 1000At timestep:     144 / 1000At timestep:     170 / 1000At timestep:     186 / 1000At timestep:     205 / 1000At timestep:     225 / 1000At timestep:     249 / 1000At timestep:     275 / 1000At timestep:     299 / 1000At timestep:     318 / 1000At timestep:     338 / 1000At timestep:     357 / 1000At timestep:     389 / 1000At timestep:     409 / 1000At timestep:     424 / 1000At timestep:     439 / 1000At timestep:     457 / 1000At timestep:     476 / 1000At timestep:     500 / 1000At timestep:     518 / 1000At timestep:     541 / 1000At timestep:     572 / 1000At timestep:     590 / 1000At timestep:     614 / 1000At timestep:     637 / 1000At timestep:     659 / 1000At timestep:     677 / 1000At timestep:     698 / 1000At timestep:     719 / 1000At timestep:     742 / 1000At timestep:     766 / 1000At timestep:     793 / 1000At timestep:     813 / 1000At timestep:     836 / 1000At timestep:     853 / 1000At timestep:     871 / 1000At timestep:     893 / 1000At timestep:     913 / 1000At timestep:     935 / 1000At timestep:     960 / 1000At timestep:     981 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -27.899999618530273
Train_BestReturn : -26.139999389648438
TimeSinceStart : 671.9505488872528
Exploitation Critic Loss : 0.2142738550901413
Exploration Critic Loss : 16805504.0
Exploration Model Loss : 23.268646240234375
Eval_AverageReturn : -20.80434799194336
Eval_StdReturn : 3.6690218448638916
Eval_MaxReturn : -14.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 21.804347826086957
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -28.110001
best mean reward -26.139999
running time 689.437120
At timestep:     30 / 1000At timestep:     42 / 1000At timestep:     64 / 1000At timestep:     91 / 1000At timestep:     117 / 1000At timestep:     142 / 1000At timestep:     165 / 1000At timestep:     186 / 1000At timestep:     211 / 1000At timestep:     233 / 1000At timestep:     253 / 1000At timestep:     272 / 1000At timestep:     298 / 1000At timestep:     317 / 1000At timestep:     341 / 1000At timestep:     366 / 1000At timestep:     386 / 1000At timestep:     409 / 1000At timestep:     436 / 1000At timestep:     457 / 1000At timestep:     476 / 1000At timestep:     494 / 1000At timestep:     513 / 1000At timestep:     542 / 1000At timestep:     566 / 1000At timestep:     587 / 1000At timestep:     610 / 1000At timestep:     634 / 1000At timestep:     659 / 1000At timestep:     681 / 1000At timestep:     699 / 1000At timestep:     718 / 1000At timestep:     746 / 1000At timestep:     767 / 1000At timestep:     789 / 1000At timestep:     812 / 1000At timestep:     838 / 1000At timestep:     859 / 1000At timestep:     886 / 1000At timestep:     913 / 1000At timestep:     937 / 1000At timestep:     958 / 1000At timestep:     977 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -28.110000610351562
Train_BestReturn : -26.139999389648438
TimeSinceStart : 689.4371199607849
Exploitation Critic Loss : 0.254355788230896
Exploration Critic Loss : 16178835.0
Exploration Model Loss : 23.269210815429688
Eval_AverageReturn : -21.727272033691406
Eval_StdReturn : 3.473036527633667
Eval_MaxReturn : -11.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 22.727272727272727
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -28.330000
best mean reward -26.139999
running time 706.921710
At timestep:     26 / 1000At timestep:     43 / 1000At timestep:     63 / 1000At timestep:     86 / 1000At timestep:     108 / 1000At timestep:     132 / 1000At timestep:     159 / 1000At timestep:     185 / 1000At timestep:     215 / 1000At timestep:     249 / 1000At timestep:     271 / 1000At timestep:     294 / 1000At timestep:     317 / 1000At timestep:     344 / 1000At timestep:     369 / 1000At timestep:     400 / 1000At timestep:     424 / 1000At timestep:     443 / 1000At timestep:     468 / 1000At timestep:     495 / 1000At timestep:     516 / 1000At timestep:     539 / 1000At timestep:     561 / 1000At timestep:     584 / 1000At timestep:     602 / 1000At timestep:     626 / 1000At timestep:     649 / 1000At timestep:     672 / 1000At timestep:     689 / 1000At timestep:     707 / 1000At timestep:     737 / 1000At timestep:     762 / 1000At timestep:     789 / 1000At timestep:     810 / 1000At timestep:     831 / 1000At timestep:     861 / 1000At timestep:     880 / 1000At timestep:     902 / 1000At timestep:     921 / 1000At timestep:     941 / 1000At timestep:     963 / 1000At timestep:     986 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -28.329999923706055
Train_BestReturn : -26.139999389648438
TimeSinceStart : 706.9217102527618
Exploitation Critic Loss : 0.18405374884605408
Exploration Critic Loss : 15873173.0
Exploration Model Loss : 23.269760131835938
Eval_AverageReturn : -22.465116500854492
Eval_StdReturn : 3.811541795730591
Eval_MaxReturn : -16.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 23.46511627906977
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -27.450001
best mean reward -26.139999
running time 724.787591
At timestep:     28 / 1000At timestep:     51 / 1000At timestep:     74 / 1000At timestep:     98 / 1000At timestep:     118 / 1000At timestep:     141 / 1000At timestep:     165 / 1000At timestep:     185 / 1000At timestep:     206 / 1000At timestep:     230 / 1000At timestep:     252 / 1000At timestep:     272 / 1000At timestep:     301 / 1000At timestep:     325 / 1000At timestep:     346 / 1000At timestep:     366 / 1000At timestep:     396 / 1000At timestep:     419 / 1000At timestep:     445 / 1000At timestep:     466 / 1000At timestep:     487 / 1000At timestep:     513 / 1000At timestep:     530 / 1000At timestep:     560 / 1000At timestep:     586 / 1000At timestep:     608 / 1000At timestep:     627 / 1000At timestep:     650 / 1000At timestep:     672 / 1000At timestep:     698 / 1000At timestep:     723 / 1000At timestep:     748 / 1000At timestep:     766 / 1000At timestep:     781 / 1000At timestep:     807 / 1000At timestep:     826 / 1000At timestep:     850 / 1000At timestep:     879 / 1000At timestep:     911 / 1000At timestep:     940 / 1000At timestep:     967 / 1000At timestep:     999 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -27.450000762939453
Train_BestReturn : -26.139999389648438
TimeSinceStart : 724.7875909805298
Exploitation Critic Loss : 0.16795951128005981
Exploration Critic Loss : 15633002.0
Exploration Model Loss : 23.270294189453125
Eval_AverageReturn : -22.76744270324707
Eval_StdReturn : 3.898939609527588
Eval_MaxReturn : -14.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 23.767441860465116
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -27.150000
best mean reward -26.139999
running time 742.494170
At timestep:     21 / 1000At timestep:     45 / 1000At timestep:     67 / 1000At timestep:     95 / 1000At timestep:     120 / 1000At timestep:     137 / 1000At timestep:     167 / 1000At timestep:     189 / 1000At timestep:     220 / 1000At timestep:     234 / 1000At timestep:     260 / 1000At timestep:     285 / 1000At timestep:     310 / 1000At timestep:     337 / 1000At timestep:     357 / 1000At timestep:     375 / 1000At timestep:     400 / 1000At timestep:     422 / 1000At timestep:     444 / 1000At timestep:     461 / 1000At timestep:     481 / 1000At timestep:     508 / 1000At timestep:     533 / 1000At timestep:     555 / 1000At timestep:     580 / 1000At timestep:     607 / 1000At timestep:     624 / 1000At timestep:     646 / 1000At timestep:     667 / 1000At timestep:     686 / 1000At timestep:     707 / 1000At timestep:     730 / 1000At timestep:     747 / 1000At timestep:     768 / 1000At timestep:     790 / 1000At timestep:     812 / 1000At timestep:     834 / 1000At timestep:     854 / 1000At timestep:     876 / 1000At timestep:     899 / 1000At timestep:     920 / 1000At timestep:     946 / 1000At timestep:     971 / 1000At timestep:     992 / 1000At timestep:     1017 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q1_env2_rnd_PointmassMedium-v0_21-11-2022_15-57-41/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -27.149999618530273
Train_BestReturn : -26.139999389648438
TimeSinceStart : 742.4941704273224
Exploitation Critic Loss : 1.1367801427841187
Exploration Critic Loss : 19287564.0
Exploration Model Loss : 23.270858764648438
Eval_AverageReturn : -21.600000381469727
Eval_StdReturn : 3.5175747871398926
Eval_MaxReturn : -13.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 22.6
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env2_random_PointmassMedium-v0_21-11-2022_16-10-24 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env2_random_PointmassMedium-v0_21-11-2022_16-10-24
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002180
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0021800994873046875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 5.440987
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 5.44098687171936
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 10.874119
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -150.0
TimeSinceStart : 10.874119281768799
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -147.800003
best mean reward -inf
running time 16.230621
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -147.8000030517578
TimeSinceStart : 16.230620861053467
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -148.269226
best mean reward -inf
running time 21.375948
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -148.26922607421875
TimeSinceStart : 21.375948190689087
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -147.272720
best mean reward -inf
running time 26.828242
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -147.27272033691406
TimeSinceStart : 26.82824206352234
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -146.146347
best mean reward -inf
running time 32.320550
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -146.14634704589844
TimeSinceStart : 32.320549726486206
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -145.375000
best mean reward -inf
running time 37.710475
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -145.375
TimeSinceStart : 37.710474729537964
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -145.888885
best mean reward -inf
running time 42.927908
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -145.88888549804688
TimeSinceStart : 42.927908182144165
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -145.803284
best mean reward -inf
running time 48.255478
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -145.80328369140625
TimeSinceStart : 48.25547814369202
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -145.529419
best mean reward -inf
running time 53.522434
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -145.5294189453125
TimeSinceStart : 53.52243375778198
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -145.946671
best mean reward -inf
running time 65.170242
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -145.94667053222656
TimeSinceStart : 65.17024230957031
Exploitation Critic Loss : 0.029515409842133522
Exploration Critic Loss : 745523.8125
Exploration Model Loss : 43.86043930053711
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -146.246918
best mean reward -inf
running time 76.817304
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -146.24691772460938
TimeSinceStart : 76.81730389595032
Exploitation Critic Loss : 0.341413676738739
Exploration Critic Loss : 13344136.0
Exploration Model Loss : 74.4615478515625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -146.022720
best mean reward -inf
running time 88.260115
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -146.02272033691406
TimeSinceStart : 88.26011538505554
Exploitation Critic Loss : 0.6295351982116699
Exploration Critic Loss : 9691432.0
Exploration Model Loss : 57.51573181152344
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -146.315796
best mean reward -inf
running time 99.994354
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -146.3157958984375
TimeSinceStart : 99.99435424804688
Exploitation Critic Loss : 0.3006271719932556
Exploration Critic Loss : 10800019.0
Exploration Model Loss : 33.35821533203125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -145.169998
best mean reward -145.169998
running time 111.527933
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -145.1699981689453
Train_BestReturn : -145.1699981689453
TimeSinceStart : 111.52793335914612
Exploitation Critic Loss : 0.33518025279045105
Exploration Critic Loss : 9797047.0
Exploration Model Loss : 28.406814575195312
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -139.720001
best mean reward -139.720001
running time 123.297603
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -139.72000122070312
Train_BestReturn : -139.72000122070312
TimeSinceStart : 123.29760313034058
Exploitation Critic Loss : 1.3061254024505615
Exploration Critic Loss : 13172798.0
Exploration Model Loss : 24.385833740234375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -135.289993
best mean reward -135.289993
running time 135.003931
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     372 / 1000At timestep:     446 / 1000At timestep:     596 / 1000At timestep:     633 / 1000At timestep:     659 / 1000At timestep:     809 / 1000At timestep:     866 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -135.2899932861328
Train_BestReturn : -135.2899932861328
TimeSinceStart : 135.0039312839508
Exploitation Critic Loss : 0.9588239192962646
Exploration Critic Loss : 10548258.0
Exploration Model Loss : 23.299041748046875
Eval_AverageReturn : -101.0999984741211
Eval_StdReturn : 50.709861755371094
Eval_MaxReturn : -25.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 101.6
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -127.750000
best mean reward -127.750000
running time 147.413934
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -127.75
Train_BestReturn : -127.75
TimeSinceStart : 147.41393446922302
Exploitation Critic Loss : 1.6069985628128052
Exploration Critic Loss : 15751143.0
Exploration Model Loss : 22.935806274414062
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -111.870003
best mean reward -111.870003
running time 160.144313
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -111.87000274658203
Train_BestReturn : -111.87000274658203
TimeSinceStart : 160.1443133354187
Exploitation Critic Loss : 0.25931960344314575
Exploration Critic Loss : 14898688.0
Exploration Model Loss : 22.805618286132812
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -98.269997
best mean reward -98.269997
running time 172.812141
At timestep:     52 / 1000At timestep:     82 / 1000At timestep:     110 / 1000At timestep:     173 / 1000At timestep:     215 / 1000At timestep:     241 / 1000At timestep:     294 / 1000At timestep:     341 / 1000At timestep:     429 / 1000At timestep:     454 / 1000At timestep:     487 / 1000At timestep:     531 / 1000At timestep:     586 / 1000At timestep:     631 / 1000At timestep:     683 / 1000At timestep:     728 / 1000At timestep:     758 / 1000At timestep:     807 / 1000At timestep:     848 / 1000At timestep:     901 / 1000At timestep:     921 / 1000At timestep:     977 / 1000At timestep:     997 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -98.2699966430664
Train_BestReturn : -98.2699966430664
TimeSinceStart : 172.81214141845703
Exploitation Critic Loss : 1.5539278984069824
Exploration Critic Loss : 14830640.0
Exploration Model Loss : 22.757965087890625
Eval_AverageReturn : -42.29166793823242
Eval_StdReturn : 15.136817932128906
Eval_MaxReturn : -19.0
Eval_MinReturn : -87.0
Eval_AverageEpLen : 43.291666666666664
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -71.290001
best mean reward -71.290001
running time 188.363222
At timestep:     76 / 1000At timestep:     154 / 1000At timestep:     186 / 1000At timestep:     272 / 1000At timestep:     300 / 1000At timestep:     324 / 1000At timestep:     353 / 1000At timestep:     387 / 1000At timestep:     418 / 1000At timestep:     454 / 1000At timestep:     503 / 1000At timestep:     558 / 1000At timestep:     589 / 1000At timestep:     618 / 1000At timestep:     683 / 1000At timestep:     723 / 1000At timestep:     748 / 1000At timestep:     788 / 1000At timestep:     839 / 1000At timestep:     889 / 1000At timestep:     921 / 1000At timestep:     957 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -71.29000091552734
Train_BestReturn : -71.29000091552734
TimeSinceStart : 188.3632218837738
Exploitation Critic Loss : 0.18974348902702332
Exploration Critic Loss : 17127098.0
Exploration Model Loss : 22.740020751953125
Eval_AverageReturn : -43.34782791137695
Eval_StdReturn : 17.84865951538086
Eval_MaxReturn : -23.0
Eval_MinReturn : -85.0
Eval_AverageEpLen : 44.34782608695652
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -55.750000
best mean reward -55.750000
running time 203.389196
At timestep:     29 / 1000At timestep:     59 / 1000At timestep:     83 / 1000At timestep:     116 / 1000At timestep:     151 / 1000At timestep:     186 / 1000At timestep:     204 / 1000At timestep:     234 / 1000At timestep:     271 / 1000At timestep:     313 / 1000At timestep:     344 / 1000At timestep:     387 / 1000At timestep:     426 / 1000At timestep:     463 / 1000At timestep:     484 / 1000At timestep:     510 / 1000At timestep:     528 / 1000At timestep:     568 / 1000At timestep:     590 / 1000At timestep:     615 / 1000At timestep:     650 / 1000At timestep:     678 / 1000At timestep:     697 / 1000At timestep:     718 / 1000At timestep:     765 / 1000At timestep:     794 / 1000At timestep:     824 / 1000At timestep:     845 / 1000At timestep:     878 / 1000At timestep:     910 / 1000At timestep:     964 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -55.75
Train_BestReturn : -55.75
TimeSinceStart : 203.38919639587402
Exploitation Critic Loss : 1.0711116790771484
Exploration Critic Loss : 11997480.0
Exploration Model Loss : 22.733932495117188
Eval_AverageReturn : -31.03125
Eval_StdReturn : 9.973353385925293
Eval_MaxReturn : -17.0
Eval_MinReturn : -60.0
Eval_AverageEpLen : 32.03125
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -47.070000
best mean reward -47.070000
running time 219.448416
At timestep:     32 / 1000At timestep:     57 / 1000At timestep:     84 / 1000At timestep:     124 / 1000At timestep:     148 / 1000At timestep:     183 / 1000At timestep:     241 / 1000At timestep:     287 / 1000At timestep:     313 / 1000At timestep:     342 / 1000At timestep:     367 / 1000At timestep:     394 / 1000At timestep:     430 / 1000At timestep:     461 / 1000At timestep:     479 / 1000At timestep:     507 / 1000At timestep:     529 / 1000At timestep:     549 / 1000At timestep:     566 / 1000At timestep:     587 / 1000At timestep:     608 / 1000At timestep:     675 / 1000At timestep:     709 / 1000At timestep:     733 / 1000At timestep:     765 / 1000At timestep:     790 / 1000At timestep:     856 / 1000At timestep:     880 / 1000At timestep:     906 / 1000At timestep:     940 / 1000At timestep:     977 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -47.06999969482422
Train_BestReturn : -47.06999969482422
TimeSinceStart : 219.44841623306274
Exploitation Critic Loss : 0.18924826383590698
Exploration Critic Loss : 12928940.0
Exploration Model Loss : 22.73175048828125
Eval_AverageReturn : -30.71875
Eval_StdReturn : 12.250597953796387
Eval_MaxReturn : -16.0
Eval_MinReturn : -66.0
Eval_AverageEpLen : 31.71875
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -41.180000
best mean reward -41.180000
running time 235.822872
At timestep:     34 / 1000At timestep:     65 / 1000At timestep:     101 / 1000At timestep:     131 / 1000At timestep:     180 / 1000At timestep:     215 / 1000At timestep:     254 / 1000At timestep:     302 / 1000At timestep:     325 / 1000At timestep:     365 / 1000At timestep:     385 / 1000At timestep:     417 / 1000At timestep:     435 / 1000At timestep:     495 / 1000At timestep:     514 / 1000At timestep:     541 / 1000At timestep:     562 / 1000At timestep:     599 / 1000At timestep:     675 / 1000At timestep:     696 / 1000At timestep:     727 / 1000At timestep:     777 / 1000At timestep:     799 / 1000At timestep:     838 / 1000At timestep:     878 / 1000At timestep:     905 / 1000At timestep:     936 / 1000At timestep:     971 / 1000At timestep:     991 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -41.18000030517578
Train_BestReturn : -41.18000030517578
TimeSinceStart : 235.82287192344666
Exploitation Critic Loss : 0.26532694697380066
Exploration Critic Loss : 8991465.0
Exploration Model Loss : 22.730758666992188
Eval_AverageReturn : -33.0
Eval_StdReturn : 12.780192375183105
Eval_MaxReturn : -17.0
Eval_MinReturn : -75.0
Eval_AverageEpLen : 34.0
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -40.810001
best mean reward -40.810001
running time 251.812380
At timestep:     32 / 1000At timestep:     81 / 1000At timestep:     112 / 1000At timestep:     159 / 1000At timestep:     212 / 1000At timestep:     273 / 1000At timestep:     311 / 1000At timestep:     355 / 1000At timestep:     391 / 1000At timestep:     447 / 1000At timestep:     483 / 1000At timestep:     531 / 1000At timestep:     578 / 1000At timestep:     606 / 1000At timestep:     665 / 1000At timestep:     704 / 1000At timestep:     757 / 1000At timestep:     823 / 1000At timestep:     857 / 1000At timestep:     878 / 1000At timestep:     959 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -40.810001373291016
Train_BestReturn : -40.810001373291016
TimeSinceStart : 251.81237983703613
Exploitation Critic Loss : 0.24637752771377563
Exploration Critic Loss : 11057742.0
Exploration Model Loss : 22.730606079101562
Eval_AverageReturn : -45.04545593261719
Eval_StdReturn : 13.696308135986328
Eval_MaxReturn : -20.0
Eval_MinReturn : -80.0
Eval_AverageEpLen : 46.04545454545455
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -39.560001
best mean reward -39.560001
running time 266.894913
At timestep:     22 / 1000At timestep:     53 / 1000At timestep:     82 / 1000At timestep:     120 / 1000At timestep:     167 / 1000At timestep:     207 / 1000At timestep:     230 / 1000At timestep:     278 / 1000At timestep:     314 / 1000At timestep:     364 / 1000At timestep:     389 / 1000At timestep:     433 / 1000At timestep:     455 / 1000At timestep:     506 / 1000At timestep:     529 / 1000At timestep:     554 / 1000At timestep:     585 / 1000At timestep:     651 / 1000At timestep:     681 / 1000At timestep:     700 / 1000At timestep:     747 / 1000At timestep:     768 / 1000At timestep:     808 / 1000At timestep:     855 / 1000At timestep:     887 / 1000At timestep:     924 / 1000At timestep:     965 / 1000At timestep:     1030 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -39.560001373291016
Train_BestReturn : -39.560001373291016
TimeSinceStart : 266.8949131965637
Exploitation Critic Loss : 0.20430608093738556
Exploration Critic Loss : 11697606.0
Exploration Model Loss : 22.730850219726562
Eval_AverageReturn : -35.78571319580078
Eval_StdReturn : 12.55660629272461
Eval_MaxReturn : -18.0
Eval_MinReturn : -65.0
Eval_AverageEpLen : 36.785714285714285
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -37.439999
best mean reward -37.439999
running time 283.115140
At timestep:     34 / 1000At timestep:     67 / 1000At timestep:     87 / 1000At timestep:     128 / 1000At timestep:     147 / 1000At timestep:     172 / 1000At timestep:     208 / 1000At timestep:     233 / 1000At timestep:     256 / 1000At timestep:     287 / 1000At timestep:     324 / 1000At timestep:     352 / 1000At timestep:     378 / 1000At timestep:     432 / 1000At timestep:     464 / 1000At timestep:     490 / 1000At timestep:     510 / 1000At timestep:     542 / 1000At timestep:     567 / 1000At timestep:     586 / 1000At timestep:     627 / 1000At timestep:     668 / 1000At timestep:     691 / 1000At timestep:     733 / 1000At timestep:     757 / 1000At timestep:     801 / 1000At timestep:     830 / 1000At timestep:     856 / 1000At timestep:     894 / 1000At timestep:     957 / 1000At timestep:     988 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -37.439998626708984
Train_BestReturn : -37.439998626708984
TimeSinceStart : 283.1151399612427
Exploitation Critic Loss : 0.9304629564285278
Exploration Critic Loss : 12323038.0
Exploration Model Loss : 22.731216430664062
Eval_AverageReturn : -30.59375
Eval_StdReturn : 10.036992073059082
Eval_MaxReturn : -18.0
Eval_MinReturn : -62.0
Eval_AverageEpLen : 31.59375
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -37.049999
best mean reward -37.049999
running time 299.576435
At timestep:     30 / 1000At timestep:     64 / 1000At timestep:     91 / 1000At timestep:     109 / 1000At timestep:     130 / 1000At timestep:     162 / 1000At timestep:     204 / 1000At timestep:     228 / 1000At timestep:     251 / 1000At timestep:     293 / 1000At timestep:     319 / 1000At timestep:     345 / 1000At timestep:     372 / 1000At timestep:     400 / 1000At timestep:     436 / 1000At timestep:     473 / 1000At timestep:     500 / 1000At timestep:     543 / 1000At timestep:     566 / 1000At timestep:     587 / 1000At timestep:     619 / 1000At timestep:     645 / 1000At timestep:     663 / 1000At timestep:     687 / 1000At timestep:     711 / 1000At timestep:     757 / 1000At timestep:     778 / 1000At timestep:     837 / 1000At timestep:     892 / 1000At timestep:     913 / 1000At timestep:     935 / 1000At timestep:     956 / 1000At timestep:     977 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -37.04999923706055
Train_BestReturn : -37.04999923706055
TimeSinceStart : 299.57643461227417
Exploitation Critic Loss : 0.9487713575363159
Exploration Critic Loss : 10684900.0
Exploration Model Loss : 22.731582641601562
Eval_AverageReturn : -28.47058868408203
Eval_StdReturn : 10.038851737976074
Eval_MaxReturn : -17.0
Eval_MinReturn : -58.0
Eval_AverageEpLen : 29.470588235294116
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -35.110001
best mean reward -35.110001
running time 315.756996
At timestep:     18 / 1000At timestep:     44 / 1000At timestep:     69 / 1000At timestep:     94 / 1000At timestep:     112 / 1000At timestep:     136 / 1000At timestep:     160 / 1000At timestep:     185 / 1000At timestep:     202 / 1000At timestep:     234 / 1000At timestep:     260 / 1000At timestep:     281 / 1000At timestep:     300 / 1000At timestep:     324 / 1000At timestep:     350 / 1000At timestep:     377 / 1000At timestep:     391 / 1000At timestep:     411 / 1000At timestep:     436 / 1000At timestep:     456 / 1000At timestep:     483 / 1000At timestep:     506 / 1000At timestep:     534 / 1000At timestep:     556 / 1000At timestep:     588 / 1000At timestep:     607 / 1000At timestep:     631 / 1000At timestep:     666 / 1000At timestep:     695 / 1000At timestep:     717 / 1000At timestep:     741 / 1000At timestep:     767 / 1000At timestep:     798 / 1000At timestep:     817 / 1000At timestep:     850 / 1000At timestep:     876 / 1000At timestep:     898 / 1000At timestep:     919 / 1000At timestep:     941 / 1000At timestep:     960 / 1000At timestep:     988 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -35.11000061035156
Train_BestReturn : -35.11000061035156
TimeSinceStart : 315.756995677948
Exploitation Critic Loss : 0.20905131101608276
Exploration Critic Loss : 10031838.0
Exploration Model Loss : 22.731918334960938
Eval_AverageReturn : -23.190475463867188
Eval_StdReturn : 4.578615188598633
Eval_MaxReturn : -13.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 24.19047619047619
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -34.130001
best mean reward -34.130001
running time 332.490768
At timestep:     30 / 1000At timestep:     53 / 1000At timestep:     83 / 1000At timestep:     126 / 1000At timestep:     151 / 1000At timestep:     198 / 1000At timestep:     223 / 1000At timestep:     253 / 1000At timestep:     280 / 1000At timestep:     322 / 1000At timestep:     350 / 1000At timestep:     381 / 1000At timestep:     415 / 1000At timestep:     449 / 1000At timestep:     506 / 1000At timestep:     541 / 1000At timestep:     586 / 1000At timestep:     606 / 1000At timestep:     650 / 1000At timestep:     681 / 1000At timestep:     712 / 1000At timestep:     743 / 1000At timestep:     769 / 1000At timestep:     792 / 1000At timestep:     823 / 1000At timestep:     869 / 1000At timestep:     886 / 1000At timestep:     918 / 1000At timestep:     939 / 1000At timestep:     981 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -34.130001068115234
Train_BestReturn : -34.130001068115234
TimeSinceStart : 332.4907681941986
Exploitation Critic Loss : 0.5426515340805054
Exploration Critic Loss : 8718406.0
Exploration Model Loss : 22.732284545898438
Eval_AverageReturn : -31.387096405029297
Eval_StdReturn : 9.188623428344727
Eval_MaxReturn : -16.0
Eval_MinReturn : -56.0
Eval_AverageEpLen : 32.38709677419355
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -33.049999
best mean reward -33.049999
running time 348.445936
At timestep:     23 / 1000At timestep:     50 / 1000At timestep:     73 / 1000At timestep:     91 / 1000At timestep:     123 / 1000At timestep:     143 / 1000At timestep:     159 / 1000At timestep:     181 / 1000At timestep:     200 / 1000At timestep:     221 / 1000At timestep:     244 / 1000At timestep:     267 / 1000At timestep:     284 / 1000At timestep:     315 / 1000At timestep:     343 / 1000At timestep:     366 / 1000At timestep:     388 / 1000At timestep:     412 / 1000At timestep:     436 / 1000At timestep:     455 / 1000At timestep:     490 / 1000At timestep:     513 / 1000At timestep:     540 / 1000At timestep:     564 / 1000At timestep:     582 / 1000At timestep:     610 / 1000At timestep:     637 / 1000At timestep:     652 / 1000At timestep:     672 / 1000At timestep:     694 / 1000At timestep:     718 / 1000At timestep:     745 / 1000At timestep:     773 / 1000At timestep:     790 / 1000At timestep:     818 / 1000At timestep:     838 / 1000At timestep:     863 / 1000At timestep:     885 / 1000At timestep:     914 / 1000At timestep:     938 / 1000At timestep:     965 / 1000At timestep:     992 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -33.04999923706055
Train_BestReturn : -33.04999923706055
TimeSinceStart : 348.44593620300293
Exploitation Critic Loss : 0.86221843957901
Exploration Critic Loss : 10817638.0
Exploration Model Loss : 22.732650756835938
Eval_AverageReturn : -22.674419403076172
Eval_StdReturn : 4.370732307434082
Eval_MaxReturn : -14.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 23.674418604651162
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -31.969999
best mean reward -31.969999
running time 366.039803
At timestep:     24 / 1000At timestep:     48 / 1000At timestep:     70 / 1000At timestep:     96 / 1000At timestep:     119 / 1000At timestep:     148 / 1000At timestep:     170 / 1000At timestep:     197 / 1000At timestep:     218 / 1000At timestep:     241 / 1000At timestep:     264 / 1000At timestep:     284 / 1000At timestep:     305 / 1000At timestep:     328 / 1000At timestep:     350 / 1000At timestep:     375 / 1000At timestep:     399 / 1000At timestep:     416 / 1000At timestep:     440 / 1000At timestep:     459 / 1000At timestep:     485 / 1000At timestep:     505 / 1000At timestep:     528 / 1000At timestep:     544 / 1000At timestep:     562 / 1000At timestep:     580 / 1000At timestep:     600 / 1000At timestep:     624 / 1000At timestep:     649 / 1000At timestep:     670 / 1000At timestep:     692 / 1000At timestep:     710 / 1000At timestep:     744 / 1000At timestep:     769 / 1000At timestep:     792 / 1000At timestep:     814 / 1000At timestep:     838 / 1000At timestep:     854 / 1000At timestep:     875 / 1000At timestep:     897 / 1000At timestep:     927 / 1000At timestep:     949 / 1000At timestep:     967 / 1000At timestep:     993 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -31.969999313354492
Train_BestReturn : -31.969999313354492
TimeSinceStart : 366.0398027896881
Exploitation Critic Loss : 0.18546389043331146
Exploration Critic Loss : 11484159.0
Exploration Model Loss : 22.7330322265625
Eval_AverageReturn : -21.46666717529297
Eval_StdReturn : 3.587632894515991
Eval_MaxReturn : -15.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 22.466666666666665
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -30.850000
best mean reward -30.850000
running time 384.260542
At timestep:     19 / 1000At timestep:     39 / 1000At timestep:     64 / 1000At timestep:     84 / 1000At timestep:     114 / 1000At timestep:     143 / 1000At timestep:     159 / 1000At timestep:     182 / 1000At timestep:     202 / 1000At timestep:     227 / 1000At timestep:     246 / 1000At timestep:     265 / 1000At timestep:     285 / 1000At timestep:     306 / 1000At timestep:     325 / 1000At timestep:     343 / 1000At timestep:     368 / 1000At timestep:     398 / 1000At timestep:     421 / 1000At timestep:     444 / 1000At timestep:     467 / 1000At timestep:     489 / 1000At timestep:     512 / 1000At timestep:     541 / 1000At timestep:     567 / 1000At timestep:     597 / 1000At timestep:     621 / 1000At timestep:     652 / 1000At timestep:     673 / 1000At timestep:     704 / 1000At timestep:     726 / 1000At timestep:     751 / 1000At timestep:     783 / 1000At timestep:     798 / 1000At timestep:     823 / 1000At timestep:     849 / 1000At timestep:     877 / 1000At timestep:     902 / 1000At timestep:     927 / 1000At timestep:     950 / 1000At timestep:     980 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -30.850000381469727
Train_BestReturn : -30.850000381469727
TimeSinceStart : 384.26054191589355
Exploitation Critic Loss : 0.42966198921203613
Exploration Critic Loss : 9785936.0
Exploration Model Loss : 22.733428955078125
Eval_AverageReturn : -22.880952835083008
Eval_StdReturn : 4.2661590576171875
Eval_MaxReturn : -14.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 23.88095238095238
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -29.980000
best mean reward -29.980000
running time 402.138956
At timestep:     20 / 1000At timestep:     37 / 1000At timestep:     60 / 1000At timestep:     83 / 1000At timestep:     106 / 1000At timestep:     124 / 1000At timestep:     158 / 1000At timestep:     183 / 1000At timestep:     205 / 1000At timestep:     233 / 1000At timestep:     265 / 1000At timestep:     287 / 1000At timestep:     311 / 1000At timestep:     336 / 1000At timestep:     359 / 1000At timestep:     382 / 1000At timestep:     408 / 1000At timestep:     437 / 1000At timestep:     466 / 1000At timestep:     490 / 1000At timestep:     513 / 1000At timestep:     542 / 1000At timestep:     570 / 1000At timestep:     592 / 1000At timestep:     616 / 1000At timestep:     641 / 1000At timestep:     662 / 1000At timestep:     682 / 1000At timestep:     705 / 1000At timestep:     730 / 1000At timestep:     754 / 1000At timestep:     774 / 1000At timestep:     801 / 1000At timestep:     825 / 1000At timestep:     846 / 1000At timestep:     876 / 1000At timestep:     899 / 1000At timestep:     925 / 1000At timestep:     952 / 1000At timestep:     979 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -29.979999542236328
Train_BestReturn : -29.979999542236328
TimeSinceStart : 402.1389558315277
Exploitation Critic Loss : 0.8449972867965698
Exploration Critic Loss : 8149519.0
Exploration Model Loss : 22.733779907226562
Eval_AverageReturn : -23.439023971557617
Eval_StdReturn : 3.5272374153137207
Eval_MaxReturn : -16.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 24.4390243902439
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -29.150000
best mean reward -29.150000
running time 419.582620
At timestep:     24 / 1000At timestep:     73 / 1000At timestep:     99 / 1000At timestep:     125 / 1000At timestep:     150 / 1000At timestep:     170 / 1000At timestep:     195 / 1000At timestep:     219 / 1000At timestep:     237 / 1000At timestep:     267 / 1000At timestep:     297 / 1000At timestep:     318 / 1000At timestep:     355 / 1000At timestep:     376 / 1000At timestep:     400 / 1000At timestep:     435 / 1000At timestep:     457 / 1000At timestep:     475 / 1000At timestep:     497 / 1000At timestep:     518 / 1000At timestep:     543 / 1000At timestep:     567 / 1000At timestep:     592 / 1000At timestep:     610 / 1000At timestep:     628 / 1000At timestep:     651 / 1000At timestep:     677 / 1000At timestep:     704 / 1000At timestep:     728 / 1000At timestep:     752 / 1000At timestep:     783 / 1000At timestep:     809 / 1000At timestep:     830 / 1000At timestep:     857 / 1000At timestep:     874 / 1000At timestep:     900 / 1000At timestep:     928 / 1000At timestep:     952 / 1000At timestep:     970 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -29.149999618530273
Train_BestReturn : -29.149999618530273
TimeSinceStart : 419.58262038230896
Exploitation Critic Loss : 0.8124864101409912
Exploration Critic Loss : 9014124.0
Exploration Model Loss : 22.734085083007812
Eval_AverageReturn : -24.100000381469727
Eval_StdReturn : 5.99082612991333
Eval_MaxReturn : -16.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 25.1
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -28.450001
best mean reward -28.450001
running time 437.592942
At timestep:     19 / 1000At timestep:     43 / 1000At timestep:     65 / 1000At timestep:     92 / 1000At timestep:     112 / 1000At timestep:     143 / 1000At timestep:     167 / 1000At timestep:     187 / 1000At timestep:     213 / 1000At timestep:     235 / 1000At timestep:     256 / 1000At timestep:     288 / 1000At timestep:     308 / 1000At timestep:     328 / 1000At timestep:     350 / 1000At timestep:     371 / 1000At timestep:     402 / 1000At timestep:     428 / 1000At timestep:     451 / 1000At timestep:     475 / 1000At timestep:     494 / 1000At timestep:     516 / 1000At timestep:     542 / 1000At timestep:     565 / 1000At timestep:     589 / 1000At timestep:     612 / 1000At timestep:     633 / 1000At timestep:     660 / 1000At timestep:     681 / 1000At timestep:     705 / 1000At timestep:     726 / 1000At timestep:     753 / 1000At timestep:     774 / 1000At timestep:     795 / 1000At timestep:     821 / 1000At timestep:     843 / 1000At timestep:     868 / 1000At timestep:     897 / 1000At timestep:     928 / 1000At timestep:     953 / 1000At timestep:     971 / 1000At timestep:     998 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -28.450000762939453
Train_BestReturn : -28.450000762939453
TimeSinceStart : 437.592942237854
Exploitation Critic Loss : 0.22919125854969025
Exploration Critic Loss : 7928647.0
Exploration Model Loss : 22.734420776367188
Eval_AverageReturn : -22.604650497436523
Eval_StdReturn : 3.635352849960327
Eval_MaxReturn : -16.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 23.6046511627907
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -27.969999
best mean reward -27.969999
running time 456.078933
At timestep:     22 / 1000At timestep:     48 / 1000At timestep:     75 / 1000At timestep:     103 / 1000At timestep:     126 / 1000At timestep:     147 / 1000At timestep:     172 / 1000At timestep:     202 / 1000At timestep:     222 / 1000At timestep:     251 / 1000At timestep:     278 / 1000At timestep:     310 / 1000At timestep:     334 / 1000At timestep:     357 / 1000At timestep:     392 / 1000At timestep:     410 / 1000At timestep:     436 / 1000At timestep:     457 / 1000At timestep:     478 / 1000At timestep:     502 / 1000At timestep:     527 / 1000At timestep:     550 / 1000At timestep:     572 / 1000At timestep:     598 / 1000At timestep:     625 / 1000At timestep:     650 / 1000At timestep:     676 / 1000At timestep:     701 / 1000At timestep:     722 / 1000At timestep:     744 / 1000At timestep:     771 / 1000At timestep:     788 / 1000At timestep:     809 / 1000At timestep:     832 / 1000At timestep:     851 / 1000At timestep:     876 / 1000At timestep:     902 / 1000At timestep:     925 / 1000At timestep:     951 / 1000At timestep:     972 / 1000At timestep:     993 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -27.969999313354492
Train_BestReturn : -27.969999313354492
TimeSinceStart : 456.07893323898315
Exploitation Critic Loss : 0.23860383033752441
Exploration Critic Loss : 11241541.0
Exploration Model Loss : 22.7344970703125
Eval_AverageReturn : -23.14285659790039
Eval_StdReturn : 3.5894782543182373
Eval_MaxReturn : -16.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 24.142857142857142
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -27.770000
best mean reward -27.770000
running time 474.280752
At timestep:     28 / 1000At timestep:     46 / 1000At timestep:     63 / 1000At timestep:     91 / 1000At timestep:     120 / 1000At timestep:     140 / 1000At timestep:     164 / 1000At timestep:     196 / 1000At timestep:     218 / 1000At timestep:     247 / 1000At timestep:     283 / 1000At timestep:     310 / 1000At timestep:     336 / 1000At timestep:     361 / 1000At timestep:     384 / 1000At timestep:     412 / 1000At timestep:     434 / 1000At timestep:     451 / 1000At timestep:     475 / 1000At timestep:     497 / 1000At timestep:     518 / 1000At timestep:     542 / 1000At timestep:     564 / 1000At timestep:     587 / 1000At timestep:     607 / 1000At timestep:     632 / 1000At timestep:     660 / 1000At timestep:     686 / 1000At timestep:     704 / 1000At timestep:     726 / 1000At timestep:     751 / 1000At timestep:     772 / 1000At timestep:     792 / 1000At timestep:     816 / 1000At timestep:     841 / 1000At timestep:     861 / 1000At timestep:     883 / 1000At timestep:     904 / 1000At timestep:     931 / 1000At timestep:     953 / 1000At timestep:     974 / 1000At timestep:     991 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -27.770000457763672
Train_BestReturn : -27.770000457763672
TimeSinceStart : 474.28075194358826
Exploitation Critic Loss : 0.3851834535598755
Exploration Critic Loss : 9898957.0
Exploration Model Loss : 22.734695434570312
Eval_AverageReturn : -22.627906799316406
Eval_StdReturn : 4.023323059082031
Eval_MaxReturn : -16.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 23.627906976744185
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -27.400000
best mean reward -27.400000
running time 492.630864
At timestep:     27 / 1000At timestep:     52 / 1000At timestep:     73 / 1000At timestep:     95 / 1000At timestep:     119 / 1000At timestep:     148 / 1000At timestep:     167 / 1000At timestep:     186 / 1000At timestep:     210 / 1000At timestep:     232 / 1000At timestep:     253 / 1000At timestep:     281 / 1000At timestep:     307 / 1000At timestep:     329 / 1000At timestep:     345 / 1000At timestep:     363 / 1000At timestep:     388 / 1000At timestep:     413 / 1000At timestep:     454 / 1000At timestep:     485 / 1000At timestep:     502 / 1000At timestep:     530 / 1000At timestep:     554 / 1000At timestep:     581 / 1000At timestep:     604 / 1000At timestep:     625 / 1000At timestep:     652 / 1000At timestep:     679 / 1000At timestep:     701 / 1000At timestep:     728 / 1000At timestep:     749 / 1000At timestep:     770 / 1000At timestep:     791 / 1000At timestep:     811 / 1000At timestep:     844 / 1000At timestep:     870 / 1000At timestep:     892 / 1000At timestep:     913 / 1000At timestep:     936 / 1000At timestep:     956 / 1000At timestep:     974 / 1000At timestep:     999 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -27.399999618530273
Train_BestReturn : -27.399999618530273
TimeSinceStart : 492.630863904953
Exploitation Critic Loss : 0.23515553772449493
Exploration Critic Loss : 9162176.0
Exploration Model Loss : 22.735076904296875
Eval_AverageReturn : -22.79069709777832
Eval_StdReturn : 4.53184175491333
Eval_MaxReturn : -15.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 23.790697674418606
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -28.000000
best mean reward -27.400000
running time 510.384071
At timestep:     21 / 1000At timestep:     44 / 1000At timestep:     60 / 1000At timestep:     85 / 1000At timestep:     114 / 1000At timestep:     136 / 1000At timestep:     157 / 1000At timestep:     183 / 1000At timestep:     204 / 1000At timestep:     227 / 1000At timestep:     257 / 1000At timestep:     285 / 1000At timestep:     303 / 1000At timestep:     325 / 1000At timestep:     347 / 1000At timestep:     366 / 1000At timestep:     387 / 1000At timestep:     411 / 1000At timestep:     437 / 1000At timestep:     461 / 1000At timestep:     482 / 1000At timestep:     504 / 1000At timestep:     522 / 1000At timestep:     536 / 1000At timestep:     561 / 1000At timestep:     583 / 1000At timestep:     610 / 1000At timestep:     635 / 1000At timestep:     661 / 1000At timestep:     680 / 1000At timestep:     700 / 1000At timestep:     729 / 1000At timestep:     752 / 1000At timestep:     771 / 1000At timestep:     792 / 1000At timestep:     823 / 1000At timestep:     851 / 1000At timestep:     877 / 1000At timestep:     898 / 1000At timestep:     924 / 1000At timestep:     951 / 1000At timestep:     985 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -28.0
Train_BestReturn : -27.399999618530273
TimeSinceStart : 510.3840706348419
Exploitation Critic Loss : 0.19654503464698792
Exploration Critic Loss : 7598107.0
Exploration Model Loss : 22.735275268554688
Eval_AverageReturn : -22.372093200683594
Eval_StdReturn : 4.092098236083984
Eval_MaxReturn : -13.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 23.372093023255815
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -29.330000
best mean reward -27.400000
running time 527.755912
At timestep:     21 / 1000At timestep:     39 / 1000At timestep:     65 / 1000At timestep:     91 / 1000At timestep:     105 / 1000At timestep:     128 / 1000At timestep:     150 / 1000At timestep:     171 / 1000At timestep:     194 / 1000At timestep:     220 / 1000At timestep:     240 / 1000At timestep:     260 / 1000At timestep:     285 / 1000At timestep:     303 / 1000At timestep:     325 / 1000At timestep:     348 / 1000At timestep:     381 / 1000At timestep:     400 / 1000At timestep:     421 / 1000At timestep:     445 / 1000At timestep:     469 / 1000At timestep:     494 / 1000At timestep:     512 / 1000At timestep:     544 / 1000At timestep:     573 / 1000At timestep:     603 / 1000At timestep:     626 / 1000At timestep:     648 / 1000At timestep:     679 / 1000At timestep:     701 / 1000At timestep:     725 / 1000At timestep:     747 / 1000At timestep:     769 / 1000At timestep:     789 / 1000At timestep:     802 / 1000At timestep:     822 / 1000At timestep:     858 / 1000At timestep:     875 / 1000At timestep:     894 / 1000At timestep:     914 / 1000At timestep:     933 / 1000At timestep:     954 / 1000At timestep:     977 / 1000At timestep:     999 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -29.329999923706055
Train_BestReturn : -27.399999618530273
TimeSinceStart : 527.7559115886688
Exploitation Critic Loss : 0.4063871502876282
Exploration Critic Loss : 10018178.0
Exploration Model Loss : 22.7354736328125
Eval_AverageReturn : -21.64444351196289
Eval_StdReturn : 4.590597152709961
Eval_MaxReturn : -12.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 22.644444444444446
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -29.700001
best mean reward -27.400000
running time 545.422165
At timestep:     20 / 1000At timestep:     51 / 1000At timestep:     71 / 1000At timestep:     96 / 1000At timestep:     127 / 1000At timestep:     143 / 1000At timestep:     167 / 1000At timestep:     181 / 1000At timestep:     203 / 1000At timestep:     227 / 1000At timestep:     246 / 1000At timestep:     265 / 1000At timestep:     283 / 1000At timestep:     311 / 1000At timestep:     334 / 1000At timestep:     354 / 1000At timestep:     375 / 1000At timestep:     397 / 1000At timestep:     415 / 1000At timestep:     449 / 1000At timestep:     472 / 1000At timestep:     501 / 1000At timestep:     524 / 1000At timestep:     545 / 1000At timestep:     569 / 1000At timestep:     589 / 1000At timestep:     612 / 1000At timestep:     635 / 1000At timestep:     662 / 1000At timestep:     680 / 1000At timestep:     703 / 1000At timestep:     726 / 1000At timestep:     747 / 1000At timestep:     770 / 1000At timestep:     795 / 1000At timestep:     813 / 1000At timestep:     841 / 1000At timestep:     861 / 1000At timestep:     884 / 1000At timestep:     901 / 1000At timestep:     931 / 1000At timestep:     952 / 1000At timestep:     974 / 1000At timestep:     993 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -29.700000762939453
Train_BestReturn : -27.399999618530273
TimeSinceStart : 545.4221653938293
Exploitation Critic Loss : 0.18721450865268707
Exploration Critic Loss : 9121956.0
Exploration Model Loss : 22.735580444335938
Eval_AverageReturn : -21.511110305786133
Eval_StdReturn : 4.182621002197266
Eval_MaxReturn : -13.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 22.511111111111113
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -29.250000
best mean reward -27.400000
running time 563.432935
At timestep:     26 / 1000At timestep:     50 / 1000At timestep:     69 / 1000At timestep:     89 / 1000At timestep:     107 / 1000At timestep:     133 / 1000At timestep:     165 / 1000At timestep:     196 / 1000At timestep:     215 / 1000At timestep:     240 / 1000At timestep:     260 / 1000At timestep:     284 / 1000At timestep:     304 / 1000At timestep:     329 / 1000At timestep:     355 / 1000At timestep:     378 / 1000At timestep:     401 / 1000At timestep:     420 / 1000At timestep:     443 / 1000At timestep:     470 / 1000At timestep:     493 / 1000At timestep:     515 / 1000At timestep:     539 / 1000At timestep:     561 / 1000At timestep:     593 / 1000At timestep:     629 / 1000At timestep:     648 / 1000At timestep:     669 / 1000At timestep:     695 / 1000At timestep:     715 / 1000At timestep:     734 / 1000At timestep:     757 / 1000At timestep:     777 / 1000At timestep:     799 / 1000At timestep:     823 / 1000At timestep:     848 / 1000At timestep:     869 / 1000At timestep:     893 / 1000At timestep:     919 / 1000At timestep:     942 / 1000At timestep:     965 / 1000At timestep:     999 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -29.25
Train_BestReturn : -27.399999618530273
TimeSinceStart : 563.4329345226288
Exploitation Critic Loss : 0.2633768916130066
Exploration Critic Loss : 9783306.0
Exploration Model Loss : 22.735549926757812
Eval_AverageReturn : -22.813953399658203
Eval_StdReturn : 4.12172794342041
Eval_MaxReturn : -17.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 23.813953488372093
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -27.520000
best mean reward -27.400000
running time 581.370176
At timestep:     23 / 1000At timestep:     41 / 1000At timestep:     61 / 1000At timestep:     89 / 1000At timestep:     109 / 1000At timestep:     128 / 1000At timestep:     160 / 1000At timestep:     181 / 1000At timestep:     210 / 1000At timestep:     227 / 1000At timestep:     251 / 1000At timestep:     277 / 1000At timestep:     298 / 1000At timestep:     314 / 1000At timestep:     338 / 1000At timestep:     359 / 1000At timestep:     383 / 1000At timestep:     408 / 1000At timestep:     427 / 1000At timestep:     461 / 1000At timestep:     479 / 1000At timestep:     503 / 1000At timestep:     525 / 1000At timestep:     550 / 1000At timestep:     581 / 1000At timestep:     600 / 1000At timestep:     622 / 1000At timestep:     648 / 1000At timestep:     681 / 1000At timestep:     708 / 1000At timestep:     733 / 1000At timestep:     756 / 1000At timestep:     778 / 1000At timestep:     796 / 1000At timestep:     815 / 1000At timestep:     838 / 1000At timestep:     868 / 1000At timestep:     894 / 1000At timestep:     922 / 1000At timestep:     954 / 1000At timestep:     976 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -27.520000457763672
Train_BestReturn : -27.399999618530273
TimeSinceStart : 581.3701758384705
Exploitation Critic Loss : 0.2601216733455658
Exploration Critic Loss : 10619158.0
Exploration Model Loss : 22.735595703125
Eval_AverageReturn : -22.880952835083008
Eval_StdReturn : 4.583627223968506
Eval_MaxReturn : -15.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 23.88095238095238
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -27.940001
best mean reward -27.400000
running time 598.670372
At timestep:     22 / 1000At timestep:     49 / 1000At timestep:     71 / 1000At timestep:     87 / 1000At timestep:     106 / 1000At timestep:     127 / 1000At timestep:     145 / 1000At timestep:     169 / 1000At timestep:     196 / 1000At timestep:     216 / 1000At timestep:     234 / 1000At timestep:     259 / 1000At timestep:     279 / 1000At timestep:     300 / 1000At timestep:     322 / 1000At timestep:     347 / 1000At timestep:     369 / 1000At timestep:     393 / 1000At timestep:     417 / 1000At timestep:     444 / 1000At timestep:     465 / 1000At timestep:     485 / 1000At timestep:     516 / 1000At timestep:     540 / 1000At timestep:     565 / 1000At timestep:     595 / 1000At timestep:     621 / 1000At timestep:     643 / 1000At timestep:     672 / 1000At timestep:     694 / 1000At timestep:     716 / 1000At timestep:     744 / 1000At timestep:     770 / 1000At timestep:     788 / 1000At timestep:     810 / 1000At timestep:     835 / 1000At timestep:     859 / 1000At timestep:     882 / 1000At timestep:     904 / 1000At timestep:     923 / 1000At timestep:     953 / 1000At timestep:     979 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -27.940000534057617
Train_BestReturn : -27.399999618530273
TimeSinceStart : 598.6703722476959
Exploitation Critic Loss : 0.2861962914466858
Exploration Critic Loss : 9801786.0
Exploration Model Loss : 22.735580444335938
Eval_AverageReturn : -22.279069900512695
Eval_StdReturn : 3.4796791076660156
Eval_MaxReturn : -15.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 23.27906976744186
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -28.290001
best mean reward -27.400000
running time 616.291784
At timestep:     23 / 1000At timestep:     42 / 1000At timestep:     65 / 1000At timestep:     89 / 1000At timestep:     117 / 1000At timestep:     139 / 1000At timestep:     156 / 1000At timestep:     177 / 1000At timestep:     201 / 1000At timestep:     219 / 1000At timestep:     244 / 1000At timestep:     262 / 1000At timestep:     285 / 1000At timestep:     306 / 1000At timestep:     327 / 1000At timestep:     350 / 1000At timestep:     373 / 1000At timestep:     400 / 1000At timestep:     420 / 1000At timestep:     441 / 1000At timestep:     465 / 1000At timestep:     485 / 1000At timestep:     505 / 1000At timestep:     526 / 1000At timestep:     547 / 1000At timestep:     569 / 1000At timestep:     592 / 1000At timestep:     614 / 1000At timestep:     641 / 1000At timestep:     659 / 1000At timestep:     683 / 1000At timestep:     701 / 1000At timestep:     725 / 1000At timestep:     747 / 1000At timestep:     770 / 1000At timestep:     788 / 1000At timestep:     811 / 1000At timestep:     836 / 1000At timestep:     855 / 1000At timestep:     880 / 1000At timestep:     902 / 1000At timestep:     928 / 1000At timestep:     951 / 1000At timestep:     970 / 1000At timestep:     995 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -28.290000915527344
Train_BestReturn : -27.399999618530273
TimeSinceStart : 616.291784286499
Exploitation Critic Loss : 0.1724756956100464
Exploration Critic Loss : 10013640.0
Exploration Model Loss : 22.735580444335938
Eval_AverageReturn : -21.021739959716797
Eval_StdReturn : 2.6904754638671875
Eval_MaxReturn : -16.0
Eval_MinReturn : -27.0
Eval_AverageEpLen : 22.02173913043478
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -29.250000
best mean reward -27.400000
running time 634.148801
At timestep:     26 / 1000At timestep:     43 / 1000At timestep:     64 / 1000At timestep:     87 / 1000At timestep:     107 / 1000At timestep:     128 / 1000At timestep:     157 / 1000At timestep:     176 / 1000At timestep:     194 / 1000At timestep:     218 / 1000At timestep:     236 / 1000At timestep:     261 / 1000At timestep:     288 / 1000At timestep:     309 / 1000At timestep:     328 / 1000At timestep:     348 / 1000At timestep:     375 / 1000At timestep:     394 / 1000At timestep:     419 / 1000At timestep:     444 / 1000At timestep:     469 / 1000At timestep:     490 / 1000At timestep:     509 / 1000At timestep:     535 / 1000At timestep:     560 / 1000At timestep:     582 / 1000At timestep:     603 / 1000At timestep:     623 / 1000At timestep:     655 / 1000At timestep:     678 / 1000At timestep:     706 / 1000At timestep:     724 / 1000At timestep:     747 / 1000At timestep:     767 / 1000At timestep:     790 / 1000At timestep:     818 / 1000At timestep:     839 / 1000At timestep:     862 / 1000At timestep:     879 / 1000At timestep:     905 / 1000At timestep:     923 / 1000At timestep:     942 / 1000At timestep:     965 / 1000At timestep:     989 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -29.25
Train_BestReturn : -27.399999618530273
TimeSinceStart : 634.1488010883331
Exploitation Critic Loss : 0.18838058412075043
Exploration Critic Loss : 9827178.0
Exploration Model Loss : 22.73565673828125
Eval_AverageReturn : -21.488889694213867
Eval_StdReturn : 3.4936275482177734
Eval_MaxReturn : -16.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 22.488888888888887
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -28.330000
best mean reward -27.400000
running time 652.339356
At timestep:     25 / 1000At timestep:     46 / 1000At timestep:     75 / 1000At timestep:     100 / 1000At timestep:     126 / 1000At timestep:     148 / 1000At timestep:     174 / 1000At timestep:     195 / 1000At timestep:     216 / 1000At timestep:     233 / 1000At timestep:     256 / 1000At timestep:     274 / 1000At timestep:     294 / 1000At timestep:     311 / 1000At timestep:     337 / 1000At timestep:     362 / 1000At timestep:     383 / 1000At timestep:     403 / 1000At timestep:     423 / 1000At timestep:     459 / 1000At timestep:     479 / 1000At timestep:     513 / 1000At timestep:     541 / 1000At timestep:     561 / 1000At timestep:     580 / 1000At timestep:     596 / 1000At timestep:     621 / 1000At timestep:     649 / 1000At timestep:     668 / 1000At timestep:     692 / 1000At timestep:     718 / 1000At timestep:     749 / 1000At timestep:     768 / 1000At timestep:     788 / 1000At timestep:     807 / 1000At timestep:     826 / 1000At timestep:     847 / 1000At timestep:     870 / 1000At timestep:     899 / 1000At timestep:     917 / 1000At timestep:     941 / 1000At timestep:     963 / 1000At timestep:     991 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -28.329999923706055
Train_BestReturn : -27.399999618530273
TimeSinceStart : 652.3393559455872
Exploitation Critic Loss : 0.8412675857543945
Exploration Critic Loss : 11299224.0
Exploration Model Loss : 22.736083984375
Eval_AverageReturn : -22.113636016845703
Eval_StdReturn : 4.498564720153809
Eval_MaxReturn : -15.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 23.113636363636363
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -27.850000
best mean reward -27.400000
running time 670.670211
At timestep:     15 / 1000At timestep:     42 / 1000At timestep:     66 / 1000At timestep:     92 / 1000At timestep:     125 / 1000At timestep:     146 / 1000At timestep:     166 / 1000At timestep:     187 / 1000At timestep:     212 / 1000At timestep:     226 / 1000At timestep:     245 / 1000At timestep:     265 / 1000At timestep:     289 / 1000At timestep:     317 / 1000At timestep:     336 / 1000At timestep:     361 / 1000At timestep:     389 / 1000At timestep:     412 / 1000At timestep:     439 / 1000At timestep:     463 / 1000At timestep:     488 / 1000At timestep:     511 / 1000At timestep:     529 / 1000At timestep:     552 / 1000At timestep:     574 / 1000At timestep:     593 / 1000At timestep:     621 / 1000At timestep:     644 / 1000At timestep:     666 / 1000At timestep:     686 / 1000At timestep:     708 / 1000At timestep:     729 / 1000At timestep:     750 / 1000At timestep:     773 / 1000At timestep:     794 / 1000At timestep:     816 / 1000At timestep:     839 / 1000At timestep:     860 / 1000At timestep:     888 / 1000At timestep:     913 / 1000At timestep:     938 / 1000At timestep:     964 / 1000At timestep:     981 / 1000At timestep:     1002 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q1_env2_random_PointmassMedium-v0_21-11-2022_16-10-24/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -27.850000381469727
Train_BestReturn : -27.399999618530273
TimeSinceStart : 670.670211315155
Exploitation Critic Loss : 0.5756925344467163
Exploration Critic Loss : 13417083.0
Exploration Model Loss : 22.736419677734375
Eval_AverageReturn : -21.772727966308594
Eval_StdReturn : 3.642324686050415
Eval_MaxReturn : -13.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 22.772727272727273
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_alg_med_PointmassMedium-v0_21-11-2022_16-29-39 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_alg_med_PointmassMedium-v0_21-11-2022_16-29-39
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001537
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0015370845794677734
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 3.899573
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 3.8995730876922607
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 7.824420
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -150.0
TimeSinceStart : 7.824419736862183
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -148.800003
best mean reward -inf
running time 11.762054
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -148.8000030517578
TimeSinceStart : 11.762054443359375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -149.076920
best mean reward -inf
running time 15.644889
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -149.07691955566406
TimeSinceStart : 15.64488935470581
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -146.470581
best mean reward -inf
running time 19.729292
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -146.4705810546875
TimeSinceStart : 19.729292154312134
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -147.000000
best mean reward -inf
running time 23.717687
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -147.0
TimeSinceStart : 23.717687129974365
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -145.437500
best mean reward -inf
running time 28.271612
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -145.4375
TimeSinceStart : 28.271612405776978
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -145.944443
best mean reward -inf
running time 32.424988
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -145.94444274902344
TimeSinceStart : 32.42498803138733
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -145.901642
best mean reward -inf
running time 36.743176
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -145.90164184570312
TimeSinceStart : 36.743175983428955
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -146.044113
best mean reward -inf
running time 41.052828
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -146.0441131591797
TimeSinceStart : 41.05282759666443
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -146.413330
best mean reward -inf
running time 50.713838
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -146.413330078125
TimeSinceStart : 50.71383762359619
Exploitation Critic Loss : 0.08192884176969528
Exploration Critic Loss : 909250.9375
Exploration Model Loss : 54.93998336791992
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -146.679016
best mean reward -inf
running time 60.402085
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -146.67901611328125
TimeSinceStart : 60.402085304260254
Exploitation Critic Loss : 0.3594663441181183
Exploration Critic Loss : 9586894.0
Exploration Model Loss : 61.05644226074219
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -144.415726
best mean reward -inf
running time 70.280317
At timestep:     56 / 1000At timestep:     106 / 1000At timestep:     129 / 1000At timestep:     189 / 1000At timestep:     218 / 1000At timestep:     290 / 1000At timestep:     324 / 1000At timestep:     392 / 1000At timestep:     442 / 1000At timestep:     592 / 1000At timestep:     646 / 1000At timestep:     724 / 1000At timestep:     830 / 1000At timestep:     980 / 1000At timestep:     1090 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -144.4157257080078
TimeSinceStart : 70.28031730651855
Exploitation Critic Loss : 0.1800939291715622
Exploration Critic Loss : 10495026.0
Exploration Model Loss : 39.847782135009766
Eval_AverageReturn : -71.80000305175781
Eval_StdReturn : 38.73878479003906
Eval_MaxReturn : -22.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 72.66666666666667
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -137.619995
best mean reward -137.619995
running time 81.450370
At timestep:     70 / 1000At timestep:     145 / 1000At timestep:     220 / 1000At timestep:     345 / 1000At timestep:     452 / 1000At timestep:     523 / 1000At timestep:     621 / 1000At timestep:     675 / 1000At timestep:     825 / 1000At timestep:     933 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -137.6199951171875
Train_BestReturn : -137.6199951171875
TimeSinceStart : 81.45036959648132
Exploitation Critic Loss : 0.5946024656295776
Exploration Critic Loss : 8167402.0
Exploration Model Loss : 41.571353912353516
Eval_AverageReturn : -90.36363983154297
Eval_StdReturn : 27.749767303466797
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 91.27272727272727
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -132.240005
best mean reward -132.240005
running time 92.102727
At timestep:     79 / 1000At timestep:     229 / 1000At timestep:     379 / 1000At timestep:     529 / 1000At timestep:     679 / 1000At timestep:     829 / 1000At timestep:     959 / 1000At timestep:     1109 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -132.24000549316406
Train_BestReturn : -132.24000549316406
TimeSinceStart : 92.10272741317749
Exploitation Critic Loss : 0.44791316986083984
Exploration Critic Loss : 8175220.0
Exploration Model Loss : 39.301788330078125
Eval_AverageReturn : -138.375
Eval_StdReturn : 23.832422256469727
Eval_MaxReturn : -78.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.625
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -123.320000
best mean reward -123.320000
running time 102.921770
At timestep:     38 / 1000At timestep:     62 / 1000At timestep:     132 / 1000At timestep:     249 / 1000At timestep:     276 / 1000At timestep:     341 / 1000At timestep:     391 / 1000At timestep:     487 / 1000At timestep:     546 / 1000At timestep:     574 / 1000At timestep:     622 / 1000At timestep:     650 / 1000At timestep:     740 / 1000At timestep:     797 / 1000At timestep:     833 / 1000At timestep:     898 / 1000At timestep:     975 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -123.31999969482422
Train_BestReturn : -123.31999969482422
TimeSinceStart : 102.9217700958252
Exploitation Critic Loss : 0.560646116733551
Exploration Critic Loss : 8060662.0
Exploration Model Loss : 33.79811096191406
Eval_AverageReturn : -56.0
Eval_StdReturn : 25.331140518188477
Eval_MaxReturn : -23.0
Eval_MinReturn : -116.0
Eval_AverageEpLen : 57.0
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -114.779999
best mean reward -114.779999
running time 114.675416
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -114.77999877929688
Train_BestReturn : -114.77999877929688
TimeSinceStart : 114.67541575431824
Exploitation Critic Loss : 1.9670242071151733
Exploration Critic Loss : 8039038.5
Exploration Model Loss : 42.321136474609375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -93.059998
best mean reward -93.059998
running time 126.387061
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -93.05999755859375
Train_BestReturn : -93.05999755859375
TimeSinceStart : 126.38706064224243
Exploitation Critic Loss : 0.7416495084762573
Exploration Critic Loss : 7726645.0
Exploration Model Loss : 23.135650634765625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -71.589996
best mean reward -71.589996
running time 137.928944
At timestep:     35 / 1000At timestep:     55 / 1000At timestep:     97 / 1000At timestep:     144 / 1000At timestep:     181 / 1000At timestep:     228 / 1000At timestep:     256 / 1000At timestep:     327 / 1000At timestep:     356 / 1000At timestep:     380 / 1000At timestep:     427 / 1000At timestep:     467 / 1000At timestep:     502 / 1000At timestep:     544 / 1000At timestep:     586 / 1000At timestep:     629 / 1000At timestep:     658 / 1000At timestep:     692 / 1000At timestep:     721 / 1000At timestep:     754 / 1000At timestep:     788 / 1000At timestep:     835 / 1000At timestep:     897 / 1000At timestep:     920 / 1000At timestep:     937 / 1000At timestep:     969 / 1000At timestep:     992 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -71.58999633789062
Train_BestReturn : -71.58999633789062
TimeSinceStart : 137.92894387245178
Exploitation Critic Loss : 0.7665638327598572
Exploration Critic Loss : 13852023.0
Exploration Model Loss : 34.93351745605469
Eval_AverageReturn : -35.60714340209961
Eval_StdReturn : 11.805989265441895
Eval_MaxReturn : -16.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 36.607142857142854
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -55.599998
best mean reward -55.599998
running time 151.871369
At timestep:     31 / 1000At timestep:     57 / 1000At timestep:     88 / 1000At timestep:     152 / 1000At timestep:     173 / 1000At timestep:     205 / 1000At timestep:     247 / 1000At timestep:     268 / 1000At timestep:     303 / 1000At timestep:     325 / 1000At timestep:     356 / 1000At timestep:     383 / 1000At timestep:     399 / 1000At timestep:     418 / 1000At timestep:     436 / 1000At timestep:     460 / 1000At timestep:     481 / 1000At timestep:     537 / 1000At timestep:     562 / 1000At timestep:     590 / 1000At timestep:     647 / 1000At timestep:     673 / 1000At timestep:     709 / 1000At timestep:     732 / 1000At timestep:     762 / 1000At timestep:     786 / 1000At timestep:     830 / 1000At timestep:     862 / 1000At timestep:     887 / 1000At timestep:     927 / 1000At timestep:     957 / 1000At timestep:     989 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -55.599998474121094
Train_BestReturn : -55.599998474121094
TimeSinceStart : 151.87136888504028
Exploitation Critic Loss : 1.8537547588348389
Exploration Critic Loss : 16037107.0
Exploration Model Loss : 30.710556030273438
Eval_AverageReturn : -29.606060028076172
Eval_StdReturn : 11.186949729919434
Eval_MaxReturn : -15.0
Eval_MinReturn : -63.0
Eval_AverageEpLen : 30.606060606060606
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -44.759998
best mean reward -44.759998
running time 166.716928
At timestep:     22 / 1000At timestep:     49 / 1000At timestep:     88 / 1000At timestep:     114 / 1000At timestep:     165 / 1000At timestep:     196 / 1000At timestep:     281 / 1000At timestep:     333 / 1000At timestep:     365 / 1000At timestep:     399 / 1000At timestep:     428 / 1000At timestep:     452 / 1000At timestep:     496 / 1000At timestep:     560 / 1000At timestep:     601 / 1000At timestep:     631 / 1000At timestep:     671 / 1000At timestep:     736 / 1000At timestep:     790 / 1000At timestep:     825 / 1000At timestep:     873 / 1000At timestep:     898 / 1000At timestep:     953 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -44.7599983215332
Train_BestReturn : -44.7599983215332
TimeSinceStart : 166.71692776679993
Exploitation Critic Loss : 0.8339985609054565
Exploration Critic Loss : 22170424.0
Exploration Model Loss : 29.5770263671875
Eval_AverageReturn : -41.66666793823242
Eval_StdReturn : 16.30609893798828
Eval_MaxReturn : -21.0
Eval_MinReturn : -84.0
Eval_AverageEpLen : 42.666666666666664
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -40.119999
best mean reward -40.119999
running time 180.656699
At timestep:     29 / 1000At timestep:     65 / 1000At timestep:     87 / 1000At timestep:     123 / 1000At timestep:     177 / 1000At timestep:     207 / 1000At timestep:     228 / 1000At timestep:     275 / 1000At timestep:     307 / 1000At timestep:     331 / 1000At timestep:     350 / 1000At timestep:     377 / 1000At timestep:     396 / 1000At timestep:     427 / 1000At timestep:     462 / 1000At timestep:     487 / 1000At timestep:     529 / 1000At timestep:     559 / 1000At timestep:     593 / 1000At timestep:     631 / 1000At timestep:     680 / 1000At timestep:     709 / 1000At timestep:     760 / 1000At timestep:     797 / 1000At timestep:     836 / 1000At timestep:     869 / 1000At timestep:     897 / 1000At timestep:     927 / 1000At timestep:     965 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -40.119998931884766
Train_BestReturn : -40.119998931884766
TimeSinceStart : 180.65669918060303
Exploitation Critic Loss : 0.9722704291343689
Exploration Critic Loss : 18134992.0
Exploration Model Loss : 29.198623657226562
Eval_AverageReturn : -32.86666488647461
Eval_StdReturn : 9.401182174682617
Eval_MaxReturn : -18.0
Eval_MinReturn : -53.0
Eval_AverageEpLen : 33.86666666666667
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -36.959999
best mean reward -36.959999
running time 195.240863
At timestep:     32 / 1000At timestep:     58 / 1000At timestep:     85 / 1000At timestep:     126 / 1000At timestep:     162 / 1000At timestep:     216 / 1000At timestep:     246 / 1000At timestep:     288 / 1000At timestep:     313 / 1000At timestep:     342 / 1000At timestep:     385 / 1000At timestep:     409 / 1000At timestep:     460 / 1000At timestep:     494 / 1000At timestep:     518 / 1000At timestep:     552 / 1000At timestep:     592 / 1000At timestep:     637 / 1000At timestep:     662 / 1000At timestep:     681 / 1000At timestep:     710 / 1000At timestep:     754 / 1000At timestep:     778 / 1000At timestep:     849 / 1000At timestep:     885 / 1000At timestep:     906 / 1000At timestep:     940 / 1000At timestep:     987 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -36.959999084472656
Train_BestReturn : -36.959999084472656
TimeSinceStart : 195.2408628463745
Exploitation Critic Loss : 0.2728993594646454
Exploration Critic Loss : 17067296.0
Exploration Model Loss : 29.064483642578125
Eval_AverageReturn : -33.75862121582031
Eval_StdReturn : 11.562886238098145
Eval_MaxReturn : -18.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 34.758620689655174
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -37.380001
best mean reward -36.959999
running time 209.909604
At timestep:     22 / 1000At timestep:     52 / 1000At timestep:     73 / 1000At timestep:     109 / 1000At timestep:     134 / 1000At timestep:     163 / 1000At timestep:     217 / 1000At timestep:     263 / 1000At timestep:     334 / 1000At timestep:     385 / 1000At timestep:     421 / 1000At timestep:     451 / 1000At timestep:     506 / 1000At timestep:     523 / 1000At timestep:     548 / 1000At timestep:     585 / 1000At timestep:     609 / 1000At timestep:     636 / 1000At timestep:     657 / 1000At timestep:     710 / 1000At timestep:     736 / 1000At timestep:     784 / 1000At timestep:     810 / 1000At timestep:     852 / 1000At timestep:     879 / 1000At timestep:     898 / 1000At timestep:     926 / 1000At timestep:     979 / 1000At timestep:     1003 / 1000