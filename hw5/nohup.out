


LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_dqn_PointmassMedium-v0_21-11-2022_21-48-43 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_dqn_PointmassMedium-v0_21-11-2022_21-48-43
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002245
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0022449493408203125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -145.000000
best mean reward -inf
running time 4.858802
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -145.0
TimeSinceStart : 4.858802318572998
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -147.692307
best mean reward -inf
running time 9.817425
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -147.6923065185547
TimeSinceStart : 9.817424774169922
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -148.500000
best mean reward -inf
running time 22.405549
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -148.5
TimeSinceStart : 22.40554928779602
Exploitation Critic Loss : 0.3527056574821472
Exploration Critic Loss : 1483813.5
Exploration Model Loss : 90.3641128540039
Exploitation Data q-values : 0.5046315789222717
Exploitation OOD q-values : 2.1488709449768066
Exploitation CQL Loss : 0.3527056574821472
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -148.846161
best mean reward -inf
running time 35.383298
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -148.84616088867188
TimeSinceStart : 35.38329768180847
Exploitation Critic Loss : 0.13331115245819092
Exploration Critic Loss : 6319525.5
Exploration Model Loss : 33.679405212402344
Exploitation Data q-values : 1.1894845962524414
Exploitation OOD q-values : 2.8796963691711426
Exploitation CQL Loss : 0.13331115245819092
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -149.090912
best mean reward -inf
running time 48.128552
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -149.09091186523438
TimeSinceStart : 48.128551721572876
Exploitation Critic Loss : 0.5847658514976501
Exploration Critic Loss : 7462857.0
Exploration Model Loss : 29.371231079101562
Exploitation Data q-values : 2.412165641784668
Exploitation OOD q-values : 4.258904457092285
Exploitation CQL Loss : 0.5847658514976501
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -149.250000
best mean reward -inf
running time 61.152179
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -149.25
TimeSinceStart : 61.152179479599
Exploitation Critic Loss : 1.1949682235717773
Exploration Critic Loss : 11547420.0
Exploration Model Loss : 23.626449584960938
Exploitation Data q-values : 4.824070930480957
Exploitation OOD q-values : 6.818946361541748
Exploitation CQL Loss : 1.1949682235717773
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -149.347824
best mean reward -inf
running time 74.133192
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -149.3478240966797
TimeSinceStart : 74.13319206237793
Exploitation Critic Loss : 1.5148794651031494
Exploration Critic Loss : 15179275.0
Exploration Model Loss : 22.31939697265625
Exploitation Data q-values : 7.760841369628906
Exploitation OOD q-values : 10.041659355163574
Exploitation CQL Loss : 1.5148794651031494
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -149.433960
best mean reward -inf
running time 87.891235
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -149.4339599609375
TimeSinceStart : 87.89123487472534
Exploitation Critic Loss : 2.4024062156677246
Exploration Critic Loss : 13991382.0
Exploration Model Loss : 21.901123046875
Exploitation Data q-values : 11.257026672363281
Exploitation OOD q-values : 13.777750015258789
Exploitation CQL Loss : 2.4024062156677246
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -149.500000
best mean reward -inf
running time 101.096516
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -149.5
TimeSinceStart : 101.09651589393616
Exploitation Critic Loss : 4.631613731384277
Exploration Critic Loss : 14638198.0
Exploration Model Loss : 21.754013061523438
Exploitation Data q-values : 14.634136199951172
Exploitation OOD q-values : 17.451053619384766
Exploitation CQL Loss : 4.631613731384277
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -148.850754
best mean reward -inf
running time 113.950066
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -148.8507537841797
TimeSinceStart : 113.95006585121155
Exploitation Critic Loss : 11.774015426635742
Exploration Critic Loss : 19235704.0
Exploration Model Loss : 21.701522827148438
Exploitation Data q-values : 18.22599220275879
Exploitation OOD q-values : 20.918495178222656
Exploitation CQL Loss : 11.774015426635742
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -148.945206
best mean reward -inf
running time 126.978703
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -148.94520568847656
TimeSinceStart : 126.97870302200317
Exploitation Critic Loss : 142.3060760498047
Exploration Critic Loss : 28317668.0
Exploration Model Loss : 21.6824951171875
Exploitation Data q-values : 21.445587158203125
Exploitation OOD q-values : 25.327720642089844
Exploitation CQL Loss : 142.3060760498047
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -149.037506
best mean reward -inf
running time 140.721269
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -149.03750610351562
TimeSinceStart : 140.72126936912537
Exploitation Critic Loss : 33.85567855834961
Exploration Critic Loss : 12451552.0
Exploration Model Loss : 21.675689697265625
Exploitation Data q-values : 26.81133270263672
Exploitation OOD q-values : 30.756986618041992
Exploitation CQL Loss : 33.85567855834961
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -149.114944
best mean reward -inf
running time 154.314456
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -149.1149444580078
TimeSinceStart : 154.31445622444153
Exploitation Critic Loss : 176.22821044921875
Exploration Critic Loss : 29524220.0
Exploration Model Loss : 21.673614501953125
Exploitation Data q-values : 36.78428649902344
Exploitation OOD q-values : 43.96866989135742
Exploitation CQL Loss : 176.22821044921875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -149.172043
best mean reward -inf
running time 167.332767
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -149.1720428466797
TimeSinceStart : 167.3327672481537
Exploitation Critic Loss : 105.91458892822266
Exploration Critic Loss : 28685058.0
Exploration Model Loss : 21.672805786132812
Exploitation Data q-values : 54.488853454589844
Exploitation OOD q-values : 66.06423950195312
Exploitation CQL Loss : 105.91458892822266
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -148.630005
best mean reward -inf
running time 180.657928
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -148.6300048828125
TimeSinceStart : 180.6579282283783
Exploitation Critic Loss : 71.30377197265625
Exploration Critic Loss : 21630080.0
Exploration Model Loss : 21.672500610351562
Exploitation Data q-values : 69.77426147460938
Exploitation OOD q-values : 78.82948303222656
Exploitation CQL Loss : 71.30377197265625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -148.929993
best mean reward -148.929993
running time 193.801264
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -148.92999267578125
Train_BestReturn : -148.92999267578125
TimeSinceStart : 193.80126404762268
Exploitation Critic Loss : 90.54804992675781
Exploration Critic Loss : 16044239.0
Exploration Model Loss : 21.6724853515625
Exploitation Data q-values : 80.69046020507812
Exploitation OOD q-values : 90.44816589355469
Exploitation CQL Loss : 90.54804992675781
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -148.929993
best mean reward -148.929993
running time 206.196914
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -148.92999267578125
Train_BestReturn : -148.92999267578125
TimeSinceStart : 206.19691371917725
Exploitation Critic Loss : 146.3123321533203
Exploration Critic Loss : 13649972.0
Exploration Model Loss : 21.672119140625
Exploitation Data q-values : 94.8160400390625
Exploitation OOD q-values : 106.62261962890625
Exploitation CQL Loss : 146.3123321533203
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -148.929993
best mean reward -148.929993
running time 217.255707
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -148.92999267578125
Train_BestReturn : -148.92999267578125
TimeSinceStart : 217.2557065486908
Exploitation Critic Loss : 150.3827362060547
Exploration Critic Loss : 15592040.0
Exploration Model Loss : 21.671539306640625
Exploitation Data q-values : 119.59542846679688
Exploitation OOD q-values : 133.05731201171875
Exploitation CQL Loss : 150.3827362060547
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -148.929993
best mean reward -148.929993
running time 228.405136
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -148.92999267578125
Train_BestReturn : -148.92999267578125
TimeSinceStart : 228.40513586997986
Exploitation Critic Loss : 3365.115234375
Exploration Critic Loss : 18120050.0
Exploration Model Loss : 21.670867919921875
Exploitation Data q-values : 148.2999725341797
Exploitation OOD q-values : 164.2803192138672
Exploitation CQL Loss : 3365.115234375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -148.699997
best mean reward -148.699997
running time 239.405649
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -148.6999969482422
Train_BestReturn : -148.6999969482422
TimeSinceStart : 239.40564894676208
Exploitation Critic Loss : 288.078369140625
Exploration Critic Loss : 31159000.0
Exploration Model Loss : 21.670257568359375
Exploitation Data q-values : 153.0691680908203
Exploitation OOD q-values : 164.76986694335938
Exploitation CQL Loss : 288.078369140625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -148.699997
best mean reward -148.699997
running time 250.210313
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -148.6999969482422
Train_BestReturn : -148.6999969482422
TimeSinceStart : 250.21031308174133
Exploitation Critic Loss : 1243.11279296875
Exploration Critic Loss : 17281838.0
Exploration Model Loss : 21.669830322265625
Exploitation Data q-values : 174.32723999023438
Exploitation OOD q-values : 188.31558227539062
Exploitation CQL Loss : 1243.11279296875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -148.699997
best mean reward -148.699997
running time 260.987399
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -148.6999969482422
Train_BestReturn : -148.6999969482422
TimeSinceStart : 260.9873991012573
Exploitation Critic Loss : 9228.548828125
Exploration Critic Loss : 16605288.0
Exploration Model Loss : 21.669235229492188
Exploitation Data q-values : 202.5787353515625
Exploitation OOD q-values : 224.638916015625
Exploitation CQL Loss : 9228.548828125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -148.699997
best mean reward -148.699997
running time 271.752993
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -148.6999969482422
Train_BestReturn : -148.6999969482422
TimeSinceStart : 271.7529926300049
Exploitation Critic Loss : 3216.247802734375
Exploration Critic Loss : 20148660.0
Exploration Model Loss : 21.668731689453125
Exploitation Data q-values : 241.00428771972656
Exploitation OOD q-values : 273.01910400390625
Exploitation CQL Loss : 3216.247802734375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -148.699997
best mean reward -148.699997
running time 282.660168
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -148.6999969482422
Train_BestReturn : -148.6999969482422
TimeSinceStart : 282.6601676940918
Exploitation Critic Loss : 605.4375610351562
Exploration Critic Loss : 31875782.0
Exploration Model Loss : 21.668487548828125
Exploitation Data q-values : 278.3635559082031
Exploitation OOD q-values : 307.31365966796875
Exploitation CQL Loss : 605.4375610351562
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -149.169998
best mean reward -148.699997
running time 293.411175
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     873 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -149.1699981689453
Train_BestReturn : -148.6999969482422
TimeSinceStart : 293.4111752510071
Exploitation Critic Loss : 617.9197998046875
Exploration Critic Loss : 30187908.0
Exploration Model Loss : 21.667938232421875
Exploitation Data q-values : 311.4688415527344
Exploitation OOD q-values : 343.69940185546875
Exploitation CQL Loss : 617.9197998046875
Eval_AverageReturn : -146.0
Eval_StdReturn : 9.797959327697754
Eval_MaxReturn : -122.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 146.14285714285714
Buffer size : 10001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -149.169998
best mean reward -148.699997
running time 304.176826
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -149.1699981689453
Train_BestReturn : -148.6999969482422
TimeSinceStart : 304.1768264770508
Exploitation Critic Loss : 1417.79833984375
Exploration Critic Loss : 21153740.0
Exploration Model Loss : 21.66729736328125
Exploitation Data q-values : 345.20574951171875
Exploitation OOD q-values : 376.7418212890625
Exploitation CQL Loss : 1417.79833984375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -149.169998
best mean reward -148.699997
running time 315.194532
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -149.1699981689453
Train_BestReturn : -148.6999969482422
TimeSinceStart : 315.19453167915344
Exploitation Critic Loss : 1175.092529296875
Exploration Critic Loss : 23707532.0
Exploration Model Loss : 21.6668701171875
Exploitation Data q-values : 396.3910217285156
Exploitation OOD q-values : 434.07940673828125
Exploitation CQL Loss : 1175.092529296875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -149.169998
best mean reward -148.699997
running time 326.064911
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     418 / 1000At timestep:     537 / 1000At timestep:     687 / 1000At timestep:     725 / 1000At timestep:     875 / 1000At timestep:     976 / 1000At timestep:     1126 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -149.1699981689453
Train_BestReturn : -148.6999969482422
TimeSinceStart : 326.06491136550903
Exploitation Critic Loss : 3573.45947265625
Exploration Critic Loss : 19509550.0
Exploration Model Loss : 21.66644287109375
Exploitation Data q-values : 439.01165771484375
Exploitation OOD q-values : 476.12939453125
Exploitation CQL Loss : 3573.45947265625
Eval_AverageReturn : -124.66666412353516
Eval_StdReturn : 35.91038131713867
Eval_MaxReturn : -37.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 125.11111111111111
Buffer size : 10001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -149.169998
best mean reward -148.699997
running time 337.217199
At timestep:     79 / 1000At timestep:     206 / 1000At timestep:     272 / 1000At timestep:     338 / 1000At timestep:     386 / 1000At timestep:     462 / 1000At timestep:     516 / 1000At timestep:     605 / 1000At timestep:     697 / 1000At timestep:     770 / 1000At timestep:     844 / 1000At timestep:     891 / 1000At timestep:     956 / 1000At timestep:     1034 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -149.1699981689453
Train_BestReturn : -148.6999969482422
TimeSinceStart : 337.2171993255615
Exploitation Critic Loss : 3057.462158203125
Exploration Critic Loss : 22510888.0
Exploration Model Loss : 21.665847778320312
Exploitation Data q-values : 438.2685852050781
Exploitation OOD q-values : 470.35205078125
Exploitation CQL Loss : 3057.462158203125
Eval_AverageReturn : -72.85713958740234
Eval_StdReturn : 19.671796798706055
Eval_MaxReturn : -46.0
Eval_MinReturn : -126.0
Eval_AverageEpLen : 73.85714285714286
Buffer size : 10001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -149.770004
best mean reward -148.699997
running time 349.346607
At timestep:     150 / 1000At timestep:     272 / 1000At timestep:     356 / 1000At timestep:     506 / 1000At timestep:     656 / 1000At timestep:     806 / 1000At timestep:     956 / 1000At timestep:     1106 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -149.77000427246094
Train_BestReturn : -148.6999969482422
TimeSinceStart : 349.34660720825195
Exploitation Critic Loss : 3341.07275390625
Exploration Critic Loss : 18753798.0
Exploration Model Loss : 21.66522216796875
Exploitation Data q-values : 491.47869873046875
Exploitation OOD q-values : 530.7442626953125
Exploitation CQL Loss : 3341.07275390625
Eval_AverageReturn : -138.0
Eval_StdReturn : 22.8527889251709
Eval_MaxReturn : -83.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.25
Buffer size : 10001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -149.770004
best mean reward -148.699997
running time 360.488620
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1047 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -149.77000427246094
Train_BestReturn : -148.6999969482422
TimeSinceStart : 360.48861956596375
Exploitation Critic Loss : 9832.0166015625
Exploration Critic Loss : 24978056.0
Exploration Model Loss : 21.664688110351562
Exploitation Data q-values : 511.5767517089844
Exploitation OOD q-values : 559.0738525390625
Exploitation CQL Loss : 9832.0166015625
Eval_AverageReturn : -149.42857360839844
Eval_StdReturn : 1.3997083902359009
Eval_MaxReturn : -146.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 149.57142857142858
Buffer size : 10001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -149.770004
best mean reward -148.699997
running time 371.723186
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -149.77000427246094
Train_BestReturn : -148.6999969482422
TimeSinceStart : 371.72318625450134
Exploitation Critic Loss : 5213.2783203125
Exploration Critic Loss : 16557866.0
Exploration Model Loss : 21.664077758789062
Exploitation Data q-values : 568.6412963867188
Exploitation OOD q-values : 620.5197143554688
Exploitation CQL Loss : 5213.2783203125
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 10001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -149.770004
best mean reward -148.699997
running time 382.780927
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -149.77000427246094
Train_BestReturn : -148.6999969482422
TimeSinceStart : 382.78092670440674
Exploitation Critic Loss : 5254.828125
Exploration Critic Loss : 22763900.0
Exploration Model Loss : 21.663497924804688
Exploitation Data q-values : 655.8712158203125
Exploitation OOD q-values : 714.4566650390625
Exploitation CQL Loss : 5254.828125
Eval_AverageReturn : -144.14285278320312
Eval_StdReturn : 14.34701156616211
Eval_MaxReturn : -109.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 144.28571428571428
Buffer size : 10001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -149.770004
best mean reward -148.699997
running time 393.845608
At timestep:     150 / 1000At timestep:     275 / 1000At timestep:     425 / 1000At timestep:     575 / 1000At timestep:     691 / 1000At timestep:     841 / 1000At timestep:     954 / 1000At timestep:     994 / 1000At timestep:     1144 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -149.77000427246094
Train_BestReturn : -148.6999969482422
TimeSinceStart : 393.84560799598694
Exploitation Critic Loss : 4998.2216796875
Exploration Critic Loss : 17647508.0
Exploration Model Loss : 21.66290283203125
Exploitation Data q-values : 724.706298828125
Exploitation OOD q-values : 779.8355712890625
Exploitation CQL Loss : 4998.2216796875
Eval_AverageReturn : -126.66666412353516
Eval_StdReturn : 34.58644485473633
Eval_MaxReturn : -39.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 127.11111111111111
Buffer size : 10001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 405.299191
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     732 / 1000At timestep:     882 / 1000At timestep:     983 / 1000At timestep:     1133 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 405.2991909980774
Exploitation Critic Loss : 13765.046875
Exploration Critic Loss : 20845340.0
Exploration Model Loss : 21.662322998046875
Exploitation Data q-values : 793.395751953125
Exploitation OOD q-values : 860.04443359375
Exploitation CQL Loss : 13765.046875
Eval_AverageReturn : -141.375
Eval_StdReturn : 16.82956886291504
Eval_MaxReturn : -100.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 141.625
Buffer size : 10001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 416.931080
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 416.93108010292053
Exploitation Critic Loss : 9265.125
Exploration Critic Loss : 26570002.0
Exploration Model Loss : 21.661712646484375
Exploitation Data q-values : 872.4388427734375
Exploitation OOD q-values : 945.0865478515625
Exploitation CQL Loss : 9265.125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 428.035185
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     874 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 428.0351846218109
Exploitation Critic Loss : 6905.048828125
Exploration Critic Loss : 10661746.0
Exploration Model Loss : 21.661148071289062
Exploitation Data q-values : 928.416015625
Exploitation OOD q-values : 1006.9532470703125
Exploitation CQL Loss : 6905.048828125
Eval_AverageReturn : -146.14285278320312
Eval_StdReturn : 9.448031425476074
Eval_MaxReturn : -123.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 146.28571428571428
Buffer size : 10001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 438.978014
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     577 / 1000At timestep:     727 / 1000At timestep:     877 / 1000At timestep:     994 / 1000At timestep:     1144 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 438.97801423072815
Exploitation Critic Loss : 28769.84765625
Exploration Critic Loss : 14099642.0
Exploration Model Loss : 21.660614013671875
Exploitation Data q-values : 1032.753662109375
Exploitation OOD q-values : 1115.98095703125
Exploitation CQL Loss : 28769.84765625
Eval_AverageReturn : -142.75
Eval_StdReturn : 12.803808212280273
Eval_MaxReturn : -116.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 143.0
Buffer size : 10001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 449.876206
At timestep:     150 / 1000At timestep:     261 / 1000At timestep:     411 / 1000At timestep:     561 / 1000At timestep:     711 / 1000At timestep:     861 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 449.8762056827545
Exploitation Critic Loss : 18691.97265625
Exploration Critic Loss : 21155708.0
Exploration Model Loss : 21.66021728515625
Exploitation Data q-values : 1096.2666015625
Exploitation OOD q-values : 1190.935546875
Exploitation CQL Loss : 18691.97265625
Eval_AverageReturn : -144.2857208251953
Eval_StdReturn : 13.997082710266113
Eval_MaxReturn : -110.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 144.42857142857142
Buffer size : 10001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 460.822295
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 460.82229495048523
Exploitation Critic Loss : 44094.95703125
Exploration Critic Loss : 23637404.0
Exploration Model Loss : 21.659622192382812
Exploitation Data q-values : 1195.9583740234375
Exploitation OOD q-values : 1292.2962646484375
Exploitation CQL Loss : 44094.95703125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 471.765168
At timestep:     71 / 1000At timestep:     221 / 1000At timestep:     371 / 1000At timestep:     445 / 1000At timestep:     503 / 1000At timestep:     653 / 1000At timestep:     758 / 1000At timestep:     908 / 1000At timestep:     1058 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 471.7651677131653
Exploitation Critic Loss : 71285.078125
Exploration Critic Loss : 15170317.0
Exploration Model Loss : 21.659591674804688
Exploitation Data q-values : 1279.878173828125
Exploitation OOD q-values : 1365.236083984375
Exploitation CQL Loss : 71285.078125
Eval_AverageReturn : -117.11111450195312
Eval_StdReturn : 38.526885986328125
Eval_MaxReturn : -57.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 117.55555555555556
Buffer size : 10001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 482.856922
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 482.8569219112396
Exploitation Critic Loss : 14560.48046875
Exploration Critic Loss : 18951668.0
Exploration Model Loss : 21.65899658203125
Exploitation Data q-values : 1317.8076171875
Exploitation OOD q-values : 1410.0687255859375
Exploitation CQL Loss : 14560.48046875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 493.602644
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 493.6026439666748
Exploitation Critic Loss : 38185.09375
Exploration Critic Loss : 22106264.0
Exploration Model Loss : 21.658493041992188
Exploitation Data q-values : 1418.165283203125
Exploitation OOD q-values : 1522.3583984375
Exploitation CQL Loss : 38185.09375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 504.272679
At timestep:     30 / 1000At timestep:     180 / 1000At timestep:     330 / 1000At timestep:     480 / 1000At timestep:     630 / 1000At timestep:     780 / 1000At timestep:     930 / 1000At timestep:     1080 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 504.2726786136627
Exploitation Critic Loss : 8732.0654296875
Exploration Critic Loss : 13692852.0
Exploration Model Loss : 21.657943725585938
Exploitation Data q-values : 1469.4588623046875
Exploitation OOD q-values : 1572.141845703125
Exploitation CQL Loss : 8732.0654296875
Eval_AverageReturn : -134.875
Eval_StdReturn : 40.01698684692383
Eval_MaxReturn : -29.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 135.0
Buffer size : 10001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -150.000000
best mean reward -148.699997
running time 515.243749
At timestep:     125 / 1000At timestep:     275 / 1000At timestep:     425 / 1000At timestep:     575 / 1000At timestep:     682 / 1000At timestep:     774 / 1000At timestep:     924 / 1000At timestep:     1074 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -150.0
Train_BestReturn : -148.6999969482422
TimeSinceStart : 515.2437493801117
Exploitation Critic Loss : 27688.765625
Exploration Critic Loss : 18327386.0
Exploration Model Loss : 21.657363891601562
Exploitation Data q-values : 1536.868896484375
Exploitation OOD q-values : 1661.193603515625
Exploitation CQL Loss : 27688.765625
Eval_AverageReturn : -133.875
Eval_StdReturn : 22.396636962890625
Eval_MaxReturn : -91.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 134.25
Buffer size : 10001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -149.619995
best mean reward -148.699997
running time 526.256617
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     529 / 1000At timestep:     679 / 1000At timestep:     805 / 1000At timestep:     926 / 1000At timestep:     975 / 1000At timestep:     1048 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -149.6199951171875
Train_BestReturn : -148.6999969482422
TimeSinceStart : 526.2566165924072
Exploitation Critic Loss : 41178.2109375
Exploration Critic Loss : 12353310.0
Exploration Model Loss : 21.6568603515625
Exploitation Data q-values : 1673.9437255859375
Exploitation OOD q-values : 1806.37451171875
Exploitation CQL Loss : 41178.2109375
Eval_AverageReturn : -115.88888549804688
Eval_StdReturn : 37.60154724121094
Eval_MaxReturn : -48.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 116.44444444444444
Buffer size : 10001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -149.619995
best mean reward -148.699997
running time 537.496152
At timestep:     150 / 1000At timestep:     253 / 1000At timestep:     403 / 1000At timestep:     553 / 1000At timestep:     703 / 1000At timestep:     853 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -149.6199951171875
Train_BestReturn : -148.6999969482422
TimeSinceStart : 537.4961516857147
Exploitation Critic Loss : 76868.1171875
Exploration Critic Loss : 18328404.0
Exploration Model Loss : 21.6566162109375
Exploitation Data q-values : 1715.2498779296875
Exploitation OOD q-values : 1842.452392578125
Exploitation CQL Loss : 76868.1171875
Eval_AverageReturn : -143.14285278320312
Eval_StdReturn : 16.7965030670166
Eval_MaxReturn : -102.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 143.28571428571428
Buffer size : 10001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -149.619995
best mean reward -148.699997
running time 548.894854
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     716 / 1000At timestep:     758 / 1000At timestep:     908 / 1000At timestep:     988 / 1000At timestep:     1092 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -149.6199951171875
Train_BestReturn : -148.6999969482422
TimeSinceStart : 548.8948543071747
Exploitation Critic Loss : 76472.0859375
Exploration Critic Loss : 16672925.0
Exploration Model Loss : 21.656219482421875
Exploitation Data q-values : 1887.49169921875
Exploitation OOD q-values : 2026.89599609375
Exploitation CQL Loss : 76472.0859375
Eval_AverageReturn : -120.88888549804688
Eval_StdReturn : 37.607460021972656
Eval_MaxReturn : -41.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 121.33333333333333
Buffer size : 10001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -149.619995
best mean reward -148.699997
running time 560.034372
At timestep:     45 / 1000At timestep:     195 / 1000At timestep:     275 / 1000At timestep:     425 / 1000At timestep:     522 / 1000At timestep:     607 / 1000At timestep:     700 / 1000At timestep:     784 / 1000At timestep:     922 / 1000At timestep:     1062 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q2_dqn_PointmassMedium-v0_21-11-2022_21-48-43/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -149.6199951171875
Train_BestReturn : -148.6999969482422
TimeSinceStart : 560.0343718528748
Exploitation Critic Loss : 149437.71875
Exploration Critic Loss : 17890384.0
Exploration Model Loss : 21.65557861328125
Exploitation Data q-values : 2033.1766357421875
Exploitation OOD q-values : 2240.330322265625
Exploitation CQL Loss : 149437.71875
Eval_AverageReturn : -105.4000015258789
Eval_StdReturn : 34.35171127319336
Eval_MaxReturn : -44.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 106.2
Buffer size : 10001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_dqn_PointmassMedium-v0_21-11-2022_22-03-48 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_dqn_PointmassMedium-v0_21-11-2022_22-03-48
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001451
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0014507770538330078
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 3.772560
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 3.7725603580474854
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 7.580741
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -150.0
TimeSinceStart : 7.580740928649902
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 17.188736
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -150.0
TimeSinceStart : 17.18873643875122
Exploitation Critic Loss : 4.790157254319638e-05
Exploration Critic Loss : 1388970.25
Exploration Model Loss : 86.65748596191406
Exploitation Data q-values : -0.036131348460912704
Exploitation OOD q-values : 1.5727646350860596
Exploitation CQL Loss : 4.790157254319638e-05
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 27.013266
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -150.0
TimeSinceStart : 27.01326608657837
Exploitation Critic Loss : 3.5740558814723045e-05
Exploration Critic Loss : 10297323.0
Exploration Model Loss : 46.94083786010742
Exploitation Data q-values : -0.012576473876833916
Exploitation OOD q-values : 1.5969258546829224
Exploitation CQL Loss : 3.5740558814723045e-05
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -149.666672
best mean reward -inf
running time 36.968503
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -149.6666717529297
TimeSinceStart : 36.96850252151489
Exploitation Critic Loss : 0.0011650308733806014
Exploration Critic Loss : 8676529.0
Exploration Model Loss : 38.9199104309082
Exploitation Data q-values : 0.023321527987718582
Exploitation OOD q-values : 1.6338720321655273
Exploitation CQL Loss : 0.0011650308733806014
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -149.725006
best mean reward -inf
running time 46.507832
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -149.72500610351562
TimeSinceStart : 46.50783157348633
Exploitation Critic Loss : 0.09296578913927078
Exploration Critic Loss : 9012142.0
Exploration Model Loss : 67.04654693603516
Exploitation Data q-values : 0.17397916316986084
Exploitation OOD q-values : 1.8164401054382324
Exploitation CQL Loss : 0.09296578913927078
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -149.760864
best mean reward -inf
running time 56.077517
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -149.7608642578125
TimeSinceStart : 56.07751727104187
Exploitation Critic Loss : 0.3174659311771393
Exploration Critic Loss : 12566218.0
Exploration Model Loss : 25.557907104492188
Exploitation Data q-values : 0.6340232491493225
Exploitation OOD q-values : 2.353654384613037
Exploitation CQL Loss : 0.3174659311771393
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -149.792450
best mean reward -inf
running time 65.812955
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -149.79244995117188
TimeSinceStart : 65.8129551410675
Exploitation Critic Loss : 0.9289804697036743
Exploration Critic Loss : 13017873.0
Exploration Model Loss : 22.996185302734375
Exploitation Data q-values : 1.4258050918579102
Exploitation OOD q-values : 3.2974748611450195
Exploitation CQL Loss : 0.9289804697036743
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -149.816666
best mean reward -inf
running time 75.568717
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -149.81666564941406
TimeSinceStart : 75.56871700286865
Exploitation Critic Loss : 13.639121055603027
Exploration Critic Loss : 11945956.0
Exploration Model Loss : 22.233123779296875
Exploitation Data q-values : 2.066478729248047
Exploitation OOD q-values : 3.949627637863159
Exploitation CQL Loss : 13.639121055603027
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -149.833328
best mean reward -inf
running time 85.302886
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     494 / 1000At timestep:     633 / 1000At timestep:     783 / 1000At timestep:     933 / 1000At timestep:     1083 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -149.8333282470703
TimeSinceStart : 85.30288553237915
Exploitation Critic Loss : 2.436326503753662
Exploration Critic Loss : 13133093.0
Exploration Model Loss : 21.97113037109375
Exploitation Data q-values : 5.074493885040283
Exploitation OOD q-values : 7.2287750244140625
Exploitation CQL Loss : 2.436326503753662
Eval_AverageReturn : -135.125
Eval_StdReturn : 35.04082489013672
Eval_MaxReturn : -43.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 135.375
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -149.849319
best mean reward -inf
running time 95.289378
At timestep:     150 / 1000At timestep:     202 / 1000At timestep:     265 / 1000At timestep:     345 / 1000At timestep:     421 / 1000At timestep:     543 / 1000At timestep:     693 / 1000At timestep:     772 / 1000At timestep:     838 / 1000At timestep:     959 / 1000At timestep:     1109 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -149.8493194580078
TimeSinceStart : 95.28937792778015
Exploitation Critic Loss : 6.171725273132324
Exploration Critic Loss : 14672231.0
Exploration Model Loss : 21.8773193359375
Exploitation Data q-values : 5.204023838043213
Exploitation OOD q-values : 7.245757102966309
Exploitation CQL Loss : 6.171725273132324
Eval_AverageReturn : -100.09091186523438
Eval_StdReturn : 36.87929916381836
Eval_MaxReturn : -51.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 100.81818181818181
Buffer size : 10001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -149.862503
best mean reward -inf
running time 105.624124
At timestep:     52 / 1000At timestep:     101 / 1000At timestep:     133 / 1000At timestep:     229 / 1000At timestep:     257 / 1000At timestep:     295 / 1000At timestep:     330 / 1000At timestep:     395 / 1000At timestep:     459 / 1000At timestep:     496 / 1000At timestep:     536 / 1000At timestep:     583 / 1000At timestep:     610 / 1000At timestep:     651 / 1000At timestep:     717 / 1000At timestep:     759 / 1000At timestep:     848 / 1000At timestep:     923 / 1000At timestep:     968 / 1000At timestep:     986 / 1000At timestep:     1057 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -149.8625030517578
TimeSinceStart : 105.62412357330322
Exploitation Critic Loss : 6.364410400390625
Exploration Critic Loss : 16997576.0
Exploration Model Loss : 21.842864990234375
Exploitation Data q-values : 10.450583457946777
Exploitation OOD q-values : 13.213005065917969
Exploitation CQL Loss : 6.364410400390625
Eval_AverageReturn : -49.33333206176758
Eval_StdReturn : 20.19036293029785
Eval_MaxReturn : -17.0
Eval_MinReturn : -95.0
Eval_AverageEpLen : 50.333333333333336
Buffer size : 10001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -149.872086
best mean reward -inf
running time 116.741545
At timestep:     63 / 1000At timestep:     87 / 1000At timestep:     124 / 1000At timestep:     173 / 1000At timestep:     227 / 1000At timestep:     273 / 1000At timestep:     309 / 1000At timestep:     361 / 1000At timestep:     397 / 1000At timestep:     423 / 1000At timestep:     483 / 1000At timestep:     511 / 1000At timestep:     555 / 1000At timestep:     590 / 1000At timestep:     633 / 1000At timestep:     665 / 1000At timestep:     730 / 1000At timestep:     810 / 1000At timestep:     870 / 1000At timestep:     914 / 1000At timestep:     937 / 1000At timestep:     979 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -149.87208557128906
TimeSinceStart : 116.74154472351074
Exploitation Critic Loss : 14.611618995666504
Exploration Critic Loss : 26641964.0
Exploration Model Loss : 21.8302001953125
Exploitation Data q-values : 15.757769584655762
Exploitation OOD q-values : 19.160911560058594
Exploitation CQL Loss : 14.611618995666504
Eval_AverageReturn : -44.130435943603516
Eval_StdReturn : 14.486288070678711
Eval_MaxReturn : -22.0
Eval_MinReturn : -79.0
Eval_AverageEpLen : 45.130434782608695
Buffer size : 10001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -149.881714
best mean reward -inf
running time 128.583616
At timestep:     67 / 1000At timestep:     97 / 1000At timestep:     146 / 1000At timestep:     181 / 1000At timestep:     223 / 1000At timestep:     269 / 1000At timestep:     325 / 1000At timestep:     359 / 1000At timestep:     428 / 1000At timestep:     532 / 1000At timestep:     582 / 1000At timestep:     624 / 1000At timestep:     689 / 1000At timestep:     730 / 1000At timestep:     769 / 1000At timestep:     800 / 1000At timestep:     848 / 1000At timestep:     877 / 1000At timestep:     902 / 1000At timestep:     985 / 1000At timestep:     1020 / 1000


LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_no_scale_PointmassMedium-v0_21-11-2022_22-05-57 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_no_scale_PointmassMedium-v0_21-11-2022_22-05-57
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001581
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.001581430435180664
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -146.166672
best mean reward -inf
running time 4.885090
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -146.1666717529297
TimeSinceStart : 4.885089635848999
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -147.615387
best mean reward -inf
running time 9.645785
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -149.8817138671875
TimeSinceStart : 128.58361554145813
Exploitation Critic Loss : 7.818961143493652
Exploration Critic Loss : 10270903.0
Exploration Model Loss : 21.825851440429688
Exploitation Data q-values : 21.657508850097656
Exploitation OOD q-values : 25.084423065185547
Exploitation CQL Loss : 7.818961143493652
Eval_AverageReturn : -47.57143020629883
Eval_StdReturn : 19.219463348388672
Eval_MaxReturn : -24.0
Eval_MinReturn : -103.0
Eval_AverageEpLen : 48.57142857142857
Buffer size : 10001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -149.889999
best mean reward -inf
running time 141.809855
At timestep:     31 / 1000At timestep:     71 / 1000At timestep:     133 / 1000At timestep:     177 / 1000At timestep:     236 / 1000At timestep:     345 / 1000At timestep:     402 / 1000At timestep:     436 / 1000At timestep:     473 / 1000At timestep:     541 / 1000At timestep:     584 / 1000At timestep:     625 / 1000At timestep:     716 / 1000At timestep:     753 / 1000At timestep:     830 / 1000At timestep:     906 / 1000At timestep:     948 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -147.61538696289062
TimeSinceStart : 9.645785331726074
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -148.449997
best mean reward -inf
running time 21.691351
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -149.88999938964844
TimeSinceStart : 141.80985522270203
Exploitation Critic Loss : 15.00887680053711
Exploration Critic Loss : 8843053.0
Exploration Model Loss : 21.823715209960938
Exploitation Data q-values : 30.27485466003418
Exploitation OOD q-values : 34.517215728759766
Exploitation CQL Loss : 15.00887680053711
Eval_AverageReturn : -55.22222137451172
Eval_StdReturn : 21.014692306518555
Eval_MaxReturn : -30.0
Eval_MinReturn : -108.0
Eval_AverageEpLen : 56.22222222222222
Buffer size : 10001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -149.889999
best mean reward -149.889999
running time 154.860764
At timestep:     41 / 1000At timestep:     73 / 1000At timestep:     113 / 1000At timestep:     148 / 1000At timestep:     196 / 1000At timestep:     257 / 1000At timestep:     297 / 1000At timestep:     366 / 1000At timestep:     476 / 1000At timestep:     543 / 1000At timestep:     587 / 1000At timestep:     616 / 1000At timestep:     674 / 1000At timestep:     730 / 1000At timestep:     778 / 1000At timestep:     823 / 1000At timestep:     860 / 1000At timestep:     903 / 1000At timestep:     998 / 1000At timestep:     1060 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -148.4499969482422
TimeSinceStart : 21.69135069847107
Exploitation Critic Loss : 0.0030804609414190054
Exploration Critic Loss : 1435599.0
Exploration Model Loss : 87.37114715576172
Exploitation Data q-values : -3.648653507232666
Exploitation OOD q-values : -2.0477118492126465
Exploitation CQL Loss : 0.16317462921142578
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -148.807693
best mean reward -inf
running time 34.208034
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -149.88999938964844
Train_BestReturn : -149.88999938964844
TimeSinceStart : 154.86076426506042
Exploitation Critic Loss : 30.720867156982422
Exploration Critic Loss : 9751234.0
Exploration Model Loss : 21.823043823242188
Exploitation Data q-values : 35.45841598510742
Exploitation OOD q-values : 39.288421630859375
Exploitation CQL Loss : 30.720867156982422
Eval_AverageReturn : -52.0
Eval_StdReturn : 20.02248764038086
Eval_MaxReturn : -28.0
Eval_MinReturn : -109.0
Eval_AverageEpLen : 53.0
Buffer size : 10001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -149.889999
best mean reward -149.889999
running time 169.021274
At timestep:     72 / 1000At timestep:     101 / 1000At timestep:     154 / 1000At timestep:     225 / 1000At timestep:     280 / 1000At timestep:     392 / 1000At timestep:     507 / 1000At timestep:     584 / 1000At timestep:     687 / 1000At timestep:     748 / 1000At timestep:     875 / 1000At timestep:     960 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -148.8076934814453
TimeSinceStart : 34.2080340385437
Exploitation Critic Loss : 0.09882498532533646
Exploration Critic Loss : 12570335.0
Exploration Model Loss : 83.2776870727539
Exploitation Data q-values : -5.876714706420898
Exploitation OOD q-values : -4.285386085510254
Exploitation CQL Loss : 0.25795790553092957
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -149.060608
best mean reward -inf
running time 46.971616
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -149.88999938964844
Train_BestReturn : -149.88999938964844
TimeSinceStart : 169.02127361297607
Exploitation Critic Loss : 39.93871307373047
Exploration Critic Loss : 9621186.0
Exploration Model Loss : 21.8232421875
Exploitation Data q-values : 40.00448226928711
Exploitation OOD q-values : 44.21076965332031
Exploitation CQL Loss : 39.93871307373047
Eval_AverageReturn : -76.92308044433594
Eval_StdReturn : 27.94902229309082
Eval_MaxReturn : -28.0
Eval_MinReturn : -126.0
Eval_AverageEpLen : 77.92307692307692
Buffer size : 10001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -149.889999
best mean reward -149.889999
running time 182.070350
At timestep:     70 / 1000At timestep:     180 / 1000At timestep:     220 / 1000At timestep:     256 / 1000At timestep:     273 / 1000At timestep:     306 / 1000At timestep:     353 / 1000At timestep:     396 / 1000At timestep:     446 / 1000At timestep:     496 / 1000At timestep:     536 / 1000At timestep:     597 / 1000At timestep:     624 / 1000At timestep:     688 / 1000At timestep:     753 / 1000At timestep:     833 / 1000At timestep:     869 / 1000At timestep:     966 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -149.06060791015625
TimeSinceStart : 46.97161555290222
Exploitation Critic Loss : 0.21312177181243896
Exploration Critic Loss : 8010139.0
Exploration Model Loss : 59.76203155517578
Exploitation Data q-values : -7.773531913757324
Exploitation OOD q-values : -6.197248935699463
Exploitation CQL Loss : 0.3707500696182251
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -149.225006
best mean reward -inf
running time 59.500986
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -149.88999938964844
Train_BestReturn : -149.88999938964844
TimeSinceStart : 182.07034993171692
Exploitation Critic Loss : 10.075221061706543
Exploration Critic Loss : 9217630.0
Exploration Model Loss : 21.823654174804688
Exploitation Data q-values : 45.010833740234375
Exploitation OOD q-values : 48.62940216064453
Exploitation CQL Loss : 10.075221061706543
Eval_AverageReturn : -51.68421173095703
Eval_StdReturn : 23.278749465942383
Eval_MaxReturn : -16.0
Eval_MinReturn : -109.0
Eval_AverageEpLen : 52.68421052631579
Buffer size : 10001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -149.889999
best mean reward -149.889999
running time 195.527625
At timestep:     26 / 1000At timestep:     93 / 1000At timestep:     155 / 1000At timestep:     204 / 1000At timestep:     277 / 1000At timestep:     314 / 1000At timestep:     428 / 1000At timestep:     465 / 1000At timestep:     531 / 1000At timestep:     603 / 1000At timestep:     713 / 1000At timestep:     810 / 1000At timestep:     883 / 1000At timestep:     975 / 1000At timestep:     1055 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -149.22500610351562
TimeSinceStart : 59.500986099243164
Exploitation Critic Loss : 0.3257472813129425
Exploration Critic Loss : 12526839.0
Exploration Model Loss : 88.20156860351562
Exploitation Data q-values : -9.556991577148438
Exploitation OOD q-values : -7.951412200927734
Exploitation CQL Loss : 0.4863051772117615
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -149.326080
best mean reward -inf
running time 72.257564
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -149.88999938964844
Train_BestReturn : -149.88999938964844
TimeSinceStart : 195.52762484550476
Exploitation Critic Loss : 15.392760276794434
Exploration Critic Loss : 10539507.0
Exploration Model Loss : 21.823928833007812
Exploitation Data q-values : 46.080684661865234
Exploitation OOD q-values : 49.595619201660156
Exploitation CQL Loss : 15.392760276794434
Eval_AverageReturn : -69.33333587646484
Eval_StdReturn : 25.18377113342285
Eval_MaxReturn : -25.0
Eval_MinReturn : -113.0
Eval_AverageEpLen : 70.33333333333333
Buffer size : 10001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 209.020169
At timestep:     29 / 1000At timestep:     179 / 1000At timestep:     235 / 1000At timestep:     334 / 1000At timestep:     446 / 1000At timestep:     502 / 1000At timestep:     533 / 1000At timestep:     592 / 1000At timestep:     636 / 1000At timestep:     688 / 1000At timestep:     724 / 1000At timestep:     791 / 1000At timestep:     828 / 1000At timestep:     954 / 1000At timestep:     995 / 1000At timestep:     1105 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -149.32608032226562
TimeSinceStart : 72.25756406784058
Exploitation Critic Loss : 0.4715327322483063
Exploration Critic Loss : 4751791.0
Exploration Model Loss : 30.457046508789062
Exploitation Data q-values : -10.299057006835938
Exploitation OOD q-values : -8.620258331298828
Exploitation CQL Loss : 0.639412522315979
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -149.415100
best mean reward -inf
running time 85.209645
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 209.0201690196991
Exploitation Critic Loss : 24.92306900024414
Exploration Critic Loss : 9214462.0
Exploration Model Loss : 21.82415771484375
Exploitation Data q-values : 46.29790496826172
Exploitation OOD q-values : 49.27051544189453
Exploitation CQL Loss : 24.92306900024414
Eval_AverageReturn : -68.125
Eval_StdReturn : 36.82708740234375
Eval_MaxReturn : -28.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 69.0625
Buffer size : 10001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 222.930065
At timestep:     110 / 1000At timestep:     235 / 1000At timestep:     269 / 1000At timestep:     327 / 1000At timestep:     477 / 1000At timestep:     499 / 1000At timestep:     649 / 1000At timestep:     703 / 1000At timestep:     750 / 1000At timestep:     792 / 1000At timestep:     869 / 1000At timestep:     911 / 1000At timestep:     1047 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -149.41510009765625
TimeSinceStart : 85.20964455604553
Exploitation Critic Loss : 0.18297603726387024
Exploration Critic Loss : 7043364.5
Exploration Model Loss : 37.05146789550781
Exploitation Data q-values : -10.874320983886719
Exploitation OOD q-values : -9.193729400634766
Exploitation CQL Loss : 0.3510351777076721
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -149.483337
best mean reward -inf
running time 98.179036
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 222.93006539344788
Exploitation Critic Loss : 40.57107925415039
Exploration Critic Loss : 8781087.0
Exploration Model Loss : 21.824295043945312
Exploitation Data q-values : 46.579322814941406
Exploitation OOD q-values : 49.59617233276367
Exploitation CQL Loss : 40.57107925415039
Eval_AverageReturn : -79.69230651855469
Eval_StdReturn : 45.37261962890625
Eval_MaxReturn : -21.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 80.53846153846153
Buffer size : 10001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 236.703429
At timestep:     141 / 1000At timestep:     189 / 1000At timestep:     222 / 1000At timestep:     296 / 1000At timestep:     381 / 1000At timestep:     429 / 1000At timestep:     525 / 1000At timestep:     657 / 1000At timestep:     691 / 1000At timestep:     731 / 1000At timestep:     748 / 1000At timestep:     873 / 1000At timestep:     898 / 1000At timestep:     982 / 1000At timestep:     1051 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -149.48333740234375
TimeSinceStart : 98.17903637886047
Exploitation Critic Loss : 1.109458565711975
Exploration Critic Loss : 11322773.0
Exploration Model Loss : 53.560943603515625
Exploitation Data q-values : -10.632667541503906
Exploitation OOD q-values : -8.949477195739746
Exploitation CQL Loss : 1.2777776718139648
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -149.530304
best mean reward -inf
running time 111.230599
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 236.7034294605255
Exploitation Critic Loss : 18.775753021240234
Exploration Critic Loss : 9076510.0
Exploration Model Loss : 21.824630737304688
Exploitation Data q-values : 47.400360107421875
Exploitation OOD q-values : 50.574501037597656
Exploitation CQL Loss : 18.775753021240234
Eval_AverageReturn : -69.06666564941406
Eval_StdReturn : 38.63584899902344
Eval_MaxReturn : -16.0
Eval_MinReturn : -140.0
Eval_AverageEpLen : 70.06666666666666
Buffer size : 10001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 250.630528
At timestep:     145 / 1000At timestep:     295 / 1000At timestep:     329 / 1000At timestep:     373 / 1000At timestep:     496 / 1000At timestep:     522 / 1000At timestep:     672 / 1000At timestep:     797 / 1000At timestep:     836 / 1000At timestep:     986 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -149.53030395507812
TimeSinceStart : 111.23059940338135
Exploitation Critic Loss : 0.9209157228469849
Exploration Critic Loss : 6252387.0
Exploration Model Loss : 26.756683349609375
Exploitation Data q-values : -10.446781158447266
Exploitation OOD q-values : -8.707476615905762
Exploitation CQL Loss : 1.0948460102081299
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -149.575348
best mean reward -inf
running time 124.373566
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 250.63052797317505
Exploitation Critic Loss : 5.710505485534668
Exploration Critic Loss : 9157828.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 46.550411224365234
Exploitation OOD q-values : 49.79539489746094
Exploitation CQL Loss : 5.710505485534668
Eval_AverageReturn : -92.36363983154297
Eval_StdReturn : 53.11269760131836
Eval_MaxReturn : -25.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 93.0909090909091
Buffer size : 10001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 264.070565
At timestep:     47 / 1000At timestep:     161 / 1000At timestep:     299 / 1000At timestep:     449 / 1000At timestep:     599 / 1000At timestep:     749 / 1000At timestep:     899 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -149.57534790039062
TimeSinceStart : 124.37356567382812
Exploitation Critic Loss : 1.56014084815979
Exploration Critic Loss : 7044080.5
Exploration Model Loss : 35.48908996582031
Exploitation Data q-values : -9.807884216308594
Exploitation OOD q-values : -8.160419464111328
Exploitation CQL Loss : 1.7248873710632324
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -149.612503
best mean reward -inf
running time 137.527540
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 264.0705647468567
Exploitation Critic Loss : 20.23440933227539
Exploration Critic Loss : 10104276.0
Exploration Model Loss : 21.824615478515625
Exploitation Data q-values : 48.467063903808594
Exploitation OOD q-values : 51.93964385986328
Exploitation CQL Loss : 20.23440933227539
Eval_AverageReturn : -127.125
Eval_StdReturn : 33.557552337646484
Eval_MaxReturn : -46.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 127.625
Buffer size : 10001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 276.728764
At timestep:     125 / 1000At timestep:     233 / 1000At timestep:     383 / 1000At timestep:     533 / 1000At timestep:     583 / 1000At timestep:     658 / 1000At timestep:     717 / 1000At timestep:     867 / 1000At timestep:     940 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -149.6125030517578
TimeSinceStart : 137.52753973007202
Exploitation Critic Loss : 0.3651583194732666
Exploration Critic Loss : 11813458.0
Exploration Model Loss : 52.75938415527344
Exploitation Data q-values : -9.525964736938477
Exploitation OOD q-values : -7.867125511169434
Exploitation CQL Loss : 0.531042218208313
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -149.639542
best mean reward -inf
running time 150.410577
At timestep:     32 / 1000At timestep:     106 / 1000At timestep:     256 / 1000At timestep:     301 / 1000At timestep:     378 / 1000At timestep:     440 / 1000At timestep:     487 / 1000At timestep:     554 / 1000At timestep:     588 / 1000At timestep:     678 / 1000At timestep:     732 / 1000At timestep:     830 / 1000At timestep:     899 / 1000At timestep:     935 / 1000At timestep:     969 / 1000At timestep:     1072 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 276.7287640571594
Exploitation Critic Loss : 23.074514389038086
Exploration Critic Loss : 10464734.0
Exploration Model Loss : 21.824630737304688
Exploitation Data q-values : 49.24650573730469
Exploitation OOD q-values : 52.549381256103516
Exploitation CQL Loss : 23.074514389038086
Eval_AverageReturn : -99.4000015258789
Eval_StdReturn : 39.42892074584961
Eval_MaxReturn : -49.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 100.1
Buffer size : 10001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 289.892684
At timestep:     150 / 1000At timestep:     297 / 1000At timestep:     447 / 1000At timestep:     503 / 1000At timestep:     653 / 1000At timestep:     803 / 1000At timestep:     953 / 1000At timestep:     986 / 1000At timestep:     1136 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -149.63954162597656
TimeSinceStart : 150.41057658195496
Exploitation Critic Loss : 1.4831361770629883
Exploration Critic Loss : 4339315.5
Exploration Model Loss : 26.608840942382812
Exploitation Data q-values : -9.160770416259766
Exploitation OOD q-values : -7.486804962158203
Exploitation CQL Loss : 1.6505327224731445
Eval_AverageReturn : -66.0625
Eval_StdReturn : 31.061368942260742
Eval_MaxReturn : -31.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 67.0
Buffer size : 10001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -149.666672
best mean reward -inf
running time 164.393040
At timestep:     31 / 1000At timestep:     76 / 1000At timestep:     116 / 1000At timestep:     170 / 1000At timestep:     203 / 1000At timestep:     246 / 1000At timestep:     281 / 1000At timestep:     309 / 1000At timestep:     389 / 1000At timestep:     448 / 1000At timestep:     492 / 1000At timestep:     572 / 1000At timestep:     618 / 1000At timestep:     670 / 1000At timestep:     699 / 1000At timestep:     768 / 1000At timestep:     800 / 1000At timestep:     832 / 1000At timestep:     908 / 1000At timestep:     935 / 1000At timestep:     973 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 289.8926844596863
Exploitation Critic Loss : 28.681228637695312
Exploration Critic Loss : 8246162.0
Exploration Model Loss : 21.824615478515625
Exploitation Data q-values : 48.735965728759766
Exploitation OOD q-values : 52.436988830566406
Exploitation CQL Loss : 28.681228637695312
Eval_AverageReturn : -125.88888549804688
Eval_StdReturn : 44.38829803466797
Eval_MaxReturn : -32.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 126.22222222222223
Buffer size : 10001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 302.835493
At timestep:     116 / 1000At timestep:     190 / 1000At timestep:     340 / 1000At timestep:     365 / 1000At timestep:     441 / 1000At timestep:     513 / 1000At timestep:     598 / 1000At timestep:     739 / 1000At timestep:     807 / 1000At timestep:     916 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -149.6666717529297
TimeSinceStart : 164.3930401802063
Exploitation Critic Loss : 0.638093888759613
Exploration Critic Loss : 7840766.0
Exploration Model Loss : 35.41841125488281
Exploitation Data q-values : -8.475555419921875
Exploitation OOD q-values : -6.767704963684082
Exploitation CQL Loss : 0.808879017829895
Eval_AverageReturn : -45.727272033691406
Eval_StdReturn : 16.616479873657227
Eval_MaxReturn : -26.0
Eval_MinReturn : -79.0
Eval_AverageEpLen : 46.72727272727273
Buffer size : 10001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -149.690002
best mean reward -inf
running time 178.448814
At timestep:     28 / 1000At timestep:     54 / 1000At timestep:     95 / 1000At timestep:     142 / 1000At timestep:     189 / 1000At timestep:     221 / 1000At timestep:     278 / 1000At timestep:     340 / 1000At timestep:     388 / 1000At timestep:     426 / 1000At timestep:     468 / 1000At timestep:     502 / 1000At timestep:     553 / 1000At timestep:     614 / 1000At timestep:     673 / 1000At timestep:     728 / 1000At timestep:     762 / 1000At timestep:     791 / 1000At timestep:     829 / 1000At timestep:     919 / 1000At timestep:     940 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 302.83549284935
Exploitation Critic Loss : 7.122842788696289
Exploration Critic Loss : 10653262.0
Exploration Model Loss : 21.824615478515625
Exploitation Data q-values : 48.79883575439453
Exploitation OOD q-values : 52.151336669921875
Exploitation CQL Loss : 7.122842788696289
Eval_AverageReturn : -91.0
Eval_StdReturn : 34.178672790527344
Eval_MaxReturn : -24.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 91.9090909090909
Buffer size : 10001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 315.975676
At timestep:     150 / 1000At timestep:     174 / 1000At timestep:     279 / 1000At timestep:     429 / 1000At timestep:     579 / 1000At timestep:     674 / 1000At timestep:     784 / 1000At timestep:     934 / 1000At timestep:     1073 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -149.69000244140625
TimeSinceStart : 178.44881391525269
Exploitation Critic Loss : 0.4723028540611267
Exploration Critic Loss : 11523820.0
Exploration Model Loss : 52.71919250488281
Exploitation Data q-values : -8.169301986694336
Exploitation OOD q-values : -6.537158489227295
Exploitation CQL Loss : 0.6355172991752625
Eval_AverageReturn : -44.45454406738281
Eval_StdReturn : 15.520447731018066
Eval_MaxReturn : -20.0
Eval_MinReturn : -89.0
Eval_AverageEpLen : 45.45454545454545
Buffer size : 10001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -149.919998
best mean reward -149.919998
running time 192.563272
At timestep:     60 / 1000At timestep:     105 / 1000At timestep:     152 / 1000At timestep:     189 / 1000At timestep:     228 / 1000At timestep:     259 / 1000At timestep:     371 / 1000At timestep:     419 / 1000At timestep:     491 / 1000At timestep:     555 / 1000At timestep:     625 / 1000At timestep:     673 / 1000At timestep:     715 / 1000At timestep:     757 / 1000At timestep:     907 / 1000At timestep:     957 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 315.9756758213043
Exploitation Critic Loss : 42.0753288269043
Exploration Critic Loss : 9108412.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 51.85991668701172
Exploitation OOD q-values : 55.30207824707031
Exploitation CQL Loss : 42.0753288269043
Eval_AverageReturn : -118.66666412353516
Eval_StdReturn : 39.95831298828125
Eval_MaxReturn : -23.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 119.22222222222223
Buffer size : 10001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 328.969694
At timestep:     150 / 1000At timestep:     277 / 1000At timestep:     329 / 1000At timestep:     460 / 1000At timestep:     610 / 1000At timestep:     708 / 1000At timestep:     858 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -149.9199981689453
Train_BestReturn : -149.9199981689453
TimeSinceStart : 192.5632724761963
Exploitation Critic Loss : 0.42198896408081055
Exploration Critic Loss : 7137997.5
Exploration Model Loss : 26.602218627929688
Exploitation Data q-values : -7.77699089050293
Exploitation OOD q-values : -6.051964282989502
Exploitation CQL Loss : 0.5944916605949402
Eval_AverageReturn : -58.52941131591797
Eval_StdReturn : 29.223514556884766
Eval_MaxReturn : -30.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 59.470588235294116
Buffer size : 10001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 206.288944
At timestep:     49 / 1000At timestep:     139 / 1000At timestep:     206 / 1000At timestep:     243 / 1000At timestep:     270 / 1000At timestep:     314 / 1000At timestep:     348 / 1000At timestep:     389 / 1000At timestep:     474 / 1000At timestep:     532 / 1000At timestep:     579 / 1000At timestep:     630 / 1000At timestep:     677 / 1000At timestep:     713 / 1000At timestep:     744 / 1000At timestep:     804 / 1000At timestep:     866 / 1000At timestep:     962 / 1000At timestep:     995 / 1000At timestep:     1049 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 328.9696943759918
Exploitation Critic Loss : 53.44322204589844
Exploration Critic Loss : 7506256.0
Exploration Model Loss : 21.824630737304688
Exploitation Data q-values : 51.348121643066406
Exploitation OOD q-values : 55.14258575439453
Exploitation CQL Loss : 53.44322204589844
Eval_AverageReturn : -125.5
Eval_StdReturn : 33.13608169555664
Eval_MaxReturn : -51.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 126.0
Buffer size : 10001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 342.008305
At timestep:     38 / 1000At timestep:     188 / 1000At timestep:     243 / 1000At timestep:     393 / 1000At timestep:     543 / 1000At timestep:     693 / 1000At timestep:     712 / 1000At timestep:     862 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 206.28894352912903
Exploitation Critic Loss : 0.4349380433559418
Exploration Critic Loss : 5801737.0
Exploration Model Loss : 35.41542053222656
Exploitation Data q-values : -7.70721435546875
Exploitation OOD q-values : -6.036773681640625
Exploitation CQL Loss : 0.6019821166992188
Eval_AverageReturn : -51.45000076293945
Eval_StdReturn : 19.19759178161621
Eval_MaxReturn : -26.0
Eval_MinReturn : -95.0
Eval_AverageEpLen : 52.45
Buffer size : 10001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 220.613962
At timestep:     52 / 1000At timestep:     108 / 1000At timestep:     199 / 1000At timestep:     252 / 1000At timestep:     331 / 1000At timestep:     401 / 1000At timestep:     446 / 1000At timestep:     502 / 1000At timestep:     535 / 1000At timestep:     654 / 1000At timestep:     685 / 1000At timestep:     715 / 1000At timestep:     785 / 1000At timestep:     840 / 1000At timestep:     865 / 1000At timestep:     897 / 1000At timestep:     941 / 1000At timestep:     991 / 1000At timestep:     1034 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 342.00830483436584
Exploitation Critic Loss : 66.5967025756836
Exploration Critic Loss : 8264052.0
Exploration Model Loss : 21.824630737304688
Exploitation Data q-values : 51.92601776123047
Exploitation OOD q-values : 55.29595947265625
Exploitation CQL Loss : 66.5967025756836
Eval_AverageReturn : -112.11111450195312
Eval_StdReturn : 54.25135803222656
Eval_MaxReturn : -18.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 112.44444444444444
Buffer size : 10001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 355.328325
At timestep:     85 / 1000At timestep:     192 / 1000At timestep:     302 / 1000At timestep:     409 / 1000At timestep:     559 / 1000At timestep:     630 / 1000At timestep:     780 / 1000At timestep:     821 / 1000At timestep:     913 / 1000At timestep:     1045 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 220.61396169662476
Exploitation Critic Loss : 0.4389660358428955
Exploration Critic Loss : 10924386.0
Exploration Model Loss : 52.717620849609375
Exploitation Data q-values : -7.2245893478393555
Exploitation OOD q-values : -5.556024551391602
Exploitation CQL Loss : 0.6058225035667419
Eval_AverageReturn : -53.421051025390625
Eval_StdReturn : 22.86186408996582
Eval_MaxReturn : -24.0
Eval_MinReturn : -118.0
Eval_AverageEpLen : 54.421052631578945
Buffer size : 10001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 235.101439
At timestep:     61 / 1000At timestep:     102 / 1000At timestep:     128 / 1000At timestep:     161 / 1000At timestep:     217 / 1000At timestep:     253 / 1000At timestep:     284 / 1000At timestep:     316 / 1000At timestep:     361 / 1000At timestep:     408 / 1000At timestep:     458 / 1000At timestep:     494 / 1000At timestep:     542 / 1000At timestep:     600 / 1000At timestep:     635 / 1000At timestep:     670 / 1000At timestep:     728 / 1000At timestep:     773 / 1000At timestep:     837 / 1000At timestep:     874 / 1000At timestep:     925 / 1000At timestep:     987 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 355.3283245563507
Exploitation Critic Loss : 13.240243911743164
Exploration Critic Loss : 7959156.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 52.22393035888672
Exploitation OOD q-values : 56.01493453979492
Exploitation CQL Loss : 13.240243911743164
Eval_AverageReturn : -103.69999694824219
Eval_StdReturn : 32.884796142578125
Eval_MaxReturn : -40.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 104.5
Buffer size : 10001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 369.128214
At timestep:     96 / 1000At timestep:     175 / 1000At timestep:     325 / 1000At timestep:     445 / 1000At timestep:     478 / 1000At timestep:     574 / 1000At timestep:     724 / 1000At timestep:     874 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 235.1014392375946
Exploitation Critic Loss : 0.6864160299301147
Exploration Critic Loss : 6277808.0
Exploration Model Loss : 26.602264404296875
Exploitation Data q-values : -7.037469863891602
Exploitation OOD q-values : -5.405669689178467
Exploitation CQL Loss : 0.8495960831642151
Eval_AverageReturn : -43.34782791137695
Eval_StdReturn : 11.200188636779785
Eval_MaxReturn : -25.0
Eval_MinReturn : -63.0
Eval_AverageEpLen : 44.34782608695652
Buffer size : 10001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 250.316583
At timestep:     41 / 1000At timestep:     96 / 1000At timestep:     171 / 1000At timestep:     225 / 1000At timestep:     347 / 1000At timestep:     404 / 1000At timestep:     456 / 1000At timestep:     476 / 1000At timestep:     547 / 1000At timestep:     598 / 1000At timestep:     692 / 1000At timestep:     784 / 1000At timestep:     840 / 1000At timestep:     881 / 1000At timestep:     932 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 369.1282136440277
Exploitation Critic Loss : 43.59160232543945
Exploration Critic Loss : 6745486.0
Exploration Model Loss : 21.824630737304688
Exploitation Data q-values : 54.71784210205078
Exploitation OOD q-values : 58.58536911010742
Exploitation CQL Loss : 43.59160232543945
Eval_AverageReturn : -113.22222137451172
Eval_StdReturn : 39.363765716552734
Eval_MaxReturn : -32.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 113.77777777777777
Buffer size : 10001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 382.599729
At timestep:     52 / 1000At timestep:     100 / 1000At timestep:     250 / 1000At timestep:     400 / 1000At timestep:     437 / 1000At timestep:     586 / 1000At timestep:     736 / 1000At timestep:     785 / 1000At timestep:     935 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 250.3165831565857
Exploitation Critic Loss : 0.8874467611312866
Exploration Critic Loss : 4211566.0
Exploration Model Loss : 35.41566467285156
Exploitation Data q-values : -7.020174980163574
Exploitation OOD q-values : -5.3518967628479
Exploitation CQL Loss : 1.054274559020996
Eval_AverageReturn : -61.5
Eval_StdReturn : 23.674880981445312
Eval_MaxReturn : -19.0
Eval_MinReturn : -121.0
Eval_AverageEpLen : 62.5
Buffer size : 10001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 264.280352
At timestep:     51 / 1000At timestep:     113 / 1000At timestep:     138 / 1000At timestep:     188 / 1000At timestep:     244 / 1000At timestep:     386 / 1000At timestep:     424 / 1000At timestep:     490 / 1000At timestep:     529 / 1000At timestep:     557 / 1000At timestep:     590 / 1000At timestep:     663 / 1000At timestep:     710 / 1000At timestep:     753 / 1000At timestep:     784 / 1000At timestep:     831 / 1000At timestep:     873 / 1000At timestep:     908 / 1000At timestep:     930 / 1000At timestep:     973 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 382.59972882270813
Exploitation Critic Loss : 66.46939849853516
Exploration Critic Loss : 8259356.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 55.160804748535156
Exploitation OOD q-values : 59.316402435302734
Exploitation CQL Loss : 66.46939849853516
Eval_AverageReturn : -103.30000305175781
Eval_StdReturn : 49.20782470703125
Eval_MaxReturn : -36.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 103.9
Buffer size : 10001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 395.996353
At timestep:     42 / 1000At timestep:     100 / 1000At timestep:     250 / 1000At timestep:     289 / 1000At timestep:     334 / 1000At timestep:     484 / 1000At timestep:     611 / 1000At timestep:     761 / 1000At timestep:     911 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 395.9963529109955
Exploitation Critic Loss : 54.36052322387695
Exploration Critic Loss : 8943028.0
Exploration Model Loss : 21.824615478515625
Exploitation Data q-values : 55.17875671386719
Exploitation OOD q-values : 58.69707489013672
Exploitation CQL Loss : 54.36052322387695
Eval_AverageReturn : -100.30000305175781
Eval_StdReturn : 47.95216369628906
Eval_MaxReturn : -38.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 100.9
Buffer size : 10001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 409.611792
At timestep:     61 / 1000At timestep:     211 / 1000At timestep:     361 / 1000At timestep:     464 / 1000At timestep:     588 / 1000At timestep:     738 / 1000At timestep:     888 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 264.2803521156311
Exploitation Critic Loss : 0.3505568206310272
Exploration Critic Loss : 10369720.0
Exploration Model Loss : 52.7174072265625
Exploitation Data q-values : -7.167052268981934
Exploitation OOD q-values : -5.522435188293457
Exploitation CQL Loss : 0.5150185823440552
Eval_AverageReturn : -47.52381134033203
Eval_StdReturn : 24.507761001586914
Eval_MaxReturn : -21.0
Eval_MinReturn : -141.0
Eval_AverageEpLen : 48.523809523809526
Buffer size : 10001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 278.711052
At timestep:     31 / 1000At timestep:     70 / 1000At timestep:     141 / 1000At timestep:     197 / 1000At timestep:     223 / 1000At timestep:     256 / 1000At timestep:     305 / 1000At timestep:     339 / 1000At timestep:     402 / 1000At timestep:     444 / 1000At timestep:     510 / 1000At timestep:     537 / 1000At timestep:     609 / 1000At timestep:     689 / 1000At timestep:     729 / 1000At timestep:     801 / 1000At timestep:     823 / 1000At timestep:     930 / 1000At timestep:     990 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 409.61179184913635
Exploitation Critic Loss : 15.47097396850586
Exploration Critic Loss : 8310132.0
Exploration Model Loss : 21.824615478515625
Exploitation Data q-values : 55.56300354003906
Exploitation OOD q-values : 59.58625030517578
Exploitation CQL Loss : 15.47097396850586
Eval_AverageReturn : -129.375
Eval_StdReturn : 31.08431053161621
Eval_MaxReturn : -60.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 129.75
Buffer size : 10001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 423.174419
At timestep:     78 / 1000At timestep:     228 / 1000At timestep:     261 / 1000At timestep:     411 / 1000At timestep:     462 / 1000At timestep:     551 / 1000At timestep:     701 / 1000At timestep:     719 / 1000At timestep:     869 / 1000At timestep:     955 / 1000At timestep:     1105 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 278.71105194091797
Exploitation Critic Loss : 0.17599701881408691
Exploration Critic Loss : 4156174.5
Exploration Model Loss : 26.602493286132812
Exploitation Data q-values : -7.751634120941162
Exploitation OOD q-values : -6.17975378036499
Exploitation CQL Loss : 0.33318501710891724
Eval_AverageReturn : -50.20000076293945
Eval_StdReturn : 21.710826873779297
Eval_MaxReturn : -21.0
Eval_MinReturn : -106.0
Eval_AverageEpLen : 51.2
Buffer size : 10001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 293.297291
At timestep:     60 / 1000At timestep:     97 / 1000At timestep:     166 / 1000At timestep:     234 / 1000At timestep:     267 / 1000At timestep:     300 / 1000At timestep:     328 / 1000At timestep:     384 / 1000At timestep:     453 / 1000At timestep:     506 / 1000At timestep:     561 / 1000At timestep:     586 / 1000At timestep:     628 / 1000At timestep:     667 / 1000At timestep:     707 / 1000At timestep:     737 / 1000At timestep:     761 / 1000At timestep:     805 / 1000At timestep:     841 / 1000At timestep:     907 / 1000At timestep:     1055 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 423.174418926239
Exploitation Critic Loss : 44.00281524658203
Exploration Critic Loss : 7370731.0
Exploration Model Loss : 21.824615478515625
Exploitation Data q-values : 56.54106903076172
Exploitation OOD q-values : 60.49262237548828
Exploitation CQL Loss : 44.00281524658203
Eval_AverageReturn : -99.90908813476562
Eval_StdReturn : 49.918941497802734
Eval_MaxReturn : -17.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 100.45454545454545
Buffer size : 10001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 436.816481
At timestep:     51 / 1000At timestep:     201 / 1000At timestep:     345 / 1000At timestep:     495 / 1000At timestep:     616 / 1000At timestep:     766 / 1000At timestep:     916 / 1000At timestep:     1066 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 293.2972905635834
Exploitation Critic Loss : 0.6163524389266968
Exploration Critic Loss : 5095859.5
Exploration Model Loss : 35.41590881347656
Exploitation Data q-values : -7.772505760192871
Exploitation OOD q-values : -6.189398288726807
Exploitation CQL Loss : 0.7746632099151611
Eval_AverageReturn : -49.238094329833984
Eval_StdReturn : 26.298477172851562
Eval_MaxReturn : -23.0
Eval_MinReturn : -147.0
Eval_AverageEpLen : 50.23809523809524
Buffer size : 10001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 307.908888
At timestep:     23 / 1000At timestep:     57 / 1000At timestep:     118 / 1000At timestep:     154 / 1000At timestep:     204 / 1000At timestep:     239 / 1000At timestep:     345 / 1000At timestep:     415 / 1000At timestep:     476 / 1000At timestep:     535 / 1000At timestep:     604 / 1000At timestep:     674 / 1000At timestep:     702 / 1000At timestep:     801 / 1000At timestep:     853 / 1000At timestep:     889 / 1000At timestep:     935 / 1000At timestep:     1075 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 436.81648111343384
Exploitation Critic Loss : 17.107051849365234
Exploration Critic Loss : 7607235.0
Exploration Model Loss : 21.824615478515625
Exploitation Data q-values : 57.03286361694336
Exploitation OOD q-values : 60.906883239746094
Exploitation CQL Loss : 17.107051849365234
Eval_AverageReturn : -132.875
Eval_StdReturn : 32.79267120361328
Eval_MaxReturn : -50.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 133.25
Buffer size : 10001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 450.214350
At timestep:     97 / 1000At timestep:     247 / 1000At timestep:     338 / 1000At timestep:     488 / 1000At timestep:     638 / 1000At timestep:     753 / 1000At timestep:     903 / 1000At timestep:     949 / 1000At timestep:     1099 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 307.9088878631592
Exploitation Critic Loss : 0.7047205567359924
Exploration Critic Loss : 8310771.5
Exploration Model Loss : 52.717193603515625
Exploitation Data q-values : -8.334619522094727
Exploitation OOD q-values : -6.748934745788574
Exploitation CQL Loss : 0.8632891178131104
Eval_AverageReturn : -58.72222137451172
Eval_StdReturn : 29.368928909301758
Eval_MaxReturn : -22.0
Eval_MinReturn : -139.0
Eval_AverageEpLen : 59.72222222222222
Buffer size : 10001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 322.112465
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 450.2143497467041
Exploitation Critic Loss : 7.711624622344971
Exploration Critic Loss : 5565831.0
Exploration Model Loss : 21.824630737304688
Exploitation Data q-values : 58.66584396362305
Exploitation OOD q-values : 62.719234466552734
Exploitation CQL Loss : 7.711624622344971
Eval_AverageReturn : -121.66666412353516
Eval_StdReturn : 35.91966247558594
Eval_MaxReturn : -45.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 122.11111111111111
Buffer size : 10001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 463.762555
At timestep:     150 / 1000At timestep:     197 / 1000At timestep:     347 / 1000At timestep:     497 / 1000At timestep:     542 / 1000At timestep:     692 / 1000At timestep:     842 / 1000At timestep:     992 / 1000At timestep:     1109 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 322.1124646663666
Exploitation Critic Loss : 0.5965795516967773
Exploration Critic Loss : 2713344.75
Exploration Model Loss : 26.602737426757812
Exploitation Data q-values : -8.67666244506836
Exploitation OOD q-values : -7.040012359619141
Exploitation CQL Loss : 0.7602445483207703
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 335.221633
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 463.7625548839569
Exploitation Critic Loss : 33.43974304199219
Exploration Critic Loss : 8040580.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 56.837982177734375
Exploitation OOD q-values : 60.47151565551758
Exploitation CQL Loss : 33.43974304199219
Eval_AverageReturn : -122.88888549804688
Eval_StdReturn : 42.93779754638672
Eval_MaxReturn : -44.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 123.22222222222223
Buffer size : 10001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 477.387318
At timestep:     131 / 1000At timestep:     231 / 1000At timestep:     288 / 1000At timestep:     438 / 1000At timestep:     562 / 1000At timestep:     712 / 1000At timestep:     739 / 1000At timestep:     889 / 1000At timestep:     932 / 1000At timestep:     1082 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 335.2216329574585
Exploitation Critic Loss : 0.33632317185401917
Exploration Critic Loss : 5579608.0
Exploration Model Loss : 35.4161376953125
Exploitation Data q-values : -9.221454620361328
Exploitation OOD q-values : -7.66349458694458
Exploitation CQL Loss : 0.49211928248405457
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 348.443694
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 477.38731837272644
Exploitation Critic Loss : 19.24083709716797
Exploration Critic Loss : 7367825.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 57.677154541015625
Exploitation OOD q-values : 60.92382049560547
Exploitation CQL Loss : 19.24083709716797
Eval_AverageReturn : -107.5999984741211
Eval_StdReturn : 46.506343841552734
Eval_MaxReturn : -26.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 108.2
Buffer size : 10001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 490.782249
At timestep:     104 / 1000At timestep:     132 / 1000At timestep:     282 / 1000At timestep:     311 / 1000At timestep:     415 / 1000At timestep:     444 / 1000At timestep:     594 / 1000At timestep:     744 / 1000At timestep:     805 / 1000At timestep:     955 / 1000At timestep:     1088 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 348.4436936378479
Exploitation Critic Loss : 0.6132597327232361
Exploration Critic Loss : 6522822.0
Exploration Model Loss : 52.716949462890625
Exploitation Data q-values : -9.549163818359375
Exploitation OOD q-values : -7.9431586265563965
Exploitation CQL Loss : 0.7738603949546814
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 361.320075
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 361.3200752735138
Exploitation Critic Loss : 0.5843518972396851
Exploration Critic Loss : 2937583.0
Exploration Model Loss : 26.602981567382812
Exploitation Data q-values : -10.086183547973633
Exploitation OOD q-values : -8.492133140563965
Exploitation CQL Loss : 0.7437570095062256
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 374.274492
At timestep:     150 / 1000At timestep:     211 / 1000At timestep:     361 / 1000At timestep:     511 / 1000At timestep:     559 / 1000At timestep:     709 / 1000At timestep:     859 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 490.782249212265
Exploitation Critic Loss : 17.23288345336914
Exploration Critic Loss : 6607421.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 58.77477264404297
Exploitation OOD q-values : 62.592079162597656
Exploitation CQL Loss : 17.23288345336914
Eval_AverageReturn : -98.2727279663086
Eval_StdReturn : 50.72760009765625
Eval_MaxReturn : -27.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 98.9090909090909
Buffer size : 10001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 504.414641
At timestep:     150 / 1000At timestep:     221 / 1000At timestep:     289 / 1000At timestep:     312 / 1000At timestep:     381 / 1000At timestep:     531 / 1000At timestep:     681 / 1000At timestep:     727 / 1000At timestep:     772 / 1000At timestep:     831 / 1000At timestep:     981 / 1000At timestep:     1052 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 374.27449202537537
Exploitation Critic Loss : 0.5684975385665894
Exploration Critic Loss : 6498526.5
Exploration Model Loss : 35.4163818359375
Exploitation Data q-values : -10.013999938964844
Exploitation OOD q-values : -8.381099700927734
Exploitation CQL Loss : 0.7317876219749451
Eval_AverageReturn : -125.875
Eval_StdReturn : 41.911922454833984
Eval_MaxReturn : -47.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 126.125
Buffer size : 10001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 387.741515
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 504.41464138031006
Exploitation Critic Loss : 87.4683837890625
Exploration Critic Loss : 7614198.0
Exploration Model Loss : 21.824615478515625
Exploitation Data q-values : 59.51719284057617
Exploitation OOD q-values : 63.355072021484375
Exploitation CQL Loss : 87.4683837890625
Eval_AverageReturn : -87.0
Eval_StdReturn : 46.452484130859375
Eval_MaxReturn : -22.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 87.66666666666667
Buffer size : 10001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 518.219387
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     507 / 1000At timestep:     657 / 1000At timestep:     725 / 1000At timestep:     822 / 1000At timestep:     908 / 1000At timestep:     1058 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 387.74151492118835
Exploitation Critic Loss : 0.21541061997413635
Exploration Critic Loss : 5727419.5
Exploration Model Loss : 52.716705322265625
Exploitation Data q-values : -9.88424301147461
Exploitation OOD q-values : -8.180924415588379
Exploitation CQL Loss : 0.38574257493019104
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 401.384717
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 518.2193872928619
Exploitation Critic Loss : 7.504587173461914
Exploration Critic Loss : 7153558.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 58.297462463378906
Exploitation OOD q-values : 61.454322814941406
Exploitation CQL Loss : 7.504587173461914
Eval_AverageReturn : -117.11111450195312
Eval_StdReturn : 38.19669723510742
Eval_MaxReturn : -56.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 117.55555555555556
Buffer size : 10001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 532.079665
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     679 / 1000At timestep:     795 / 1000At timestep:     945 / 1000At timestep:     1095 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 401.38471698760986
Exploitation Critic Loss : 0.35717445611953735
Exploration Critic Loss : 2883155.5
Exploration Model Loss : 26.60321044921875
Exploitation Data q-values : -8.759369850158691
Exploitation OOD q-values : -7.038843154907227
Exploitation CQL Loss : 0.5292271971702576
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 414.848654
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 532.0796649456024
Exploitation Critic Loss : 35.004425048828125
Exploration Critic Loss : 8804666.0
Exploration Model Loss : 21.824630737304688
Exploitation Data q-values : 55.72319030761719
Exploitation OOD q-values : 59.380836486816406
Exploitation CQL Loss : 35.004425048828125
Eval_AverageReturn : -136.625
Eval_StdReturn : 24.944625854492188
Eval_MaxReturn : -78.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 136.875
Buffer size : 10001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 545.621753
At timestep:     83 / 1000At timestep:     155 / 1000At timestep:     305 / 1000At timestep:     455 / 1000At timestep:     505 / 1000At timestep:     579 / 1000At timestep:     729 / 1000At timestep:     879 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 414.84865403175354
Exploitation Critic Loss : 0.778645932674408
Exploration Critic Loss : 4451387.0
Exploration Model Loss : 35.416595458984375
Exploitation Data q-values : -8.905435562133789
Exploitation OOD q-values : -7.193136692047119
Exploitation CQL Loss : 0.9498758912086487
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 428.298326
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 545.6217534542084
Exploitation Critic Loss : 134.09970092773438
Exploration Critic Loss : 12190755.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 54.47441482543945
Exploitation OOD q-values : 57.50530242919922
Exploitation CQL Loss : 134.09970092773438
Eval_AverageReturn : -113.88888549804688
Eval_StdReturn : 41.17562484741211
Eval_MaxReturn : -49.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 114.33333333333333
Buffer size : 10001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 559.007674
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 428.298326253891
Exploitation Critic Loss : 0.7507661581039429
Exploration Critic Loss : 6189543.5
Exploration Model Loss : 52.71647644042969
Exploitation Data q-values : -7.760814666748047
Exploitation OOD q-values : -6.046915531158447
Exploitation CQL Loss : 0.9221560955047607
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 441.573265
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 559.0076744556427
Exploitation Critic Loss : 23.135669708251953
Exploration Critic Loss : 10004122.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 52.94599533081055
Exploitation OOD q-values : 56.02569580078125
Exploitation CQL Loss : 23.135669708251953
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 572.570977
At timestep:     150 / 1000At timestep:     189 / 1000At timestep:     339 / 1000At timestep:     408 / 1000At timestep:     558 / 1000At timestep:     599 / 1000At timestep:     689 / 1000At timestep:     743 / 1000At timestep:     893 / 1000At timestep:     1043 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 441.573264837265
Exploitation Critic Loss : 0.47676804661750793
Exploration Critic Loss : 3445064.5
Exploration Model Loss : 26.603439331054688
Exploitation Data q-values : -7.520123481750488
Exploitation OOD q-values : -5.7796454429626465
Exploitation CQL Loss : 0.6508158445358276
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 454.818923
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 572.5709767341614
Exploitation Critic Loss : 12.070572853088379
Exploration Critic Loss : 8924250.0
Exploration Model Loss : 21.824615478515625
Exploitation Data q-values : 52.044307708740234
Exploitation OOD q-values : 54.97541809082031
Exploitation CQL Loss : 12.070572853088379
Eval_AverageReturn : -103.80000305175781
Eval_StdReturn : 48.118186950683594
Eval_MaxReturn : -38.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 104.3
Buffer size : 10001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 586.504888
At timestep:     150 / 1000At timestep:     207 / 1000At timestep:     357 / 1000At timestep:     507 / 1000At timestep:     657 / 1000At timestep:     736 / 1000At timestep:     785 / 1000At timestep:     934 / 1000At timestep:     1084 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 454.818922996521
Exploitation Critic Loss : 0.24845658242702484
Exploration Critic Loss : 3801783.25
Exploration Model Loss : 35.41685485839844
Exploitation Data q-values : -7.094687461853027
Exploitation OOD q-values : -5.450592994689941
Exploitation CQL Loss : 0.4128660559654236
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 467.910269
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     428 / 1000At timestep:     578 / 1000At timestep:     728 / 1000At timestep:     878 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 586.5048878192902
Exploitation Critic Loss : 21.615251541137695
Exploration Critic Loss : 10681483.0
Exploration Model Loss : 21.824630737304688
Exploitation Data q-values : 49.98786926269531
Exploitation OOD q-values : 52.83687210083008
Exploitation CQL Loss : 21.615251541137695
Eval_AverageReturn : -120.0
Eval_StdReturn : 42.593685150146484
Eval_MaxReturn : -48.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 120.44444444444444
Buffer size : 10001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -150.000000
best mean reward -149.889999
running time 600.041280
At timestep:     140 / 1000At timestep:     290 / 1000At timestep:     440 / 1000At timestep:     590 / 1000At timestep:     740 / 1000At timestep:     890 / 1000At timestep:     1040 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 467.91026878356934
Exploitation Critic Loss : 0.2754436135292053
Exploration Critic Loss : 12027760.0
Exploration Model Loss : 52.71600341796875
Exploitation Data q-values : -7.328535556793213
Exploitation OOD q-values : -5.678071022033691
Exploitation CQL Loss : 0.44049009680747986
Eval_AverageReturn : -146.7142791748047
Eval_StdReturn : 8.048322677612305
Eval_MaxReturn : -127.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 146.85714285714286
Buffer size : 10001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 480.774825
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q2_dqn_PointmassMedium-v0_21-11-2022_22-03-48/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -150.0
Train_BestReturn : -149.88999938964844
TimeSinceStart : 600.0412797927856
Exploitation Critic Loss : 63.420658111572266
Exploration Critic Loss : 12658658.0
Exploration Model Loss : 21.824600219726562
Exploitation Data q-values : 49.783416748046875
Exploitation OOD q-values : 52.42924880981445
Exploitation CQL Loss : 63.420658111572266
Eval_AverageReturn : -148.42857360839844
Eval_StdReturn : 3.849198579788208
Eval_MaxReturn : -139.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 148.57142857142858
Buffer size : 10001
Done logging...


usage: run_hw5_expl.py [-h]
                       [--env_name {PointmassEasy-v0,PointmassMedium-v0,PointmassHard-v0,PointmassVeryHard-v0}]
                       [--exp_name EXP_NAME]
                       [--eval_batch_size EVAL_BATCH_SIZE]
                       [--batch_size BATCH_SIZE] [--use_rnd]
                       [--num_exploration_steps NUM_EXPLORATION_STEPS]
                       [--unsupervised_exploration] [--offline_exploitation]
                       [--cql_alpha CQL_ALPHA]
                       [--exploit_rew_shift EXPLOIT_REW_SHIFT]
                       [--exploit_rew_scale EXPLOIT_REW_SCALE]
                       [--rnd_output_size RND_OUTPUT_SIZE]
                       [--rnd_n_layers RND_N_LAYERS] [--rnd_size RND_SIZE]
                       [--seed SEED] [--no_gpu] [--which_gpu WHICH_GPU]
                       [--scalar_log_freq SCALAR_LOG_FREQ] [--save_params]
                       [--use_boltzmann]
run_hw5_expl.py: error: argument --env_name: invalid choice: '*Chosen' (choose from 'PointmassEasy-v0', 'PointmassMedium-v0', 'PointmassHard-v0', 'PointmassVeryHard-v0')
usage: run_hw5_expl.py [-h]
                       [--env_name {PointmassEasy-v0,PointmassMedium-v0,PointmassHard-v0,PointmassVeryHard-v0}]
                       [--exp_name EXP_NAME]
                       [--eval_batch_size EVAL_BATCH_SIZE]
                       [--batch_size BATCH_SIZE] [--use_rnd]
                       [--num_exploration_steps NUM_EXPLORATION_STEPS]
                       [--unsupervised_exploration] [--offline_exploitation]
                       [--cql_alpha CQL_ALPHA]
                       [--exploit_rew_shift EXPLOIT_REW_SHIFT]
                       [--exploit_rew_scale EXPLOIT_REW_SCALE]
                       [--rnd_output_size RND_OUTPUT_SIZE]
                       [--rnd_n_layers RND_N_LAYERS] [--rnd_size RND_SIZE]
                       [--seed SEED] [--no_gpu] [--which_gpu WHICH_GPU]
                       [--scalar_log_freq SCALAR_LOG_FREQ] [--save_params]
                       [--use_boltzmann]
run_hw5_expl.py: error: argument --env_name: invalid choice: '*Chosen' (choose from 'PointmassEasy-v0', 'PointmassMedium-v0', 'PointmassHard-v0', 'PointmassVeryHard-v0')
usage: run_hw5_expl.py [-h]
                       [--env_name {PointmassEasy-v0,PointmassMedium-v0,PointmassHard-v0,PointmassVeryHard-v0}]
                       [--exp_name EXP_NAME]
                       [--eval_batch_size EVAL_BATCH_SIZE]
                       [--batch_size BATCH_SIZE] [--use_rnd]
                       [--num_exploration_steps NUM_EXPLORATION_STEPS]
                       [--unsupervised_exploration] [--offline_exploitation]
                       [--cql_alpha CQL_ALPHA]
                       [--exploit_rew_shift EXPLOIT_REW_SHIFT]
                       [--exploit_rew_scale EXPLOIT_REW_SCALE]
                       [--rnd_output_size RND_OUTPUT_SIZE]
                       [--rnd_n_layers RND_N_LAYERS] [--rnd_size RND_SIZE]
                       [--seed SEED] [--no_gpu] [--which_gpu WHICH_GPU]
                       [--scalar_log_freq SCALAR_LOG_FREQ] [--save_params]
                       [--use_boltzmann]
run_hw5_expl.py: error: argument --env_name: invalid choice: '*Chosen' (choose from 'PointmassEasy-v0', 'PointmassMedium-v0', 'PointmassHard-v0', 'PointmassVeryHard-v0')
Train_EnvstepsSoFar : 37001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 480.77482533454895
Exploitation Critic Loss : 1.0787081718444824
Exploration Critic Loss : 5893504.0
Exploration Model Loss : 26.603927612304688
Exploitation Data q-values : -7.634766578674316
Exploitation OOD q-values : -6.010706424713135
Exploitation CQL Loss : 1.2411141395568848
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 493.549849
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 493.54984879493713
Exploitation Critic Loss : 0.6670112013816833
Exploration Critic Loss : 5713319.0
Exploration Model Loss : 35.41731262207031
Exploitation Data q-values : -7.528299808502197
Exploitation OOD q-values : -5.959500312805176
Exploitation CQL Loss : 0.8238911032676697
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 505.391890
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 505.39188957214355
Exploitation Critic Loss : 0.4003446400165558
Exploration Critic Loss : 12474134.0
Exploration Model Loss : 52.71575927734375
Exploitation Data q-values : -7.7048797607421875
Exploitation OOD q-values : -6.045931339263916
Exploitation CQL Loss : 0.5662394762039185
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 516.722070
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     818 / 1000At timestep:     968 / 1000At timestep:     1118 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 516.722069978714
Exploitation Critic Loss : 0.5517075061798096
Exploration Critic Loss : 6290711.0
Exploration Model Loss : 26.604171752929688
Exploitation Data q-values : -7.6535868644714355
Exploitation OOD q-values : -6.009486198425293
Exploitation CQL Loss : 0.7161175608634949
Eval_AverageReturn : -139.625
Eval_StdReturn : 27.449670791625977
Eval_MaxReturn : -67.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.75
Buffer size : 10001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 528.063215
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     513 / 1000At timestep:     663 / 1000At timestep:     759 / 1000At timestep:     909 / 1000At timestep:     1059 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 528.0632150173187
Exploitation Critic Loss : 0.16966935992240906
Exploration Critic Loss : 5484756.0
Exploration Model Loss : 35.41754150390625
Exploitation Data q-values : -8.261691093444824
Exploitation OOD q-values : -6.619470596313477
Exploitation CQL Loss : 0.33389145135879517
Eval_AverageReturn : -132.125
Eval_StdReturn : 32.04074478149414
Eval_MaxReturn : -62.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 132.375
Buffer size : 10001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 539.265843
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 539.2658429145813
Exploitation Critic Loss : 0.17286697030067444
Exploration Critic Loss : 8494770.0
Exploration Model Loss : 52.71551513671875
Exploitation Data q-values : -8.591136932373047
Exploitation OOD q-values : -6.980005741119385
Exploitation CQL Loss : 0.33398008346557617
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 550.448884
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     533 / 1000At timestep:     683 / 1000At timestep:     791 / 1000At timestep:     941 / 1000At timestep:     1091 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 550.4488837718964
Exploitation Critic Loss : 0.18271219730377197
Exploration Critic Loss : 5859193.0
Exploration Model Loss : 26.604400634765625
Exploitation Data q-values : -8.603806495666504
Exploitation OOD q-values : -6.932500839233398
Exploitation CQL Loss : 0.34984278678894043
Eval_AverageReturn : -136.125
Eval_StdReturn : 24.831621170043945
Eval_MaxReturn : -82.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 136.375
Buffer size : 10001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 561.634143
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 561.6341426372528
Exploitation Critic Loss : 0.36568284034729004
Exploration Critic Loss : 6773595.0
Exploration Model Loss : 35.41780090332031
Exploitation Data q-values : -8.355748176574707
Exploitation OOD q-values : -6.677482604980469
Exploitation CQL Loss : 0.533509373664856
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 572.727862
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     491 / 1000At timestep:     641 / 1000At timestep:     791 / 1000At timestep:     941 / 1000At timestep:     1091 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 572.7278623580933
Exploitation Critic Loss : 0.7107630968093872
Exploration Critic Loss : 10063168.0
Exploration Model Loss : 52.71528625488281
Exploitation Data q-values : -8.409339904785156
Exploitation OOD q-values : -6.743463516235352
Exploitation CQL Loss : 0.8773506879806519
Eval_AverageReturn : -136.25
Eval_StdReturn : 36.37908172607422
Eval_MaxReturn : -40.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 136.375
Buffer size : 10001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 583.854096
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 583.8540961742401
Exploitation Critic Loss : 0.28404873609542847
Exploration Critic Loss : 6572216.0
Exploration Model Loss : 26.604644775390625
Exploitation Data q-values : -8.001497268676758
Exploitation OOD q-values : -6.278430938720703
Exploitation CQL Loss : 0.4563553035259247
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 594.855477
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000


LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_numsteps_5000_PointmassMedium-v0_21-11-2022_22-16-04 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_numsteps_5000_PointmassMedium-v0_21-11-2022_22-16-04
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001892
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 594.8554773330688
Exploitation Critic Loss : 0.8419683575630188
Exploration Critic Loss : 10652454.0
Exploration Model Loss : 35.417999267578125
Exploitation Data q-values : -7.45169734954834
Exploitation OOD q-values : -5.823462963104248
Exploitation CQL Loss : 1.0047917366027832
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 606.703218
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0018918514251708984
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 4.922278
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 4.9222776889801025
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -148.230774
best mean reward -inf
running time 10.095896
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 606.703218460083
Exploitation Critic Loss : 1.0957379341125488
Exploration Critic Loss : 12335754.0
Exploration Model Loss : 52.715057373046875
Exploitation Data q-values : -7.817815780639648
Exploitation OOD q-values : -6.173694610595703
Exploitation CQL Loss : 1.2601500749588013
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -150.000000
best mean reward -149.919998
running time 618.938212
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -148.23077392578125
TimeSinceStart : 10.095896482467651
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -148.850006
best mean reward -inf
running time 23.499023
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q2_cql_no_scale_PointmassMedium-v0_21-11-2022_22-05-57/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -150.0
Train_BestReturn : -149.9199981689453
TimeSinceStart : 618.9382121562958
Exploitation Critic Loss : 0.33893099427223206
Exploration Critic Loss : 8417260.0
Exploration Model Loss : 26.604843139648438
Exploitation Data q-values : -8.05424690246582
Exploitation OOD q-values : -6.414554595947266
Exploitation CQL Loss : 0.502900242805481
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...


./q2.sh: 8: e: not found



LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_numsteps_5000_PointmassMedium-v0_21-11-2022_22-16-33 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_numsteps_5000_PointmassMedium-v0_21-11-2022_22-16-33
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.003127
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.003126859664916992
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -148.500000
best mean reward -inf
running time 5.684017
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -148.85000610351562
TimeSinceStart : 23.499022722244263
Exploitation Critic Loss : 0.2646973729133606
Exploration Critic Loss : 991903.9375
Exploration Model Loss : 55.79180145263672
Exploitation Data q-values : 0.7034355401992798
Exploitation OOD q-values : 2.331545829772949
Exploitation CQL Loss : 0.4275084137916565
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -148.653839
best mean reward -inf
running time 37.327660
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -148.5
TimeSinceStart : 5.684016942977905
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -148.307693
best mean reward -inf
running time 10.991313
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -148.65383911132812
TimeSinceStart : 37.32766032218933
Exploitation Critic Loss : 1.7887139320373535
Exploration Critic Loss : 6963271.5
Exploration Model Loss : 49.6634521484375
Exploitation Data q-values : 2.9743945598602295
Exploitation OOD q-values : 4.813018798828125
Exploitation CQL Loss : 1.972576379776001
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -148.939392
best mean reward -inf
running time 50.213045
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -148.3076934814453
TimeSinceStart : 10.991313219070435
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -148.899994
best mean reward -inf
running time 24.188676
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -148.93939208984375
TimeSinceStart : 50.21304535865784
Exploitation Critic Loss : 0.7839934229850769
Exploration Critic Loss : 19093794.0
Exploration Model Loss : 128.22366333007812
Exploitation Data q-values : 3.9834232330322266
Exploitation OOD q-values : 5.725699424743652
Exploitation CQL Loss : 0.9582210779190063
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -148.550003
best mean reward -inf
running time 63.435222
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -148.89999389648438
TimeSinceStart : 24.188676357269287
Exploitation Critic Loss : 2.9480249881744385
Exploration Critic Loss : 1015121.6875
Exploration Model Loss : 61.3299560546875
Exploitation Data q-values : 1.6292176246643066
Exploitation OOD q-values : 3.5641493797302246
Exploitation CQL Loss : 3.1415181159973145
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -149.153839
best mean reward -inf
running time 37.205251
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -148.5500030517578
TimeSinceStart : 63.435222148895264
Exploitation Critic Loss : 1.461291790008545
Exploration Critic Loss : 4678852.0
Exploration Model Loss : 28.836349487304688
Exploitation Data q-values : 6.149183750152588
Exploitation OOD q-values : 7.953125
Exploitation CQL Loss : 1.641685962677002
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -147.638290
best mean reward -inf
running time 76.806168
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -149.15383911132812
TimeSinceStart : 37.20525121688843
Exploitation Critic Loss : 3.517636775970459
Exploration Critic Loss : 9365510.0
Exploration Model Loss : 32.77082824707031
Exploitation Data q-values : 4.325223445892334
Exploitation OOD q-values : 6.527056694030762
Exploitation CQL Loss : 3.7378201484680176
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -149.333328
best mean reward -inf
running time 50.759432
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -147.63829040527344
TimeSinceStart : 76.80616784095764
Exploitation Critic Loss : 0.8911484479904175
Exploration Critic Loss : 7061743.5
Exploration Model Loss : 32.86146926879883
Exploitation Data q-values : 5.946039199829102
Exploitation OOD q-values : 7.655837535858154
Exploitation CQL Loss : 1.0621283054351807
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -145.833328
best mean reward -inf
running time 90.753862
At timestep:     83 / 1000At timestep:     189 / 1000At timestep:     261 / 1000At timestep:     307 / 1000At timestep:     379 / 1000At timestep:     471 / 1000At timestep:     567 / 1000At timestep:     661 / 1000At timestep:     691 / 1000At timestep:     778 / 1000At timestep:     878 / 1000At timestep:     945 / 1000At timestep:     1095 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -149.3333282470703
TimeSinceStart : 50.75943160057068
Exploitation Critic Loss : 3.1515121459960938
Exploration Critic Loss : 10643056.0
Exploration Model Loss : 67.35389709472656
Exploitation Data q-values : 6.685607433319092
Exploitation OOD q-values : 9.635523796081543
Exploitation CQL Loss : 3.4465036392211914
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -149.100006
best mean reward -inf
running time 64.525722
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -145.8333282470703
TimeSinceStart : 90.75386166572571
Exploitation Critic Loss : 6.034753799438477
Exploration Critic Loss : 10095092.0
Exploration Model Loss : 24.269485473632812
Exploitation Data q-values : 6.603508949279785
Exploitation OOD q-values : 8.36291790008545
Exploitation CQL Loss : 6.210694789886475
Eval_AverageReturn : -83.30769348144531
Eval_StdReturn : 28.3966007232666
Eval_MaxReturn : -29.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 84.23076923076923
Buffer size : 5001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -146.311478
best mean reward -inf
running time 105.109788
At timestep:     96 / 1000At timestep:     131 / 1000At timestep:     236 / 1000At timestep:     263 / 1000At timestep:     373 / 1000At timestep:     407 / 1000At timestep:     461 / 1000At timestep:     611 / 1000At timestep:     728 / 1000At timestep:     808 / 1000At timestep:     834 / 1000At timestep:     914 / 1000At timestep:     968 / 1000At timestep:     1081 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -149.10000610351562
TimeSinceStart : 64.5257215499878
Exploitation Critic Loss : 12.411090850830078
Exploration Critic Loss : 8223230.5
Exploration Model Loss : 33.35990905761719
Exploitation Data q-values : 16.48089599609375
Exploitation OOD q-values : 20.783573150634766
Exploitation CQL Loss : 12.841358184814453
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -149.217392
best mean reward -inf
running time 77.590101
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -146.3114776611328
TimeSinceStart : 105.10978817939758
Exploitation Critic Loss : 0.8311952352523804
Exploration Critic Loss : 14388988.0
Exploration Model Loss : 21.274642944335938
Exploitation Data q-values : 7.557980537414551
Exploitation OOD q-values : 9.408418655395508
Exploitation CQL Loss : 1.0162389278411865
Eval_AverageReturn : -76.28571319580078
Eval_StdReturn : 38.167728424072266
Eval_MaxReturn : -25.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 77.21428571428571
Buffer size : 5001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -146.691177
best mean reward -inf
running time 119.013450
At timestep:     59 / 1000At timestep:     117 / 1000At timestep:     163 / 1000At timestep:     209 / 1000At timestep:     242 / 1000At timestep:     291 / 1000At timestep:     361 / 1000At timestep:     413 / 1000At timestep:     507 / 1000At timestep:     555 / 1000At timestep:     630 / 1000At timestep:     675 / 1000At timestep:     702 / 1000At timestep:     757 / 1000At timestep:     800 / 1000At timestep:     842 / 1000At timestep:     881 / 1000At timestep:     938 / 1000At timestep:     978 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -149.21739196777344
TimeSinceStart : 77.59010052680969
Exploitation Critic Loss : 24.83061408996582
Exploration Critic Loss : 14759564.0
Exploration Model Loss : 38.00755310058594
Exploitation Data q-values : 25.121780395507812
Exploitation OOD q-values : 29.816356658935547
Exploitation CQL Loss : 25.300071716308594
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -149.320755
best mean reward -inf
running time 90.778182
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     573 / 1000At timestep:     723 / 1000At timestep:     873 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -146.69117736816406
TimeSinceStart : 119.01345038414001
Exploitation Critic Loss : 2.1005194187164307
Exploration Critic Loss : 13354068.0
Exploration Model Loss : 20.405960083007812
Exploitation Data q-values : 9.295568466186523
Exploitation OOD q-values : 11.267599105834961
Exploitation CQL Loss : 2.297722339630127
Eval_AverageReturn : -50.45000076293945
Eval_StdReturn : 14.759657859802246
Eval_MaxReturn : -26.0
Eval_MinReturn : -93.0
Eval_AverageEpLen : 51.45
Buffer size : 5001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -146.959457
best mean reward -inf
running time 133.903524
At timestep:     60 / 1000At timestep:     89 / 1000At timestep:     150 / 1000At timestep:     173 / 1000At timestep:     212 / 1000At timestep:     246 / 1000At timestep:     273 / 1000At timestep:     324 / 1000At timestep:     368 / 1000At timestep:     396 / 1000At timestep:     474 / 1000At timestep:     525 / 1000At timestep:     567 / 1000At timestep:     621 / 1000At timestep:     679 / 1000At timestep:     702 / 1000At timestep:     821 / 1000At timestep:     878 / 1000At timestep:     968 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -149.3207550048828
TimeSinceStart : 90.7781822681427
Exploitation Critic Loss : 15.269691467285156
Exploration Critic Loss : 18874646.0
Exploration Model Loss : 33.02738952636719
Exploitation Data q-values : 35.65744400024414
Exploitation OOD q-values : 40.67167663574219
Exploitation CQL Loss : 15.77111530303955
Eval_AverageReturn : -146.0
Eval_StdReturn : 9.797959327697754
Eval_MaxReturn : -122.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 146.14285714285714
Buffer size : 5001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -149.399994
best mean reward -inf
running time 104.347879
At timestep:     51 / 1000At timestep:     137 / 1000At timestep:     164 / 1000At timestep:     205 / 1000At timestep:     240 / 1000At timestep:     265 / 1000At timestep:     325 / 1000At timestep:     346 / 1000At timestep:     395 / 1000At timestep:     502 / 1000At timestep:     567 / 1000At timestep:     614 / 1000At timestep:     670 / 1000At timestep:     722 / 1000At timestep:     817 / 1000At timestep:     883 / 1000At timestep:     911 / 1000At timestep:     984 / 1000At timestep:     1069 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -146.95945739746094
TimeSinceStart : 133.90352392196655
Exploitation Critic Loss : 6.096296787261963
Exploration Critic Loss : 13013738.0
Exploration Model Loss : 20.110382080078125
Exploitation Data q-values : 11.740716934204102
Exploitation OOD q-values : 13.950640678405762
Exploitation CQL Loss : 6.317289352416992
Eval_AverageReturn : -49.0
Eval_StdReturn : 23.674880981445312
Eval_MaxReturn : -22.0
Eval_MinReturn : -118.0
Eval_AverageEpLen : 50.0
Buffer size : 5001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -147.222229
best mean reward -inf
running time 148.594271
At timestep:     53 / 1000At timestep:     94 / 1000At timestep:     142 / 1000At timestep:     169 / 1000At timestep:     195 / 1000At timestep:     240 / 1000At timestep:     274 / 1000At timestep:     294 / 1000At timestep:     342 / 1000At timestep:     398 / 1000At timestep:     454 / 1000At timestep:     498 / 1000At timestep:     526 / 1000At timestep:     562 / 1000At timestep:     629 / 1000At timestep:     704 / 1000At timestep:     737 / 1000At timestep:     802 / 1000At timestep:     850 / 1000At timestep:     902 / 1000At timestep:     929 / 1000At timestep:     948 / 1000At timestep:     986 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -149.39999389648438
TimeSinceStart : 104.34787893295288
Exploitation Critic Loss : 20.315797805786133
Exploration Critic Loss : 15275294.0
Exploration Model Loss : 31.755569458007812
Exploitation Data q-values : 45.95301818847656
Exploitation OOD q-values : 51.46283721923828
Exploitation CQL Loss : 20.866779327392578
Eval_AverageReturn : -55.26315689086914
Eval_StdReturn : 24.056604385375977
Eval_MaxReturn : -20.0
Eval_MinReturn : -106.0
Eval_AverageEpLen : 56.26315789473684
Buffer size : 5001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -149.454544
best mean reward -inf
running time 118.865674
At timestep:     35 / 1000At timestep:     51 / 1000At timestep:     74 / 1000At timestep:     126 / 1000At timestep:     177 / 1000At timestep:     243 / 1000At timestep:     290 / 1000At timestep:     320 / 1000At timestep:     347 / 1000At timestep:     373 / 1000At timestep:     415 / 1000At timestep:     457 / 1000At timestep:     487 / 1000At timestep:     524 / 1000At timestep:     599 / 1000At timestep:     642 / 1000At timestep:     717 / 1000At timestep:     749 / 1000At timestep:     789 / 1000At timestep:     830 / 1000At timestep:     863 / 1000At timestep:     919 / 1000At timestep:     938 / 1000At timestep:     968 / 1000At timestep:     991 / 1000At timestep:     1047 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -147.22222900390625
TimeSinceStart : 148.59427094459534
Exploitation Critic Loss : 2.2587215900421143
Exploration Critic Loss : 13478652.0
Exploration Model Loss : 20.0037841796875
Exploitation Data q-values : 12.026888847351074
Exploitation OOD q-values : 14.195320129394531
Exploitation CQL Loss : 2.47556471824646
Eval_AverageReturn : -41.875
Eval_StdReturn : 14.538204193115234
Eval_MaxReturn : -18.0
Eval_MinReturn : -74.0
Eval_AverageEpLen : 42.875
Buffer size : 5001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -147.443176
best mean reward -inf
running time 163.496948
At timestep:     61 / 1000At timestep:     106 / 1000At timestep:     136 / 1000At timestep:     212 / 1000At timestep:     266 / 1000At timestep:     314 / 1000At timestep:     357 / 1000At timestep:     391 / 1000At timestep:     414 / 1000At timestep:     461 / 1000At timestep:     501 / 1000At timestep:     542 / 1000At timestep:     591 / 1000At timestep:     610 / 1000At timestep:     673 / 1000At timestep:     720 / 1000At timestep:     773 / 1000At timestep:     823 / 1000At timestep:     870 / 1000At timestep:     913 / 1000At timestep:     948 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -149.4545440673828
TimeSinceStart : 118.86567449569702
Exploitation Critic Loss : 28.423118591308594
Exploration Critic Loss : 17543908.0
Exploration Model Loss : 31.338333129882812
Exploitation Data q-values : 51.41901397705078
Exploitation OOD q-values : 56.193359375
Exploitation CQL Loss : 28.90055274963379
Eval_AverageReturn : -39.269229888916016
Eval_StdReturn : 15.658758163452148
Eval_MaxReturn : -15.0
Eval_MinReturn : -74.0
Eval_AverageEpLen : 40.26923076923077
Buffer size : 5001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -149.506851
best mean reward -inf
running time 134.017268
At timestep:     67 / 1000At timestep:     124 / 1000At timestep:     151 / 1000At timestep:     192 / 1000At timestep:     258 / 1000At timestep:     283 / 1000At timestep:     311 / 1000At timestep:     343 / 1000At timestep:     387 / 1000At timestep:     424 / 1000At timestep:     460 / 1000At timestep:     489 / 1000At timestep:     538 / 1000At timestep:     596 / 1000At timestep:     622 / 1000At timestep:     655 / 1000At timestep:     702 / 1000At timestep:     734 / 1000At timestep:     780 / 1000At timestep:     845 / 1000At timestep:     885 / 1000At timestep:     914 / 1000At timestep:     942 / 1000At timestep:     965 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -147.44317626953125
TimeSinceStart : 163.49694800376892
Exploitation Critic Loss : 2.106597661972046
Exploration Critic Loss : 15264735.0
Exploration Model Loss : 19.964996337890625
Exploitation Data q-values : 13.94705581665039
Exploitation OOD q-values : 16.150821685791016
Exploitation CQL Loss : 2.3269741535186768
Eval_AverageReturn : -44.8636360168457
Eval_StdReturn : 12.867490768432617
Eval_MaxReturn : -18.0
Eval_MinReturn : -75.0
Eval_AverageEpLen : 45.86363636363637
Buffer size : 5001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -147.606384
best mean reward -inf
running time 178.003890
At timestep:     37 / 1000At timestep:     85 / 1000At timestep:     125 / 1000At timestep:     169 / 1000At timestep:     225 / 1000At timestep:     264 / 1000At timestep:     283 / 1000At timestep:     331 / 1000At timestep:     392 / 1000At timestep:     451 / 1000At timestep:     508 / 1000At timestep:     594 / 1000At timestep:     646 / 1000At timestep:     689 / 1000At timestep:     736 / 1000At timestep:     824 / 1000At timestep:     872 / 1000At timestep:     982 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -149.50685119628906
TimeSinceStart : 134.01726818084717
Exploitation Critic Loss : 17.705642700195312
Exploration Critic Loss : 17801702.0
Exploration Model Loss : 31.190841674804688
Exploitation Data q-values : 58.66630172729492
Exploitation OOD q-values : 62.63482666015625
Exploitation CQL Loss : 18.102495193481445
Eval_AverageReturn : -39.31999969482422
Eval_StdReturn : 13.331827163696289
Eval_MaxReturn : -22.0
Eval_MinReturn : -66.0
Eval_AverageEpLen : 40.32
Buffer size : 5001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -149.550003
best mean reward -inf
running time 149.339453
At timestep:     34 / 1000At timestep:     76 / 1000At timestep:     116 / 1000At timestep:     184 / 1000At timestep:     227 / 1000At timestep:     256 / 1000At timestep:     293 / 1000At timestep:     340 / 1000At timestep:     412 / 1000At timestep:     461 / 1000At timestep:     486 / 1000At timestep:     514 / 1000At timestep:     542 / 1000At timestep:     562 / 1000At timestep:     585 / 1000At timestep:     662 / 1000At timestep:     695 / 1000At timestep:     736 / 1000At timestep:     770 / 1000At timestep:     817 / 1000At timestep:     886 / 1000At timestep:     916 / 1000At timestep:     951 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -147.60638427734375
TimeSinceStart : 178.0038902759552
Exploitation Critic Loss : 2.13134765625
Exploration Critic Loss : 9792216.0
Exploration Model Loss : 19.950851440429688
Exploitation Data q-values : 16.050155639648438
Exploitation OOD q-values : 18.33440399169922
Exploitation CQL Loss : 2.3597726821899414
Eval_AverageReturn : -52.47368240356445
Eval_StdReturn : 20.666189193725586
Eval_MaxReturn : -18.0
Eval_MinReturn : -109.0
Eval_AverageEpLen : 53.473684210526315
Buffer size : 5001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -147.750000
best mean reward -147.750000
running time 192.785484
At timestep:     88 / 1000At timestep:     130 / 1000At timestep:     208 / 1000At timestep:     228 / 1000At timestep:     267 / 1000At timestep:     314 / 1000At timestep:     380 / 1000At timestep:     417 / 1000At timestep:     468 / 1000At timestep:     508 / 1000At timestep:     550 / 1000At timestep:     591 / 1000At timestep:     631 / 1000At timestep:     723 / 1000At timestep:     763 / 1000At timestep:     818 / 1000At timestep:     849 / 1000At timestep:     892 / 1000At timestep:     990 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -149.5500030517578
TimeSinceStart : 149.33945298194885
Exploitation Critic Loss : 62.017059326171875
Exploration Critic Loss : 18772400.0
Exploration Model Loss : 31.137359619140625
Exploitation Data q-values : 56.458656311035156
Exploitation OOD q-values : 60.34009552001953
Exploitation CQL Loss : 62.40520477294922
Eval_AverageReturn : -41.29166793823242
Eval_StdReturn : 16.1231689453125
Eval_MaxReturn : -19.0
Eval_MinReturn : -76.0
Eval_AverageEpLen : 42.291666666666664
Buffer size : 5001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -149.581390
best mean reward -inf
running time 164.948811
At timestep:     63 / 1000At timestep:     108 / 1000At timestep:     153 / 1000At timestep:     225 / 1000At timestep:     261 / 1000At timestep:     353 / 1000At timestep:     377 / 1000At timestep:     407 / 1000At timestep:     438 / 1000At timestep:     456 / 1000At timestep:     502 / 1000At timestep:     527 / 1000At timestep:     633 / 1000At timestep:     672 / 1000At timestep:     739 / 1000At timestep:     761 / 1000At timestep:     811 / 1000At timestep:     867 / 1000At timestep:     909 / 1000At timestep:     970 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -147.75
Train_BestReturn : -147.75
TimeSinceStart : 192.7854835987091
Exploitation Critic Loss : 3.666576862335205
Exploration Critic Loss : 15010397.0
Exploration Model Loss : 19.945266723632812
Exploitation Data q-values : 17.959484100341797
Exploitation OOD q-values : 20.37839698791504
Exploitation CQL Loss : 3.908468246459961
Eval_AverageReturn : -50.599998474121094
Eval_StdReturn : 20.86720085144043
Eval_MaxReturn : -19.0
Eval_MinReturn : -97.0
Eval_AverageEpLen : 51.6
Buffer size : 5001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -147.750000
best mean reward -147.750000
running time 207.548201
At timestep:     31 / 1000At timestep:     63 / 1000At timestep:     114 / 1000At timestep:     141 / 1000At timestep:     238 / 1000At timestep:     273 / 1000At timestep:     391 / 1000At timestep:     414 / 1000At timestep:     504 / 1000At timestep:     534 / 1000At timestep:     598 / 1000At timestep:     627 / 1000At timestep:     682 / 1000At timestep:     722 / 1000At timestep:     774 / 1000At timestep:     813 / 1000At timestep:     906 / 1000At timestep:     939 / 1000At timestep:     994 / 1000At timestep:     1040 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -149.58139038085938
TimeSinceStart : 164.94881129264832
Exploitation Critic Loss : 87.2415771484375
Exploration Critic Loss : 11479796.0
Exploration Model Loss : 31.118637084960938
Exploitation Data q-values : 58.96190643310547
Exploitation OOD q-values : 62.798973083496094
Exploitation CQL Loss : 87.62528228759766
Eval_AverageReturn : -46.80952453613281
Eval_StdReturn : 22.330034255981445
Eval_MaxReturn : -17.0
Eval_MinReturn : -105.0
Eval_AverageEpLen : 47.80952380952381
Buffer size : 5001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -149.612900
best mean reward -inf
running time 180.305562
At timestep:     48 / 1000At timestep:     88 / 1000At timestep:     117 / 1000At timestep:     142 / 1000At timestep:     177 / 1000At timestep:     222 / 1000At timestep:     269 / 1000At timestep:     316 / 1000At timestep:     353 / 1000At timestep:     419 / 1000At timestep:     466 / 1000At timestep:     535 / 1000At timestep:     575 / 1000At timestep:     622 / 1000At timestep:     666 / 1000At timestep:     718 / 1000At timestep:     762 / 1000At timestep:     791 / 1000At timestep:     821 / 1000At timestep:     860 / 1000At timestep:     914 / 1000At timestep:     933 / 1000At timestep:     978 / 1000At timestep:     1068 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -147.75
Train_BestReturn : -147.75
TimeSinceStart : 207.54820108413696
Exploitation Critic Loss : 2.7410101890563965
Exploration Critic Loss : 54464288.0
Exploration Model Loss : 19.942657470703125
Exploitation Data q-values : 22.420589447021484
Exploitation OOD q-values : 24.852890014648438
Exploitation CQL Loss : 2.9842400550842285
Eval_AverageReturn : -51.0
Eval_StdReturn : 26.4461727142334
Eval_MaxReturn : -22.0
Eval_MinReturn : -117.0
Eval_AverageEpLen : 52.0
Buffer size : 5001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -147.979996
best mean reward -147.750000
running time 222.322922
At timestep:     20 / 1000At timestep:     43 / 1000At timestep:     113 / 1000At timestep:     149 / 1000At timestep:     186 / 1000At timestep:     219 / 1000At timestep:     244 / 1000At timestep:     280 / 1000At timestep:     344 / 1000At timestep:     385 / 1000At timestep:     439 / 1000At timestep:     466 / 1000At timestep:     489 / 1000At timestep:     555 / 1000At timestep:     586 / 1000At timestep:     642 / 1000At timestep:     672 / 1000At timestep:     720 / 1000At timestep:     768 / 1000At timestep:     797 / 1000At timestep:     830 / 1000At timestep:     880 / 1000At timestep:     954 / 1000At timestep:     979 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -149.61289978027344
TimeSinceStart : 180.30556178092957
Exploitation Critic Loss : 43.87452697753906
Exploration Critic Loss : 28195802.0
Exploration Model Loss : 31.111495971679688
Exploitation Data q-values : 63.10651397705078
Exploitation OOD q-values : 66.58221435546875
Exploitation CQL Loss : 44.22209548950195
Eval_AverageReturn : -43.5
Eval_StdReturn : 14.798648834228516
Eval_MaxReturn : -18.0
Eval_MinReturn : -89.0
Eval_AverageEpLen : 44.5
Buffer size : 5001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -149.639999
best mean reward -inf
running time 195.766051
At timestep:     52 / 1000At timestep:     129 / 1000At timestep:     151 / 1000At timestep:     196 / 1000At timestep:     273 / 1000At timestep:     307 / 1000At timestep:     333 / 1000At timestep:     370 / 1000At timestep:     403 / 1000At timestep:     453 / 1000At timestep:     501 / 1000At timestep:     542 / 1000At timestep:     600 / 1000At timestep:     613 / 1000At timestep:     653 / 1000At timestep:     700 / 1000At timestep:     735 / 1000At timestep:     780 / 1000At timestep:     809 / 1000At timestep:     853 / 1000At timestep:     913 / 1000At timestep:     996 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -147.97999572753906
Train_BestReturn : -147.75
TimeSinceStart : 222.32292199134827
Exploitation Critic Loss : 11.61465072631836
Exploration Critic Loss : 22119920.0
Exploration Model Loss : 19.94195556640625
Exploitation Data q-values : 22.32857894897461
Exploitation OOD q-values : 24.82634735107422
Exploitation CQL Loss : 11.86442756652832
Eval_AverageReturn : -39.36000061035156
Eval_StdReturn : 15.650891304016113
Eval_MaxReturn : -19.0
Eval_MinReturn : -73.0
Eval_AverageEpLen : 40.36
Buffer size : 5001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -147.979996
best mean reward -147.750000
running time 237.632761
At timestep:     101 / 1000At timestep:     127 / 1000At timestep:     187 / 1000At timestep:     250 / 1000At timestep:     325 / 1000At timestep:     455 / 1000At timestep:     471 / 1000At timestep:     496 / 1000At timestep:     567 / 1000At timestep:     634 / 1000At timestep:     716 / 1000At timestep:     740 / 1000At timestep:     781 / 1000At timestep:     816 / 1000At timestep:     855 / 1000At timestep:     882 / 1000At timestep:     931 / 1000At timestep:     997 / 1000At timestep:     1074 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -149.63999938964844
TimeSinceStart : 195.76605129241943
Exploitation Critic Loss : 6.840494632720947
Exploration Critic Loss : 11778721.0
Exploration Model Loss : 31.108016967773438
Exploitation Data q-values : 59.47821044921875
Exploitation OOD q-values : 62.109519958496094
Exploitation CQL Loss : 7.103625297546387
Eval_AverageReturn : -43.869564056396484
Eval_StdReturn : 17.060771942138672
Eval_MaxReturn : -12.0
Eval_MinReturn : -82.0
Eval_AverageEpLen : 44.869565217391305
Buffer size : 5001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -149.729996
best mean reward -149.729996
running time 210.851385
At timestep:     36 / 1000At timestep:     123 / 1000At timestep:     151 / 1000At timestep:     192 / 1000At timestep:     245 / 1000At timestep:     332 / 1000At timestep:     460 / 1000At timestep:     494 / 1000At timestep:     540 / 1000At timestep:     604 / 1000At timestep:     640 / 1000At timestep:     663 / 1000At timestep:     721 / 1000At timestep:     762 / 1000At timestep:     807 / 1000At timestep:     845 / 1000At timestep:     904 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -147.97999572753906
Train_BestReturn : -147.75
TimeSinceStart : 237.63276052474976
Exploitation Critic Loss : 3.262831687927246
Exploration Critic Loss : 12383724.0
Exploration Model Loss : 19.94207763671875
Exploitation Data q-values : 22.284561157226562
Exploitation OOD q-values : 24.67752456665039
Exploitation CQL Loss : 3.5021278858184814
Eval_AverageReturn : -55.52631759643555
Eval_StdReturn : 28.850557327270508
Eval_MaxReturn : -15.0
Eval_MinReturn : -129.0
Eval_AverageEpLen : 56.526315789473685
Buffer size : 5001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -148.100006
best mean reward -147.750000
running time 252.544074
At timestep:     29 / 1000At timestep:     52 / 1000At timestep:     87 / 1000At timestep:     121 / 1000At timestep:     167 / 1000At timestep:     186 / 1000At timestep:     236 / 1000At timestep:     272 / 1000At timestep:     303 / 1000At timestep:     342 / 1000At timestep:     380 / 1000At timestep:     417 / 1000At timestep:     469 / 1000At timestep:     512 / 1000At timestep:     560 / 1000At timestep:     614 / 1000At timestep:     654 / 1000At timestep:     669 / 1000At timestep:     700 / 1000At timestep:     747 / 1000At timestep:     781 / 1000At timestep:     830 / 1000At timestep:     862 / 1000At timestep:     892 / 1000At timestep:     925 / 1000At timestep:     976 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -149.72999572753906
Train_BestReturn : -149.72999572753906
TimeSinceStart : 210.85138511657715
Exploitation Critic Loss : 21.49374008178711
Exploration Critic Loss : 8768128.0
Exploration Model Loss : 31.107406616210938
Exploitation Data q-values : 53.72178649902344
Exploitation OOD q-values : 56.05249786376953
Exploitation CQL Loss : 21.7268123626709
Eval_AverageReturn : -55.11111068725586
Eval_StdReturn : 27.732627868652344
Eval_MaxReturn : -22.0
Eval_MinReturn : -127.0
Eval_AverageEpLen : 56.111111111111114
Buffer size : 5001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -149.860001
best mean reward -149.729996
running time 225.618565
At timestep:     28 / 1000At timestep:     57 / 1000At timestep:     83 / 1000At timestep:     107 / 1000At timestep:     157 / 1000At timestep:     216 / 1000At timestep:     245 / 1000At timestep:     304 / 1000At timestep:     338 / 1000At timestep:     376 / 1000At timestep:     406 / 1000At timestep:     441 / 1000At timestep:     507 / 1000At timestep:     575 / 1000At timestep:     615 / 1000At timestep:     652 / 1000At timestep:     710 / 1000At timestep:     734 / 1000At timestep:     764 / 1000At timestep:     796 / 1000At timestep:     846 / 1000At timestep:     887 / 1000At timestep:     953 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -148.10000610351562
Train_BestReturn : -147.75
TimeSinceStart : 252.5440742969513
Exploitation Critic Loss : 9.156184196472168
Exploration Critic Loss : 18454104.0
Exploration Model Loss : 19.9422607421875
Exploitation Data q-values : 25.254344940185547
Exploitation OOD q-values : 27.549053192138672
Exploitation CQL Loss : 9.385655403137207
Eval_AverageReturn : -36.25925827026367
Eval_StdReturn : 9.953940391540527
Eval_MaxReturn : -14.0
Eval_MinReturn : -53.0
Eval_AverageEpLen : 37.25925925925926
Buffer size : 5001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -148.100006
best mean reward -147.750000
running time 268.367627
At timestep:     107 / 1000At timestep:     165 / 1000At timestep:     236 / 1000At timestep:     266 / 1000At timestep:     376 / 1000At timestep:     417 / 1000At timestep:     452 / 1000At timestep:     473 / 1000At timestep:     499 / 1000At timestep:     575 / 1000At timestep:     606 / 1000At timestep:     630 / 1000At timestep:     693 / 1000At timestep:     730 / 1000At timestep:     816 / 1000At timestep:     841 / 1000At timestep:     880 / 1000At timestep:     923 / 1000At timestep:     966 / 1000At timestep:     997 / 1000At timestep:     1033 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -149.86000061035156
Train_BestReturn : -149.72999572753906
TimeSinceStart : 225.61856532096863
Exploitation Critic Loss : 32.604515075683594
Exploration Critic Loss : 10150100.0
Exploration Model Loss : 31.107345581054688
Exploitation Data q-values : 49.730186462402344
Exploitation OOD q-values : 51.777374267578125
Exploitation CQL Loss : 32.809234619140625
Eval_AverageReturn : -41.375
Eval_StdReturn : 14.798120498657227
Eval_MaxReturn : -23.0
Eval_MinReturn : -67.0
Eval_AverageEpLen : 42.375
Buffer size : 5001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -149.860001
best mean reward -149.729996
running time 241.505782
At timestep:     112 / 1000At timestep:     149 / 1000At timestep:     195 / 1000At timestep:     235 / 1000At timestep:     284 / 1000At timestep:     323 / 1000At timestep:     378 / 1000At timestep:     459 / 1000At timestep:     532 / 1000At timestep:     597 / 1000At timestep:     671 / 1000At timestep:     767 / 1000At timestep:     789 / 1000At timestep:     841 / 1000At timestep:     890 / 1000At timestep:     957 / 1000At timestep:     988 / 1000At timestep:     1047 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -148.10000610351562
Train_BestReturn : -147.75
TimeSinceStart : 268.3676266670227
Exploitation Critic Loss : 6.61993408203125
Exploration Critic Loss : 31917392.0
Exploration Model Loss : 19.942352294921875
Exploitation Data q-values : 24.75680160522461
Exploitation OOD q-values : 26.956893920898438
Exploitation CQL Loss : 6.8399434089660645
Eval_AverageReturn : -48.19047546386719
Eval_StdReturn : 26.013952255249023
Eval_MaxReturn : -20.0
Eval_MinReturn : -109.0
Eval_AverageEpLen : 49.19047619047619
Buffer size : 5001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -148.330002
best mean reward -147.750000
running time 283.129278
At timestep:     45 / 1000At timestep:     75 / 1000At timestep:     132 / 1000At timestep:     162 / 1000At timestep:     196 / 1000At timestep:     233 / 1000At timestep:     267 / 1000At timestep:     327 / 1000At timestep:     376 / 1000At timestep:     415 / 1000At timestep:     443 / 1000At timestep:     515 / 1000At timestep:     557 / 1000At timestep:     580 / 1000At timestep:     625 / 1000At timestep:     656 / 1000At timestep:     710 / 1000At timestep:     736 / 1000At timestep:     758 / 1000At timestep:     806 / 1000At timestep:     844 / 1000At timestep:     913 / 1000At timestep:     944 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -149.86000061035156
Train_BestReturn : -149.72999572753906
TimeSinceStart : 241.5057818889618
Exploitation Critic Loss : 36.34152603149414
Exploration Critic Loss : 10595352.0
Exploration Model Loss : 31.107009887695312
Exploitation Data q-values : 46.19068908691406
Exploitation OOD q-values : 48.274169921875
Exploitation CQL Loss : 36.54987335205078
Eval_AverageReturn : -57.16666793823242
Eval_StdReturn : 22.36378288269043
Eval_MaxReturn : -21.0
Eval_MinReturn : -111.0
Eval_AverageEpLen : 58.166666666666664
Buffer size : 5001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -149.679993
best mean reward -149.679993
running time 256.165698
At timestep:     46 / 1000At timestep:     106 / 1000At timestep:     168 / 1000At timestep:     201 / 1000At timestep:     232 / 1000At timestep:     272 / 1000At timestep:     324 / 1000At timestep:     370 / 1000At timestep:     428 / 1000At timestep:     458 / 1000At timestep:     532 / 1000At timestep:     610 / 1000At timestep:     641 / 1000At timestep:     714 / 1000At timestep:     763 / 1000At timestep:     814 / 1000At timestep:     888 / 1000At timestep:     917 / 1000At timestep:     950 / 1000At timestep:     991 / 1000At timestep:     1040 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -148.3300018310547
Train_BestReturn : -147.75
TimeSinceStart : 283.129278421402
Exploitation Critic Loss : 3.496731758117676
Exploration Critic Loss : 17061164.0
Exploration Model Loss : 19.942855834960938
Exploitation Data q-values : 26.36419105529785
Exploitation OOD q-values : 28.777938842773438
Exploitation CQL Loss : 3.7381067276000977
Eval_AverageReturn : -41.04166793823242
Eval_StdReturn : 14.252131462097168
Eval_MaxReturn : -21.0
Eval_MinReturn : -71.0
Eval_AverageEpLen : 42.041666666666664
Buffer size : 5001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -148.860001
best mean reward -147.750000
running time 298.042688
At timestep:     42 / 1000At timestep:     74 / 1000At timestep:     105 / 1000At timestep:     132 / 1000At timestep:     158 / 1000At timestep:     221 / 1000At timestep:     241 / 1000At timestep:     280 / 1000At timestep:     321 / 1000At timestep:     355 / 1000At timestep:     431 / 1000At timestep:     460 / 1000At timestep:     485 / 1000At timestep:     513 / 1000At timestep:     561 / 1000At timestep:     581 / 1000At timestep:     608 / 1000At timestep:     630 / 1000At timestep:     658 / 1000At timestep:     700 / 1000At timestep:     728 / 1000At timestep:     774 / 1000At timestep:     831 / 1000At timestep:     867 / 1000At timestep:     895 / 1000At timestep:     921 / 1000At timestep:     955 / 1000At timestep:     977 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -149.67999267578125
Train_BestReturn : -149.67999267578125
TimeSinceStart : 256.16569781303406
Exploitation Critic Loss : 24.43311882019043
Exploration Critic Loss : 21210892.0
Exploration Model Loss : 31.107147216796875
Exploitation Data q-values : 42.35600280761719
Exploitation OOD q-values : 44.54538345336914
Exploitation CQL Loss : 24.652057647705078
Eval_AverageReturn : -48.52381134033203
Eval_StdReturn : 15.6284818649292
Eval_MaxReturn : -28.0
Eval_MinReturn : -77.0
Eval_AverageEpLen : 49.523809523809526
Buffer size : 5001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -149.679993
best mean reward -149.679993
running time 270.887866
At timestep:     29 / 1000At timestep:     79 / 1000At timestep:     117 / 1000At timestep:     218 / 1000At timestep:     238 / 1000At timestep:     283 / 1000At timestep:     309 / 1000At timestep:     373 / 1000At timestep:     440 / 1000At timestep:     462 / 1000At timestep:     499 / 1000At timestep:     546 / 1000At timestep:     594 / 1000At timestep:     628 / 1000At timestep:     655 / 1000At timestep:     753 / 1000At timestep:     786 / 1000At timestep:     862 / 1000At timestep:     894 / 1000At timestep:     914 / 1000At timestep:     948 / 1000At timestep:     996 / 1000At timestep:     1048 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -148.86000061035156
Train_BestReturn : -147.75
TimeSinceStart : 298.04268765449524
Exploitation Critic Loss : 1.2840441465377808
Exploration Critic Loss : 27881572.0
Exploration Model Loss : 19.943328857421875
Exploitation Data q-values : 27.095468521118164
Exploitation OOD q-values : 29.476394653320312
Exploitation CQL Loss : 1.5221368074417114
Eval_AverageReturn : -33.58620834350586
Eval_StdReturn : 13.002697944641113
Eval_MaxReturn : -19.0
Eval_MinReturn : -75.0
Eval_AverageEpLen : 34.58620689655172
Buffer size : 5001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 313.532116
At timestep:     77 / 1000At timestep:     109 / 1000At timestep:     139 / 1000At timestep:     192 / 1000At timestep:     244 / 1000At timestep:     292 / 1000At timestep:     330 / 1000At timestep:     378 / 1000At timestep:     419 / 1000At timestep:     465 / 1000At timestep:     499 / 1000At timestep:     534 / 1000At timestep:     575 / 1000At timestep:     620 / 1000At timestep:     644 / 1000At timestep:     663 / 1000At timestep:     702 / 1000At timestep:     740 / 1000At timestep:     772 / 1000At timestep:     812 / 1000At timestep:     854 / 1000At timestep:     885 / 1000At timestep:     940 / 1000At timestep:     960 / 1000At timestep:     1033 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -149.67999267578125
Train_BestReturn : -149.67999267578125
TimeSinceStart : 270.8878662586212
Exploitation Critic Loss : 2.267256021499634
Exploration Critic Loss : 20853028.0
Exploration Model Loss : 31.107025146484375
Exploitation Data q-values : 40.41822052001953
Exploitation OOD q-values : 42.87371063232422
Exploitation CQL Loss : 2.5128049850463867
Eval_AverageReturn : -44.565216064453125
Eval_StdReturn : 22.094314575195312
Eval_MaxReturn : -19.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 45.56521739130435
Buffer size : 5001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -149.419998
best mean reward -149.419998
running time 285.968831
At timestep:     22 / 1000At timestep:     86 / 1000At timestep:     120 / 1000At timestep:     149 / 1000At timestep:     205 / 1000At timestep:     240 / 1000At timestep:     318 / 1000At timestep:     345 / 1000At timestep:     420 / 1000At timestep:     449 / 1000At timestep:     474 / 1000At timestep:     515 / 1000At timestep:     550 / 1000At timestep:     606 / 1000At timestep:     656 / 1000At timestep:     687 / 1000At timestep:     713 / 1000At timestep:     762 / 1000At timestep:     789 / 1000At timestep:     838 / 1000At timestep:     889 / 1000At timestep:     981 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 313.5321156978607
Exploitation Critic Loss : 5.875152111053467
Exploration Critic Loss : 15355986.0
Exploration Model Loss : 19.94366455078125
Exploitation Data q-values : 25.502586364746094
Exploitation OOD q-values : 27.719633102416992
Exploitation CQL Loss : 6.096857070922852
Eval_AverageReturn : -40.31999969482422
Eval_StdReturn : 13.599176406860352
Eval_MaxReturn : -18.0
Eval_MinReturn : -76.0
Eval_AverageEpLen : 41.32
Buffer size : 5001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 329.092941
At timestep:     22 / 1000At timestep:     75 / 1000At timestep:     102 / 1000At timestep:     147 / 1000At timestep:     176 / 1000At timestep:     212 / 1000At timestep:     258 / 1000At timestep:     283 / 1000At timestep:     309 / 1000At timestep:     337 / 1000At timestep:     391 / 1000At timestep:     427 / 1000At timestep:     472 / 1000At timestep:     491 / 1000At timestep:     531 / 1000At timestep:     569 / 1000At timestep:     615 / 1000At timestep:     667 / 1000At timestep:     686 / 1000At timestep:     724 / 1000At timestep:     747 / 1000At timestep:     773 / 1000At timestep:     842 / 1000At timestep:     893 / 1000At timestep:     942 / 1000At timestep:     994 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -149.4199981689453
Train_BestReturn : -149.4199981689453
TimeSinceStart : 285.9688313007355
Exploitation Critic Loss : 2.4789681434631348
Exploration Critic Loss : 21579268.0
Exploration Model Loss : 31.10699462890625
Exploitation Data q-values : 39.34419250488281
Exploitation OOD q-values : 41.86334991455078
Exploitation CQL Loss : 2.730883836746216
Eval_AverageReturn : -43.826087951660156
Eval_StdReturn : 18.534696578979492
Eval_MaxReturn : -21.0
Eval_MinReturn : -91.0
Eval_AverageEpLen : 44.82608695652174
Buffer size : 5001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -149.419998
best mean reward -149.419998
running time 301.482633
At timestep:     33 / 1000At timestep:     56 / 1000At timestep:     114 / 1000At timestep:     140 / 1000At timestep:     178 / 1000At timestep:     228 / 1000At timestep:     265 / 1000At timestep:     290 / 1000At timestep:     335 / 1000At timestep:     362 / 1000At timestep:     419 / 1000At timestep:     446 / 1000At timestep:     471 / 1000At timestep:     503 / 1000At timestep:     526 / 1000At timestep:     595 / 1000At timestep:     612 / 1000At timestep:     633 / 1000At timestep:     691 / 1000At timestep:     723 / 1000At timestep:     754 / 1000At timestep:     774 / 1000At timestep:     818 / 1000At timestep:     846 / 1000At timestep:     911 / 1000At timestep:     940 / 1000At timestep:     975 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 329.0929412841797
Exploitation Critic Loss : 4.169858932495117
Exploration Critic Loss : 21332384.0
Exploration Model Loss : 19.943756103515625
Exploitation Data q-values : 27.294296264648438
Exploitation OOD q-values : 29.771068572998047
Exploitation CQL Loss : 4.41753625869751
Eval_AverageReturn : -37.44444274902344
Eval_StdReturn : 12.699762344360352
Eval_MaxReturn : -18.0
Eval_MinReturn : -68.0
Eval_AverageEpLen : 38.44444444444444
Buffer size : 5001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 344.599872
At timestep:     45 / 1000At timestep:     83 / 1000At timestep:     132 / 1000At timestep:     160 / 1000At timestep:     208 / 1000At timestep:     246 / 1000At timestep:     274 / 1000At timestep:     305 / 1000At timestep:     341 / 1000At timestep:     391 / 1000At timestep:     480 / 1000At timestep:     514 / 1000At timestep:     585 / 1000At timestep:     609 / 1000At timestep:     633 / 1000At timestep:     666 / 1000At timestep:     710 / 1000At timestep:     752 / 1000At timestep:     837 / 1000At timestep:     886 / 1000At timestep:     966 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -149.4199981689453
Train_BestReturn : -149.4199981689453
TimeSinceStart : 301.4826328754425
Exploitation Critic Loss : 2.544585943222046
Exploration Critic Loss : 23337364.0
Exploration Model Loss : 31.10662841796875
Exploitation Data q-values : 36.998756408691406
Exploitation OOD q-values : 39.47332763671875
Exploitation CQL Loss : 2.7920432090759277
Eval_AverageReturn : -34.96428680419922
Eval_StdReturn : 14.097826957702637
Eval_MaxReturn : -16.0
Eval_MinReturn : -68.0
Eval_AverageEpLen : 35.964285714285715
Buffer size : 5001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -149.419998
best mean reward -149.419998
running time 317.117221
At timestep:     63 / 1000At timestep:     115 / 1000At timestep:     153 / 1000At timestep:     207 / 1000At timestep:     245 / 1000At timestep:     288 / 1000At timestep:     316 / 1000At timestep:     363 / 1000At timestep:     392 / 1000At timestep:     420 / 1000At timestep:     443 / 1000At timestep:     466 / 1000At timestep:     502 / 1000At timestep:     537 / 1000At timestep:     582 / 1000At timestep:     604 / 1000At timestep:     621 / 1000At timestep:     642 / 1000At timestep:     668 / 1000At timestep:     702 / 1000At timestep:     723 / 1000At timestep:     758 / 1000At timestep:     823 / 1000At timestep:     864 / 1000At timestep:     892 / 1000At timestep:     946 / 1000At timestep:     990 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 344.5998718738556
Exploitation Critic Loss : 12.427909851074219
Exploration Critic Loss : 23263706.0
Exploration Model Loss : 19.94403076171875
Exploitation Data q-values : 27.142133712768555
Exploitation OOD q-values : 29.430301666259766
Exploitation CQL Loss : 12.656726837158203
Eval_AverageReturn : -44.727272033691406
Eval_StdReturn : 18.618616104125977
Eval_MaxReturn : -23.0
Eval_MinReturn : -88.0
Eval_AverageEpLen : 45.72727272727273
Buffer size : 5001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 359.523354
At timestep:     64 / 1000At timestep:     99 / 1000At timestep:     118 / 1000At timestep:     152 / 1000At timestep:     185 / 1000At timestep:     220 / 1000At timestep:     289 / 1000At timestep:     314 / 1000At timestep:     357 / 1000At timestep:     393 / 1000At timestep:     433 / 1000At timestep:     456 / 1000At timestep:     511 / 1000At timestep:     561 / 1000At timestep:     588 / 1000At timestep:     614 / 1000At timestep:     639 / 1000At timestep:     692 / 1000At timestep:     716 / 1000At timestep:     761 / 1000At timestep:     793 / 1000At timestep:     826 / 1000At timestep:     869 / 1000At timestep:     905 / 1000At timestep:     938 / 1000At timestep:     981 / 1000At timestep:     1036 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -149.4199981689453
Train_BestReturn : -149.4199981689453
TimeSinceStart : 317.1172206401825
Exploitation Critic Loss : 3.255354881286621
Exploration Critic Loss : 25246722.0
Exploration Model Loss : 31.106170654296875
Exploitation Data q-values : 37.497901916503906
Exploitation OOD q-values : 40.22660827636719
Exploitation CQL Loss : 3.528225898742676
Eval_AverageReturn : -36.07143020629883
Eval_StdReturn : 12.853372573852539
Eval_MaxReturn : -16.0
Eval_MinReturn : -64.0
Eval_AverageEpLen : 37.07142857142857
Buffer size : 5001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -149.419998
best mean reward -149.419998
running time 332.909065
At timestep:     40 / 1000At timestep:     61 / 1000At timestep:     108 / 1000At timestep:     138 / 1000At timestep:     170 / 1000At timestep:     189 / 1000At timestep:     219 / 1000At timestep:     237 / 1000At timestep:     274 / 1000At timestep:     309 / 1000At timestep:     330 / 1000At timestep:     346 / 1000At timestep:     377 / 1000At timestep:     403 / 1000At timestep:     453 / 1000At timestep:     490 / 1000At timestep:     514 / 1000At timestep:     553 / 1000At timestep:     571 / 1000At timestep:     614 / 1000At timestep:     652 / 1000At timestep:     674 / 1000At timestep:     696 / 1000At timestep:     722 / 1000At timestep:     801 / 1000At timestep:     845 / 1000At timestep:     898 / 1000At timestep:     927 / 1000At timestep:     958 / 1000At timestep:     977 / 1000At timestep:     1018 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q2_cql_numsteps_5000_PointmassMedium-v0_21-11-2022_22-16-33/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 24001
Train_AverageReturn : -149.4199981689453
Train_BestReturn : -149.4199981689453
TimeSinceStart : 332.9090654850006
Exploitation Critic Loss : 3.669429063796997
Exploration Critic Loss : 26489614.0
Exploration Model Loss : 31.10589599609375
Exploitation Data q-values : 36.90095901489258
Exploitation OOD q-values : 39.62031936645508
Exploitation CQL Loss : 3.9413647651672363
Eval_AverageReturn : -31.838708877563477
Eval_StdReturn : 13.0929594039917
Eval_MaxReturn : -15.0
Eval_MinReturn : -78.0
Eval_AverageEpLen : 32.83870967741935
Buffer size : 5001
Done logging...


Traceback (most recent call last):
  File "cs285/scripts/run_hw5_expl.py", line 134, in <module>
    main()
  File "cs285/scripts/run_hw5_expl.py", line 130, in main
    trainer.run_training_loop()
  File "cs285/scripts/run_hw5_expl.py", line 37, in run_training_loop
    eval_policy = self.rl_trainer.agent.actor,
  File "/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py", line 201, in run_training_loop
    all_logs = self.train_agent()
  File "/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py", line 260, in train_agent
    train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)
  File "/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/agents/explore_or_exploit_agent.py", line 89, in train
    mixed_reward, terminal_n)
  File "/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/critics/dqn_critic.py", line 87, in update
    return {'Training Loss': ptu.to_numpy(loss)}
  File "/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/pytorch_util.py", line 92, in to_numpy
    return tensor.to('cpu').detach().numpy()
KeyboardInterrupt
Train_EnvstepsSoFar : 26001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 359.5233542919159
Exploitation Critic Loss : 9.793583869934082
Exploration Critic Loss : 25256456.0
Exploration Model Loss : 19.944366455078125
Exploitation Data q-values : 26.307334899902344
Exploitation OOD q-values : 28.679218292236328
Exploitation CQL Loss : 10.03077220916748
Eval_AverageReturn : -37.37036895751953
Eval_StdReturn : 12.540742874145508
Eval_MaxReturn : -18.0
Eval_MinReturn : -68.0
Eval_AverageEpLen : 38.370370370370374
Buffer size : 5001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 375.093310
At timestep:     30 / 1000At timestep:     57 / 1000At timestep:     166 / 1000At timestep:     193 / 1000At timestep:     214 / 1000At timestep:     240 / 1000At timestep:     269 / 1000At timestep:     297 / 1000At timestep:     323 / 1000At timestep:     348 / 1000At timestep:     375 / 1000At timestep:     398 / 1000At timestep:     434 / 1000At timestep:     477 / 1000At timestep:     536 / 1000At timestep:     586 / 1000At timestep:     618 / 1000At timestep:     650 / 1000At timestep:     707 / 1000At timestep:     741 / 1000At timestep:     775 / 1000At timestep:     814 / 1000At timestep:     844 / 1000At timestep:     884 / 1000At timestep:     917 / 1000At timestep:     941 / 1000At timestep:     970 / 1000At timestep:     996 / 1000At timestep:     1045 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 375.0933096408844
Exploitation Critic Loss : 11.504501342773438
Exploration Critic Loss : 36633676.0
Exploration Model Loss : 19.9443359375
Exploitation Data q-values : 27.489437103271484
Exploitation OOD q-values : 29.811969757080078
Exploitation CQL Loss : 11.736754417419434
Eval_AverageReturn : -35.034481048583984
Eval_StdReturn : 16.838958740234375
Eval_MaxReturn : -20.0
Eval_MinReturn : -108.0
Eval_AverageEpLen : 36.03448275862069
Buffer size : 5001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 388.862407
At timestep:     34 / 1000At timestep:     61 / 1000At timestep:     87 / 1000At timestep:     124 / 1000At timestep:     147 / 1000At timestep:     177 / 1000At timestep:     223 / 1000At timestep:     265 / 1000At timestep:     300 / 1000At timestep:     323 / 1000At timestep:     343 / 1000At timestep:     366 / 1000At timestep:     394 / 1000At timestep:     430 / 1000At timestep:     458 / 1000At timestep:     489 / 1000At timestep:     511 / 1000At timestep:     544 / 1000At timestep:     589 / 1000At timestep:     611 / 1000At timestep:     646 / 1000At timestep:     682 / 1000At timestep:     699 / 1000At timestep:     730 / 1000At timestep:     782 / 1000At timestep:     820 / 1000At timestep:     860 / 1000At timestep:     892 / 1000At timestep:     943 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 388.86240696907043
Exploitation Critic Loss : 3.8346073627471924
Exploration Critic Loss : 15982968.0
Exploration Model Loss : 19.944366455078125
Exploitation Data q-values : 28.23908233642578
Exploitation OOD q-values : 30.58609390258789
Exploitation CQL Loss : 4.069308280944824
Eval_AverageReturn : -32.63333511352539
Eval_StdReturn : 10.634796142578125
Eval_MaxReturn : -16.0
Eval_MinReturn : -65.0
Eval_AverageEpLen : 33.63333333333333
Buffer size : 5001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 402.877640
At timestep:     80 / 1000At timestep:     137 / 1000At timestep:     167 / 1000At timestep:     204 / 1000At timestep:     245 / 1000At timestep:     301 / 1000At timestep:     345 / 1000At timestep:     364 / 1000At timestep:     394 / 1000At timestep:     417 / 1000At timestep:     459 / 1000At timestep:     487 / 1000At timestep:     540 / 1000At timestep:     581 / 1000At timestep:     602 / 1000At timestep:     654 / 1000At timestep:     696 / 1000At timestep:     731 / 1000At timestep:     755 / 1000At timestep:     796 / 1000At timestep:     830 / 1000At timestep:     864 / 1000At timestep:     910 / 1000At timestep:     956 / 1000At timestep:     975 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 402.87764024734497
Exploitation Critic Loss : 1.4351260662078857
Exploration Critic Loss : 14767080.0
Exploration Model Loss : 19.944503784179688
Exploitation Data q-values : 27.201871871948242
Exploitation OOD q-values : 29.563121795654297
Exploitation CQL Loss : 1.6712509393692017
Eval_AverageReturn : -37.653846740722656
Eval_StdReturn : 13.654929161071777
Eval_MaxReturn : -18.0
Eval_MinReturn : -79.0
Eval_AverageEpLen : 38.65384615384615
Buffer size : 5001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 416.462874
At timestep:     32 / 1000At timestep:     112 / 1000At timestep:     138 / 1000At timestep:     180 / 1000At timestep:     229 / 1000At timestep:     260 / 1000At timestep:     297 / 1000At timestep:     318 / 1000At timestep:     348 / 1000At timestep:     387 / 1000At timestep:     410 / 1000At timestep:     448 / 1000At timestep:     495 / 1000At timestep:     525 / 1000At timestep:     559 / 1000At timestep:     608 / 1000At timestep:     651 / 1000At timestep:     695 / 1000At timestep:     744 / 1000At timestep:     775 / 1000At timestep:     811 / 1000At timestep:     840 / 1000At timestep:     891 / 1000At timestep:     924 / 1000At timestep:     956 / 1000At timestep:     995 / 1000At timestep:     1034 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 416.4628736972809
Exploitation Critic Loss : 11.885741233825684
Exploration Critic Loss : 13047383.0
Exploration Model Loss : 19.94488525390625
Exploitation Data q-values : 28.729137420654297
Exploitation OOD q-values : 30.98137664794922
Exploitation CQL Loss : 12.110965728759766
Eval_AverageReturn : -37.296295166015625
Eval_StdReturn : 11.446541786193848
Eval_MaxReturn : -20.0
Eval_MinReturn : -79.0
Eval_AverageEpLen : 38.2962962962963
Buffer size : 5001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 429.799828
At timestep:     53 / 1000At timestep:     92 / 1000At timestep:     114 / 1000At timestep:     150 / 1000At timestep:     174 / 1000At timestep:     214 / 1000At timestep:     249 / 1000At timestep:     284 / 1000At timestep:     352 / 1000At timestep:     394 / 1000At timestep:     412 / 1000At timestep:     440 / 1000At timestep:     459 / 1000At timestep:     481 / 1000At timestep:     532 / 1000At timestep:     561 / 1000At timestep:     584 / 1000At timestep:     624 / 1000At timestep:     652 / 1000At timestep:     700 / 1000At timestep:     756 / 1000At timestep:     802 / 1000At timestep:     849 / 1000At timestep:     868 / 1000At timestep:     910 / 1000At timestep:     950 / 1000At timestep:     989 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 429.7998278141022
Exploitation Critic Loss : 2.7396578788757324
Exploration Critic Loss : 11231767.0
Exploration Model Loss : 19.945297241210938
Exploitation Data q-values : 26.920440673828125
Exploitation OOD q-values : 29.193897247314453
Exploitation CQL Loss : 2.967003583908081
Eval_AverageReturn : -35.25
Eval_StdReturn : 12.428725242614746
Eval_MaxReturn : -17.0
Eval_MinReturn : -67.0
Eval_AverageEpLen : 36.25
Buffer size : 5001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 443.237884
At timestep:     60 / 1000At timestep:     95 / 1000At timestep:     144 / 1000At timestep:     194 / 1000At timestep:     223 / 1000At timestep:     267 / 1000At timestep:     302 / 1000At timestep:     340 / 1000At timestep:     364 / 1000At timestep:     417 / 1000At timestep:     453 / 1000At timestep:     480 / 1000At timestep:     523 / 1000At timestep:     553 / 1000At timestep:     578 / 1000At timestep:     622 / 1000At timestep:     657 / 1000At timestep:     693 / 1000At timestep:     731 / 1000At timestep:     808 / 1000At timestep:     832 / 1000At timestep:     871 / 1000At timestep:     906 / 1000At timestep:     931 / 1000At timestep:     959 / 1000At timestep:     976 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 443.23788380622864
Exploitation Critic Loss : 2.226957321166992
Exploration Critic Loss : 17448192.0
Exploration Model Loss : 19.945648193359375
Exploitation Data q-values : 27.572647094726562
Exploitation OOD q-values : 29.846702575683594
Exploitation CQL Loss : 2.4543628692626953
Eval_AverageReturn : -36.62963104248047
Eval_StdReturn : 12.433968544006348
Eval_MaxReturn : -16.0
Eval_MinReturn : -76.0
Eval_AverageEpLen : 37.629629629629626
Buffer size : 5001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 455.897932
At timestep:     27 / 1000At timestep:     67 / 1000At timestep:     102 / 1000At timestep:     130 / 1000At timestep:     190 / 1000At timestep:     218 / 1000At timestep:     248 / 1000At timestep:     290 / 1000At timestep:     347 / 1000At timestep:     372 / 1000At timestep:     409 / 1000At timestep:     457 / 1000At timestep:     497 / 1000At timestep:     541 / 1000At timestep:     583 / 1000At timestep:     633 / 1000At timestep:     657 / 1000At timestep:     708 / 1000At timestep:     766 / 1000At timestep:     795 / 1000At timestep:     845 / 1000At timestep:     895 / 1000At timestep:     940 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 455.89793157577515
Exploitation Critic Loss : 5.122730255126953
Exploration Critic Loss : 35746640.0
Exploration Model Loss : 19.94586181640625
Exploitation Data q-values : 30.409791946411133
Exploitation OOD q-values : 32.79301452636719
Exploitation CQL Loss : 5.361052513122559
Eval_AverageReturn : -41.33333206176758
Eval_StdReturn : 12.75626277923584
Eval_MaxReturn : -23.0
Eval_MinReturn : -75.0
Eval_AverageEpLen : 42.333333333333336
Buffer size : 5001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 468.209905
At timestep:     32 / 1000At timestep:     71 / 1000At timestep:     119 / 1000At timestep:     149 / 1000At timestep:     181 / 1000At timestep:     211 / 1000At timestep:     255 / 1000At timestep:     312 / 1000At timestep:     337 / 1000At timestep:     371 / 1000At timestep:     409 / 1000At timestep:     437 / 1000At timestep:     487 / 1000At timestep:     517 / 1000At timestep:     561 / 1000At timestep:     596 / 1000At timestep:     615 / 1000At timestep:     656 / 1000At timestep:     681 / 1000At timestep:     721 / 1000At timestep:     749 / 1000At timestep:     786 / 1000At timestep:     819 / 1000At timestep:     860 / 1000At timestep:     897 / 1000At timestep:     919 / 1000At timestep:     972 / 1000At timestep:     997 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 468.2099049091339
Exploitation Critic Loss : 2.3710148334503174
Exploration Critic Loss : 27955744.0
Exploration Model Loss : 19.946136474609375
Exploitation Data q-values : 29.972400665283203
Exploitation OOD q-values : 32.326988220214844
Exploitation CQL Loss : 2.606473922729492
Eval_AverageReturn : -34.27586364746094
Eval_StdReturn : 9.273233413696289
Eval_MaxReturn : -18.0
Eval_MinReturn : -56.0
Eval_AverageEpLen : 35.275862068965516
Buffer size : 5001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 480.875600
At timestep:     33 / 1000At timestep:     73 / 1000At timestep:     143 / 1000At timestep:     172 / 1000At timestep:     227 / 1000At timestep:     266 / 1000At timestep:     308 / 1000At timestep:     327 / 1000At timestep:     352 / 1000At timestep:     392 / 1000At timestep:     432 / 1000At timestep:     465 / 1000At timestep:     489 / 1000At timestep:     511 / 1000At timestep:     541 / 1000At timestep:     577 / 1000At timestep:     604 / 1000At timestep:     635 / 1000At timestep:     663 / 1000At timestep:     719 / 1000At timestep:     758 / 1000At timestep:     801 / 1000At timestep:     851 / 1000At timestep:     868 / 1000At timestep:     928 / 1000At timestep:     966 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 480.8756003379822
Exploitation Critic Loss : 17.86429214477539
Exploration Critic Loss : 17888996.0
Exploration Model Loss : 19.946609497070312
Exploitation Data q-values : 27.152591705322266
Exploitation OOD q-values : 29.651718139648438
Exploitation CQL Loss : 18.11420440673828
Eval_AverageReturn : -36.48147964477539
Eval_StdReturn : 12.573843002319336
Eval_MaxReturn : -16.0
Eval_MinReturn : -69.0
Eval_AverageEpLen : 37.48148148148148
Buffer size : 5001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 493.455903
At timestep:     31 / 1000At timestep:     60 / 1000At timestep:     95 / 1000At timestep:     119 / 1000At timestep:     145 / 1000At timestep:     185 / 1000At timestep:     220 / 1000At timestep:     252 / 1000At timestep:     276 / 1000At timestep:     295 / 1000At timestep:     324 / 1000At timestep:     377 / 1000At timestep:     401 / 1000At timestep:     429 / 1000At timestep:     461 / 1000At timestep:     505 / 1000At timestep:     551 / 1000At timestep:     579 / 1000At timestep:     621 / 1000At timestep:     649 / 1000At timestep:     670 / 1000At timestep:     699 / 1000At timestep:     746 / 1000At timestep:     768 / 1000At timestep:     804 / 1000At timestep:     827 / 1000At timestep:     858 / 1000At timestep:     881 / 1000At timestep:     919 / 1000At timestep:     951 / 1000At timestep:     978 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 493.4559030532837
Exploitation Critic Loss : 17.604202270507812
Exploration Critic Loss : 11191004.0
Exploration Model Loss : 19.947006225585938
Exploitation Data q-values : 28.654165267944336
Exploitation OOD q-values : 31.044071197509766
Exploitation CQL Loss : 17.84319305419922
Eval_AverageReturn : -31.4375
Eval_StdReturn : 9.519511222839355
Eval_MaxReturn : -18.0
Eval_MinReturn : -59.0
Eval_AverageEpLen : 32.4375
Buffer size : 5001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 506.427158
At timestep:     42 / 1000At timestep:     65 / 1000At timestep:     89 / 1000At timestep:     127 / 1000At timestep:     160 / 1000At timestep:     200 / 1000At timestep:     231 / 1000At timestep:     265 / 1000At timestep:     322 / 1000At timestep:     347 / 1000At timestep:     407 / 1000At timestep:     452 / 1000At timestep:     486 / 1000At timestep:     517 / 1000At timestep:     555 / 1000At timestep:     600 / 1000At timestep:     631 / 1000At timestep:     655 / 1000At timestep:     697 / 1000At timestep:     752 / 1000At timestep:     783 / 1000At timestep:     808 / 1000At timestep:     837 / 1000At timestep:     860 / 1000At timestep:     894 / 1000At timestep:     920 / 1000At timestep:     940 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 506.4271583557129
Exploitation Critic Loss : 8.702021598815918
Exploration Critic Loss : 12235646.0
Exploration Model Loss : 19.947402954101562
Exploitation Data q-values : 30.4254093170166
Exploitation OOD q-values : 33.0069694519043
Exploitation CQL Loss : 8.960177421569824
Eval_AverageReturn : -34.71428680419922
Eval_StdReturn : 11.344781875610352
Eval_MaxReturn : -19.0
Eval_MinReturn : -59.0
Eval_AverageEpLen : 35.714285714285715
Buffer size : 5001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 518.849506
At timestep:     38 / 1000At timestep:     74 / 1000At timestep:     101 / 1000At timestep:     164 / 1000At timestep:     218 / 1000At timestep:     262 / 1000At timestep:     296 / 1000At timestep:     333 / 1000At timestep:     359 / 1000At timestep:     429 / 1000At timestep:     486 / 1000At timestep:     519 / 1000At timestep:     549 / 1000At timestep:     578 / 1000At timestep:     600 / 1000At timestep:     643 / 1000At timestep:     707 / 1000At timestep:     739 / 1000At timestep:     779 / 1000At timestep:     813 / 1000At timestep:     854 / 1000At timestep:     890 / 1000At timestep:     921 / 1000At timestep:     955 / 1000At timestep:     988 / 1000At timestep:     1036 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 518.8495056629181
Exploitation Critic Loss : 2.623551845550537
Exploration Critic Loss : 11879205.0
Exploration Model Loss : 19.947555541992188
Exploitation Data q-values : 31.806774139404297
Exploitation OOD q-values : 34.450199127197266
Exploitation CQL Loss : 2.88789439201355
Eval_AverageReturn : -38.846153259277344
Eval_StdReturn : 12.224478721618652
Eval_MaxReturn : -21.0
Eval_MinReturn : -69.0
Eval_AverageEpLen : 39.84615384615385
Buffer size : 5001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 531.244502
At timestep:     19 / 1000At timestep:     64 / 1000At timestep:     88 / 1000At timestep:     132 / 1000At timestep:     171 / 1000At timestep:     214 / 1000At timestep:     247 / 1000At timestep:     286 / 1000At timestep:     321 / 1000At timestep:     343 / 1000At timestep:     364 / 1000At timestep:     411 / 1000At timestep:     442 / 1000At timestep:     463 / 1000At timestep:     497 / 1000At timestep:     523 / 1000At timestep:     549 / 1000At timestep:     587 / 1000At timestep:     629 / 1000At timestep:     656 / 1000At timestep:     679 / 1000At timestep:     724 / 1000At timestep:     761 / 1000At timestep:     799 / 1000At timestep:     826 / 1000At timestep:     851 / 1000At timestep:     885 / 1000At timestep:     920 / 1000At timestep:     944 / 1000At timestep:     963 / 1000At timestep:     997 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 531.2445023059845
Exploitation Critic Loss : 16.87904930114746
Exploration Critic Loss : 22675530.0
Exploration Model Loss : 19.94757080078125
Exploitation Data q-values : 33.116058349609375
Exploitation OOD q-values : 35.78577423095703
Exploitation CQL Loss : 17.146020889282227
Eval_AverageReturn : -31.0
Eval_StdReturn : 8.366600036621094
Eval_MaxReturn : -18.0
Eval_MinReturn : -46.0
Eval_AverageEpLen : 32.0
Buffer size : 5001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 544.096580
At timestep:     36 / 1000At timestep:     85 / 1000At timestep:     120 / 1000At timestep:     188 / 1000At timestep:     223 / 1000At timestep:     256 / 1000At timestep:     292 / 1000At timestep:     323 / 1000At timestep:     351 / 1000At timestep:     381 / 1000At timestep:     416 / 1000At timestep:     476 / 1000At timestep:     505 / 1000At timestep:     539 / 1000At timestep:     571 / 1000At timestep:     609 / 1000At timestep:     635 / 1000At timestep:     657 / 1000At timestep:     683 / 1000At timestep:     724 / 1000At timestep:     762 / 1000At timestep:     794 / 1000At timestep:     815 / 1000At timestep:     852 / 1000At timestep:     902 / 1000At timestep:     923 / 1000At timestep:     957 / 1000At timestep:     999 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 544.0965797901154
Exploitation Critic Loss : 9.338640213012695
Exploration Critic Loss : 20659784.0
Exploration Model Loss : 19.947830200195312
Exploitation Data q-values : 33.91455841064453
Exploitation OOD q-values : 36.735877990722656
Exploitation CQL Loss : 9.620772361755371
Eval_AverageReturn : -34.7931022644043
Eval_StdReturn : 10.363661766052246
Eval_MaxReturn : -20.0
Eval_MinReturn : -67.0
Eval_AverageEpLen : 35.793103448275865
Buffer size : 5001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 556.664793
At timestep:     35 / 1000At timestep:     75 / 1000At timestep:     103 / 1000At timestep:     121 / 1000At timestep:     154 / 1000At timestep:     180 / 1000At timestep:     203 / 1000At timestep:     227 / 1000At timestep:     268 / 1000At timestep:     299 / 1000At timestep:     345 / 1000At timestep:     376 / 1000At timestep:     414 / 1000At timestep:     452 / 1000At timestep:     490 / 1000At timestep:     521 / 1000At timestep:     561 / 1000At timestep:     593 / 1000At timestep:     619 / 1000At timestep:     655 / 1000At timestep:     676 / 1000At timestep:     694 / 1000At timestep:     722 / 1000At timestep:     760 / 1000At timestep:     794 / 1000At timestep:     831 / 1000At timestep:     863 / 1000At timestep:     897 / 1000At timestep:     917 / 1000At timestep:     963 / 1000At timestep:     998 / 1000At timestep:     1036 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 556.6647925376892
Exploitation Critic Loss : 15.621432304382324
Exploration Critic Loss : 16604556.0
Exploration Model Loss : 19.948089599609375
Exploitation Data q-values : 35.764671325683594
Exploitation OOD q-values : 38.56727981567383
Exploitation CQL Loss : 15.901693344116211
Eval_AverageReturn : -31.375
Eval_StdReturn : 7.402491092681885
Eval_MaxReturn : -17.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 32.375
Buffer size : 5001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 569.469991
At timestep:     44 / 1000At timestep:     88 / 1000At timestep:     111 / 1000At timestep:     145 / 1000At timestep:     165 / 1000At timestep:     215 / 1000At timestep:     240 / 1000At timestep:     273 / 1000At timestep:     322 / 1000At timestep:     362 / 1000At timestep:     382 / 1000At timestep:     433 / 1000At timestep:     470 / 1000At timestep:     512 / 1000At timestep:     550 / 1000At timestep:     593 / 1000At timestep:     654 / 1000At timestep:     694 / 1000At timestep:     724 / 1000At timestep:     754 / 1000At timestep:     782 / 1000At timestep:     821 / 1000At timestep:     851 / 1000At timestep:     887 / 1000At timestep:     926 / 1000At timestep:     957 / 1000At timestep:     985 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 569.4699907302856
Exploitation Critic Loss : 17.59650993347168
Exploration Critic Loss : 13439852.0
Exploration Model Loss : 19.948379516601562
Exploitation Data q-values : 37.372413635253906
Exploitation OOD q-values : 40.23837661743164
Exploitation CQL Loss : 17.883106231689453
Eval_AverageReturn : -35.75
Eval_StdReturn : 9.649666786193848
Eval_MaxReturn : -19.0
Eval_MinReturn : -60.0
Eval_AverageEpLen : 36.75
Buffer size : 5001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 582.062068
At timestep:     39 / 1000At timestep:     61 / 1000At timestep:     93 / 1000At timestep:     152 / 1000At timestep:     173 / 1000At timestep:     192 / 1000At timestep:     215 / 1000At timestep:     241 / 1000At timestep:     285 / 1000At timestep:     321 / 1000At timestep:     338 / 1000At timestep:     381 / 1000At timestep:     423 / 1000At timestep:     443 / 1000At timestep:     486 / 1000At timestep:     517 / 1000At timestep:     552 / 1000At timestep:     579 / 1000At timestep:     609 / 1000At timestep:     650 / 1000At timestep:     685 / 1000At timestep:     712 / 1000At timestep:     749 / 1000At timestep:     779 / 1000At timestep:     809 / 1000At timestep:     831 / 1000At timestep:     860 / 1000At timestep:     893 / 1000At timestep:     939 / 1000At timestep:     975 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 582.0620684623718
Exploitation Critic Loss : 6.4175639152526855
Exploration Critic Loss : 12945380.0
Exploration Model Loss : 19.948638916015625
Exploitation Data q-values : 36.603919982910156
Exploitation OOD q-values : 39.29133605957031
Exploitation CQL Loss : 6.686305522918701
Eval_AverageReturn : -31.612903594970703
Eval_StdReturn : 9.279451370239258
Eval_MaxReturn : -16.0
Eval_MinReturn : -58.0
Eval_AverageEpLen : 32.61290322580645
Buffer size : 5001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 594.863066
At timestep:     27 / 1000At timestep:     59 / 1000At timestep:     77 / 1000At timestep:     111 / 1000At timestep:     140 / 1000At timestep:     179 / 1000At timestep:     207 / 1000At timestep:     228 / 1000At timestep:     260 / 1000At timestep:     287 / 1000At timestep:     314 / 1000At timestep:     329 / 1000At timestep:     361 / 1000At timestep:     390 / 1000At timestep:     417 / 1000At timestep:     444 / 1000At timestep:     474 / 1000At timestep:     497 / 1000At timestep:     517 / 1000At timestep:     561 / 1000At timestep:     590 / 1000At timestep:     612 / 1000At timestep:     644 / 1000At timestep:     680 / 1000At timestep:     708 / 1000At timestep:     747 / 1000At timestep:     778 / 1000At timestep:     792 / 1000At timestep:     824 / 1000At timestep:     858 / 1000At timestep:     883 / 1000At timestep:     915 / 1000At timestep:     948 / 1000At timestep:     968 / 1000At timestep:     997 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 594.8630659580231
Exploitation Critic Loss : 23.350072860717773
Exploration Critic Loss : 16857220.0
Exploration Model Loss : 19.948638916015625
Exploitation Data q-values : 37.83280944824219
Exploitation OOD q-values : 40.68446350097656
Exploitation CQL Loss : 23.635238647460938
Eval_AverageReturn : -27.25
Eval_StdReturn : 6.563260555267334
Eval_MaxReturn : -13.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 28.25
Buffer size : 5001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 608.231035
At timestep:     32 / 1000At timestep:     59 / 1000At timestep:     85 / 1000At timestep:     116 / 1000At timestep:     143 / 1000At timestep:     166 / 1000At timestep:     199 / 1000At timestep:     229 / 1000At timestep:     254 / 1000At timestep:     292 / 1000At timestep:     334 / 1000At timestep:     379 / 1000At timestep:     415 / 1000At timestep:     449 / 1000At timestep:     479 / 1000At timestep:     527 / 1000At timestep:     552 / 1000At timestep:     582 / 1000At timestep:     632 / 1000At timestep:     660 / 1000At timestep:     681 / 1000At timestep:     707 / 1000At timestep:     731 / 1000At timestep:     754 / 1000At timestep:     775 / 1000At timestep:     809 / 1000At timestep:     844 / 1000At timestep:     891 / 1000At timestep:     915 / 1000At timestep:     938 / 1000At timestep:     986 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 608.2310345172882
Exploitation Critic Loss : 23.780105590820312
Exploration Critic Loss : 14613612.0
Exploration Model Loss : 19.948837280273438
Exploitation Data q-values : 38.52677536010742
Exploitation OOD q-values : 41.662960052490234
Exploitation CQL Loss : 24.09372329711914
Eval_AverageReturn : -30.9375
Eval_StdReturn : 8.437037467956543
Eval_MaxReturn : -20.0
Eval_MinReturn : -49.0
Eval_AverageEpLen : 31.9375
Buffer size : 5001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 621.203490
At timestep:     19 / 1000At timestep:     49 / 1000At timestep:     82 / 1000At timestep:     117 / 1000At timestep:     148 / 1000At timestep:     170 / 1000At timestep:     213 / 1000At timestep:     240 / 1000At timestep:     256 / 1000At timestep:     288 / 1000At timestep:     312 / 1000At timestep:     340 / 1000At timestep:     361 / 1000At timestep:     388 / 1000At timestep:     415 / 1000At timestep:     436 / 1000At timestep:     453 / 1000At timestep:     487 / 1000At timestep:     509 / 1000At timestep:     532 / 1000At timestep:     556 / 1000At timestep:     584 / 1000At timestep:     629 / 1000At timestep:     654 / 1000At timestep:     680 / 1000At timestep:     719 / 1000At timestep:     777 / 1000At timestep:     807 / 1000At timestep:     823 / 1000At timestep:     858 / 1000At timestep:     879 / 1000At timestep:     911 / 1000At timestep:     940 / 1000At timestep:     967 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 621.2034902572632
Exploitation Critic Loss : 24.79080581665039
Exploration Critic Loss : 14003564.0
Exploration Model Loss : 19.949005126953125
Exploitation Data q-values : 39.48551559448242
Exploitation OOD q-values : 42.647560119628906
Exploitation CQL Loss : 25.107009887695312
Eval_AverageReturn : -27.657142639160156
Eval_StdReturn : 8.578853607177734
Eval_MaxReturn : -15.0
Eval_MinReturn : -57.0
Eval_AverageEpLen : 28.65714285714286
Buffer size : 5001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 634.333224
At timestep:     23 / 1000At timestep:     49 / 1000At timestep:     68 / 1000At timestep:     92 / 1000At timestep:     122 / 1000At timestep:     148 / 1000At timestep:     171 / 1000At timestep:     195 / 1000At timestep:     229 / 1000At timestep:     266 / 1000At timestep:     300 / 1000At timestep:     339 / 1000At timestep:     362 / 1000At timestep:     387 / 1000At timestep:     410 / 1000At timestep:     438 / 1000At timestep:     465 / 1000At timestep:     495 / 1000At timestep:     526 / 1000At timestep:     560 / 1000At timestep:     596 / 1000At timestep:     622 / 1000At timestep:     651 / 1000At timestep:     679 / 1000At timestep:     700 / 1000At timestep:     725 / 1000At timestep:     747 / 1000At timestep:     771 / 1000At timestep:     795 / 1000At timestep:     827 / 1000At timestep:     853 / 1000At timestep:     911 / 1000At timestep:     937 / 1000At timestep:     964 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 634.3332242965698
Exploitation Critic Loss : 15.162164688110352
Exploration Critic Loss : 11289827.0
Exploration Model Loss : 19.949325561523438
Exploitation Data q-values : 39.973487854003906
Exploitation OOD q-values : 43.1287841796875
Exploitation CQL Loss : 15.477694511413574
Eval_AverageReturn : -27.600000381469727
Eval_StdReturn : 7.063791275024414
Eval_MaxReturn : -18.0
Eval_MinReturn : -57.0
Eval_AverageEpLen : 28.6
Buffer size : 5001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 647.526361
At timestep:     28 / 1000At timestep:     49 / 1000At timestep:     73 / 1000At timestep:     99 / 1000At timestep:     127 / 1000At timestep:     156 / 1000At timestep:     180 / 1000At timestep:     220 / 1000At timestep:     263 / 1000At timestep:     298 / 1000At timestep:     319 / 1000At timestep:     342 / 1000At timestep:     365 / 1000At timestep:     389 / 1000At timestep:     420 / 1000At timestep:     464 / 1000At timestep:     497 / 1000At timestep:     515 / 1000At timestep:     539 / 1000At timestep:     569 / 1000At timestep:     595 / 1000At timestep:     629 / 1000At timestep:     653 / 1000At timestep:     686 / 1000At timestep:     711 / 1000At timestep:     736 / 1000At timestep:     764 / 1000At timestep:     788 / 1000At timestep:     823 / 1000At timestep:     858 / 1000At timestep:     881 / 1000At timestep:     909 / 1000At timestep:     931 / 1000At timestep:     968 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 647.5263612270355
Exploitation Critic Loss : 24.12515640258789
Exploration Critic Loss : 15690064.0
Exploration Model Loss : 19.949432373046875
Exploitation Data q-values : 40.30072784423828
Exploitation OOD q-values : 43.23552322387695
Exploitation CQL Loss : 24.418636322021484
Eval_AverageReturn : -27.600000381469727
Eval_StdReturn : 6.316418170928955
Eval_MaxReturn : -17.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 28.6
Buffer size : 5001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -150.000000
best mean reward -147.750000
running time 660.713270
At timestep:     47 / 1000At timestep:     82 / 1000At timestep:     110 / 1000At timestep:     136 / 1000At timestep:     159 / 1000At timestep:     184 / 1000At timestep:     226 / 1000At timestep:     253 / 1000At timestep:     301 / 1000At timestep:     325 / 1000At timestep:     362 / 1000At timestep:     383 / 1000At timestep:     412 / 1000At timestep:     431 / 1000At timestep:     452 / 1000At timestep:     489 / 1000At timestep:     518 / 1000At timestep:     548 / 1000At timestep:     573 / 1000At timestep:     608 / 1000At timestep:     634 / 1000At timestep:     676 / 1000At timestep:     694 / 1000At timestep:     718 / 1000At timestep:     737 / 1000At timestep:     763 / 1000At timestep:     781 / 1000At timestep:     809 / 1000At timestep:     838 / 1000At timestep:     873 / 1000At timestep:     897 / 1000At timestep:     931 / 1000At timestep:     967 / 1000At timestep:     997 / 1000At timestep:     1024 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q2_cql_numsteps_5000_PointmassMedium-v0_21-11-2022_22-16-04/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -150.0
Train_BestReturn : -147.75
TimeSinceStart : 660.7132701873779
Exploitation Critic Loss : 4.480497360229492
Exploration Critic Loss : 13022516.0
Exploration Model Loss : 19.949722290039062
Exploitation Data q-values : 41.14679718017578
Exploitation OOD q-values : 44.17103958129883
Exploitation CQL Loss : 4.78292179107666
Eval_AverageReturn : -28.257143020629883
Eval_StdReturn : 7.713967800140381
Eval_MaxReturn : -17.0
Eval_MinReturn : -47.0
Eval_AverageEpLen : 29.257142857142856
Buffer size : 5001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_dqn_numsteps_5000_PointmassMedium-v0_21-11-2022_22-27-21 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_dqn_numsteps_5000_PointmassMedium-v0_21-11-2022_22-27-21
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001660
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0016596317291259766
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 4.513465
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 4.513465166091919
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -144.846161
best mean reward -inf
running time 9.113545
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -144.84616088867188
TimeSinceStart : 9.113545179367065
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -146.649994
best mean reward -inf
running time 19.830886
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -146.64999389648438
TimeSinceStart : 19.830886125564575
Exploitation Critic Loss : 1.3774898052215576
Exploration Critic Loss : 990907.875
Exploration Model Loss : 53.24404525756836
Exploitation Data q-values : 0.8570339679718018
Exploitation OOD q-values : 2.902038097381592
Exploitation CQL Loss : 1.3774898052215576
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -147.518524
best mean reward -inf
running time 31.011088
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -147.51852416992188
TimeSinceStart : 31.011087656021118
Exploitation Critic Loss : 0.9613120555877686
Exploration Critic Loss : 5683170.5
Exploration Model Loss : 60.30759048461914
Exploitation Data q-values : 3.2918453216552734
Exploitation OOD q-values : 5.441035747528076
Exploitation CQL Loss : 0.9613120555877686
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -147.969696
best mean reward -inf
running time 42.001437
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -147.96969604492188
TimeSinceStart : 42.0014374256134
Exploitation Critic Loss : 5.096776962280273
Exploration Critic Loss : 12640203.0
Exploration Model Loss : 111.49342346191406
Exploitation Data q-values : 6.488076210021973
Exploitation OOD q-values : 8.847882270812988
Exploitation CQL Loss : 5.096776962280273
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -148.324997
best mean reward -inf
running time 53.024373
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -148.3249969482422
TimeSinceStart : 53.024373054504395
Exploitation Critic Loss : 1.8077466487884521
Exploration Critic Loss : 9486978.0
Exploration Model Loss : 36.08589553833008
Exploitation Data q-values : 12.386632919311523
Exploitation OOD q-values : 15.134622573852539
Exploitation CQL Loss : 1.8077466487884521
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -148.574463
best mean reward -inf
running time 63.882474
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -148.574462890625
TimeSinceStart : 63.8824737071991
Exploitation Critic Loss : 2.812471389770508
Exploration Critic Loss : 8632962.0
Exploration Model Loss : 45.293060302734375
Exploitation Data q-values : 15.805435180664062
Exploitation OOD q-values : 18.35546875
Exploitation CQL Loss : 2.812471389770508
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -148.735855
best mean reward -inf
running time 74.564711
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -148.73585510253906
TimeSinceStart : 74.56471109390259
Exploitation Critic Loss : 7.272730827331543
Exploration Critic Loss : 11902946.0
Exploration Model Loss : 59.58013916015625
Exploitation Data q-values : 17.984798431396484
Exploitation OOD q-values : 20.396507263183594
Exploitation CQL Loss : 7.272730827331543
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -148.883331
best mean reward -inf
running time 85.522569
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -148.88333129882812
TimeSinceStart : 85.52256917953491
Exploitation Critic Loss : 6.196702003479004
Exploration Critic Loss : 6259135.0
Exploration Model Loss : 25.355667114257812
Exploitation Data q-values : 21.344152450561523
Exploitation OOD q-values : 23.822071075439453
Exploitation CQL Loss : 6.196702003479004
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -149.000000
best mean reward -inf
running time 96.257106
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -149.0
TimeSinceStart : 96.25710558891296
Exploitation Critic Loss : 8.686284065246582
Exploration Critic Loss : 9564625.0
Exploration Model Loss : 33.726470947265625
Exploitation Data q-values : 24.201324462890625
Exploitation OOD q-values : 27.902416229248047
Exploitation CQL Loss : 8.686284065246582
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -149.082199
best mean reward -inf
running time 106.946447
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -149.0821990966797
TimeSinceStart : 106.94644713401794
Exploitation Critic Loss : 6.582695007324219
Exploration Critic Loss : 14917400.0
Exploration Model Loss : 55.010345458984375
Exploitation Data q-values : 29.825559616088867
Exploitation OOD q-values : 34.44805145263672
Exploitation CQL Loss : 6.582695007324219
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -149.162506
best mean reward -inf
running time 117.849921
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -149.16250610351562
TimeSinceStart : 117.84992146492004
Exploitation Critic Loss : 10.243989944458008
Exploration Critic Loss : 5771768.0
Exploration Model Loss : 24.557754516601562
Exploitation Data q-values : 36.395301818847656
Exploitation OOD q-values : 40.95098876953125
Exploitation CQL Loss : 10.243989944458008
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -149.229889
best mean reward -inf
running time 128.841076
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -149.22988891601562
TimeSinceStart : 128.84107565879822
Exploitation Critic Loss : 21.910329818725586
Exploration Critic Loss : 9357246.0
Exploration Model Loss : 33.350982666015625
Exploitation Data q-values : 45.234474182128906
Exploitation OOD q-values : 50.85604476928711
Exploitation CQL Loss : 21.910329818725586
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -149.279572
best mean reward -inf
running time 139.692221
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -149.27957153320312
TimeSinceStart : 139.6922209262848
Exploitation Critic Loss : 14.898172378540039
Exploration Critic Loss : 11649599.0
Exploration Model Loss : 54.8109130859375
Exploitation Data q-values : 49.27683639526367
Exploitation OOD q-values : 53.98704528808594
Exploitation CQL Loss : 14.898172378540039
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -149.330002
best mean reward -inf
running time 150.629739
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -149.3300018310547
TimeSinceStart : 150.62973856925964
Exploitation Critic Loss : 9.538352966308594
Exploration Critic Loss : 6294576.0
Exploration Model Loss : 24.519699096679688
Exploitation Data q-values : 57.2050666809082
Exploitation OOD q-values : 61.910919189453125
Exploitation CQL Loss : 9.538352966308594
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -149.330002
best mean reward -149.330002
running time 161.709042
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -149.3300018310547
Train_BestReturn : -149.3300018310547
TimeSinceStart : 161.70904231071472
Exploitation Critic Loss : 23.33608627319336
Exploration Critic Loss : 11943142.0
Exploration Model Loss : 33.3319091796875
Exploitation Data q-values : 55.043670654296875
Exploitation OOD q-values : 58.46152877807617
Exploitation CQL Loss : 23.33608627319336
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 172.549101
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     787 / 1000At timestep:     861 / 1000At timestep:     897 / 1000At timestep:     994 / 1000At timestep:     1046 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 172.5491008758545
Exploitation Critic Loss : 25.50544548034668
Exploration Critic Loss : 22828908.0
Exploration Model Loss : 54.802978515625
Exploitation Data q-values : 58.18666076660156
Exploitation OOD q-values : 61.20751190185547
Exploitation CQL Loss : 25.50544548034668
Eval_AverageReturn : -104.0999984741211
Eval_StdReturn : 48.784114837646484
Eval_MaxReturn : -35.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 104.6
Buffer size : 5001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 183.775476
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 183.77547597885132
Exploitation Critic Loss : 38.62564468383789
Exploration Critic Loss : 4435739.0
Exploration Model Loss : 24.513946533203125
Exploitation Data q-values : 55.33925247192383
Exploitation OOD q-values : 57.936859130859375
Exploitation CQL Loss : 38.62564468383789
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 194.652044
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 194.65204429626465
Exploitation Critic Loss : 2.5629420280456543
Exploration Critic Loss : 5631676.0
Exploration Model Loss : 33.32752990722656
Exploitation Data q-values : 50.311683654785156
Exploitation OOD q-values : 52.481101989746094
Exploitation CQL Loss : 2.5629420280456543
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 205.458590
At timestep:     84 / 1000At timestep:     219 / 1000At timestep:     312 / 1000At timestep:     354 / 1000At timestep:     448 / 1000At timestep:     538 / 1000At timestep:     659 / 1000At timestep:     692 / 1000At timestep:     723 / 1000At timestep:     782 / 1000At timestep:     840 / 1000At timestep:     937 / 1000At timestep:     982 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 205.45859003067017
Exploitation Critic Loss : 21.662431716918945
Exploration Critic Loss : 9874256.0
Exploration Model Loss : 54.806243896484375
Exploitation Data q-values : 46.469154357910156
Exploitation OOD q-values : 48.743011474609375
Exploitation CQL Loss : 21.662431716918945
Eval_AverageReturn : -72.0
Eval_StdReturn : 32.28666305541992
Eval_MaxReturn : -30.0
Eval_MinReturn : -134.0
Eval_AverageEpLen : 73.0
Buffer size : 5001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 216.971924
At timestep:     77 / 1000At timestep:     98 / 1000At timestep:     175 / 1000At timestep:     218 / 1000At timestep:     271 / 1000At timestep:     331 / 1000At timestep:     404 / 1000At timestep:     427 / 1000At timestep:     543 / 1000At timestep:     590 / 1000At timestep:     708 / 1000At timestep:     777 / 1000At timestep:     825 / 1000At timestep:     885 / 1000At timestep:     978 / 1000At timestep:     1114 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 216.97192430496216
Exploitation Critic Loss : 2.141045331954956
Exploration Critic Loss : 3468693.75
Exploration Model Loss : 24.514175415039062
Exploitation Data q-values : 42.172752380371094
Exploitation OOD q-values : 44.41157531738281
Exploitation CQL Loss : 2.141045331954956
Eval_AverageReturn : -68.625
Eval_StdReturn : 31.870588302612305
Eval_MaxReturn : -20.0
Eval_MinReturn : -135.0
Eval_AverageEpLen : 69.625
Buffer size : 5001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 228.512521
At timestep:     25 / 1000At timestep:     79 / 1000At timestep:     114 / 1000At timestep:     155 / 1000At timestep:     192 / 1000At timestep:     216 / 1000At timestep:     243 / 1000At timestep:     277 / 1000At timestep:     317 / 1000At timestep:     350 / 1000At timestep:     375 / 1000At timestep:     402 / 1000At timestep:     474 / 1000At timestep:     541 / 1000At timestep:     587 / 1000At timestep:     641 / 1000At timestep:     674 / 1000At timestep:     700 / 1000At timestep:     746 / 1000At timestep:     780 / 1000At timestep:     816 / 1000At timestep:     859 / 1000At timestep:     886 / 1000At timestep:     915 / 1000At timestep:     944 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 228.5125207901001
Exploitation Critic Loss : 10.71800422668457
Exploration Critic Loss : 5404305.0
Exploration Model Loss : 33.32752990722656
Exploitation Data q-values : 39.432247161865234
Exploitation OOD q-values : 41.82500457763672
Exploitation CQL Loss : 10.71800422668457
Eval_AverageReturn : -37.96154022216797
Eval_StdReturn : 13.743475914001465
Eval_MaxReturn : -23.0
Eval_MinReturn : -71.0
Eval_AverageEpLen : 38.96153846153846
Buffer size : 5001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 240.705805
At timestep:     36 / 1000At timestep:     66 / 1000At timestep:     87 / 1000At timestep:     114 / 1000At timestep:     148 / 1000At timestep:     179 / 1000At timestep:     213 / 1000At timestep:     247 / 1000At timestep:     277 / 1000At timestep:     328 / 1000At timestep:     356 / 1000At timestep:     384 / 1000At timestep:     409 / 1000At timestep:     444 / 1000At timestep:     483 / 1000At timestep:     527 / 1000At timestep:     573 / 1000At timestep:     613 / 1000At timestep:     655 / 1000At timestep:     688 / 1000At timestep:     713 / 1000At timestep:     769 / 1000At timestep:     789 / 1000At timestep:     817 / 1000At timestep:     841 / 1000At timestep:     868 / 1000At timestep:     886 / 1000At timestep:     935 / 1000At timestep:     970 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 240.70580458641052
Exploitation Critic Loss : 15.23246955871582
Exploration Critic Loss : 9942084.0
Exploration Model Loss : 54.805419921875
Exploitation Data q-values : 38.61491394042969
Exploitation OOD q-values : 40.95621871948242
Exploitation CQL Loss : 15.23246955871582
Eval_AverageReturn : -32.46666717529297
Eval_StdReturn : 9.120429039001465
Eval_MaxReturn : -17.0
Eval_MinReturn : -55.0
Eval_AverageEpLen : 33.46666666666667
Buffer size : 5001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 253.596689
At timestep:     24 / 1000At timestep:     49 / 1000At timestep:     73 / 1000At timestep:     106 / 1000At timestep:     137 / 1000At timestep:     156 / 1000At timestep:     193 / 1000At timestep:     230 / 1000At timestep:     257 / 1000At timestep:     291 / 1000At timestep:     318 / 1000At timestep:     364 / 1000At timestep:     388 / 1000At timestep:     433 / 1000At timestep:     462 / 1000At timestep:     513 / 1000At timestep:     539 / 1000At timestep:     572 / 1000At timestep:     605 / 1000At timestep:     636 / 1000At timestep:     659 / 1000At timestep:     683 / 1000At timestep:     724 / 1000At timestep:     759 / 1000At timestep:     800 / 1000At timestep:     849 / 1000At timestep:     876 / 1000At timestep:     911 / 1000At timestep:     955 / 1000At timestep:     994 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 253.59668898582458
Exploitation Critic Loss : 26.4090576171875
Exploration Critic Loss : 3642552.25
Exploration Model Loss : 24.514724731445312
Exploitation Data q-values : 37.92068862915039
Exploitation OOD q-values : 40.51091766357422
Exploitation CQL Loss : 26.4090576171875
Eval_AverageReturn : -32.25806427001953
Eval_StdReturn : 8.226502418518066
Eval_MaxReturn : -18.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 33.25806451612903
Buffer size : 5001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 266.655257
At timestep:     31 / 1000At timestep:     48 / 1000At timestep:     72 / 1000At timestep:     104 / 1000At timestep:     151 / 1000At timestep:     176 / 1000At timestep:     206 / 1000At timestep:     233 / 1000At timestep:     259 / 1000At timestep:     281 / 1000At timestep:     300 / 1000At timestep:     333 / 1000At timestep:     373 / 1000At timestep:     395 / 1000At timestep:     418 / 1000At timestep:     441 / 1000At timestep:     473 / 1000At timestep:     519 / 1000At timestep:     556 / 1000At timestep:     604 / 1000At timestep:     645 / 1000At timestep:     676 / 1000At timestep:     712 / 1000At timestep:     738 / 1000At timestep:     770 / 1000At timestep:     810 / 1000At timestep:     850 / 1000At timestep:     901 / 1000At timestep:     943 / 1000At timestep:     971 / 1000At timestep:     1035 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 266.65525698661804
Exploitation Critic Loss : 19.62063980102539
Exploration Critic Loss : 4901941.0
Exploration Model Loss : 33.327178955078125
Exploitation Data q-values : 37.34010314941406
Exploitation OOD q-values : 39.86480712890625
Exploitation CQL Loss : 19.62063980102539
Eval_AverageReturn : -32.3870964050293
Eval_StdReturn : 10.480942726135254
Eval_MaxReturn : -16.0
Eval_MinReturn : -63.0
Eval_AverageEpLen : 33.38709677419355
Buffer size : 5001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 279.385453
At timestep:     43 / 1000At timestep:     74 / 1000At timestep:     95 / 1000At timestep:     131 / 1000At timestep:     158 / 1000At timestep:     184 / 1000At timestep:     215 / 1000At timestep:     249 / 1000At timestep:     287 / 1000At timestep:     324 / 1000At timestep:     377 / 1000At timestep:     407 / 1000At timestep:     430 / 1000At timestep:     458 / 1000At timestep:     499 / 1000At timestep:     531 / 1000At timestep:     556 / 1000At timestep:     591 / 1000At timestep:     614 / 1000At timestep:     655 / 1000At timestep:     682 / 1000At timestep:     719 / 1000At timestep:     735 / 1000At timestep:     774 / 1000At timestep:     798 / 1000At timestep:     823 / 1000At timestep:     851 / 1000At timestep:     875 / 1000At timestep:     925 / 1000At timestep:     985 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 279.3854525089264
Exploitation Critic Loss : 40.07148742675781
Exploration Critic Loss : 8474807.0
Exploration Model Loss : 54.807464599609375
Exploitation Data q-values : 37.97631072998047
Exploitation OOD q-values : 40.636085510253906
Exploitation CQL Loss : 40.07148742675781
Eval_AverageReturn : -31.516128540039062
Eval_StdReturn : 9.767857551574707
Eval_MaxReturn : -15.0
Eval_MinReturn : -59.0
Eval_AverageEpLen : 32.516129032258064
Buffer size : 5001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 292.495743
At timestep:     51 / 1000At timestep:     79 / 1000At timestep:     98 / 1000At timestep:     128 / 1000At timestep:     160 / 1000At timestep:     196 / 1000At timestep:     226 / 1000At timestep:     282 / 1000At timestep:     311 / 1000At timestep:     367 / 1000At timestep:     392 / 1000At timestep:     423 / 1000At timestep:     456 / 1000At timestep:     480 / 1000At timestep:     533 / 1000At timestep:     565 / 1000At timestep:     596 / 1000At timestep:     624 / 1000At timestep:     663 / 1000At timestep:     695 / 1000At timestep:     718 / 1000At timestep:     736 / 1000At timestep:     762 / 1000At timestep:     807 / 1000At timestep:     855 / 1000At timestep:     904 / 1000At timestep:     934 / 1000At timestep:     955 / 1000At timestep:     989 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 292.4957432746887
Exploitation Critic Loss : 2.3881583213806152
Exploration Critic Loss : 3226872.0
Exploration Model Loss : 24.512680053710938
Exploitation Data q-values : 37.2404670715332
Exploitation OOD q-values : 39.92052459716797
Exploitation CQL Loss : 2.3881583213806152
Eval_AverageReturn : -33.0
Eval_StdReturn : 10.630146026611328
Eval_MaxReturn : -17.0
Eval_MinReturn : -55.0
Eval_AverageEpLen : 34.0
Buffer size : 5001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 305.393400
At timestep:     29 / 1000At timestep:     49 / 1000At timestep:     84 / 1000At timestep:     115 / 1000At timestep:     147 / 1000At timestep:     180 / 1000At timestep:     214 / 1000At timestep:     230 / 1000At timestep:     259 / 1000At timestep:     287 / 1000At timestep:     305 / 1000At timestep:     334 / 1000At timestep:     352 / 1000At timestep:     390 / 1000At timestep:     431 / 1000At timestep:     466 / 1000At timestep:     506 / 1000At timestep:     566 / 1000At timestep:     587 / 1000At timestep:     616 / 1000At timestep:     643 / 1000At timestep:     680 / 1000At timestep:     714 / 1000At timestep:     748 / 1000At timestep:     781 / 1000At timestep:     807 / 1000At timestep:     836 / 1000At timestep:     870 / 1000At timestep:     894 / 1000At timestep:     943 / 1000At timestep:     968 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 305.39339995384216
Exploitation Critic Loss : 25.49334144592285
Exploration Critic Loss : 5616995.0
Exploration Model Loss : 33.32623291015625
Exploitation Data q-values : 37.47135543823242
Exploitation OOD q-values : 40.60690689086914
Exploitation CQL Loss : 25.49334144592285
Eval_AverageReturn : -30.3125
Eval_StdReturn : 8.755132675170898
Eval_MaxReturn : -15.0
Eval_MinReturn : -59.0
Eval_AverageEpLen : 31.3125
Buffer size : 5001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 318.378837
At timestep:     26 / 1000At timestep:     44 / 1000At timestep:     68 / 1000At timestep:     92 / 1000At timestep:     127 / 1000At timestep:     150 / 1000At timestep:     184 / 1000At timestep:     216 / 1000At timestep:     241 / 1000At timestep:     274 / 1000At timestep:     317 / 1000At timestep:     336 / 1000At timestep:     360 / 1000At timestep:     379 / 1000At timestep:     405 / 1000At timestep:     444 / 1000At timestep:     471 / 1000At timestep:     510 / 1000At timestep:     545 / 1000At timestep:     570 / 1000At timestep:     591 / 1000At timestep:     620 / 1000At timestep:     657 / 1000At timestep:     700 / 1000At timestep:     732 / 1000At timestep:     759 / 1000At timestep:     781 / 1000At timestep:     805 / 1000At timestep:     824 / 1000At timestep:     850 / 1000At timestep:     873 / 1000At timestep:     900 / 1000At timestep:     935 / 1000At timestep:     969 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 318.37883710861206
Exploitation Critic Loss : 7.189901351928711
Exploration Critic Loss : 9667834.0
Exploration Model Loss : 54.80670166015625
Exploitation Data q-values : 40.66163635253906
Exploitation OOD q-values : 43.401058197021484
Exploitation CQL Loss : 7.189901351928711
Eval_AverageReturn : -27.742856979370117
Eval_StdReturn : 6.941975116729736
Eval_MaxReturn : -17.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 28.742857142857144
Buffer size : 5001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 333.294035
At timestep:     21 / 1000At timestep:     46 / 1000At timestep:     91 / 1000At timestep:     117 / 1000At timestep:     145 / 1000At timestep:     169 / 1000At timestep:     187 / 1000At timestep:     230 / 1000At timestep:     274 / 1000At timestep:     305 / 1000At timestep:     327 / 1000At timestep:     363 / 1000At timestep:     384 / 1000At timestep:     426 / 1000At timestep:     451 / 1000At timestep:     477 / 1000At timestep:     512 / 1000At timestep:     544 / 1000At timestep:     580 / 1000At timestep:     603 / 1000At timestep:     626 / 1000At timestep:     656 / 1000At timestep:     687 / 1000At timestep:     708 / 1000At timestep:     737 / 1000At timestep:     777 / 1000At timestep:     800 / 1000At timestep:     817 / 1000At timestep:     841 / 1000At timestep:     864 / 1000At timestep:     904 / 1000At timestep:     934 / 1000At timestep:     960 / 1000At timestep:     984 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 333.2940351963043
Exploitation Critic Loss : 26.545454025268555
Exploration Critic Loss : 3546594.5
Exploration Model Loss : 24.513427734375
Exploitation Data q-values : 40.979496002197266
Exploitation OOD q-values : 43.85000991821289
Exploitation CQL Loss : 26.545454025268555
Eval_AverageReturn : -27.828571319580078
Eval_StdReturn : 7.636325359344482
Eval_MaxReturn : -16.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 28.82857142857143
Buffer size : 5001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 348.114567
At timestep:     44 / 1000At timestep:     73 / 1000At timestep:     118 / 1000At timestep:     156 / 1000At timestep:     172 / 1000At timestep:     203 / 1000At timestep:     222 / 1000At timestep:     240 / 1000At timestep:     277 / 1000At timestep:     302 / 1000At timestep:     336 / 1000At timestep:     350 / 1000At timestep:     388 / 1000At timestep:     428 / 1000At timestep:     456 / 1000At timestep:     488 / 1000At timestep:     505 / 1000At timestep:     527 / 1000At timestep:     554 / 1000At timestep:     573 / 1000At timestep:     606 / 1000At timestep:     634 / 1000At timestep:     661 / 1000At timestep:     683 / 1000At timestep:     706 / 1000At timestep:     726 / 1000At timestep:     754 / 1000At timestep:     777 / 1000At timestep:     808 / 1000At timestep:     833 / 1000At timestep:     854 / 1000At timestep:     887 / 1000At timestep:     914 / 1000At timestep:     941 / 1000At timestep:     991 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 348.1145672798157
Exploitation Critic Loss : 13.995363235473633
Exploration Critic Loss : 4546251.0
Exploration Model Loss : 33.32740783691406
Exploitation Data q-values : 41.83233642578125
Exploitation OOD q-values : 44.69976806640625
Exploitation CQL Loss : 13.995363235473633
Eval_AverageReturn : -27.52777862548828
Eval_StdReturn : 8.513016700744629
Eval_MaxReturn : -13.0
Eval_MinReturn : -49.0
Eval_AverageEpLen : 28.52777777777778
Buffer size : 5001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 363.636664
At timestep:     23 / 1000At timestep:     63 / 1000At timestep:     108 / 1000At timestep:     132 / 1000At timestep:     160 / 1000At timestep:     199 / 1000At timestep:     235 / 1000At timestep:     252 / 1000At timestep:     292 / 1000At timestep:     325 / 1000At timestep:     345 / 1000At timestep:     380 / 1000At timestep:     404 / 1000At timestep:     439 / 1000At timestep:     465 / 1000At timestep:     483 / 1000At timestep:     513 / 1000At timestep:     550 / 1000At timestep:     571 / 1000At timestep:     601 / 1000At timestep:     627 / 1000At timestep:     666 / 1000At timestep:     703 / 1000At timestep:     725 / 1000At timestep:     759 / 1000At timestep:     798 / 1000At timestep:     828 / 1000At timestep:     861 / 1000At timestep:     886 / 1000At timestep:     923 / 1000At timestep:     938 / 1000At timestep:     966 / 1000At timestep:     992 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 363.6366641521454
Exploitation Critic Loss : 16.297203063964844
Exploration Critic Loss : 6332378.0
Exploration Model Loss : 54.805694580078125
Exploitation Data q-values : 42.11908721923828
Exploitation OOD q-values : 45.05991744995117
Exploitation CQL Loss : 16.297203063964844
Eval_AverageReturn : -29.52941131591797
Eval_StdReturn : 7.982463836669922
Eval_MaxReturn : -14.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 30.529411764705884
Buffer size : 5001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 380.215411
At timestep:     27 / 1000At timestep:     67 / 1000At timestep:     85 / 1000At timestep:     111 / 1000At timestep:     136 / 1000At timestep:     173 / 1000At timestep:     196 / 1000At timestep:     234 / 1000At timestep:     261 / 1000At timestep:     299 / 1000At timestep:     326 / 1000At timestep:     365 / 1000At timestep:     397 / 1000At timestep:     421 / 1000At timestep:     467 / 1000At timestep:     501 / 1000At timestep:     554 / 1000At timestep:     596 / 1000At timestep:     644 / 1000At timestep:     675 / 1000At timestep:     690 / 1000At timestep:     728 / 1000At timestep:     780 / 1000At timestep:     809 / 1000At timestep:     832 / 1000At timestep:     871 / 1000At timestep:     890 / 1000At timestep:     912 / 1000At timestep:     938 / 1000At timestep:     967 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 380.21541142463684
Exploitation Critic Loss : 34.972801208496094
Exploration Critic Loss : 4815678.0
Exploration Model Loss : 24.514434814453125
Exploitation Data q-values : 43.65519332885742
Exploitation OOD q-values : 46.677886962890625
Exploitation CQL Loss : 34.972801208496094
Eval_AverageReturn : -31.70967674255371
Eval_StdReturn : 9.994172096252441
Eval_MaxReturn : -14.0
Eval_MinReturn : -52.0
Eval_AverageEpLen : 32.70967741935484
Buffer size : 5001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 395.933472
At timestep:     24 / 1000At timestep:     57 / 1000At timestep:     105 / 1000At timestep:     154 / 1000At timestep:     202 / 1000At timestep:     243 / 1000At timestep:     259 / 1000At timestep:     280 / 1000At timestep:     329 / 1000At timestep:     377 / 1000At timestep:     396 / 1000At timestep:     424 / 1000At timestep:     449 / 1000At timestep:     492 / 1000At timestep:     521 / 1000At timestep:     544 / 1000At timestep:     570 / 1000At timestep:     628 / 1000At timestep:     665 / 1000At timestep:     687 / 1000At timestep:     719 / 1000At timestep:     740 / 1000At timestep:     765 / 1000At timestep:     783 / 1000At timestep:     811 / 1000At timestep:     828 / 1000At timestep:     869 / 1000At timestep:     894 / 1000At timestep:     930 / 1000At timestep:     965 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 395.93347239494324
Exploitation Critic Loss : 2.6125495433807373
Exploration Critic Loss : 4015836.5
Exploration Model Loss : 33.32798767089844
Exploitation Data q-values : 42.54280471801758
Exploitation OOD q-values : 45.276084899902344
Exploitation CQL Loss : 2.6125495433807373
Eval_AverageReturn : -31.677419662475586
Eval_StdReturn : 11.57691478729248
Eval_MaxReturn : -15.0
Eval_MinReturn : -57.0
Eval_AverageEpLen : 32.67741935483871
Buffer size : 5001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 411.855432
At timestep:     19 / 1000At timestep:     46 / 1000At timestep:     81 / 1000At timestep:     110 / 1000At timestep:     141 / 1000At timestep:     168 / 1000At timestep:     209 / 1000At timestep:     234 / 1000At timestep:     272 / 1000At timestep:     305 / 1000At timestep:     327 / 1000At timestep:     349 / 1000At timestep:     371 / 1000At timestep:     393 / 1000At timestep:     417 / 1000At timestep:     441 / 1000At timestep:     466 / 1000At timestep:     495 / 1000At timestep:     525 / 1000At timestep:     571 / 1000At timestep:     603 / 1000At timestep:     626 / 1000At timestep:     648 / 1000At timestep:     669 / 1000At timestep:     701 / 1000At timestep:     724 / 1000At timestep:     757 / 1000At timestep:     785 / 1000At timestep:     825 / 1000At timestep:     864 / 1000At timestep:     882 / 1000At timestep:     926 / 1000At timestep:     960 / 1000At timestep:     994 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 411.8554320335388
Exploitation Critic Loss : 14.618412017822266
Exploration Critic Loss : 9660106.0
Exploration Model Loss : 54.80511474609375
Exploitation Data q-values : 43.78411865234375
Exploitation OOD q-values : 46.96540451049805
Exploitation CQL Loss : 14.618412017822266
Eval_AverageReturn : -28.22857093811035
Eval_StdReturn : 7.155759334564209
Eval_MaxReturn : -17.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 29.228571428571428
Buffer size : 5001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 428.823494
At timestep:     25 / 1000At timestep:     43 / 1000At timestep:     89 / 1000At timestep:     119 / 1000At timestep:     146 / 1000At timestep:     178 / 1000At timestep:     213 / 1000At timestep:     236 / 1000At timestep:     282 / 1000At timestep:     301 / 1000At timestep:     352 / 1000At timestep:     380 / 1000At timestep:     413 / 1000At timestep:     460 / 1000At timestep:     485 / 1000At timestep:     509 / 1000At timestep:     558 / 1000At timestep:     578 / 1000At timestep:     606 / 1000At timestep:     643 / 1000At timestep:     674 / 1000At timestep:     700 / 1000At timestep:     733 / 1000At timestep:     769 / 1000At timestep:     799 / 1000At timestep:     827 / 1000At timestep:     850 / 1000At timestep:     865 / 1000At timestep:     899 / 1000At timestep:     930 / 1000At timestep:     951 / 1000At timestep:     976 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 428.82349395751953
Exploitation Critic Loss : 21.792984008789062
Exploration Critic Loss : 4804464.5
Exploration Model Loss : 24.5146484375
Exploitation Data q-values : 43.38190460205078
Exploitation OOD q-values : 46.26326370239258
Exploitation CQL Loss : 21.792984008789062
Eval_AverageReturn : -29.393939971923828
Eval_StdReturn : 9.001479148864746
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 30.393939393939394
Buffer size : 5001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 445.013478
At timestep:     23 / 1000At timestep:     58 / 1000At timestep:     81 / 1000At timestep:     106 / 1000At timestep:     123 / 1000At timestep:     166 / 1000At timestep:     188 / 1000At timestep:     236 / 1000At timestep:     261 / 1000At timestep:     279 / 1000At timestep:     305 / 1000At timestep:     329 / 1000At timestep:     351 / 1000At timestep:     372 / 1000At timestep:     427 / 1000At timestep:     456 / 1000At timestep:     490 / 1000At timestep:     516 / 1000At timestep:     547 / 1000At timestep:     569 / 1000At timestep:     596 / 1000At timestep:     619 / 1000At timestep:     679 / 1000At timestep:     720 / 1000At timestep:     759 / 1000At timestep:     785 / 1000At timestep:     816 / 1000At timestep:     837 / 1000At timestep:     862 / 1000At timestep:     906 / 1000At timestep:     934 / 1000At timestep:     953 / 1000At timestep:     984 / 1000At timestep:     1035 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 445.0134780406952
Exploitation Critic Loss : 27.28164291381836
Exploration Critic Loss : 5649448.0
Exploration Model Loss : 33.32823181152344
Exploitation Data q-values : 45.56874084472656
Exploitation OOD q-values : 48.34326934814453
Exploitation CQL Loss : 27.28164291381836
Eval_AverageReturn : -29.441177368164062
Eval_StdReturn : 10.849752426147461
Eval_MaxReturn : -16.0
Eval_MinReturn : -59.0
Eval_AverageEpLen : 30.441176470588236
Buffer size : 5001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 461.622741
At timestep:     27 / 1000At timestep:     62 / 1000At timestep:     88 / 1000At timestep:     124 / 1000At timestep:     144 / 1000At timestep:     171 / 1000At timestep:     194 / 1000At timestep:     217 / 1000At timestep:     235 / 1000At timestep:     264 / 1000At timestep:     293 / 1000At timestep:     313 / 1000At timestep:     358 / 1000At timestep:     402 / 1000At timestep:     429 / 1000At timestep:     454 / 1000At timestep:     485 / 1000At timestep:     513 / 1000At timestep:     539 / 1000At timestep:     569 / 1000At timestep:     597 / 1000At timestep:     621 / 1000At timestep:     655 / 1000At timestep:     673 / 1000At timestep:     703 / 1000At timestep:     738 / 1000At timestep:     781 / 1000At timestep:     804 / 1000At timestep:     831 / 1000At timestep:     867 / 1000At timestep:     891 / 1000At timestep:     929 / 1000At timestep:     958 / 1000At timestep:     986 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 461.62274074554443
Exploitation Critic Loss : 13.828556060791016
Exploration Critic Loss : 7612767.0
Exploration Model Loss : 54.80560302734375
Exploitation Data q-values : 43.621788024902344
Exploitation OOD q-values : 46.2823371887207
Exploitation CQL Loss : 13.828556060791016
Eval_AverageReturn : -28.200000762939453
Eval_StdReturn : 6.8067193031311035
Eval_MaxReturn : -17.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 29.2
Buffer size : 5001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -149.800003
best mean reward -149.330002
running time 478.759101
At timestep:     24 / 1000At timestep:     54 / 1000At timestep:     87 / 1000At timestep:     126 / 1000At timestep:     193 / 1000At timestep:     256 / 1000At timestep:     286 / 1000At timestep:     320 / 1000At timestep:     385 / 1000At timestep:     412 / 1000At timestep:     438 / 1000At timestep:     458 / 1000At timestep:     494 / 1000At timestep:     532 / 1000At timestep:     557 / 1000At timestep:     595 / 1000At timestep:     618 / 1000At timestep:     641 / 1000At timestep:     674 / 1000At timestep:     717 / 1000At timestep:     747 / 1000At timestep:     766 / 1000At timestep:     794 / 1000At timestep:     833 / 1000At timestep:     874 / 1000At timestep:     911 / 1000At timestep:     950 / 1000At timestep:     992 / 1000At timestep:     1045 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -149.8000030517578
Train_BestReturn : -149.3300018310547
TimeSinceStart : 478.7591013908386
Exploitation Critic Loss : 30.242855072021484
Exploration Critic Loss : 5567707.0
Exploration Model Loss : 24.514114379882812
Exploitation Data q-values : 43.613800048828125
Exploitation OOD q-values : 46.73625564575195
Exploitation CQL Loss : 30.242855072021484
Eval_AverageReturn : -35.034481048583984
Eval_StdReturn : 12.441539764404297
Eval_MaxReturn : -18.0
Eval_MinReturn : -66.0
Eval_AverageEpLen : 36.03448275862069
Buffer size : 5001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 495.008538
At timestep:     29 / 1000At timestep:     66 / 1000At timestep:     107 / 1000At timestep:     125 / 1000At timestep:     153 / 1000At timestep:     210 / 1000At timestep:     254 / 1000At timestep:     297 / 1000At timestep:     340 / 1000At timestep:     372 / 1000At timestep:     399 / 1000At timestep:     429 / 1000At timestep:     463 / 1000At timestep:     504 / 1000At timestep:     535 / 1000At timestep:     573 / 1000At timestep:     607 / 1000At timestep:     644 / 1000At timestep:     674 / 1000At timestep:     704 / 1000At timestep:     734 / 1000At timestep:     761 / 1000At timestep:     790 / 1000At timestep:     814 / 1000At timestep:     857 / 1000At timestep:     898 / 1000At timestep:     922 / 1000At timestep:     945 / 1000At timestep:     976 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 495.0085382461548
Exploitation Critic Loss : 16.17441177368164
Exploration Critic Loss : 6330111.0
Exploration Model Loss : 33.327545166015625
Exploitation Data q-values : 44.44832992553711
Exploitation OOD q-values : 47.678550720214844
Exploitation CQL Loss : 16.17441177368164
Eval_AverageReturn : -32.56666564941406
Eval_StdReturn : 7.965271949768066
Eval_MaxReturn : -17.0
Eval_MinReturn : -56.0
Eval_AverageEpLen : 33.56666666666667
Buffer size : 5001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 511.332979
At timestep:     38 / 1000At timestep:     97 / 1000At timestep:     128 / 1000At timestep:     154 / 1000At timestep:     174 / 1000At timestep:     191 / 1000At timestep:     223 / 1000At timestep:     260 / 1000At timestep:     300 / 1000At timestep:     330 / 1000At timestep:     394 / 1000At timestep:     414 / 1000At timestep:     433 / 1000At timestep:     461 / 1000At timestep:     493 / 1000At timestep:     517 / 1000At timestep:     544 / 1000At timestep:     593 / 1000At timestep:     609 / 1000At timestep:     636 / 1000At timestep:     656 / 1000At timestep:     691 / 1000At timestep:     722 / 1000At timestep:     742 / 1000At timestep:     788 / 1000At timestep:     812 / 1000At timestep:     843 / 1000At timestep:     867 / 1000At timestep:     901 / 1000At timestep:     923 / 1000At timestep:     949 / 1000At timestep:     975 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 511.3329792022705
Exploitation Critic Loss : 29.024938583374023
Exploration Critic Loss : 8807566.0
Exploration Model Loss : 54.80560302734375
Exploitation Data q-values : 45.12604522705078
Exploitation OOD q-values : 48.33196258544922
Exploitation CQL Loss : 29.024938583374023
Eval_AverageReturn : -29.363636016845703
Eval_StdReturn : 11.025639533996582
Eval_MaxReturn : -15.0
Eval_MinReturn : -63.0
Eval_AverageEpLen : 30.363636363636363
Buffer size : 5001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 527.592459
At timestep:     43 / 1000At timestep:     77 / 1000At timestep:     114 / 1000At timestep:     141 / 1000At timestep:     170 / 1000At timestep:     212 / 1000At timestep:     234 / 1000At timestep:     265 / 1000At timestep:     291 / 1000At timestep:     316 / 1000At timestep:     354 / 1000At timestep:     395 / 1000At timestep:     433 / 1000At timestep:     464 / 1000At timestep:     497 / 1000At timestep:     524 / 1000At timestep:     546 / 1000At timestep:     574 / 1000At timestep:     600 / 1000At timestep:     630 / 1000At timestep:     666 / 1000At timestep:     705 / 1000At timestep:     731 / 1000At timestep:     764 / 1000At timestep:     808 / 1000At timestep:     829 / 1000At timestep:     850 / 1000At timestep:     880 / 1000At timestep:     902 / 1000At timestep:     938 / 1000At timestep:     995 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 527.5924589633942
Exploitation Critic Loss : 38.53681945800781
Exploration Critic Loss : 7707569.5
Exploration Model Loss : 24.514511108398438
Exploitation Data q-values : 45.55490493774414
Exploitation OOD q-values : 48.347511291503906
Exploitation CQL Loss : 38.53681945800781
Eval_AverageReturn : -31.25
Eval_StdReturn : 8.027297019958496
Eval_MaxReturn : -20.0
Eval_MinReturn : -56.0
Eval_AverageEpLen : 32.25
Buffer size : 5001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 543.943176
At timestep:     22 / 1000At timestep:     57 / 1000At timestep:     107 / 1000At timestep:     153 / 1000At timestep:     219 / 1000At timestep:     247 / 1000At timestep:     270 / 1000At timestep:     298 / 1000At timestep:     351 / 1000At timestep:     372 / 1000At timestep:     406 / 1000At timestep:     430 / 1000At timestep:     450 / 1000At timestep:     475 / 1000At timestep:     513 / 1000At timestep:     535 / 1000At timestep:     581 / 1000At timestep:     603 / 1000At timestep:     649 / 1000At timestep:     670 / 1000At timestep:     695 / 1000At timestep:     726 / 1000At timestep:     747 / 1000At timestep:     783 / 1000At timestep:     820 / 1000At timestep:     841 / 1000At timestep:     873 / 1000At timestep:     944 / 1000At timestep:     965 / 1000At timestep:     986 / 1000At timestep:     1041 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 543.9431757926941
Exploitation Critic Loss : 36.568199157714844
Exploration Critic Loss : 8178887.5
Exploration Model Loss : 33.32807922363281
Exploitation Data q-values : 45.560028076171875
Exploitation OOD q-values : 48.493350982666016
Exploitation CQL Loss : 36.568199157714844
Eval_AverageReturn : -32.58064651489258
Eval_StdReturn : 13.932496070861816
Eval_MaxReturn : -19.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 33.58064516129032
Buffer size : 5001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 559.825867
At timestep:     33 / 1000At timestep:     48 / 1000At timestep:     69 / 1000At timestep:     88 / 1000At timestep:     117 / 1000At timestep:     136 / 1000At timestep:     163 / 1000At timestep:     195 / 1000At timestep:     212 / 1000At timestep:     251 / 1000At timestep:     274 / 1000At timestep:     336 / 1000At timestep:     355 / 1000At timestep:     378 / 1000At timestep:     399 / 1000At timestep:     426 / 1000At timestep:     454 / 1000At timestep:     493 / 1000At timestep:     514 / 1000At timestep:     538 / 1000At timestep:     592 / 1000At timestep:     628 / 1000At timestep:     683 / 1000At timestep:     706 / 1000At timestep:     743 / 1000At timestep:     775 / 1000At timestep:     823 / 1000At timestep:     863 / 1000At timestep:     893 / 1000At timestep:     917 / 1000At timestep:     943 / 1000At timestep:     979 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 559.8258666992188
Exploitation Critic Loss : 15.259721755981445
Exploration Critic Loss : 8328557.5
Exploration Model Loss : 54.804840087890625
Exploitation Data q-values : 46.230079650878906
Exploitation OOD q-values : 49.155181884765625
Exploitation CQL Loss : 15.259721755981445
Eval_AverageReturn : -29.939393997192383
Eval_StdReturn : 11.444039344787598
Eval_MaxReturn : -14.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 30.939393939393938
Buffer size : 5001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 575.916474
At timestep:     32 / 1000At timestep:     61 / 1000At timestep:     85 / 1000At timestep:     133 / 1000At timestep:     176 / 1000At timestep:     206 / 1000At timestep:     272 / 1000At timestep:     292 / 1000At timestep:     326 / 1000At timestep:     359 / 1000At timestep:     401 / 1000At timestep:     427 / 1000At timestep:     458 / 1000At timestep:     500 / 1000At timestep:     520 / 1000At timestep:     550 / 1000At timestep:     574 / 1000At timestep:     618 / 1000At timestep:     642 / 1000At timestep:     661 / 1000At timestep:     713 / 1000At timestep:     773 / 1000At timestep:     806 / 1000At timestep:     844 / 1000At timestep:     899 / 1000At timestep:     922 / 1000At timestep:     955 / 1000At timestep:     986 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 575.9164741039276
Exploitation Critic Loss : 13.285270690917969
Exploration Critic Loss : 8021963.5
Exploration Model Loss : 24.514572143554688
Exploitation Data q-values : 45.42090606689453
Exploitation OOD q-values : 48.19447326660156
Exploitation CQL Loss : 13.285270690917969
Eval_AverageReturn : -33.75862121582031
Eval_StdReturn : 12.173048973083496
Eval_MaxReturn : -18.0
Eval_MinReturn : -65.0
Eval_AverageEpLen : 34.758620689655174
Buffer size : 5001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 591.567642
At timestep:     37 / 1000At timestep:     74 / 1000At timestep:     131 / 1000At timestep:     163 / 1000At timestep:     196 / 1000At timestep:     219 / 1000At timestep:     252 / 1000At timestep:     275 / 1000At timestep:     299 / 1000At timestep:     327 / 1000At timestep:     350 / 1000At timestep:     372 / 1000At timestep:     400 / 1000At timestep:     430 / 1000At timestep:     462 / 1000At timestep:     520 / 1000At timestep:     576 / 1000At timestep:     607 / 1000At timestep:     629 / 1000At timestep:     676 / 1000At timestep:     702 / 1000At timestep:     734 / 1000At timestep:     775 / 1000At timestep:     805 / 1000At timestep:     818 / 1000At timestep:     852 / 1000At timestep:     889 / 1000At timestep:     913 / 1000At timestep:     927 / 1000At timestep:     977 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 591.5676424503326
Exploitation Critic Loss : 18.724851608276367
Exploration Critic Loss : 8803046.0
Exploration Model Loss : 33.328125
Exploitation Data q-values : 46.47152328491211
Exploitation OOD q-values : 49.324951171875
Exploitation CQL Loss : 18.724851608276367
Eval_AverageReturn : -31.354839324951172
Eval_StdReturn : 11.292442321777344
Eval_MaxReturn : -12.0
Eval_MinReturn : -57.0
Eval_AverageEpLen : 32.354838709677416
Buffer size : 5001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -150.000000
best mean reward -149.330002
running time 607.432238
At timestep:     43 / 1000At timestep:     70 / 1000At timestep:     113 / 1000At timestep:     142 / 1000At timestep:     180 / 1000At timestep:     207 / 1000At timestep:     249 / 1000At timestep:     287 / 1000At timestep:     311 / 1000At timestep:     351 / 1000At timestep:     390 / 1000At timestep:     420 / 1000At timestep:     447 / 1000At timestep:     476 / 1000At timestep:     498 / 1000At timestep:     522 / 1000At timestep:     563 / 1000At timestep:     604 / 1000At timestep:     636 / 1000At timestep:     699 / 1000At timestep:     734 / 1000At timestep:     773 / 1000At timestep:     797 / 1000At timestep:     828 / 1000At timestep:     866 / 1000At timestep:     889 / 1000At timestep:     912 / 1000At timestep:     976 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -150.0
Train_BestReturn : -149.3300018310547
TimeSinceStart : 607.4322383403778
Exploitation Critic Loss : 18.573013305664062
Exploration Critic Loss : 9810587.0
Exploration Model Loss : 54.805633544921875
Exploitation Data q-values : 47.8667106628418
Exploitation OOD q-values : 51.35865020751953
Exploitation CQL Loss : 18.573013305664062
Eval_AverageReturn : -33.75862121582031
Eval_StdReturn : 10.337928771972656
Eval_MaxReturn : -21.0
Eval_MinReturn : -63.0
Eval_AverageEpLen : 34.758620689655174
Buffer size : 5001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -149.910004
best mean reward -149.330002
running time 623.189442
At timestep:     64 / 1000At timestep:     88 / 1000At timestep:     123 / 1000At timestep:     154 / 1000At timestep:     186 / 1000At timestep:     232 / 1000At timestep:     261 / 1000At timestep:     287 / 1000At timestep:     314 / 1000At timestep:     359 / 1000At timestep:     387 / 1000At timestep:     431 / 1000At timestep:     453 / 1000At timestep:     481 / 1000At timestep:     509 / 1000At timestep:     533 / 1000At timestep:     603 / 1000At timestep:     653 / 1000At timestep:     678 / 1000At timestep:     713 / 1000At timestep:     746 / 1000At timestep:     786 / 1000At timestep:     819 / 1000At timestep:     850 / 1000At timestep:     896 / 1000At timestep:     929 / 1000At timestep:     957 / 1000At timestep:     985 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -149.91000366210938
Train_BestReturn : -149.3300018310547
TimeSinceStart : 623.1894416809082
Exploitation Critic Loss : 28.1885986328125
Exploration Critic Loss : 8912063.0
Exploration Model Loss : 24.513565063476562
Exploitation Data q-values : 44.63886260986328
Exploitation OOD q-values : 47.70728302001953
Exploitation CQL Loss : 28.1885986328125
Eval_AverageReturn : -34.13793182373047
Eval_StdReturn : 11.340267181396484
Eval_MaxReturn : -21.0
Eval_MinReturn : -69.0
Eval_AverageEpLen : 35.13793103448276
Buffer size : 5001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -149.910004
best mean reward -149.330002
running time 638.898362
At timestep:     35 / 1000At timestep:     59 / 1000At timestep:     96 / 1000At timestep:     120 / 1000At timestep:     146 / 1000At timestep:     165 / 1000At timestep:     187 / 1000At timestep:     230 / 1000At timestep:     263 / 1000At timestep:     293 / 1000At timestep:     332 / 1000At timestep:     368 / 1000At timestep:     389 / 1000At timestep:     430 / 1000At timestep:     462 / 1000At timestep:     491 / 1000At timestep:     515 / 1000At timestep:     542 / 1000At timestep:     572 / 1000At timestep:     593 / 1000At timestep:     622 / 1000At timestep:     675 / 1000At timestep:     704 / 1000At timestep:     731 / 1000At timestep:     755 / 1000At timestep:     774 / 1000At timestep:     795 / 1000At timestep:     813 / 1000At timestep:     834 / 1000At timestep:     864 / 1000At timestep:     901 / 1000At timestep:     926 / 1000At timestep:     953 / 1000At timestep:     996 / 1000At timestep:     1022 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q2_dqn_numsteps_5000_PointmassMedium-v0_21-11-2022_22-27-21/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -149.91000366210938
Train_BestReturn : -149.3300018310547
TimeSinceStart : 638.8983623981476
Exploitation Critic Loss : 42.630245208740234
Exploration Critic Loss : 10874574.0
Exploration Model Loss : 33.32623291015625
Exploitation Data q-values : 45.9996337890625
Exploitation OOD q-values : 48.733970642089844
Exploitation CQL Loss : 42.630245208740234
Eval_AverageReturn : -28.200000762939453
Eval_StdReturn : 7.970660209655762
Eval_MaxReturn : -17.0
Eval_MinReturn : -52.0
Eval_AverageEpLen : 29.2
Buffer size : 5001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_numsteps_15000_PointmassMedium-v0_21-11-2022_22-38-20 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_numsteps_15000_PointmassMedium-v0_21-11-2022_22-38-20
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001966
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0019655227661132812
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 5.461415
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 5.461414813995361
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -147.923080
best mean reward -inf
running time 11.068258
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -147.92308044433594
TimeSinceStart : 11.068258285522461
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -148.649994
best mean reward -inf
running time 24.750140
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -148.64999389648438
TimeSinceStart : 24.750139951705933
Exploitation Critic Loss : 0.7272948026657104
Exploration Critic Loss : 1129178.25
Exploration Model Loss : 71.00654602050781
Exploitation Data q-values : 1.0564587116241455
Exploitation OOD q-values : 2.7824959754943848
Exploitation CQL Loss : 0.8998985290527344
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -148.961533
best mean reward -inf
running time 38.189702
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -148.96153259277344
TimeSinceStart : 38.18970203399658
Exploitation Critic Loss : 0.26855894923210144
Exploration Critic Loss : 8844376.0
Exploration Model Loss : 43.80195999145508
Exploitation Data q-values : 1.6999423503875732
Exploitation OOD q-values : 3.4590892791748047
Exploitation CQL Loss : 0.4444736838340759
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -149.181824
best mean reward -inf
running time 52.637260
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -149.18182373046875
TimeSinceStart : 52.63725972175598
Exploitation Critic Loss : 0.9905271530151367
Exploration Critic Loss : 8926296.0
Exploration Model Loss : 66.28165435791016
Exploitation Data q-values : 2.990598201751709
Exploitation OOD q-values : 4.711735725402832
Exploitation CQL Loss : 1.162640929222107
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -149.324997
best mean reward -inf
running time 66.568793
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -149.3249969482422
TimeSinceStart : 66.56879258155823
Exploitation Critic Loss : 11.71717357635498
Exploration Critic Loss : 4188172.0
Exploration Model Loss : 21.459062576293945
Exploitation Data q-values : 5.191617012023926
Exploitation OOD q-values : 7.223959445953369
Exploitation CQL Loss : 11.920408248901367
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -149.413040
best mean reward -inf
running time 80.049338
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -149.4130401611328
TimeSinceStart : 80.04933834075928
Exploitation Critic Loss : 8.341699600219727
Exploration Critic Loss : 11775928.0
Exploration Model Loss : 26.761703491210938
Exploitation Data q-values : 9.067390441894531
Exploitation OOD q-values : 12.344715118408203
Exploitation CQL Loss : 8.669431686401367
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -149.490570
best mean reward -inf
running time 93.694266
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -149.49057006835938
TimeSinceStart : 93.69426560401917
Exploitation Critic Loss : 6.858318328857422
Exploration Critic Loss : 18157980.0
Exploration Model Loss : 23.670440673828125
Exploitation Data q-values : 14.490489959716797
Exploitation OOD q-values : 18.577796936035156
Exploitation CQL Loss : 7.2670488357543945
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -149.550003
best mean reward -inf
running time 107.296638
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -149.5500030517578
TimeSinceStart : 107.29663753509521
Exploitation Critic Loss : 34.73097229003906
Exploration Critic Loss : 14147158.0
Exploration Model Loss : 22.803451538085938
Exploitation Data q-values : 29.03866195678711
Exploitation OOD q-values : 34.987937927246094
Exploitation CQL Loss : 35.32590103149414
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -149.590912
best mean reward -inf
running time 120.778852
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000