


LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_unsupervised_lam0.1_PointmassMedium-v0_22-11-2022_21-21-48 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_unsupervised_lam0.1_PointmassMedium-v0_22-11-2022_21-21-48
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.003304
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.00330352783203125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 7.736769
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     666 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 7.736768960952759
Eval_AverageReturn : -139.375
Eval_StdReturn : 28.111106872558594
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.5
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 15.770829
At timestep:     54 / 1000At timestep:     204 / 1000At timestep:     354 / 1000At timestep:     504 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -150.0
TimeSinceStart : 15.770829200744629
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 46.464963
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -150.0
TimeSinceStart : 46.46496343612671
Exploration Critic Loss : 1012343.75
Exploitation Critic Loss : 0.00011949745385209098
Exploration Model Loss : 62.042945861816406
Actor Loss : 1.1926782131195068
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 76.252453
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -150.0
TimeSinceStart : 76.25245332717896
Exploration Critic Loss : 9842531.0
Exploitation Critic Loss : 0.03459673374891281
Exploration Model Loss : 46.220298767089844
Actor Loss : 0.9748734831809998
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -147.090912
best mean reward -inf
running time 106.587132
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -147.09091186523438
TimeSinceStart : 106.58713173866272
Exploration Critic Loss : 6715688.0
Exploitation Critic Loss : 0.07507026195526123
Exploration Model Loss : 32.93437194824219
Actor Loss : 0.7975425124168396
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -146.048782
best mean reward -inf
running time 138.226613
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -146.0487823486328
TimeSinceStart : 138.2266128063202
Exploration Critic Loss : 9640146.0
Exploitation Critic Loss : 0.03266199305653572
Exploration Model Loss : 45.98236083984375
Actor Loss : 0.6468390822410583
Eval_AverageReturn : -149.85714721679688
Eval_StdReturn : 0.3499270975589752
Eval_MaxReturn : -149.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -146.553192
best mean reward -inf
running time 170.084225
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -146.55319213867188
TimeSinceStart : 170.08422470092773
Exploration Critic Loss : 9910542.0
Exploitation Critic Loss : 0.07829578220844269
Exploration Model Loss : 28.94525146484375
Actor Loss : 0.5910142660140991
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -139.649124
best mean reward -inf
running time 208.137013
At timestep:     150 / 1000At timestep:     238 / 1000At timestep:     388 / 1000At timestep:     538 / 1000At timestep:     680 / 1000At timestep:     830 / 1000At timestep:     942 / 1000At timestep:     1092 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -139.6491241455078
TimeSinceStart : 208.1370131969452
Exploration Critic Loss : 8263030.0
Exploitation Critic Loss : 0.1289547085762024
Exploration Model Loss : 27.903167724609375
Actor Loss : 0.5142830014228821
Eval_AverageReturn : -136.125
Eval_StdReturn : 22.44681167602539
Eval_MaxReturn : -87.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 136.5
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -138.230774
best mean reward -inf
running time 245.801994
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -138.23077392578125
TimeSinceStart : 245.8019938468933
Exploration Critic Loss : 10794523.0
Exploitation Critic Loss : 0.048077408224344254
Exploration Model Loss : 22.369781494140625
Actor Loss : 0.4771268367767334
Eval_AverageReturn : -145.0
Eval_StdReturn : 12.247448921203613
Eval_MaxReturn : -115.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 145.14285714285714
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -133.675674
best mean reward -inf
running time 283.748994
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     424 / 1000At timestep:     574 / 1000At timestep:     724 / 1000At timestep:     843 / 1000At timestep:     993 / 1000At timestep:     1143 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -133.67567443847656
TimeSinceStart : 283.7489938735962
Exploration Critic Loss : 14126215.0
Exploitation Critic Loss : 0.053343817591667175
Exploration Model Loss : 20.997055053710938
Actor Loss : 0.43696728348731995
Eval_AverageReturn : -142.625
Eval_StdReturn : 12.83488941192627
Eval_MaxReturn : -118.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 142.875
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -130.710846
best mean reward -inf
running time 322.495677
At timestep:     100 / 1000At timestep:     250 / 1000At timestep:     400 / 1000At timestep:     540 / 1000At timestep:     690 / 1000At timestep:     840 / 1000At timestep:     990 / 1000At timestep:     1140 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -130.71084594726562
TimeSinceStart : 322.49567699432373
Exploration Critic Loss : 17216180.0
Exploitation Critic Loss : 0.05919395387172699
Exploration Model Loss : 20.5478515625
Actor Loss : 0.41789019107818604
Eval_AverageReturn : -142.25
Eval_StdReturn : 16.738801956176758
Eval_MaxReturn : -99.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 142.5
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -131.142853
best mean reward -inf
running time 360.233795
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     365 / 1000At timestep:     515 / 1000At timestep:     665 / 1000At timestep:     815 / 1000At timestep:     965 / 1000At timestep:     1115 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -131.14285278320312
TimeSinceStart : 360.2337951660156
Exploration Critic Loss : 14466218.0
Exploitation Critic Loss : 0.0846657007932663
Exploration Model Loss : 20.389312744140625
Actor Loss : 0.3834421932697296
Eval_AverageReturn : -139.25
Eval_StdReturn : 28.44182586669922
Eval_MaxReturn : -64.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.375
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -131.244904
best mean reward -inf
running time 399.750114
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     393 / 1000At timestep:     543 / 1000At timestep:     693 / 1000At timestep:     843 / 1000At timestep:     993 / 1000At timestep:     1143 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -131.24490356445312
TimeSinceStart : 399.75011444091797
Exploration Critic Loss : 15978546.0
Exploitation Critic Loss : 0.1924591362476349
Exploration Model Loss : 20.331329345703125
Actor Loss : 0.37637490034103394
Eval_AverageReturn : -142.75
Eval_StdReturn : 19.181697845458984
Eval_MaxReturn : -92.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 142.875
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -125.019997
best mean reward -125.019997
running time 437.961344
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -125.0199966430664
Train_BestReturn : -125.0199966430664
TimeSinceStart : 437.96134400367737
Exploration Critic Loss : 9506048.0
Exploitation Critic Loss : 0.21146801114082336
Exploration Model Loss : 20.310043334960938
Actor Loss : 0.3773568272590637
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -116.489998
best mean reward -116.489998
running time 476.312279
At timestep:     150 / 1000At timestep:     221 / 1000At timestep:     327 / 1000At timestep:     425 / 1000At timestep:     575 / 1000At timestep:     684 / 1000At timestep:     834 / 1000At timestep:     984 / 1000At timestep:     1134 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -116.48999786376953
Train_BestReturn : -116.48999786376953
TimeSinceStart : 476.31227922439575
Exploration Critic Loss : 18086336.0
Exploitation Critic Loss : 0.17462487518787384
Exploration Model Loss : 20.302047729492188
Actor Loss : 0.36057719588279724
Eval_AverageReturn : -125.55555725097656
Eval_StdReturn : 29.09796142578125
Eval_MaxReturn : -70.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 126.0
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -99.820000
best mean reward -99.820000
running time 515.357220
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     572 / 1000At timestep:     679 / 1000At timestep:     784 / 1000At timestep:     890 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -99.81999969482422
Train_BestReturn : -99.81999969482422
TimeSinceStart : 515.3572201728821
Exploration Critic Loss : 8244560.0
Exploitation Critic Loss : 0.0871381163597107
Exploration Model Loss : 20.298873901367188
Actor Loss : 0.3633624315261841
Eval_AverageReturn : -127.125
Eval_StdReturn : 19.662384033203125
Eval_MaxReturn : -104.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 127.75
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -82.180000
best mean reward -82.180000
running time 554.914810
At timestep:     75 / 1000At timestep:     225 / 1000At timestep:     375 / 1000At timestep:     424 / 1000At timestep:     503 / 1000At timestep:     653 / 1000At timestep:     803 / 1000At timestep:     953 / 1000At timestep:     1059 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -82.18000030517578
Train_BestReturn : -82.18000030517578
TimeSinceStart : 554.9148097038269
Exploration Critic Loss : 7326497.0
Exploitation Critic Loss : 0.02591589093208313
Exploration Model Loss : 20.297958374023438
Actor Loss : 0.40196388959884644
Eval_AverageReturn : -117.22222137451172
Eval_StdReturn : 39.04350280761719
Eval_MaxReturn : -48.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 117.66666666666667
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -64.290001
best mean reward -64.290001
running time 597.504311
At timestep:     129 / 1000At timestep:     188 / 1000At timestep:     286 / 1000At timestep:     329 / 1000At timestep:     436 / 1000At timestep:     586 / 1000At timestep:     736 / 1000At timestep:     774 / 1000At timestep:     891 / 1000At timestep:     963 / 1000At timestep:     1045 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -64.29000091552734
Train_BestReturn : -64.29000091552734
TimeSinceStart : 597.5043110847473
Exploration Critic Loss : 9950211.0
Exploitation Critic Loss : 0.2535628080368042
Exploration Model Loss : 20.29742431640625
Actor Loss : 0.4012889266014099
Eval_AverageReturn : -94.18181610107422
Eval_StdReturn : 38.15237045288086
Eval_MaxReturn : -37.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 95.0
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -44.529999
best mean reward -44.529999
running time 639.659690
At timestep:     95 / 1000At timestep:     245 / 1000At timestep:     293 / 1000At timestep:     375 / 1000At timestep:     515 / 1000At timestep:     608 / 1000At timestep:     693 / 1000At timestep:     843 / 1000At timestep:     935 / 1000At timestep:     1078 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -44.529998779296875
Train_BestReturn : -44.529998779296875
TimeSinceStart : 639.6596899032593
Exploration Critic Loss : 8339685.0
Exploitation Critic Loss : 0.1724582463502884
Exploration Model Loss : 20.297195434570312
Actor Loss : 0.4086294174194336
Eval_AverageReturn : -107.0
Eval_StdReturn : 33.766845703125
Eval_MaxReturn : -47.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 107.8
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -39.209999
best mean reward -39.209999
running time 680.115413
At timestep:     62 / 1000At timestep:     118 / 1000At timestep:     268 / 1000At timestep:     320 / 1000At timestep:     444 / 1000At timestep:     500 / 1000At timestep:     615 / 1000At timestep:     719 / 1000At timestep:     835 / 1000At timestep:     985 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -39.209999084472656
Train_BestReturn : -39.209999084472656
TimeSinceStart : 680.1154127120972
Exploration Critic Loss : 8356391.0
Exploitation Critic Loss : 0.1282733678817749
Exploration Model Loss : 20.297042846679688
Actor Loss : 0.41248780488967896
Eval_AverageReturn : -94.63636016845703
Eval_StdReturn : 36.81717300415039
Eval_MaxReturn : -51.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 95.45454545454545
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -38.060001
best mean reward -38.060001
running time 722.216312
At timestep:     62 / 1000At timestep:     163 / 1000At timestep:     239 / 1000At timestep:     284 / 1000At timestep:     369 / 1000At timestep:     420 / 1000At timestep:     497 / 1000At timestep:     546 / 1000At timestep:     637 / 1000At timestep:     755 / 1000At timestep:     811 / 1000At timestep:     889 / 1000At timestep:     954 / 1000At timestep:     1036 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -38.060001373291016
Train_BestReturn : -38.060001373291016
TimeSinceStart : 722.2163119316101
Exploration Critic Loss : 8615158.0
Exploitation Critic Loss : 0.2700870931148529
Exploration Model Loss : 20.296920776367188
Actor Loss : 0.44702619314193726
Eval_AverageReturn : -73.0
Eval_StdReturn : 20.19900894165039
Eval_MaxReturn : -44.0
Eval_MinReturn : -117.0
Eval_AverageEpLen : 74.0
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -37.400002
best mean reward -37.400002
running time 764.227960
At timestep:     56 / 1000At timestep:     109 / 1000At timestep:     197 / 1000At timestep:     243 / 1000At timestep:     393 / 1000At timestep:     457 / 1000At timestep:     503 / 1000At timestep:     556 / 1000At timestep:     610 / 1000At timestep:     667 / 1000At timestep:     724 / 1000At timestep:     817 / 1000At timestep:     922 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -37.400001525878906
Train_BestReturn : -37.400001525878906
TimeSinceStart : 764.2279603481293
Exploration Critic Loss : 7921059.5
Exploitation Critic Loss : 0.2074858397245407
Exploration Model Loss : 20.29681396484375
Actor Loss : 0.4705033004283905
Eval_AverageReturn : -71.92857360839844
Eval_StdReturn : 29.10440444946289
Eval_MaxReturn : -45.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 72.85714285714286
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -36.639999
best mean reward -36.639999
running time 806.911700
At timestep:     99 / 1000At timestep:     199 / 1000At timestep:     251 / 1000At timestep:     344 / 1000At timestep:     376 / 1000At timestep:     428 / 1000At timestep:     504 / 1000At timestep:     594 / 1000At timestep:     744 / 1000At timestep:     806 / 1000At timestep:     951 / 1000At timestep:     1047 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -36.63999938964844
Train_BestReturn : -36.63999938964844
TimeSinceStart : 806.9117002487183
Exploration Critic Loss : 8312032.0
Exploitation Critic Loss : 0.11666524410247803
Exploration Model Loss : 20.296676635742188
Actor Loss : 0.501465916633606
Eval_AverageReturn : -86.33333587646484
Eval_StdReturn : 34.30095672607422
Eval_MaxReturn : -31.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 87.25
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -34.650002
best mean reward -34.650002
running time 851.172364
At timestep:     54 / 1000At timestep:     130 / 1000At timestep:     174 / 1000At timestep:     225 / 1000At timestep:     296 / 1000At timestep:     446 / 1000At timestep:     596 / 1000At timestep:     663 / 1000At timestep:     711 / 1000At timestep:     785 / 1000At timestep:     870 / 1000At timestep:     913 / 1000At timestep:     948 / 1000At timestep:     1054 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -34.650001525878906
Train_BestReturn : -34.650001525878906
TimeSinceStart : 851.1723644733429
Exploration Critic Loss : 9270346.0
Exploitation Critic Loss : 0.16920071840286255
Exploration Model Loss : 20.2965087890625
Actor Loss : 0.47711092233657837
Eval_AverageReturn : -74.35713958740234
Eval_StdReturn : 35.730350494384766
Eval_MaxReturn : -34.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 75.28571428571429
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -33.310001
best mean reward -33.310001
running time 894.210881
At timestep:     61 / 1000At timestep:     101 / 1000At timestep:     201 / 1000At timestep:     249 / 1000At timestep:     302 / 1000At timestep:     350 / 1000At timestep:     401 / 1000At timestep:     454 / 1000At timestep:     598 / 1000At timestep:     659 / 1000At timestep:     728 / 1000At timestep:     787 / 1000At timestep:     829 / 1000At timestep:     923 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -33.310001373291016
Train_BestReturn : -33.310001373291016
TimeSinceStart : 894.2108812332153
Exploration Critic Loss : 8734460.0
Exploitation Critic Loss : 0.14138326048851013
Exploration Model Loss : 20.296310424804688
Actor Loss : 0.5725631713867188
Eval_AverageReturn : -65.66666412353516
Eval_StdReturn : 26.719947814941406
Eval_MaxReturn : -39.0
Eval_MinReturn : -143.0
Eval_AverageEpLen : 66.66666666666667
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -31.280001
best mean reward -31.280001
running time 937.595564
At timestep:     51 / 1000At timestep:     111 / 1000At timestep:     172 / 1000At timestep:     240 / 1000At timestep:     308 / 1000At timestep:     368 / 1000At timestep:     406 / 1000At timestep:     493 / 1000At timestep:     580 / 1000At timestep:     632 / 1000At timestep:     661 / 1000At timestep:     712 / 1000At timestep:     792 / 1000At timestep:     840 / 1000At timestep:     923 / 1000At timestep:     1044 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -31.280000686645508
Train_BestReturn : -31.280000686645508
TimeSinceStart : 937.5955636501312
Exploration Critic Loss : 8270560.0
Exploitation Critic Loss : 0.14634904265403748
Exploration Model Loss : 20.29620361328125
Actor Loss : 0.5541126132011414
Eval_AverageReturn : -64.25
Eval_StdReturn : 21.810260772705078
Eval_MaxReturn : -28.0
Eval_MinReturn : -120.0
Eval_AverageEpLen : 65.25
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -30.230000
best mean reward -30.230000
running time 979.614026
At timestep:     28 / 1000At timestep:     98 / 1000At timestep:     139 / 1000At timestep:     192 / 1000At timestep:     252 / 1000At timestep:     315 / 1000At timestep:     356 / 1000At timestep:     393 / 1000At timestep:     532 / 1000At timestep:     595 / 1000At timestep:     724 / 1000At timestep:     780 / 1000At timestep:     810 / 1000At timestep:     843 / 1000At timestep:     903 / 1000At timestep:     960 / 1000At timestep:     991 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -30.229999542236328
Train_BestReturn : -30.229999542236328
TimeSinceStart : 979.614025592804
Exploration Critic Loss : 9916056.0
Exploitation Critic Loss : 0.1301891803741455
Exploration Model Loss : 20.296066284179688
Actor Loss : 0.5368881225585938
Eval_AverageReturn : -55.83333206176758
Eval_StdReturn : 30.315471649169922
Eval_MaxReturn : -27.0
Eval_MinReturn : -138.0
Eval_AverageEpLen : 56.833333333333336
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -28.280001
best mean reward -28.280001
running time 1024.167968
At timestep:     57 / 1000At timestep:     106 / 1000At timestep:     154 / 1000At timestep:     208 / 1000At timestep:     250 / 1000At timestep:     291 / 1000At timestep:     355 / 1000At timestep:     398 / 1000At timestep:     445 / 1000At timestep:     497 / 1000At timestep:     557 / 1000At timestep:     619 / 1000At timestep:     692 / 1000At timestep:     796 / 1000At timestep:     872 / 1000At timestep:     902 / 1000At timestep:     962 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -28.280000686645508
Train_BestReturn : -28.280000686645508
TimeSinceStart : 1024.1679680347443
Exploration Critic Loss : 7800019.0
Exploitation Critic Loss : 0.09659349918365479
Exploration Model Loss : 20.295944213867188
Actor Loss : 0.5756794810295105
Eval_AverageReturn : -55.33333206176758
Eval_StdReturn : 16.02082061767578
Eval_MaxReturn : -29.0
Eval_MinReturn : -103.0
Eval_AverageEpLen : 56.333333333333336
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -27.879999
best mean reward -27.879999
running time 1069.535750
At timestep:     48 / 1000At timestep:     107 / 1000At timestep:     147 / 1000At timestep:     175 / 1000At timestep:     259 / 1000At timestep:     319 / 1000At timestep:     367 / 1000At timestep:     420 / 1000At timestep:     460 / 1000At timestep:     541 / 1000At timestep:     614 / 1000At timestep:     724 / 1000At timestep:     757 / 1000At timestep:     796 / 1000At timestep:     838 / 1000At timestep:     876 / 1000At timestep:     929 / 1000At timestep:     975 / 1000At timestep:     1043 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -27.8799991607666
Train_BestReturn : -27.8799991607666
TimeSinceStart : 1069.5357496738434
Exploration Critic Loss : 8140855.5
Exploitation Critic Loss : 0.13249549269676208
Exploration Model Loss : 20.295791625976562
Actor Loss : 0.5409056544303894
Eval_AverageReturn : -53.894737243652344
Eval_StdReturn : 20.05228614807129
Eval_MaxReturn : -27.0
Eval_MinReturn : -109.0
Eval_AverageEpLen : 54.89473684210526
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -27.340000
best mean reward -27.340000
running time 1113.511749
At timestep:     73 / 1000At timestep:     148 / 1000At timestep:     201 / 1000At timestep:     253 / 1000At timestep:     296 / 1000At timestep:     347 / 1000At timestep:     384 / 1000At timestep:     434 / 1000At timestep:     541 / 1000At timestep:     591 / 1000At timestep:     658 / 1000At timestep:     704 / 1000At timestep:     797 / 1000At timestep:     825 / 1000At timestep:     894 / 1000At timestep:     951 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -27.34000015258789
Train_BestReturn : -27.34000015258789
TimeSinceStart : 1113.5117485523224
Exploration Critic Loss : 8730142.0
Exploitation Critic Loss : 0.11519942432641983
Exploration Model Loss : 20.295700073242188
Actor Loss : 0.5746287703514099
Eval_AverageReturn : -59.05882263183594
Eval_StdReturn : 19.346593856811523
Eval_MaxReturn : -27.0
Eval_MinReturn : -106.0
Eval_AverageEpLen : 60.05882352941177
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -27.010000
best mean reward -27.010000
running time 1158.125614
At timestep:     106 / 1000At timestep:     191 / 1000At timestep:     268 / 1000At timestep:     311 / 1000At timestep:     370 / 1000At timestep:     406 / 1000At timestep:     448 / 1000At timestep:     521 / 1000At timestep:     563 / 1000At timestep:     611 / 1000At timestep:     669 / 1000At timestep:     708 / 1000At timestep:     743 / 1000At timestep:     818 / 1000At timestep:     860 / 1000At timestep:     925 / 1000At timestep:     971 / 1000At timestep:     1033 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -27.010000228881836
Train_BestReturn : -27.010000228881836
TimeSinceStart : 1158.1256141662598
Exploration Critic Loss : 6241786.0
Exploitation Critic Loss : 0.1461624801158905
Exploration Model Loss : 20.2955322265625
Actor Loss : 0.5695580840110779
Eval_AverageReturn : -56.38888931274414
Eval_StdReturn : 19.11700439453125
Eval_MaxReturn : -34.0
Eval_MinReturn : -105.0
Eval_AverageEpLen : 57.388888888888886
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -27.150000
best mean reward -27.010000
running time 1201.366138
At timestep:     44 / 1000At timestep:     95 / 1000At timestep:     137 / 1000At timestep:     166 / 1000At timestep:     209 / 1000At timestep:     254 / 1000At timestep:     339 / 1000At timestep:     374 / 1000At timestep:     426 / 1000At timestep:     480 / 1000At timestep:     573 / 1000At timestep:     661 / 1000At timestep:     704 / 1000At timestep:     758 / 1000At timestep:     785 / 1000At timestep:     847 / 1000At timestep:     928 / 1000At timestep:     999 / 1000At timestep:     1045 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -27.149999618530273
Train_BestReturn : -27.010000228881836
TimeSinceStart : 1201.3661379814148
Exploration Critic Loss : 7576256.0
Exploitation Critic Loss : 0.1506269872188568
Exploration Model Loss : 20.295379638671875
Actor Loss : 0.5435258150100708
Eval_AverageReturn : -54.0
Eval_StdReturn : 19.276382446289062
Eval_MaxReturn : -26.0
Eval_MinReturn : -92.0
Eval_AverageEpLen : 55.0
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -26.370001
best mean reward -26.370001
running time 1244.755137
At timestep:     77 / 1000At timestep:     146 / 1000At timestep:     179 / 1000At timestep:     199 / 1000At timestep:     236 / 1000At timestep:     290 / 1000At timestep:     337 / 1000At timestep:     382 / 1000At timestep:     451 / 1000At timestep:     504 / 1000At timestep:     540 / 1000At timestep:     649 / 1000At timestep:     706 / 1000At timestep:     747 / 1000At timestep:     806 / 1000At timestep:     836 / 1000At timestep:     869 / 1000At timestep:     916 / 1000At timestep:     961 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -26.3700008392334
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1244.7551369667053
Exploration Critic Loss : 9674106.0
Exploitation Critic Loss : 0.08381447196006775
Exploration Model Loss : 20.295211791992188
Actor Loss : 0.5468495488166809
Eval_AverageReturn : -49.650001525878906
Eval_StdReturn : 19.332033157348633
Eval_MaxReturn : -19.0
Eval_MinReturn : -108.0
Eval_AverageEpLen : 50.65
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -27.190001
best mean reward -26.370001
running time 1289.316754
At timestep:     42 / 1000At timestep:     95 / 1000At timestep:     128 / 1000At timestep:     188 / 1000At timestep:     225 / 1000At timestep:     276 / 1000At timestep:     307 / 1000At timestep:     347 / 1000At timestep:     404 / 1000At timestep:     443 / 1000At timestep:     499 / 1000At timestep:     570 / 1000At timestep:     588 / 1000At timestep:     642 / 1000At timestep:     684 / 1000At timestep:     741 / 1000At timestep:     772 / 1000At timestep:     808 / 1000At timestep:     846 / 1000At timestep:     893 / 1000At timestep:     954 / 1000At timestep:     991 / 1000At timestep:     1047 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -27.190000534057617
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1289.316754102707
Exploration Critic Loss : 9262390.0
Exploitation Critic Loss : 0.1313832402229309
Exploration Model Loss : 20.295089721679688
Actor Loss : 0.5665099620819092
Eval_AverageReturn : -44.5217399597168
Eval_StdReturn : 12.278895378112793
Eval_MaxReturn : -17.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 45.52173913043478
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -26.580000
best mean reward -26.370001
running time 1335.051275
At timestep:     46 / 1000At timestep:     85 / 1000At timestep:     150 / 1000At timestep:     174 / 1000At timestep:     216 / 1000At timestep:     249 / 1000At timestep:     293 / 1000At timestep:     333 / 1000At timestep:     387 / 1000At timestep:     437 / 1000At timestep:     474 / 1000At timestep:     524 / 1000At timestep:     619 / 1000At timestep:     681 / 1000At timestep:     720 / 1000At timestep:     761 / 1000At timestep:     795 / 1000At timestep:     835 / 1000At timestep:     860 / 1000At timestep:     930 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -26.579999923706055
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1335.0512747764587
Exploration Critic Loss : 7468491.0
Exploitation Critic Loss : 0.09663307666778564
Exploration Model Loss : 20.294967651367188
Actor Loss : 0.5588986277580261
Eval_AverageReturn : -47.9523811340332
Eval_StdReturn : 19.204355239868164
Eval_MaxReturn : -23.0
Eval_MinReturn : -97.0
Eval_AverageEpLen : 48.95238095238095
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -27.040001
best mean reward -26.370001
running time 1378.869905
At timestep:     41 / 1000At timestep:     96 / 1000At timestep:     129 / 1000At timestep:     188 / 1000At timestep:     251 / 1000At timestep:     300 / 1000At timestep:     330 / 1000At timestep:     371 / 1000At timestep:     426 / 1000At timestep:     459 / 1000At timestep:     498 / 1000At timestep:     553 / 1000At timestep:     611 / 1000At timestep:     649 / 1000At timestep:     692 / 1000At timestep:     740 / 1000At timestep:     813 / 1000At timestep:     860 / 1000At timestep:     904 / 1000At timestep:     944 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -27.040000915527344
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1378.8699052333832
Exploration Critic Loss : 8444477.0
Exploitation Critic Loss : 0.09704712778329849
Exploration Model Loss : 20.294845581054688
Actor Loss : 0.5205855369567871
Eval_AverageReturn : -46.80952453613281
Eval_StdReturn : 10.957141876220703
Eval_MaxReturn : -29.0
Eval_MinReturn : -72.0
Eval_AverageEpLen : 47.80952380952381
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -26.379999
best mean reward -26.370001
running time 1423.001377
At timestep:     51 / 1000At timestep:     84 / 1000At timestep:     124 / 1000At timestep:     174 / 1000At timestep:     219 / 1000At timestep:     285 / 1000At timestep:     310 / 1000At timestep:     358 / 1000At timestep:     410 / 1000At timestep:     460 / 1000At timestep:     504 / 1000At timestep:     552 / 1000At timestep:     594 / 1000At timestep:     633 / 1000At timestep:     673 / 1000At timestep:     710 / 1000At timestep:     753 / 1000At timestep:     799 / 1000At timestep:     840 / 1000At timestep:     883 / 1000At timestep:     934 / 1000At timestep:     980 / 1000At timestep:     1045 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -26.3799991607666
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1423.0013768672943
Exploration Critic Loss : 8259118.0
Exploitation Critic Loss : 0.09393435716629028
Exploration Model Loss : 20.294723510742188
Actor Loss : 0.5362258553504944
Eval_AverageReturn : -44.434783935546875
Eval_StdReturn : 8.736859321594238
Eval_MaxReturn : -24.0
Eval_MinReturn : -65.0
Eval_AverageEpLen : 45.43478260869565
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -26.440001
best mean reward -26.370001
running time 1466.851060
At timestep:     42 / 1000At timestep:     89 / 1000At timestep:     184 / 1000At timestep:     222 / 1000At timestep:     255 / 1000At timestep:     288 / 1000At timestep:     335 / 1000At timestep:     363 / 1000At timestep:     418 / 1000At timestep:     457 / 1000At timestep:     487 / 1000At timestep:     515 / 1000At timestep:     557 / 1000At timestep:     599 / 1000At timestep:     649 / 1000At timestep:     704 / 1000At timestep:     776 / 1000At timestep:     829 / 1000At timestep:     876 / 1000At timestep:     915 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -26.440000534057617
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1466.8510599136353
Exploration Critic Loss : 9076842.0
Exploitation Critic Loss : 0.1233789399266243
Exploration Model Loss : 20.294570922851562
Actor Loss : 0.5062937140464783
Eval_AverageReturn : -47.19047546386719
Eval_StdReturn : 18.592416763305664
Eval_MaxReturn : -27.0
Eval_MinReturn : -96.0
Eval_AverageEpLen : 48.19047619047619
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -26.980000
best mean reward -26.370001
running time 1511.855143
At timestep:     43 / 1000At timestep:     86 / 1000At timestep:     134 / 1000At timestep:     173 / 1000At timestep:     206 / 1000At timestep:     259 / 1000At timestep:     304 / 1000At timestep:     353 / 1000At timestep:     402 / 1000At timestep:     454 / 1000At timestep:     519 / 1000At timestep:     609 / 1000At timestep:     644 / 1000At timestep:     685 / 1000At timestep:     761 / 1000At timestep:     789 / 1000At timestep:     817 / 1000At timestep:     873 / 1000At timestep:     927 / 1000At timestep:     992 / 1000At timestep:     1091 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -26.979999542236328
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1511.8551433086395
Exploration Critic Loss : 9276932.0
Exploitation Critic Loss : 0.07596978545188904
Exploration Model Loss : 20.294448852539062
Actor Loss : 0.517611563205719
Eval_AverageReturn : -50.9523811340332
Eval_StdReturn : 18.159284591674805
Eval_MaxReturn : -27.0
Eval_MinReturn : -98.0
Eval_AverageEpLen : 51.95238095238095
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -27.010000
best mean reward -26.370001
running time 1555.578297
At timestep:     67 / 1000At timestep:     142 / 1000At timestep:     167 / 1000At timestep:     212 / 1000At timestep:     256 / 1000At timestep:     313 / 1000At timestep:     362 / 1000At timestep:     420 / 1000At timestep:     454 / 1000At timestep:     501 / 1000At timestep:     545 / 1000At timestep:     592 / 1000At timestep:     616 / 1000At timestep:     656 / 1000At timestep:     727 / 1000At timestep:     771 / 1000At timestep:     798 / 1000At timestep:     845 / 1000At timestep:     917 / 1000At timestep:     991 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -27.010000228881836
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1555.578296661377
Exploration Critic Loss : 10006380.0
Exploitation Critic Loss : 0.12409353256225586
Exploration Model Loss : 20.294448852539062
Actor Loss : 0.5626707673072815
Eval_AverageReturn : -47.85714340209961
Eval_StdReturn : 15.541911125183105
Eval_MaxReturn : -23.0
Eval_MinReturn : -74.0
Eval_AverageEpLen : 48.857142857142854
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -26.570000
best mean reward -26.370001
running time 1598.435659
At timestep:     28 / 1000At timestep:     66 / 1000At timestep:     106 / 1000At timestep:     158 / 1000At timestep:     200 / 1000At timestep:     236 / 1000At timestep:     273 / 1000At timestep:     319 / 1000At timestep:     370 / 1000At timestep:     401 / 1000At timestep:     453 / 1000At timestep:     495 / 1000At timestep:     519 / 1000At timestep:     557 / 1000At timestep:     590 / 1000At timestep:     630 / 1000At timestep:     677 / 1000At timestep:     742 / 1000At timestep:     775 / 1000At timestep:     820 / 1000At timestep:     854 / 1000At timestep:     903 / 1000At timestep:     944 / 1000At timestep:     978 / 1000At timestep:     1042 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -26.56999969482422
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1598.4356586933136
Exploration Critic Loss : 9129461.0
Exploitation Critic Loss : 0.09618616104125977
Exploration Model Loss : 20.294448852539062
Actor Loss : 0.5256887078285217
Eval_AverageReturn : -40.68000030517578
Eval_StdReturn : 9.84568977355957
Eval_MaxReturn : -23.0
Eval_MinReturn : -64.0
Eval_AverageEpLen : 41.68
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -27.020000
best mean reward -26.370001
running time 1643.119578
At timestep:     47 / 1000At timestep:     91 / 1000At timestep:     128 / 1000At timestep:     174 / 1000At timestep:     218 / 1000At timestep:     260 / 1000At timestep:     290 / 1000At timestep:     338 / 1000At timestep:     384 / 1000At timestep:     419 / 1000At timestep:     449 / 1000At timestep:     483 / 1000At timestep:     536 / 1000At timestep:     573 / 1000At timestep:     605 / 1000At timestep:     664 / 1000At timestep:     697 / 1000At timestep:     730 / 1000At timestep:     766 / 1000At timestep:     819 / 1000At timestep:     854 / 1000At timestep:     916 / 1000At timestep:     975 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -27.020000457763672
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1643.1195783615112
Exploration Critic Loss : 11288108.0
Exploitation Critic Loss : 0.0714748203754425
Exploration Model Loss : 20.294464111328125
Actor Loss : 0.5178881883621216
Eval_AverageReturn : -41.625
Eval_StdReturn : 9.455212593078613
Eval_MaxReturn : -29.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 42.625
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -26.969999
best mean reward -26.370001
running time 1688.242499
At timestep:     34 / 1000At timestep:     100 / 1000At timestep:     140 / 1000At timestep:     184 / 1000At timestep:     240 / 1000At timestep:     288 / 1000At timestep:     321 / 1000At timestep:     395 / 1000At timestep:     444 / 1000At timestep:     486 / 1000At timestep:     547 / 1000At timestep:     589 / 1000At timestep:     619 / 1000At timestep:     661 / 1000At timestep:     740 / 1000At timestep:     771 / 1000At timestep:     798 / 1000At timestep:     832 / 1000At timestep:     883 / 1000At timestep:     913 / 1000At timestep:     954 / 1000At timestep:     987 / 1000At timestep:     1046 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -26.969999313354492
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1688.2424988746643
Exploration Critic Loss : 13741812.0
Exploitation Critic Loss : 0.08434917032718658
Exploration Model Loss : 20.294448852539062
Actor Loss : 0.509821891784668
Eval_AverageReturn : -44.4782600402832
Eval_StdReturn : 14.196967124938965
Eval_MaxReturn : -26.0
Eval_MinReturn : -78.0
Eval_AverageEpLen : 45.47826086956522
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -27.559999
best mean reward -26.370001
running time 1731.616453
At timestep:     54 / 1000At timestep:     98 / 1000At timestep:     117 / 1000At timestep:     164 / 1000At timestep:     189 / 1000At timestep:     228 / 1000At timestep:     262 / 1000At timestep:     296 / 1000At timestep:     346 / 1000At timestep:     405 / 1000At timestep:     443 / 1000At timestep:     474 / 1000At timestep:     514 / 1000At timestep:     540 / 1000At timestep:     579 / 1000At timestep:     620 / 1000At timestep:     645 / 1000At timestep:     687 / 1000At timestep:     733 / 1000At timestep:     767 / 1000At timestep:     808 / 1000At timestep:     854 / 1000At timestep:     892 / 1000At timestep:     935 / 1000At timestep:     963 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -27.559999465942383
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1731.6164529323578
Exploration Critic Loss : 10073653.0
Exploitation Critic Loss : 0.07058579474687576
Exploration Model Loss : 20.294464111328125
Actor Loss : 0.5538837909698486
Eval_AverageReturn : -38.230770111083984
Eval_StdReturn : 9.830516815185547
Eval_MaxReturn : -18.0
Eval_MinReturn : -58.0
Eval_AverageEpLen : 39.23076923076923
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -26.650000
best mean reward -26.370001
running time 1777.588155
At timestep:     38 / 1000At timestep:     77 / 1000At timestep:     133 / 1000At timestep:     178 / 1000At timestep:     244 / 1000At timestep:     296 / 1000At timestep:     337 / 1000At timestep:     380 / 1000At timestep:     422 / 1000At timestep:     460 / 1000At timestep:     487 / 1000At timestep:     517 / 1000At timestep:     543 / 1000At timestep:     575 / 1000At timestep:     593 / 1000At timestep:     615 / 1000At timestep:     652 / 1000At timestep:     683 / 1000At timestep:     728 / 1000At timestep:     761 / 1000At timestep:     807 / 1000At timestep:     841 / 1000At timestep:     876 / 1000At timestep:     903 / 1000At timestep:     937 / 1000At timestep:     972 / 1000At timestep:     1044 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -26.649999618530273
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1777.588154554367
Exploration Critic Loss : 18119368.0
Exploitation Critic Loss : 0.11618342995643616
Exploration Model Loss : 20.294464111328125
Actor Loss : 0.54875248670578
Eval_AverageReturn : -37.66666793823242
Eval_StdReturn : 12.061570167541504
Eval_MaxReturn : -17.0
Eval_MinReturn : -71.0
Eval_AverageEpLen : 38.666666666666664
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -26.490000
best mean reward -26.370001
running time 1823.682491
At timestep:     77 / 1000At timestep:     117 / 1000At timestep:     154 / 1000At timestep:     187 / 1000At timestep:     224 / 1000At timestep:     262 / 1000At timestep:     324 / 1000At timestep:     369 / 1000At timestep:     436 / 1000At timestep:     478 / 1000At timestep:     512 / 1000At timestep:     553 / 1000At timestep:     580 / 1000At timestep:     615 / 1000At timestep:     673 / 1000At timestep:     725 / 1000At timestep:     746 / 1000At timestep:     786 / 1000At timestep:     817 / 1000At timestep:     859 / 1000At timestep:     893 / 1000At timestep:     934 / 1000At timestep:     971 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -26.489999771118164
Train_BestReturn : -26.3700008392334
TimeSinceStart : 1823.6824913024902
Exploration Critic Loss : 14602519.0
Exploitation Critic Loss : 0.08610465377569199
Exploration Model Loss : 20.294448852539062
Actor Loss : 0.5175710916519165
Eval_AverageReturn : -40.70833206176758
Eval_StdReturn : 12.7523832321167
Eval_MaxReturn : -20.0
Eval_MinReturn : -76.0
Eval_AverageEpLen : 41.708333333333336
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -26.190001
best mean reward -26.190001
running time 1868.526217
At timestep:     22 / 1000At timestep:     55 / 1000At timestep:     119 / 1000At timestep:     178 / 1000At timestep:     213 / 1000At timestep:     263 / 1000At timestep:     314 / 1000At timestep:     360 / 1000At timestep:     405 / 1000At timestep:     444 / 1000At timestep:     513 / 1000At timestep:     546 / 1000At timestep:     599 / 1000At timestep:     637 / 1000At timestep:     714 / 1000At timestep:     777 / 1000At timestep:     808 / 1000At timestep:     835 / 1000At timestep:     903 / 1000At timestep:     939 / 1000At timestep:     989 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -26.190000534057617
Train_BestReturn : -26.190000534057617
TimeSinceStart : 1868.5262167453766
Exploration Critic Loss : 16468623.0
Exploitation Critic Loss : 0.12697739899158478
Exploration Model Loss : 20.294464111328125
Actor Loss : 0.5409690737724304
Eval_AverageReturn : -45.681819915771484
Eval_StdReturn : 14.695039749145508
Eval_MaxReturn : -21.0
Eval_MinReturn : -76.0
Eval_AverageEpLen : 46.68181818181818
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -26.940001
best mean reward -26.190001
running time 1911.950427
At timestep:     53 / 1000At timestep:     120 / 1000At timestep:     172 / 1000At timestep:     214 / 1000At timestep:     268 / 1000At timestep:     301 / 1000At timestep:     338 / 1000At timestep:     379 / 1000At timestep:     408 / 1000At timestep:     441 / 1000At timestep:     467 / 1000At timestep:     525 / 1000At timestep:     557 / 1000At timestep:     586 / 1000At timestep:     613 / 1000At timestep:     643 / 1000At timestep:     677 / 1000At timestep:     730 / 1000At timestep:     765 / 1000At timestep:     789 / 1000At timestep:     834 / 1000At timestep:     853 / 1000At timestep:     897 / 1000At timestep:     935 / 1000At timestep:     985 / 1000At timestep:     1030 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -26.940000534057617
Train_BestReturn : -26.190000534057617
TimeSinceStart : 1911.950427055359
Exploration Critic Loss : 19442288.0
Exploitation Critic Loss : 0.13406085968017578
Exploration Model Loss : 20.294479370117188
Actor Loss : 0.5306304693222046
Eval_AverageReturn : -38.61538314819336
Eval_StdReturn : 11.665454864501953
Eval_MaxReturn : -18.0
Eval_MinReturn : -66.0
Eval_AverageEpLen : 39.61538461538461
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -27.379999
best mean reward -26.190001
running time 1956.688545
At timestep:     42 / 1000At timestep:     74 / 1000At timestep:     121 / 1000At timestep:     156 / 1000At timestep:     179 / 1000At timestep:     202 / 1000At timestep:     237 / 1000At timestep:     271 / 1000At timestep:     304 / 1000At timestep:     340 / 1000At timestep:     372 / 1000At timestep:     408 / 1000At timestep:     457 / 1000At timestep:     490 / 1000At timestep:     522 / 1000At timestep:     555 / 1000At timestep:     591 / 1000At timestep:     627 / 1000At timestep:     670 / 1000At timestep:     724 / 1000At timestep:     754 / 1000At timestep:     788 / 1000At timestep:     850 / 1000At timestep:     894 / 1000At timestep:     944 / 1000At timestep:     984 / 1000At timestep:     1028 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_medium_unsupervised_lam0.1_PointmassMedium-v0_22-11-2022_21-21-48/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -27.3799991607666
Train_BestReturn : -26.190000534057617
TimeSinceStart : 1956.6885452270508
Exploration Critic Loss : 17043068.0
Exploitation Critic Loss : 0.09369005262851715
Exploration Model Loss : 20.294464111328125
Actor Loss : 0.457926869392395
Eval_AverageReturn : -37.074073791503906
Eval_StdReturn : 8.730219841003418
Eval_MaxReturn : -22.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 38.074074074074076
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_unsupervised_lam1_PointmassMedium-v0_22-11-2022_21-55-15 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_unsupervised_lam1_PointmassMedium-v0_22-11-2022_21-55-15
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002446
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0024459362030029297
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 8.709329
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     666 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 8.70932936668396
Eval_AverageReturn : -139.375
Eval_StdReturn : 28.111106872558594
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.5
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 17.808749
At timestep:     54 / 1000At timestep:     204 / 1000At timestep:     354 / 1000At timestep:     504 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -150.0
TimeSinceStart : 17.808748960494995
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 57.812074
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -150.0
TimeSinceStart : 57.812074184417725
Exploration Critic Loss : 636351.5625
Exploitation Critic Loss : 0.008098328486084938
Exploration Model Loss : 34.890682220458984
Actor Loss : 1.1953988075256348
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 95.488534
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -150.0
TimeSinceStart : 95.48853421211243
Exploration Critic Loss : 9871808.0
Exploitation Critic Loss : 0.03412877395749092
Exploration Model Loss : 40.6683235168457
Actor Loss : 0.987343430519104
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -145.088242
best mean reward -inf
running time 134.723270
At timestep:     150 / 1000At timestep:     287 / 1000At timestep:     437 / 1000At timestep:     587 / 1000At timestep:     737 / 1000At timestep:     887 / 1000At timestep:     1037 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -145.08824157714844
TimeSinceStart : 134.7232701778412
Exploration Critic Loss : 7905903.0
Exploitation Critic Loss : 0.09735724329948425
Exploration Model Loss : 59.021522521972656
Actor Loss : 0.8315657377243042
Eval_AverageReturn : -148.0
Eval_StdReturn : 4.898979663848877
Eval_MaxReturn : -136.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 148.14285714285714
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -144.073166
best mean reward -inf
running time 173.607214
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -144.0731658935547
TimeSinceStart : 173.60721373558044
Exploration Critic Loss : 5709345.0
Exploitation Critic Loss : 0.033738575875759125
Exploration Model Loss : 41.08531188964844
Actor Loss : 0.7135101556777954
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -142.979172
best mean reward -inf
running time 210.110804
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -142.9791717529297
TimeSinceStart : 210.11080360412598
Exploration Critic Loss : 11778367.0
Exploitation Critic Loss : 0.041504502296447754
Exploration Model Loss : 54.80958557128906
Actor Loss : 0.5976889729499817
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -139.508774
best mean reward -inf
running time 250.414222
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     890 / 1000At timestep:     1040 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -139.50877380371094
TimeSinceStart : 250.41422200202942
Exploration Critic Loss : 5921626.5
Exploitation Critic Loss : 0.238901287317276
Exploration Model Loss : 27.415054321289062
Actor Loss : 0.5533186197280884
Eval_AverageReturn : -148.42857360839844
Eval_StdReturn : 3.849198341369629
Eval_MaxReturn : -139.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 148.57142857142858
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -137.600006
best mean reward -inf
running time 287.773990
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     874 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -137.60000610351562
TimeSinceStart : 287.7739896774292
Exploration Critic Loss : 8375964.5
Exploitation Critic Loss : 0.12428154051303864
Exploration Model Loss : 35.99082946777344
Actor Loss : 0.508766233921051
Eval_AverageReturn : -146.14285278320312
Eval_StdReturn : 9.448031425476074
Eval_MaxReturn : -123.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 146.28571428571428
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -137.722229
best mean reward -inf
running time 328.201412
At timestep:     120 / 1000At timestep:     211 / 1000At timestep:     361 / 1000At timestep:     511 / 1000At timestep:     650 / 1000At timestep:     800 / 1000At timestep:     936 / 1000At timestep:     1082 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -137.72222900390625
TimeSinceStart : 328.2014117240906
Exploration Critic Loss : 11706758.0
Exploitation Critic Loss : 0.10361392796039581
Exploration Model Loss : 52.466461181640625
Actor Loss : 0.4885408580303192
Eval_AverageReturn : -134.625
Eval_StdReturn : 19.56998634338379
Eval_MaxReturn : -90.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 135.25
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -131.927704
best mean reward -inf
running time 368.925920
At timestep:     139 / 1000At timestep:     239 / 1000At timestep:     389 / 1000At timestep:     539 / 1000At timestep:     689 / 1000At timestep:     839 / 1000At timestep:     989 / 1000At timestep:     1139 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -131.92770385742188
TimeSinceStart : 368.92591977119446
Exploration Critic Loss : 5590364.5
Exploitation Critic Loss : 0.1676572859287262
Exploration Model Loss : 26.988113403320312
Actor Loss : 0.46745434403419495
Eval_AverageReturn : -142.125
Eval_StdReturn : 16.76631736755371
Eval_MaxReturn : -99.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 142.375
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -124.458336
best mean reward -inf
running time 409.199809
At timestep:     141 / 1000At timestep:     229 / 1000At timestep:     345 / 1000At timestep:     495 / 1000At timestep:     645 / 1000At timestep:     795 / 1000At timestep:     938 / 1000At timestep:     1088 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -124.45833587646484
TimeSinceStart : 409.19980907440186
Exploration Critic Loss : 8495496.0
Exploitation Critic Loss : 0.3440166115760803
Exploration Model Loss : 35.78974914550781
Actor Loss : 0.4616456925868988
Eval_AverageReturn : -135.5
Eval_StdReturn : 21.435951232910156
Eval_MaxReturn : -87.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 136.0
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -111.139999
best mean reward -111.139999
running time 450.232761
At timestep:     150 / 1000At timestep:     290 / 1000At timestep:     440 / 1000At timestep:     529 / 1000At timestep:     679 / 1000At timestep:     759 / 1000At timestep:     909 / 1000At timestep:     1037 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -111.13999938964844
Train_BestReturn : -111.13999938964844
TimeSinceStart : 450.2327609062195
Exploration Critic Loss : 11875221.0
Exploitation Critic Loss : 0.19800905883312225
Exploration Model Loss : 52.358642578125
Actor Loss : 0.4518253207206726
Eval_AverageReturn : -129.125
Eval_StdReturn : 27.4974422454834
Eval_MaxReturn : -79.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 129.625
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -91.860001
best mean reward -91.860001
running time 491.287796
At timestep:     136 / 1000At timestep:     183 / 1000At timestep:     277 / 1000At timestep:     427 / 1000At timestep:     577 / 1000At timestep:     723 / 1000At timestep:     873 / 1000At timestep:     931 / 1000At timestep:     1081 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -91.86000061035156
Train_BestReturn : -91.86000061035156
TimeSinceStart : 491.2877962589264
Exploration Critic Loss : 5941586.5
Exploitation Critic Loss : 0.16755038499832153
Exploration Model Loss : 26.966964721679688
Actor Loss : 0.42755094170570374
Eval_AverageReturn : -119.55555725097656
Eval_StdReturn : 40.30746078491211
Eval_MaxReturn : -46.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 120.11111111111111
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -71.690002
best mean reward -71.690002
running time 532.005019
At timestep:     92 / 1000At timestep:     192 / 1000At timestep:     342 / 1000At timestep:     453 / 1000At timestep:     603 / 1000At timestep:     753 / 1000At timestep:     903 / 1000At timestep:     1053 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -71.69000244140625
Train_BestReturn : -71.69000244140625
TimeSinceStart : 532.0050187110901
Exploration Critic Loss : 8115374.0
Exploitation Critic Loss : 0.12729129195213318
Exploration Model Loss : 35.77726745605469
Actor Loss : 0.46264466643333435
Eval_AverageReturn : -131.125
Eval_StdReturn : 24.578636169433594
Eval_MaxReturn : -91.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 131.625
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -49.570000
best mean reward -49.570000
running time 574.526155
At timestep:     150 / 1000At timestep:     274 / 1000At timestep:     356 / 1000At timestep:     506 / 1000At timestep:     588 / 1000At timestep:     632 / 1000At timestep:     778 / 1000At timestep:     903 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -49.56999969482422
Train_BestReturn : -49.56999969482422
TimeSinceStart : 574.526154756546
Exploration Critic Loss : 10389285.0
Exploitation Critic Loss : 0.18590714037418365
Exploration Model Loss : 52.35711669921875
Actor Loss : 0.4401146173477173
Eval_AverageReturn : -114.66666412353516
Eval_StdReturn : 35.57464599609375
Eval_MaxReturn : -43.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 115.44444444444444
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -41.799999
best mean reward -41.799999
running time 616.143155
At timestep:     104 / 1000At timestep:     163 / 1000At timestep:     313 / 1000At timestep:     412 / 1000At timestep:     475 / 1000At timestep:     534 / 1000At timestep:     661 / 1000At timestep:     748 / 1000At timestep:     795 / 1000At timestep:     945 / 1000At timestep:     1095 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -41.79999923706055
Train_BestReturn : -41.79999923706055
TimeSinceStart : 616.1431550979614
Exploration Critic Loss : 4989970.5
Exploitation Critic Loss : 0.10403063893318176
Exploration Model Loss : 26.960464477539062
Actor Loss : 0.44866642355918884
Eval_AverageReturn : -98.81818389892578
Eval_StdReturn : 38.4443359375
Eval_MaxReturn : -46.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 99.54545454545455
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -38.720001
best mean reward -38.720001
running time 658.242501
At timestep:     104 / 1000At timestep:     193 / 1000At timestep:     343 / 1000At timestep:     420 / 1000At timestep:     491 / 1000At timestep:     559 / 1000At timestep:     616 / 1000At timestep:     713 / 1000At timestep:     763 / 1000At timestep:     895 / 1000At timestep:     966 / 1000At timestep:     1061 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -38.720001220703125
Train_BestReturn : -38.720001220703125
TimeSinceStart : 658.2425014972687
Exploration Critic Loss : 5753234.0
Exploitation Critic Loss : 0.1426674723625183
Exploration Model Loss : 35.772491455078125
Actor Loss : 0.44928571581840515
Eval_AverageReturn : -87.5
Eval_StdReturn : 28.567171096801758
Eval_MaxReturn : -49.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 88.41666666666667
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -38.669998
best mean reward -38.669998
running time 700.400554
At timestep:     59 / 1000At timestep:     89 / 1000At timestep:     132 / 1000At timestep:     180 / 1000At timestep:     299 / 1000At timestep:     449 / 1000At timestep:     562 / 1000At timestep:     655 / 1000At timestep:     751 / 1000At timestep:     799 / 1000At timestep:     830 / 1000At timestep:     921 / 1000At timestep:     967 / 1000At timestep:     1047 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -38.66999816894531
Train_BestReturn : -38.66999816894531
TimeSinceStart : 700.4005539417267
Exploration Critic Loss : 13019730.0
Exploitation Critic Loss : 0.2137119174003601
Exploration Model Loss : 52.3607177734375
Actor Loss : 0.4449514150619507
Eval_AverageReturn : -73.85713958740234
Eval_StdReturn : 35.598670959472656
Eval_MaxReturn : -29.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 74.78571428571429
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -38.959999
best mean reward -38.669998
running time 742.397442
At timestep:     60 / 1000At timestep:     140 / 1000At timestep:     197 / 1000At timestep:     255 / 1000At timestep:     382 / 1000At timestep:     446 / 1000At timestep:     513 / 1000At timestep:     637 / 1000At timestep:     696 / 1000At timestep:     748 / 1000At timestep:     824 / 1000At timestep:     846 / 1000At timestep:     931 / 1000At timestep:     1081 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -38.959999084472656
Train_BestReturn : -38.66999816894531
TimeSinceStart : 742.3974416255951
Exploration Critic Loss : 4315880.5
Exploitation Critic Loss : 0.15288710594177246
Exploration Model Loss : 26.959030151367188
Actor Loss : 0.49122369289398193
Eval_AverageReturn : -76.28571319580078
Eval_StdReturn : 33.32830047607422
Eval_MaxReturn : -21.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 77.21428571428571
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -36.389999
best mean reward -36.389999
running time 785.366733
At timestep:     81 / 1000At timestep:     142 / 1000At timestep:     207 / 1000At timestep:     253 / 1000At timestep:     370 / 1000At timestep:     510 / 1000At timestep:     548 / 1000At timestep:     693 / 1000At timestep:     752 / 1000At timestep:     782 / 1000At timestep:     865 / 1000At timestep:     924 / 1000At timestep:     984 / 1000At timestep:     1069 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -36.38999938964844
Train_BestReturn : -36.38999938964844
TimeSinceStart : 785.3667333126068
Exploration Critic Loss : 6672573.0
Exploitation Critic Loss : 0.15173617005348206
Exploration Model Loss : 35.772247314453125
Actor Loss : 0.5018986463546753
Eval_AverageReturn : -75.35713958740234
Eval_StdReturn : 34.15428924560547
Eval_MaxReturn : -29.0
Eval_MinReturn : -144.0
Eval_AverageEpLen : 76.35714285714286
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -31.309999
best mean reward -31.309999
running time 829.826841
At timestep:     49 / 1000At timestep:     106 / 1000At timestep:     185 / 1000At timestep:     236 / 1000At timestep:     290 / 1000At timestep:     358 / 1000At timestep:     422 / 1000At timestep:     503 / 1000At timestep:     631 / 1000At timestep:     706 / 1000At timestep:     752 / 1000At timestep:     863 / 1000At timestep:     909 / 1000At timestep:     957 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -31.309999465942383
Train_BestReturn : -31.309999465942383
TimeSinceStart : 829.826840877533
Exploration Critic Loss : 10175222.0
Exploitation Critic Loss : 0.17144931852817535
Exploration Model Loss : 52.36102294921875
Actor Loss : 0.5041961669921875
Eval_AverageReturn : -66.19999694824219
Eval_StdReturn : 23.690221786499023
Eval_MaxReturn : -45.0
Eval_MinReturn : -127.0
Eval_AverageEpLen : 67.2
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -27.879999
best mean reward -27.879999
running time 873.874858
At timestep:     53 / 1000At timestep:     106 / 1000At timestep:     151 / 1000At timestep:     212 / 1000At timestep:     362 / 1000At timestep:     414 / 1000At timestep:     480 / 1000At timestep:     511 / 1000At timestep:     556 / 1000At timestep:     595 / 1000At timestep:     671 / 1000At timestep:     754 / 1000At timestep:     904 / 1000At timestep:     937 / 1000At timestep:     993 / 1000At timestep:     1045 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -27.8799991607666
Train_BestReturn : -27.8799991607666
TimeSinceStart : 873.8748576641083
Exploration Critic Loss : 3914695.5
Exploitation Critic Loss : 0.10583288967609406
Exploration Model Loss : 26.959152221679688
Actor Loss : 0.5304734110832214
Eval_AverageReturn : -64.375
Eval_StdReturn : 34.86021041870117
Eval_MaxReturn : -30.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 65.3125
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -27.370001
best mean reward -27.370001
running time 916.420097
At timestep:     27 / 1000At timestep:     72 / 1000At timestep:     177 / 1000At timestep:     232 / 1000At timestep:     293 / 1000At timestep:     345 / 1000At timestep:     408 / 1000At timestep:     455 / 1000At timestep:     508 / 1000At timestep:     556 / 1000At timestep:     587 / 1000At timestep:     666 / 1000At timestep:     704 / 1000At timestep:     806 / 1000At timestep:     876 / 1000At timestep:     968 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -27.3700008392334
Train_BestReturn : -27.3700008392334
TimeSinceStart : 916.4200971126556
Exploration Critic Loss : 7576125.5
Exploitation Critic Loss : 0.15699070692062378
Exploration Model Loss : 35.772613525390625
Actor Loss : 0.5938742160797119
Eval_AverageReturn : -60.117645263671875
Eval_StdReturn : 22.32614517211914
Eval_MaxReturn : -26.0
Eval_MinReturn : -104.0
Eval_AverageEpLen : 61.11764705882353
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -27.250000
best mean reward -27.250000
running time 961.834399
At timestep:     65 / 1000At timestep:     128 / 1000At timestep:     165 / 1000At timestep:     236 / 1000At timestep:     279 / 1000At timestep:     399 / 1000At timestep:     543 / 1000At timestep:     579 / 1000At timestep:     648 / 1000At timestep:     712 / 1000At timestep:     760 / 1000At timestep:     826 / 1000At timestep:     862 / 1000At timestep:     903 / 1000At timestep:     932 / 1000At timestep:     971 / 1000At timestep:     1048 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -27.25
Train_BestReturn : -27.25
TimeSinceStart : 961.8343985080719
Exploration Critic Loss : 9194106.0
Exploitation Critic Loss : 0.10769152641296387
Exploration Model Loss : 52.36067199707031
Actor Loss : 0.5919032096862793
Eval_AverageReturn : -60.64706039428711
Eval_StdReturn : 29.76761245727539
Eval_MaxReturn : -28.0
Eval_MinReturn : -143.0
Eval_AverageEpLen : 61.64705882352941
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -27.240000
best mean reward -27.240000
running time 1006.029532
At timestep:     55 / 1000At timestep:     94 / 1000At timestep:     169 / 1000At timestep:     201 / 1000At timestep:     260 / 1000At timestep:     310 / 1000At timestep:     347 / 1000At timestep:     400 / 1000At timestep:     451 / 1000At timestep:     487 / 1000At timestep:     527 / 1000At timestep:     572 / 1000At timestep:     618 / 1000At timestep:     644 / 1000At timestep:     711 / 1000At timestep:     777 / 1000At timestep:     829 / 1000At timestep:     898 / 1000At timestep:     960 / 1000At timestep:     994 / 1000At timestep:     1041 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -27.239999771118164
Train_BestReturn : -27.239999771118164
TimeSinceStart : 1006.029531955719
Exploration Critic Loss : 3452727.75
Exploitation Critic Loss : 0.1347857266664505
Exploration Model Loss : 26.959091186523438
Actor Loss : 0.6139689683914185
Eval_AverageReturn : -48.57143020629883
Eval_StdReturn : 13.062383651733398
Eval_MaxReturn : -25.0
Eval_MinReturn : -74.0
Eval_AverageEpLen : 49.57142857142857
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -27.680000
best mean reward -27.240000
running time 1050.063020
At timestep:     52 / 1000At timestep:     166 / 1000At timestep:     202 / 1000At timestep:     251 / 1000At timestep:     284 / 1000At timestep:     312 / 1000At timestep:     384 / 1000At timestep:     455 / 1000At timestep:     534 / 1000At timestep:     566 / 1000At timestep:     595 / 1000At timestep:     644 / 1000At timestep:     694 / 1000At timestep:     782 / 1000At timestep:     845 / 1000At timestep:     869 / 1000At timestep:     911 / 1000At timestep:     979 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -27.68000030517578
Train_BestReturn : -27.239999771118164
TimeSinceStart : 1050.0630202293396
Exploration Critic Loss : 7182738.0
Exploitation Critic Loss : 0.09432445466518402
Exploration Model Loss : 35.77232360839844
Actor Loss : 0.5359258055686951
Eval_AverageReturn : -52.31578826904297
Eval_StdReturn : 23.312641143798828
Eval_MaxReturn : -23.0
Eval_MinReturn : -113.0
Eval_AverageEpLen : 53.31578947368421
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -27.059999
best mean reward -27.059999
running time 1095.048935
At timestep:     39 / 1000At timestep:     94 / 1000At timestep:     132 / 1000At timestep:     186 / 1000At timestep:     254 / 1000At timestep:     296 / 1000At timestep:     353 / 1000At timestep:     399 / 1000At timestep:     457 / 1000At timestep:     496 / 1000At timestep:     533 / 1000At timestep:     559 / 1000At timestep:     587 / 1000At timestep:     620 / 1000At timestep:     650 / 1000At timestep:     685 / 1000At timestep:     727 / 1000At timestep:     806 / 1000At timestep:     845 / 1000At timestep:     894 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -27.059999465942383
Train_BestReturn : -27.059999465942383
TimeSinceStart : 1095.0489354133606
Exploration Critic Loss : 8780258.0
Exploitation Critic Loss : 0.08354338258504868
Exploration Model Loss : 52.36097717285156
Actor Loss : 0.5983766317367554
Eval_AverageReturn : -47.0476188659668
Eval_StdReturn : 19.83137321472168
Eval_MaxReturn : -25.0
Eval_MinReturn : -114.0
Eval_AverageEpLen : 48.04761904761905
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -26.920000
best mean reward -26.920000
running time 1140.068190
At timestep:     54 / 1000At timestep:     135 / 1000At timestep:     182 / 1000At timestep:     249 / 1000At timestep:     280 / 1000At timestep:     330 / 1000At timestep:     383 / 1000At timestep:     422 / 1000At timestep:     465 / 1000At timestep:     498 / 1000At timestep:     588 / 1000At timestep:     638 / 1000At timestep:     664 / 1000At timestep:     708 / 1000At timestep:     738 / 1000At timestep:     793 / 1000At timestep:     832 / 1000At timestep:     871 / 1000At timestep:     898 / 1000At timestep:     987 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -26.920000076293945
Train_BestReturn : -26.920000076293945
TimeSinceStart : 1140.0681900978088
Exploration Critic Loss : 4115421.0
Exploitation Critic Loss : 0.11568669974803925
Exploration Model Loss : 26.958786010742188
Actor Loss : 0.5218187570571899
Eval_AverageReturn : -48.095237731933594
Eval_StdReturn : 18.324859619140625
Eval_MaxReturn : -25.0
Eval_MinReturn : -89.0
Eval_AverageEpLen : 49.095238095238095
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -27.340000
best mean reward -26.920000
running time 1183.378486
At timestep:     44 / 1000At timestep:     87 / 1000At timestep:     137 / 1000At timestep:     179 / 1000At timestep:     219 / 1000At timestep:     275 / 1000At timestep:     308 / 1000At timestep:     351 / 1000At timestep:     396 / 1000At timestep:     477 / 1000At timestep:     524 / 1000At timestep:     577 / 1000At timestep:     624 / 1000At timestep:     676 / 1000At timestep:     704 / 1000At timestep:     763 / 1000At timestep:     813 / 1000At timestep:     844 / 1000At timestep:     876 / 1000At timestep:     926 / 1000At timestep:     958 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -27.34000015258789
Train_BestReturn : -26.920000076293945
TimeSinceStart : 1183.3784863948822
Exploration Critic Loss : 6581615.5
Exploitation Critic Loss : 0.08108064532279968
Exploration Model Loss : 35.7725830078125
Actor Loss : 0.5704437494277954
Eval_AverageReturn : -45.727272033691406
Eval_StdReturn : 12.392013549804688
Eval_MaxReturn : -27.0
Eval_MinReturn : -80.0
Eval_AverageEpLen : 46.72727272727273
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -27.230000
best mean reward -26.920000
running time 1227.155379
At timestep:     62 / 1000At timestep:     99 / 1000At timestep:     135 / 1000At timestep:     166 / 1000At timestep:     220 / 1000At timestep:     255 / 1000At timestep:     285 / 1000At timestep:     326 / 1000At timestep:     379 / 1000At timestep:     403 / 1000At timestep:     436 / 1000At timestep:     479 / 1000At timestep:     502 / 1000At timestep:     529 / 1000At timestep:     622 / 1000At timestep:     670 / 1000At timestep:     712 / 1000At timestep:     740 / 1000At timestep:     782 / 1000At timestep:     806 / 1000At timestep:     859 / 1000At timestep:     890 / 1000At timestep:     932 / 1000At timestep:     962 / 1000At timestep:     997 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -27.229999542236328
Train_BestReturn : -26.920000076293945
TimeSinceStart : 1227.1553792953491
Exploration Critic Loss : 8484348.0
Exploitation Critic Loss : 0.09068509936332703
Exploration Model Loss : 52.360687255859375
Actor Loss : 0.5845403671264648
Eval_AverageReturn : -38.5
Eval_StdReturn : 14.684502601623535
Eval_MaxReturn : -22.0
Eval_MinReturn : -92.0
Eval_AverageEpLen : 39.5
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -26.540001
best mean reward -26.540001
running time 1273.388764
At timestep:     71 / 1000At timestep:     92 / 1000At timestep:     129 / 1000At timestep:     157 / 1000At timestep:     211 / 1000At timestep:     297 / 1000At timestep:     340 / 1000At timestep:     372 / 1000At timestep:     415 / 1000At timestep:     452 / 1000At timestep:     475 / 1000At timestep:     532 / 1000At timestep:     598 / 1000At timestep:     627 / 1000At timestep:     667 / 1000At timestep:     692 / 1000At timestep:     741 / 1000At timestep:     782 / 1000At timestep:     866 / 1000At timestep:     925 / 1000At timestep:     952 / 1000At timestep:     975 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -26.540000915527344
Train_BestReturn : -26.540000915527344
TimeSinceStart : 1273.3887639045715
Exploration Critic Loss : 6318162.0
Exploitation Critic Loss : 0.12937676906585693
Exploration Model Loss : 26.959075927734375
Actor Loss : 0.5397241115570068
Eval_AverageReturn : -42.60869598388672
Eval_StdReturn : 18.835071563720703
Eval_MaxReturn : -20.0
Eval_MinReturn : -85.0
Eval_AverageEpLen : 43.608695652173914
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -26.120001
best mean reward -26.120001
running time 1317.697388
At timestep:     69 / 1000At timestep:     143 / 1000At timestep:     205 / 1000At timestep:     236 / 1000At timestep:     278 / 1000At timestep:     306 / 1000At timestep:     341 / 1000At timestep:     383 / 1000At timestep:     422 / 1000At timestep:     486 / 1000At timestep:     526 / 1000At timestep:     603 / 1000At timestep:     648 / 1000At timestep:     685 / 1000At timestep:     741 / 1000At timestep:     802 / 1000At timestep:     850 / 1000At timestep:     908 / 1000At timestep:     940 / 1000At timestep:     970 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -26.1200008392334
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1317.6973884105682
Exploration Critic Loss : 7674672.0
Exploitation Critic Loss : 0.12416638433933258
Exploration Model Loss : 35.77227783203125
Actor Loss : 0.6021640300750732
Eval_AverageReturn : -47.33333206176758
Eval_StdReturn : 14.694777488708496
Eval_MaxReturn : -27.0
Eval_MinReturn : -76.0
Eval_AverageEpLen : 48.333333333333336
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -26.959999
best mean reward -26.120001
running time 1361.011700
At timestep:     45 / 1000At timestep:     74 / 1000At timestep:     109 / 1000At timestep:     134 / 1000At timestep:     166 / 1000At timestep:     207 / 1000At timestep:     286 / 1000At timestep:     347 / 1000At timestep:     425 / 1000At timestep:     518 / 1000At timestep:     567 / 1000At timestep:     626 / 1000At timestep:     684 / 1000At timestep:     715 / 1000At timestep:     767 / 1000At timestep:     820 / 1000At timestep:     843 / 1000At timestep:     898 / 1000At timestep:     943 / 1000At timestep:     988 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -26.959999084472656
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1361.0117001533508
Exploration Critic Loss : 9161376.0
Exploitation Critic Loss : 0.08270574361085892
Exploration Model Loss : 52.360992431640625
Actor Loss : 0.5521748661994934
Eval_AverageReturn : -47.47618865966797
Eval_StdReturn : 18.245988845825195
Eval_MaxReturn : -22.0
Eval_MinReturn : -92.0
Eval_AverageEpLen : 48.476190476190474
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -27.219999
best mean reward -26.120001
running time 1406.201277
At timestep:     22 / 1000At timestep:     89 / 1000At timestep:     144 / 1000At timestep:     172 / 1000At timestep:     218 / 1000At timestep:     271 / 1000At timestep:     323 / 1000At timestep:     376 / 1000At timestep:     407 / 1000At timestep:     434 / 1000At timestep:     456 / 1000At timestep:     500 / 1000At timestep:     545 / 1000At timestep:     575 / 1000At timestep:     615 / 1000At timestep:     651 / 1000At timestep:     712 / 1000At timestep:     738 / 1000At timestep:     786 / 1000At timestep:     828 / 1000At timestep:     886 / 1000At timestep:     943 / 1000At timestep:     988 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -27.219999313354492
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1406.2012770175934
Exploration Critic Loss : 6856963.0
Exploitation Critic Loss : 0.14537601172924042
Exploration Model Loss : 26.958770751953125
Actor Loss : 0.5769999027252197
Eval_AverageReturn : -41.70833206176758
Eval_StdReturn : 12.660566329956055
Eval_MaxReturn : -21.0
Eval_MinReturn : -66.0
Eval_AverageEpLen : 42.708333333333336
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -27.160000
best mean reward -26.120001
running time 1449.488558
At timestep:     45 / 1000At timestep:     93 / 1000At timestep:     137 / 1000At timestep:     174 / 1000At timestep:     207 / 1000At timestep:     252 / 1000At timestep:     317 / 1000At timestep:     348 / 1000At timestep:     394 / 1000At timestep:     433 / 1000At timestep:     476 / 1000At timestep:     531 / 1000At timestep:     577 / 1000At timestep:     615 / 1000At timestep:     663 / 1000At timestep:     746 / 1000At timestep:     779 / 1000At timestep:     815 / 1000At timestep:     859 / 1000At timestep:     890 / 1000At timestep:     937 / 1000At timestep:     974 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -27.15999984741211
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1449.4885580539703
Exploration Critic Loss : 7775923.0
Exploitation Critic Loss : 0.1049579456448555
Exploration Model Loss : 35.77256774902344
Actor Loss : 0.564137876033783
Eval_AverageReturn : -42.869564056396484
Eval_StdReturn : 11.456171035766602
Eval_MaxReturn : -30.0
Eval_MinReturn : -82.0
Eval_AverageEpLen : 43.869565217391305
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -26.219999
best mean reward -26.120001
running time 1494.198878
At timestep:     36 / 1000At timestep:     83 / 1000At timestep:     132 / 1000At timestep:     198 / 1000At timestep:     254 / 1000At timestep:     300 / 1000At timestep:     331 / 1000At timestep:     387 / 1000At timestep:     429 / 1000At timestep:     468 / 1000At timestep:     537 / 1000At timestep:     580 / 1000At timestep:     631 / 1000At timestep:     693 / 1000At timestep:     732 / 1000At timestep:     779 / 1000At timestep:     812 / 1000At timestep:     833 / 1000At timestep:     885 / 1000At timestep:     939 / 1000At timestep:     984 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -26.219999313354492
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1494.1988775730133
Exploration Critic Loss : 13023286.0
Exploitation Critic Loss : 0.08646348118782043
Exploration Model Loss : 52.3607177734375
Actor Loss : 0.5514219403266907
Eval_AverageReturn : -45.90909194946289
Eval_StdReturn : 11.179969787597656
Eval_MaxReturn : -20.0
Eval_MinReturn : -68.0
Eval_AverageEpLen : 46.90909090909091
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -26.129999
best mean reward -26.120001
running time 1538.359024
At timestep:     36 / 1000At timestep:     72 / 1000At timestep:     121 / 1000At timestep:     144 / 1000At timestep:     179 / 1000At timestep:     213 / 1000At timestep:     255 / 1000At timestep:     291 / 1000At timestep:     326 / 1000At timestep:     341 / 1000At timestep:     374 / 1000At timestep:     410 / 1000At timestep:     465 / 1000At timestep:     501 / 1000At timestep:     533 / 1000At timestep:     568 / 1000At timestep:     597 / 1000At timestep:     625 / 1000At timestep:     659 / 1000At timestep:     703 / 1000At timestep:     732 / 1000At timestep:     772 / 1000At timestep:     797 / 1000At timestep:     834 / 1000At timestep:     881 / 1000At timestep:     923 / 1000At timestep:     951 / 1000At timestep:     983 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -26.1299991607666
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1538.3590240478516
Exploration Critic Loss : 6755071.0
Exploitation Critic Loss : 0.11914954334497452
Exploration Model Loss : 26.959060668945312
Actor Loss : 0.5232570171356201
Eval_AverageReturn : -34.379310607910156
Eval_StdReturn : 7.988844871520996
Eval_MaxReturn : -14.0
Eval_MinReturn : -54.0
Eval_AverageEpLen : 35.37931034482759
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -26.350000
best mean reward -26.120001
running time 1585.075759
At timestep:     50 / 1000At timestep:     107 / 1000At timestep:     136 / 1000At timestep:     175 / 1000At timestep:     216 / 1000At timestep:     251 / 1000At timestep:     286 / 1000At timestep:     318 / 1000At timestep:     384 / 1000At timestep:     420 / 1000At timestep:     458 / 1000At timestep:     492 / 1000At timestep:     539 / 1000At timestep:     581 / 1000At timestep:     637 / 1000At timestep:     672 / 1000At timestep:     709 / 1000At timestep:     755 / 1000At timestep:     780 / 1000At timestep:     823 / 1000At timestep:     876 / 1000At timestep:     915 / 1000At timestep:     957 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -26.350000381469727
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1585.0757586956024
Exploration Critic Loss : 8646036.0
Exploitation Critic Loss : 0.07650431990623474
Exploration Model Loss : 35.77227783203125
Actor Loss : 0.5373849868774414
Eval_AverageReturn : -40.75
Eval_StdReturn : 9.346344947814941
Eval_MaxReturn : -24.0
Eval_MinReturn : -65.0
Eval_AverageEpLen : 41.75
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -26.500000
best mean reward -26.120001
running time 1630.095611
At timestep:     25 / 1000At timestep:     75 / 1000At timestep:     101 / 1000At timestep:     129 / 1000At timestep:     156 / 1000At timestep:     213 / 1000At timestep:     260 / 1000At timestep:     295 / 1000At timestep:     332 / 1000At timestep:     372 / 1000At timestep:     429 / 1000At timestep:     471 / 1000At timestep:     517 / 1000At timestep:     543 / 1000At timestep:     572 / 1000At timestep:     597 / 1000At timestep:     633 / 1000At timestep:     668 / 1000At timestep:     716 / 1000At timestep:     746 / 1000At timestep:     778 / 1000At timestep:     814 / 1000At timestep:     847 / 1000At timestep:     904 / 1000At timestep:     941 / 1000At timestep:     983 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -26.5
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1630.09561085701
Exploration Critic Loss : 14699678.0
Exploitation Critic Loss : 0.13106860220432281
Exploration Model Loss : 52.36102294921875
Actor Loss : 0.5701848268508911
Eval_AverageReturn : -36.296295166015625
Eval_StdReturn : 10.131778717041016
Eval_MaxReturn : -23.0
Eval_MinReturn : -56.0
Eval_AverageEpLen : 37.2962962962963
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -26.290001
best mean reward -26.120001
running time 1674.733086
At timestep:     55 / 1000At timestep:     92 / 1000At timestep:     137 / 1000At timestep:     175 / 1000At timestep:     229 / 1000At timestep:     253 / 1000At timestep:     294 / 1000At timestep:     326 / 1000At timestep:     365 / 1000At timestep:     395 / 1000At timestep:     434 / 1000At timestep:     463 / 1000At timestep:     504 / 1000At timestep:     556 / 1000At timestep:     589 / 1000At timestep:     634 / 1000At timestep:     680 / 1000At timestep:     718 / 1000At timestep:     759 / 1000At timestep:     803 / 1000At timestep:     847 / 1000At timestep:     868 / 1000At timestep:     896 / 1000At timestep:     942 / 1000At timestep:     987 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -26.290000915527344
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1674.73308634758
Exploration Critic Loss : 10580124.0
Exploitation Critic Loss : 0.10786345601081848
Exploration Model Loss : 26.958755493164062
Actor Loss : 0.5763847827911377
Eval_AverageReturn : -38.230770111083984
Eval_StdReturn : 8.536382675170898
Eval_MaxReturn : -20.0
Eval_MinReturn : -54.0
Eval_AverageEpLen : 39.23076923076923
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -26.549999
best mean reward -26.120001
running time 1721.059806
At timestep:     34 / 1000At timestep:     80 / 1000At timestep:     123 / 1000At timestep:     158 / 1000At timestep:     197 / 1000At timestep:     244 / 1000At timestep:     289 / 1000At timestep:     319 / 1000At timestep:     374 / 1000At timestep:     421 / 1000At timestep:     472 / 1000At timestep:     525 / 1000At timestep:     582 / 1000At timestep:     628 / 1000At timestep:     678 / 1000At timestep:     717 / 1000At timestep:     750 / 1000At timestep:     816 / 1000At timestep:     873 / 1000At timestep:     903 / 1000At timestep:     954 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -26.549999237060547
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1721.0598056316376
Exploration Critic Loss : 13720550.0
Exploitation Critic Loss : 0.0871601551771164
Exploration Model Loss : 35.772613525390625
Actor Loss : 0.5626834034919739
Eval_AverageReturn : -44.95454406738281
Eval_StdReturn : 9.570205688476562
Eval_MaxReturn : -29.0
Eval_MinReturn : -65.0
Eval_AverageEpLen : 45.95454545454545
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -26.540001
best mean reward -26.120001
running time 1765.612582
At timestep:     33 / 1000At timestep:     72 / 1000At timestep:     100 / 1000At timestep:     139 / 1000At timestep:     190 / 1000At timestep:     223 / 1000At timestep:     271 / 1000At timestep:     325 / 1000At timestep:     368 / 1000At timestep:     402 / 1000At timestep:     444 / 1000At timestep:     479 / 1000At timestep:     539 / 1000At timestep:     578 / 1000At timestep:     615 / 1000At timestep:     652 / 1000At timestep:     691 / 1000At timestep:     743 / 1000At timestep:     769 / 1000At timestep:     802 / 1000At timestep:     861 / 1000At timestep:     906 / 1000At timestep:     941 / 1000At timestep:     982 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -26.540000915527344
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1765.6125819683075
Exploration Critic Loss : 13626888.0
Exploitation Critic Loss : 0.1019052118062973
Exploration Model Loss : 52.36064147949219
Actor Loss : 0.5071855783462524
Eval_AverageReturn : -40.08000183105469
Eval_StdReturn : 8.754061698913574
Eval_MaxReturn : -25.0
Eval_MinReturn : -59.0
Eval_AverageEpLen : 41.08
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -27.309999
best mean reward -26.120001
running time 1808.761812
At timestep:     37 / 1000At timestep:     73 / 1000At timestep:     101 / 1000At timestep:     126 / 1000At timestep:     170 / 1000At timestep:     201 / 1000At timestep:     256 / 1000At timestep:     341 / 1000At timestep:     380 / 1000At timestep:     454 / 1000At timestep:     502 / 1000At timestep:     536 / 1000At timestep:     590 / 1000At timestep:     635 / 1000At timestep:     669 / 1000At timestep:     717 / 1000At timestep:     755 / 1000At timestep:     799 / 1000At timestep:     837 / 1000At timestep:     901 / 1000At timestep:     985 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -27.309999465942383
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1808.7618124485016
Exploration Critic Loss : 12226839.0
Exploitation Critic Loss : 0.07807427644729614
Exploration Model Loss : 26.959121704101562
Actor Loss : 0.5093922019004822
Eval_AverageReturn : -45.04545593261719
Eval_StdReturn : 16.780609130859375
Eval_MaxReturn : -24.0
Eval_MinReturn : -84.0
Eval_AverageEpLen : 46.04545454545455
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -26.510000
best mean reward -26.120001
running time 1854.146457
At timestep:     46 / 1000At timestep:     75 / 1000At timestep:     126 / 1000At timestep:     158 / 1000At timestep:     202 / 1000At timestep:     232 / 1000At timestep:     260 / 1000At timestep:     311 / 1000At timestep:     339 / 1000At timestep:     370 / 1000At timestep:     393 / 1000At timestep:     437 / 1000At timestep:     484 / 1000At timestep:     534 / 1000At timestep:     571 / 1000At timestep:     625 / 1000At timestep:     657 / 1000At timestep:     683 / 1000At timestep:     711 / 1000At timestep:     763 / 1000At timestep:     805 / 1000At timestep:     852 / 1000At timestep:     901 / 1000At timestep:     923 / 1000At timestep:     983 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -26.510000228881836
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1854.146457195282
Exploration Critic Loss : 11924943.0
Exploitation Critic Loss : 0.07430781424045563
Exploration Model Loss : 35.772308349609375
Actor Loss : 0.5334115624427795
Eval_AverageReturn : -38.346153259277344
Eval_StdReturn : 10.745096206665039
Eval_MaxReturn : -21.0
Eval_MinReturn : -59.0
Eval_AverageEpLen : 39.34615384615385
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -26.730000
best mean reward -26.120001
running time 1900.208732
At timestep:     37 / 1000At timestep:     64 / 1000At timestep:     104 / 1000At timestep:     135 / 1000At timestep:     164 / 1000At timestep:     208 / 1000At timestep:     241 / 1000At timestep:     262 / 1000At timestep:     315 / 1000At timestep:     373 / 1000At timestep:     426 / 1000At timestep:     470 / 1000At timestep:     490 / 1000At timestep:     524 / 1000At timestep:     569 / 1000At timestep:     613 / 1000At timestep:     640 / 1000At timestep:     680 / 1000At timestep:     715 / 1000At timestep:     760 / 1000At timestep:     782 / 1000At timestep:     843 / 1000At timestep:     917 / 1000At timestep:     945 / 1000At timestep:     975 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -26.729999542236328
Train_BestReturn : -26.1200008392334
TimeSinceStart : 1900.2087316513062
Exploration Critic Loss : 13909024.0
Exploitation Critic Loss : 0.09368368238210678
Exploration Model Loss : 52.36094665527344
Actor Loss : 0.5034289360046387
Eval_AverageReturn : -38.46154022216797
Eval_StdReturn : 13.095009803771973
Eval_MaxReturn : -19.0
Eval_MinReturn : -73.0
Eval_AverageEpLen : 39.46153846153846
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -25.959999
best mean reward -25.959999
running time 1944.821123
At timestep:     34 / 1000At timestep:     69 / 1000At timestep:     95 / 1000At timestep:     139 / 1000At timestep:     178 / 1000At timestep:     216 / 1000At timestep:     260 / 1000At timestep:     296 / 1000At timestep:     327 / 1000At timestep:     367 / 1000At timestep:     394 / 1000At timestep:     428 / 1000At timestep:     463 / 1000At timestep:     511 / 1000At timestep:     548 / 1000At timestep:     619 / 1000At timestep:     664 / 1000At timestep:     708 / 1000At timestep:     770 / 1000At timestep:     806 / 1000At timestep:     839 / 1000At timestep:     872 / 1000At timestep:     924 / 1000At timestep:     960 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -25.959999084472656
Train_BestReturn : -25.959999084472656
TimeSinceStart : 1944.8211233615875
Exploration Critic Loss : 14529175.0
Exploitation Critic Loss : 0.09910818934440613
Exploration Model Loss : 26.958831787109375
Actor Loss : 0.5784705877304077
Eval_AverageReturn : -39.08000183105469
Eval_StdReturn : 9.915321350097656
Eval_MaxReturn : -25.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 40.08
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -27.180000
best mean reward -25.959999
running time 1989.831129
At timestep:     41 / 1000At timestep:     73 / 1000At timestep:     114 / 1000At timestep:     159 / 1000At timestep:     208 / 1000At timestep:     229 / 1000At timestep:     262 / 1000At timestep:     301 / 1000At timestep:     340 / 1000At timestep:     375 / 1000At timestep:     404 / 1000At timestep:     442 / 1000At timestep:     468 / 1000At timestep:     528 / 1000At timestep:     564 / 1000At timestep:     607 / 1000At timestep:     645 / 1000At timestep:     719 / 1000At timestep:     766 / 1000At timestep:     793 / 1000At timestep:     843 / 1000At timestep:     883 / 1000At timestep:     920 / 1000At timestep:     975 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -27.18000030517578
Train_BestReturn : -25.959999084472656
TimeSinceStart : 1989.831128835678
Exploration Critic Loss : 17603404.0
Exploitation Critic Loss : 0.11313138157129288
Exploration Model Loss : 35.772674560546875
Actor Loss : 0.5520731210708618
Eval_AverageReturn : -39.720001220703125
Eval_StdReturn : 11.068947792053223
Eval_MaxReturn : -20.0
Eval_MinReturn : -73.0
Eval_AverageEpLen : 40.72
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -27.139999
best mean reward -25.959999
running time 2035.595051
At timestep:     40 / 1000At timestep:     60 / 1000At timestep:     95 / 1000At timestep:     112 / 1000At timestep:     169 / 1000At timestep:     208 / 1000At timestep:     230 / 1000At timestep:     259 / 1000At timestep:     316 / 1000At timestep:     346 / 1000At timestep:     385 / 1000At timestep:     421 / 1000At timestep:     461 / 1000At timestep:     495 / 1000At timestep:     531 / 1000At timestep:     575 / 1000At timestep:     616 / 1000At timestep:     651 / 1000At timestep:     684 / 1000At timestep:     708 / 1000At timestep:     740 / 1000At timestep:     795 / 1000At timestep:     835 / 1000At timestep:     879 / 1000At timestep:     931 / 1000At timestep:     987 / 1000At timestep:     1032 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_medium_unsupervised_lam1_PointmassMedium-v0_22-11-2022_21-55-15/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -27.139999389648438
Train_BestReturn : -25.959999084472656
TimeSinceStart : 2035.595050573349
Exploration Critic Loss : 21017000.0
Exploitation Critic Loss : 0.06607145071029663
Exploration Model Loss : 52.360595703125
Actor Loss : 0.48267480731010437
Eval_AverageReturn : -37.22222137451172
Eval_StdReturn : 10.78178596496582
Eval_MaxReturn : -16.0
Eval_MinReturn : -56.0
Eval_AverageEpLen : 38.22222222222222
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_unsupervised_lam2_PointmassMedium-v0_22-11-2022_22-30-02 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_unsupervised_lam2_PointmassMedium-v0_22-11-2022_22-30-02
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002369
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0023691654205322266
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 8.710882
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     666 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 8.710882186889648
Eval_AverageReturn : -139.375
Eval_StdReturn : 28.111106872558594
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.5
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 17.586439
At timestep:     54 / 1000At timestep:     204 / 1000At timestep:     354 / 1000At timestep:     504 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -150.0
TimeSinceStart : 17.58643937110901
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 56.952096
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -150.0
TimeSinceStart : 56.95209622383118
Exploration Critic Loss : 812639.625
Exploitation Critic Loss : 0.02527538500726223
Exploration Model Loss : 46.75253677368164
Actor Loss : 1.2048916816711426
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 92.936530
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -150.0
TimeSinceStart : 92.93652987480164
Exploration Critic Loss : 17888924.0
Exploitation Critic Loss : 0.03441883251070976
Exploration Model Loss : 96.21756744384766
Actor Loss : 0.9166333079338074
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 129.963042
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -150.0
TimeSinceStart : 129.9630422592163
Exploration Critic Loss : 10322426.0
Exploitation Critic Loss : 0.025486571714282036
Exploration Model Loss : 61.667694091796875
Actor Loss : 0.7747915983200073
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 168.331702
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -150.0
TimeSinceStart : 168.33170223236084
Exploration Critic Loss : 6261642.0
Exploitation Critic Loss : 0.03319748118519783
Exploration Model Loss : 28.477706909179688
Actor Loss : 0.6401739716529846
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -147.276596
best mean reward -inf
running time 204.119290
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -147.27659606933594
TimeSinceStart : 204.11929035186768
Exploration Critic Loss : 9201046.0
Exploitation Critic Loss : 0.07793562114238739
Exploration Model Loss : 36.49615478515625
Actor Loss : 0.5684114694595337
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -145.648148
best mean reward -inf
running time 242.462947
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     703 / 1000At timestep:     853 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -145.6481475830078
TimeSinceStart : 242.46294736862183
Exploration Critic Loss : 14082379.0
Exploitation Critic Loss : 0.173756405711174
Exploration Model Loss : 52.65464782714844
Actor Loss : 0.5184242129325867
Eval_AverageReturn : -143.14285278320312
Eval_StdReturn : 16.79650115966797
Eval_MaxReturn : -102.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 143.28571428571428
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -142.428574
best mean reward -inf
running time 277.260226
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -142.42857360839844
TimeSinceStart : 277.2602264881134
Exploration Critic Loss : 6233990.0
Exploitation Critic Loss : 0.0003418578999117017
Exploration Model Loss : 27.081451416015625
Actor Loss : 0.4683217406272888
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -143.086960
best mean reward -inf
running time 315.209295
At timestep:     150 / 1000At timestep:     225 / 1000At timestep:     375 / 1000At timestep:     525 / 1000At timestep:     675 / 1000At timestep:     765 / 1000At timestep:     915 / 1000At timestep:     1065 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -143.0869598388672
TimeSinceStart : 315.20929503440857
Exploration Critic Loss : 8397022.0
Exploitation Critic Loss : 0.0567900575697422
Exploration Model Loss : 35.86540222167969
Actor Loss : 0.4394282102584839
Eval_AverageReturn : -132.875
Eval_StdReturn : 29.89748191833496
Eval_MaxReturn : -74.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 133.125
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -143.065796
best mean reward -inf
running time 353.796542
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -143.0657958984375
TimeSinceStart : 353.7965421676636
Exploration Critic Loss : 15320048.0
Exploitation Critic Loss : 0.17691238224506378
Exploration Model Loss : 52.315643310546875
Actor Loss : 0.4279356598854065
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -143.421692
best mean reward -inf
running time 390.620566
At timestep:     150 / 1000At timestep:     253 / 1000At timestep:     327 / 1000At timestep:     477 / 1000At timestep:     627 / 1000At timestep:     777 / 1000At timestep:     927 / 1000At timestep:     1077 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -143.42169189453125
TimeSinceStart : 390.62056589126587
Exploration Critic Loss : 7198680.0
Exploitation Critic Loss : 0.3193924129009247
Exploration Model Loss : 27.023941040039062
Actor Loss : 0.40194571018218994
Eval_AverageReturn : -134.375
Eval_StdReturn : 28.0175724029541
Eval_MaxReturn : -73.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 134.625
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -142.142853
best mean reward -inf
running time 429.164633
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     425 / 1000At timestep:     575 / 1000At timestep:     673 / 1000At timestep:     823 / 1000At timestep:     920 / 1000At timestep:     1070 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -142.14285278320312
TimeSinceStart : 429.16463327407837
Exploration Critic Loss : 8549638.0
Exploitation Critic Loss : 0.2066362202167511
Exploration Model Loss : 35.8355712890625
Actor Loss : 0.40623682737350464
Eval_AverageReturn : -133.375
Eval_StdReturn : 22.885244369506836
Eval_MaxReturn : -96.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 133.75
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -136.160004
best mean reward -136.160004
running time 466.480703
At timestep:     150 / 1000At timestep:     276 / 1000At timestep:     426 / 1000At timestep:     576 / 1000At timestep:     726 / 1000At timestep:     867 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -136.16000366210938
Train_BestReturn : -136.16000366210938
TimeSinceStart : 466.48070335388184
Exploration Critic Loss : 13868333.0
Exploitation Critic Loss : 0.03504856303334236
Exploration Model Loss : 52.30078125
Actor Loss : 0.3621797561645508
Eval_AverageReturn : -145.0
Eval_StdReturn : 8.864052772521973
Eval_MaxReturn : -125.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 145.28571428571428
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -128.750000
best mean reward -128.750000
running time 504.893577
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     443 / 1000At timestep:     593 / 1000At timestep:     743 / 1000At timestep:     893 / 1000At timestep:     1043 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -128.75
Train_BestReturn : -128.75
TimeSinceStart : 504.8935766220093
Exploration Critic Loss : 6563172.0
Exploitation Critic Loss : 0.17607297003269196
Exploration Model Loss : 27.019210815429688
Actor Loss : 0.3812160789966583
Eval_AverageReturn : -148.85714721679688
Eval_StdReturn : 2.799417018890381
Eval_MaxReturn : -142.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 149.0
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -117.389999
best mean reward -117.389999
running time 543.712213
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     591 / 1000At timestep:     741 / 1000At timestep:     837 / 1000At timestep:     987 / 1000At timestep:     1137 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -117.38999938964844
Train_BestReturn : -117.38999938964844
TimeSinceStart : 543.7122128009796
Exploration Critic Loss : 7623263.5
Exploitation Critic Loss : 0.12750783562660217
Exploration Model Loss : 35.83262634277344
Actor Loss : 0.3736315071582794
Eval_AverageReturn : -141.875
Eval_StdReturn : 18.016918182373047
Eval_MaxReturn : -95.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 142.125
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -107.120003
best mean reward -107.120003
running time 582.008850
At timestep:     89 / 1000At timestep:     239 / 1000At timestep:     389 / 1000At timestep:     539 / 1000At timestep:     689 / 1000At timestep:     839 / 1000At timestep:     989 / 1000At timestep:     1090 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -107.12000274658203
Train_BestReturn : -107.12000274658203
TimeSinceStart : 582.0088496208191
Exploration Critic Loss : 11398515.0
Exploitation Critic Loss : 0.1654852330684662
Exploration Model Loss : 52.30024719238281
Actor Loss : 0.3935372233390808
Eval_AverageReturn : -136.0
Eval_StdReturn : 24.433584213256836
Eval_MaxReturn : -88.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 136.25
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -92.739998
best mean reward -92.739998
running time 620.925707
At timestep:     60 / 1000At timestep:     182 / 1000At timestep:     265 / 1000At timestep:     370 / 1000At timestep:     477 / 1000At timestep:     552 / 1000At timestep:     626 / 1000At timestep:     776 / 1000At timestep:     878 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -92.73999786376953
Train_BestReturn : -92.73999786376953
TimeSinceStart : 620.9257068634033
Exploration Critic Loss : 7100453.0
Exploitation Critic Loss : 0.14303871989250183
Exploration Model Loss : 27.019256591796875
Actor Loss : 0.38540926575660706
Eval_AverageReturn : -102.0
Eval_StdReturn : 29.772470474243164
Eval_MaxReturn : -59.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 102.8
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -68.389999
best mean reward -68.389999
running time 661.885914
At timestep:     130 / 1000At timestep:     228 / 1000At timestep:     353 / 1000At timestep:     404 / 1000At timestep:     554 / 1000At timestep:     704 / 1000At timestep:     780 / 1000At timestep:     930 / 1000At timestep:     1043 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -68.38999938964844
Train_BestReturn : -68.38999938964844
TimeSinceStart : 661.8859136104584
Exploration Critic Loss : 9042258.0
Exploitation Critic Loss : 0.13470767438411713
Exploration Model Loss : 35.832244873046875
Actor Loss : 0.4048045575618744
Eval_AverageReturn : -115.22222137451172
Eval_StdReturn : 33.51542663574219
Eval_MaxReturn : -50.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 115.88888888888889
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -54.080002
best mean reward -54.080002
running time 702.194071
At timestep:     138 / 1000At timestep:     228 / 1000At timestep:     378 / 1000At timestep:     528 / 1000At timestep:     572 / 1000At timestep:     687 / 1000At timestep:     800 / 1000At timestep:     856 / 1000At timestep:     919 / 1000At timestep:     1049 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -54.08000183105469
Train_BestReturn : -54.08000183105469
TimeSinceStart : 702.1940712928772
Exploration Critic Loss : 9101744.0
Exploitation Critic Loss : 0.22872920334339142
Exploration Model Loss : 52.29981994628906
Actor Loss : 0.4434109330177307
Eval_AverageReturn : -104.0999984741211
Eval_StdReturn : 37.710609436035156
Eval_MaxReturn : -43.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 104.9
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -45.630001
best mean reward -45.630001
running time 742.128428
At timestep:     150 / 1000At timestep:     212 / 1000At timestep:     289 / 1000At timestep:     424 / 1000At timestep:     530 / 1000At timestep:     593 / 1000At timestep:     743 / 1000At timestep:     808 / 1000At timestep:     958 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -45.630001068115234
Train_BestReturn : -45.630001068115234
TimeSinceStart : 742.1284275054932
Exploration Critic Loss : 6554096.5
Exploitation Critic Loss : 0.2000894844532013
Exploration Model Loss : 27.01971435546875
Actor Loss : 0.4237923324108124
Eval_AverageReturn : -104.30000305175781
Eval_StdReturn : 36.62526321411133
Eval_MaxReturn : -61.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 105.0
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -40.740002
best mean reward -40.740002
running time 782.880188
At timestep:     64 / 1000At timestep:     111 / 1000At timestep:     233 / 1000At timestep:     336 / 1000At timestep:     397 / 1000At timestep:     465 / 1000At timestep:     590 / 1000At timestep:     684 / 1000At timestep:     805 / 1000At timestep:     916 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -40.7400016784668
Train_BestReturn : -40.7400016784668
TimeSinceStart : 782.8801884651184
Exploration Critic Loss : 7499382.0
Exploitation Critic Loss : 0.11349888145923615
Exploration Model Loss : 35.83271789550781
Actor Loss : 0.42752552032470703
Eval_AverageReturn : -92.18181610107422
Eval_StdReturn : 26.859291076660156
Eval_MaxReturn : -46.0
Eval_MinReturn : -124.0
Eval_AverageEpLen : 93.18181818181819
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -40.099998
best mean reward -40.099998
running time 823.473022
At timestep:     115 / 1000At timestep:     197 / 1000At timestep:     230 / 1000At timestep:     369 / 1000At timestep:     436 / 1000At timestep:     495 / 1000At timestep:     594 / 1000At timestep:     673 / 1000At timestep:     765 / 1000At timestep:     915 / 1000At timestep:     1030 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -40.099998474121094
Train_BestReturn : -40.099998474121094
TimeSinceStart : 823.4730224609375
Exploration Critic Loss : 13520347.0
Exploitation Critic Loss : 0.08921030163764954
Exploration Model Loss : 52.300811767578125
Actor Loss : 0.4524131417274475
Eval_AverageReturn : -92.7272720336914
Eval_StdReturn : 33.34145736694336
Eval_MaxReturn : -32.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 93.63636363636364
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -39.220001
best mean reward -39.220001
running time 863.409169
At timestep:     128 / 1000At timestep:     195 / 1000At timestep:     274 / 1000At timestep:     333 / 1000At timestep:     474 / 1000At timestep:     599 / 1000At timestep:     661 / 1000At timestep:     811 / 1000At timestep:     961 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -39.220001220703125
Train_BestReturn : -39.220001220703125
TimeSinceStart : 863.4091691970825
Exploration Critic Loss : 5575388.0
Exploitation Critic Loss : 0.24146774411201477
Exploration Model Loss : 27.02008056640625
Actor Loss : 0.43608415126800537
Eval_AverageReturn : -100.80000305175781
Eval_StdReturn : 38.65177917480469
Eval_MaxReturn : -54.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 101.6
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -36.700001
best mean reward -36.700001
running time 904.801038
At timestep:     35 / 1000At timestep:     81 / 1000At timestep:     148 / 1000At timestep:     298 / 1000At timestep:     436 / 1000At timestep:     516 / 1000At timestep:     583 / 1000At timestep:     620 / 1000At timestep:     713 / 1000At timestep:     761 / 1000At timestep:     826 / 1000At timestep:     872 / 1000At timestep:     923 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -36.70000076293945
Train_BestReturn : -36.70000076293945
TimeSinceStart : 904.8010377883911
Exploration Critic Loss : 6908252.0
Exploitation Critic Loss : 0.15594974160194397
Exploration Model Loss : 35.83317565917969
Actor Loss : 0.4898073375225067
Eval_AverageReturn : -72.42857360839844
Eval_StdReturn : 35.009620666503906
Eval_MaxReturn : -34.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 73.35714285714286
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -34.180000
best mean reward -34.180000
running time 946.634952
At timestep:     99 / 1000At timestep:     137 / 1000At timestep:     161 / 1000At timestep:     243 / 1000At timestep:     287 / 1000At timestep:     375 / 1000At timestep:     427 / 1000At timestep:     481 / 1000At timestep:     542 / 1000At timestep:     593 / 1000At timestep:     743 / 1000At timestep:     800 / 1000At timestep:     864 / 1000At timestep:     910 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -34.18000030517578
Train_BestReturn : -34.18000030517578
TimeSinceStart : 946.6349518299103
Exploration Critic Loss : 11098896.0
Exploitation Critic Loss : 0.1264580488204956
Exploration Model Loss : 52.300323486328125
Actor Loss : 0.5145161747932434
Eval_AverageReturn : -67.06666564941406
Eval_StdReturn : 31.657470703125
Eval_MaxReturn : -23.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 68.0
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -30.910000
best mean reward -30.910000
running time 987.787258
At timestep:     44 / 1000At timestep:     79 / 1000At timestep:     157 / 1000At timestep:     198 / 1000At timestep:     238 / 1000At timestep:     303 / 1000At timestep:     393 / 1000At timestep:     481 / 1000At timestep:     529 / 1000At timestep:     589 / 1000At timestep:     665 / 1000At timestep:     756 / 1000At timestep:     812 / 1000At timestep:     857 / 1000At timestep:     931 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -30.90999984741211
Train_BestReturn : -30.90999984741211
TimeSinceStart : 987.7872576713562
Exploration Critic Loss : 6049855.0
Exploitation Critic Loss : 0.19521892070770264
Exploration Model Loss : 27.0191650390625
Actor Loss : 0.5030633807182312
Eval_AverageReturn : -61.625
Eval_StdReturn : 18.513086318969727
Eval_MaxReturn : -34.0
Eval_MinReturn : -90.0
Eval_AverageEpLen : 62.625
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -31.520000
best mean reward -30.910000
running time 1031.032059
At timestep:     65 / 1000At timestep:     158 / 1000At timestep:     215 / 1000At timestep:     296 / 1000At timestep:     370 / 1000At timestep:     427 / 1000At timestep:     483 / 1000At timestep:     523 / 1000At timestep:     589 / 1000At timestep:     645 / 1000At timestep:     696 / 1000At timestep:     788 / 1000At timestep:     850 / 1000At timestep:     935 / 1000At timestep:     1063 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -31.520000457763672
Train_BestReturn : -30.90999984741211
TimeSinceStart : 1031.0320591926575
Exploration Critic Loss : 8058279.5
Exploitation Critic Loss : 0.11288364231586456
Exploration Model Loss : 35.83216857910156
Actor Loss : 0.5049453973770142
Eval_AverageReturn : -69.86666870117188
Eval_StdReturn : 21.32874870300293
Eval_MaxReturn : -39.0
Eval_MinReturn : -127.0
Eval_AverageEpLen : 70.86666666666666
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -30.370001
best mean reward -30.370001
running time 1074.813809
At timestep:     41 / 1000At timestep:     87 / 1000At timestep:     138 / 1000At timestep:     216 / 1000At timestep:     264 / 1000At timestep:     339 / 1000At timestep:     391 / 1000At timestep:     452 / 1000At timestep:     483 / 1000At timestep:     536 / 1000At timestep:     573 / 1000At timestep:     610 / 1000At timestep:     661 / 1000At timestep:     708 / 1000At timestep:     770 / 1000At timestep:     836 / 1000At timestep:     904 / 1000At timestep:     975 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -30.3700008392334
Train_BestReturn : -30.3700008392334
TimeSinceStart : 1074.8138091564178
Exploration Critic Loss : 11547425.0
Exploitation Critic Loss : 0.1373368501663208
Exploration Model Loss : 52.29988098144531
Actor Loss : 0.5540990233421326
Eval_AverageReturn : -53.105262756347656
Eval_StdReturn : 12.993499755859375
Eval_MaxReturn : -30.0
Eval_MinReturn : -77.0
Eval_AverageEpLen : 54.10526315789474
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -29.990000
best mean reward -29.990000
running time 1117.488636
At timestep:     49 / 1000At timestep:     95 / 1000At timestep:     161 / 1000At timestep:     260 / 1000At timestep:     313 / 1000At timestep:     397 / 1000At timestep:     465 / 1000At timestep:     533 / 1000At timestep:     596 / 1000At timestep:     658 / 1000At timestep:     720 / 1000At timestep:     781 / 1000At timestep:     825 / 1000At timestep:     859 / 1000At timestep:     906 / 1000At timestep:     981 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -29.989999771118164
Train_BestReturn : -29.989999771118164
TimeSinceStart : 1117.4886357784271
Exploration Critic Loss : 6220168.0
Exploitation Critic Loss : 0.13001969456672668
Exploration Model Loss : 27.019622802734375
Actor Loss : 0.5289609432220459
Eval_AverageReturn : -60.117645263671875
Eval_StdReturn : 15.25039005279541
Eval_MaxReturn : -33.0
Eval_MinReturn : -98.0
Eval_AverageEpLen : 61.11764705882353
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -29.820000
best mean reward -29.820000
running time 1160.087488
At timestep:     44 / 1000At timestep:     95 / 1000At timestep:     172 / 1000At timestep:     248 / 1000At timestep:     325 / 1000At timestep:     403 / 1000At timestep:     483 / 1000At timestep:     511 / 1000At timestep:     575 / 1000At timestep:     624 / 1000At timestep:     672 / 1000At timestep:     771 / 1000At timestep:     906 / 1000At timestep:     992 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -29.81999969482422
Train_BestReturn : -29.81999969482422
TimeSinceStart : 1160.0874881744385
Exploration Critic Loss : 7891954.0
Exploitation Critic Loss : 0.20291538536548615
Exploration Model Loss : 35.83265686035156
Actor Loss : 0.5923607349395752
Eval_AverageReturn : -67.13333129882812
Eval_StdReturn : 26.993499755859375
Eval_MaxReturn : -27.0
Eval_MinReturn : -134.0
Eval_AverageEpLen : 68.13333333333334
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -28.820000
best mean reward -28.820000
running time 1203.552910
At timestep:     46 / 1000At timestep:     78 / 1000At timestep:     147 / 1000At timestep:     179 / 1000At timestep:     256 / 1000At timestep:     336 / 1000At timestep:     384 / 1000At timestep:     416 / 1000At timestep:     492 / 1000At timestep:     546 / 1000At timestep:     601 / 1000At timestep:     632 / 1000At timestep:     678 / 1000At timestep:     743 / 1000At timestep:     791 / 1000At timestep:     853 / 1000At timestep:     892 / 1000At timestep:     930 / 1000At timestep:     972 / 1000At timestep:     1046 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -28.81999969482422
Train_BestReturn : -28.81999969482422
TimeSinceStart : 1203.5529098510742
Exploration Critic Loss : 9841283.0
Exploitation Critic Loss : 0.21434728801250458
Exploration Model Loss : 52.300872802734375
Actor Loss : 0.4974110722541809
Eval_AverageReturn : -51.29999923706055
Eval_StdReturn : 16.199073791503906
Eval_MaxReturn : -30.0
Eval_MinReturn : -79.0
Eval_AverageEpLen : 52.3
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -27.910000
best mean reward -27.910000
running time 1245.988892
At timestep:     31 / 1000At timestep:     117 / 1000At timestep:     216 / 1000At timestep:     282 / 1000At timestep:     341 / 1000At timestep:     400 / 1000At timestep:     455 / 1000At timestep:     526 / 1000At timestep:     589 / 1000At timestep:     632 / 1000At timestep:     687 / 1000At timestep:     778 / 1000At timestep:     826 / 1000At timestep:     886 / 1000At timestep:     944 / 1000At timestep:     993 / 1000At timestep:     1040 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -27.90999984741211
Train_BestReturn : -27.90999984741211
TimeSinceStart : 1245.9888916015625
Exploration Critic Loss : 6874935.0
Exploitation Critic Loss : 0.11173935234546661
Exploration Model Loss : 27.020095825195312
Actor Loss : 0.5326179265975952
Eval_AverageReturn : -60.17647171020508
Eval_StdReturn : 17.01291847229004
Eval_MaxReturn : -30.0
Eval_MinReturn : -98.0
Eval_AverageEpLen : 61.1764705882353
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -27.790001
best mean reward -27.790001
running time 1288.767497
At timestep:     78 / 1000At timestep:     111 / 1000At timestep:     149 / 1000At timestep:     181 / 1000At timestep:     229 / 1000At timestep:     282 / 1000At timestep:     353 / 1000At timestep:     421 / 1000At timestep:     468 / 1000At timestep:     499 / 1000At timestep:     545 / 1000At timestep:     578 / 1000At timestep:     636 / 1000At timestep:     697 / 1000At timestep:     766 / 1000At timestep:     795 / 1000At timestep:     853 / 1000At timestep:     891 / 1000At timestep:     935 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -27.790000915527344
Train_BestReturn : -27.790000915527344
TimeSinceStart : 1288.7674968242645
Exploration Critic Loss : 8316357.0
Exploitation Critic Loss : 0.15081775188446045
Exploration Model Loss : 35.83311462402344
Actor Loss : 0.5648505091667175
Eval_AverageReturn : -50.099998474121094
Eval_StdReturn : 16.688020706176758
Eval_MaxReturn : -28.0
Eval_MinReturn : -86.0
Eval_AverageEpLen : 51.1
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -27.230000
best mean reward -27.230000
running time 1332.847405
At timestep:     27 / 1000At timestep:     131 / 1000At timestep:     199 / 1000At timestep:     244 / 1000At timestep:     285 / 1000At timestep:     328 / 1000At timestep:     364 / 1000At timestep:     437 / 1000At timestep:     496 / 1000At timestep:     536 / 1000At timestep:     575 / 1000At timestep:     631 / 1000At timestep:     685 / 1000At timestep:     734 / 1000At timestep:     776 / 1000At timestep:     838 / 1000At timestep:     885 / 1000At timestep:     951 / 1000At timestep:     988 / 1000At timestep:     1034 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -27.229999542236328
Train_BestReturn : -27.229999542236328
TimeSinceStart : 1332.8474054336548
Exploration Critic Loss : 8073457.0
Exploitation Critic Loss : 0.1167576014995575
Exploration Model Loss : 52.30043029785156
Actor Loss : 0.5342979431152344
Eval_AverageReturn : -50.70000076293945
Eval_StdReturn : 16.739473342895508
Eval_MaxReturn : -26.0
Eval_MinReturn : -103.0
Eval_AverageEpLen : 51.7
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -27.379999
best mean reward -27.230000
running time 1374.809573
At timestep:     36 / 1000At timestep:     87 / 1000At timestep:     129 / 1000At timestep:     150 / 1000At timestep:     186 / 1000At timestep:     245 / 1000At timestep:     283 / 1000At timestep:     327 / 1000At timestep:     399 / 1000At timestep:     445 / 1000At timestep:     490 / 1000At timestep:     562 / 1000At timestep:     607 / 1000At timestep:     680 / 1000At timestep:     716 / 1000At timestep:     769 / 1000At timestep:     840 / 1000At timestep:     871 / 1000At timestep:     978 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -27.3799991607666
Train_BestReturn : -27.229999542236328
TimeSinceStart : 1374.8095729351044
Exploration Critic Loss : 6753662.5
Exploitation Critic Loss : 0.09166411310434341
Exploration Model Loss : 27.019454956054688
Actor Loss : 0.5503619909286499
Eval_AverageReturn : -49.79999923706055
Eval_StdReturn : 19.37679100036621
Eval_MaxReturn : -20.0
Eval_MinReturn : -106.0
Eval_AverageEpLen : 50.8
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -26.590000
best mean reward -26.590000
running time 1418.479777
At timestep:     40 / 1000At timestep:     92 / 1000At timestep:     171 / 1000At timestep:     230 / 1000At timestep:     273 / 1000At timestep:     319 / 1000At timestep:     377 / 1000At timestep:     432 / 1000At timestep:     481 / 1000At timestep:     529 / 1000At timestep:     588 / 1000At timestep:     646 / 1000At timestep:     694 / 1000At timestep:     734 / 1000At timestep:     774 / 1000At timestep:     817 / 1000At timestep:     863 / 1000At timestep:     900 / 1000At timestep:     930 / 1000At timestep:     981 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -26.59000015258789
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1418.4797773361206
Exploration Critic Loss : 10079756.0
Exploitation Critic Loss : 0.1743772327899933
Exploration Model Loss : 35.83323669433594
Actor Loss : 0.5682011842727661
Eval_AverageReturn : -47.57143020629883
Eval_StdReturn : 10.344408988952637
Eval_MaxReturn : -29.0
Eval_MinReturn : -78.0
Eval_AverageEpLen : 48.57142857142857
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -26.740000
best mean reward -26.590000
running time 1463.570738
At timestep:     141 / 1000At timestep:     170 / 1000At timestep:     213 / 1000At timestep:     300 / 1000At timestep:     378 / 1000At timestep:     429 / 1000At timestep:     463 / 1000At timestep:     510 / 1000At timestep:     609 / 1000At timestep:     646 / 1000At timestep:     714 / 1000At timestep:     763 / 1000At timestep:     814 / 1000At timestep:     854 / 1000At timestep:     923 / 1000At timestep:     973 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -26.739999771118164
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1463.5707378387451
Exploration Critic Loss : 11320654.0
Exploitation Critic Loss : 0.12017738819122314
Exploration Model Loss : 52.30029296875
Actor Loss : 0.571810245513916
Eval_AverageReturn : -58.588233947753906
Eval_StdReturn : 27.742637634277344
Eval_MaxReturn : -28.0
Eval_MinReturn : -140.0
Eval_AverageEpLen : 59.588235294117645
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -26.660000
best mean reward -26.590000
running time 1504.973933
At timestep:     40 / 1000At timestep:     91 / 1000At timestep:     169 / 1000At timestep:     221 / 1000At timestep:     261 / 1000At timestep:     314 / 1000At timestep:     348 / 1000At timestep:     411 / 1000At timestep:     457 / 1000At timestep:     512 / 1000At timestep:     562 / 1000At timestep:     595 / 1000At timestep:     672 / 1000At timestep:     743 / 1000At timestep:     812 / 1000At timestep:     859 / 1000At timestep:     904 / 1000At timestep:     981 / 1000At timestep:     1079 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -26.65999984741211
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1504.973932504654
Exploration Critic Loss : 8472112.0
Exploitation Critic Loss : 0.1209186315536499
Exploration Model Loss : 27.01922607421875
Actor Loss : 0.5387277007102966
Eval_AverageReturn : -55.78947448730469
Eval_StdReturn : 16.975460052490234
Eval_MaxReturn : -32.0
Eval_MinReturn : -97.0
Eval_AverageEpLen : 56.78947368421053
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -26.850000
best mean reward -26.590000
running time 1547.656883
At timestep:     46 / 1000At timestep:     78 / 1000At timestep:     160 / 1000At timestep:     235 / 1000At timestep:     287 / 1000At timestep:     335 / 1000At timestep:     404 / 1000At timestep:     434 / 1000At timestep:     472 / 1000At timestep:     512 / 1000At timestep:     574 / 1000At timestep:     655 / 1000At timestep:     725 / 1000At timestep:     774 / 1000At timestep:     808 / 1000At timestep:     839 / 1000At timestep:     870 / 1000At timestep:     924 / 1000At timestep:     956 / 1000At timestep:     989 / 1000At timestep:     1034 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -26.850000381469727
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1547.6568830013275
Exploration Critic Loss : 12205350.0
Exploitation Critic Loss : 0.14884164929389954
Exploration Model Loss : 35.832244873046875
Actor Loss : 0.5164414644241333
Eval_AverageReturn : -48.238094329833984
Eval_StdReturn : 17.08216667175293
Eval_MaxReturn : -29.0
Eval_MinReturn : -81.0
Eval_AverageEpLen : 49.23809523809524
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -27.309999
best mean reward -26.590000
running time 1591.818557
At timestep:     45 / 1000At timestep:     78 / 1000At timestep:     141 / 1000At timestep:     228 / 1000At timestep:     260 / 1000At timestep:     308 / 1000At timestep:     353 / 1000At timestep:     389 / 1000At timestep:     437 / 1000At timestep:     473 / 1000At timestep:     532 / 1000At timestep:     561 / 1000At timestep:     663 / 1000At timestep:     712 / 1000At timestep:     755 / 1000At timestep:     809 / 1000At timestep:     848 / 1000At timestep:     890 / 1000At timestep:     940 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -27.309999465942383
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1591.818556547165
Exploration Critic Loss : 14103976.0
Exploitation Critic Loss : 0.09856988489627838
Exploration Model Loss : 52.299835205078125
Actor Loss : 0.5803588628768921
Eval_AverageReturn : -49.54999923706055
Eval_StdReturn : 18.045705795288086
Eval_MaxReturn : -28.0
Eval_MinReturn : -101.0
Eval_AverageEpLen : 50.55
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -27.850000
best mean reward -26.590000
running time 1633.986249
At timestep:     82 / 1000At timestep:     137 / 1000At timestep:     168 / 1000At timestep:     207 / 1000At timestep:     254 / 1000At timestep:     299 / 1000At timestep:     355 / 1000At timestep:     387 / 1000At timestep:     445 / 1000At timestep:     490 / 1000At timestep:     531 / 1000At timestep:     575 / 1000At timestep:     636 / 1000At timestep:     670 / 1000At timestep:     709 / 1000At timestep:     745 / 1000At timestep:     835 / 1000At timestep:     867 / 1000At timestep:     903 / 1000At timestep:     950 / 1000At timestep:     990 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -27.850000381469727
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1633.9862487316132
Exploration Critic Loss : 12016085.0
Exploitation Critic Loss : 0.15293315052986145
Exploration Model Loss : 27.01971435546875
Actor Loss : 0.5272374153137207
Eval_AverageReturn : -46.227272033691406
Eval_StdReturn : 14.902506828308105
Eval_MaxReturn : -30.0
Eval_MinReturn : -89.0
Eval_AverageEpLen : 47.22727272727273
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -26.940001
best mean reward -26.590000
running time 1677.283566
At timestep:     45 / 1000At timestep:     89 / 1000At timestep:     134 / 1000At timestep:     178 / 1000At timestep:     254 / 1000At timestep:     299 / 1000At timestep:     356 / 1000At timestep:     401 / 1000At timestep:     445 / 1000At timestep:     495 / 1000At timestep:     528 / 1000At timestep:     604 / 1000At timestep:     691 / 1000At timestep:     726 / 1000At timestep:     777 / 1000At timestep:     817 / 1000At timestep:     865 / 1000At timestep:     911 / 1000At timestep:     970 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -26.940000534057617
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1677.2835655212402
Exploration Critic Loss : 12075755.0
Exploitation Critic Loss : 0.09974552690982819
Exploration Model Loss : 35.83268737792969
Actor Loss : 0.5657142400741577
Eval_AverageReturn : -50.150001525878906
Eval_StdReturn : 13.558300971984863
Eval_MaxReturn : -32.0
Eval_MinReturn : -86.0
Eval_AverageEpLen : 51.15
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -27.400000
best mean reward -26.590000
running time 1721.703310
At timestep:     45 / 1000At timestep:     101 / 1000At timestep:     147 / 1000At timestep:     188 / 1000At timestep:     238 / 1000At timestep:     301 / 1000At timestep:     347 / 1000At timestep:     378 / 1000At timestep:     405 / 1000At timestep:     459 / 1000At timestep:     478 / 1000At timestep:     515 / 1000At timestep:     540 / 1000At timestep:     580 / 1000At timestep:     625 / 1000At timestep:     665 / 1000At timestep:     718 / 1000At timestep:     761 / 1000At timestep:     793 / 1000At timestep:     841 / 1000At timestep:     886 / 1000At timestep:     908 / 1000At timestep:     946 / 1000At timestep:     989 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -27.399999618530273
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1721.7033097743988
Exploration Critic Loss : 12863624.0
Exploitation Critic Loss : 0.09807994216680527
Exploration Model Loss : 52.30082702636719
Actor Loss : 0.5349379777908325
Eval_AverageReturn : -39.599998474121094
Eval_StdReturn : 10.925200462341309
Eval_MaxReturn : -18.0
Eval_MinReturn : -62.0
Eval_AverageEpLen : 40.6
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -26.920000
best mean reward -26.590000
running time 1765.352337
At timestep:     44 / 1000At timestep:     94 / 1000At timestep:     126 / 1000At timestep:     173 / 1000At timestep:     206 / 1000At timestep:     244 / 1000At timestep:     280 / 1000At timestep:     370 / 1000At timestep:     429 / 1000At timestep:     482 / 1000At timestep:     534 / 1000At timestep:     559 / 1000At timestep:     589 / 1000At timestep:     632 / 1000At timestep:     667 / 1000At timestep:     735 / 1000At timestep:     778 / 1000At timestep:     846 / 1000At timestep:     910 / 1000At timestep:     949 / 1000At timestep:     989 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -26.920000076293945
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1765.3523371219635
Exploration Critic Loss : 15367422.0
Exploitation Critic Loss : 0.15786755084991455
Exploration Model Loss : 27.020233154296875
Actor Loss : 0.511734664440155
Eval_AverageReturn : -45.8636360168457
Eval_StdReturn : 15.003925323486328
Eval_MaxReturn : -24.0
Eval_MinReturn : -89.0
Eval_AverageEpLen : 46.86363636363637
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -26.850000
best mean reward -26.590000
running time 1808.252746
At timestep:     41 / 1000At timestep:     93 / 1000At timestep:     141 / 1000At timestep:     190 / 1000At timestep:     239 / 1000At timestep:     294 / 1000At timestep:     349 / 1000At timestep:     380 / 1000At timestep:     423 / 1000At timestep:     450 / 1000At timestep:     512 / 1000At timestep:     571 / 1000At timestep:     608 / 1000At timestep:     635 / 1000At timestep:     673 / 1000At timestep:     727 / 1000At timestep:     770 / 1000At timestep:     825 / 1000At timestep:     886 / 1000At timestep:     920 / 1000At timestep:     950 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -26.850000381469727
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1808.252746105194
Exploration Critic Loss : 14933998.0
Exploitation Critic Loss : 0.08994239568710327
Exploration Model Loss : 35.833343505859375
Actor Loss : 0.5089515447616577
Eval_AverageReturn : -44.8636360168457
Eval_StdReturn : 11.116308212280273
Eval_MaxReturn : -26.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 45.86363636363637
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -27.139999
best mean reward -26.590000
running time 1852.348382
At timestep:     59 / 1000At timestep:     99 / 1000At timestep:     119 / 1000At timestep:     146 / 1000At timestep:     186 / 1000At timestep:     220 / 1000At timestep:     261 / 1000At timestep:     307 / 1000At timestep:     342 / 1000At timestep:     423 / 1000At timestep:     466 / 1000At timestep:     507 / 1000At timestep:     544 / 1000At timestep:     627 / 1000At timestep:     649 / 1000At timestep:     691 / 1000At timestep:     725 / 1000At timestep:     767 / 1000At timestep:     805 / 1000At timestep:     836 / 1000At timestep:     869 / 1000At timestep:     909 / 1000At timestep:     964 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -27.139999389648438
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1852.3483822345734
Exploration Critic Loss : 13823134.0
Exploitation Critic Loss : 0.1211753860116005
Exploration Model Loss : 52.3001708984375
Actor Loss : 0.5857993364334106
Eval_AverageReturn : -41.41666793823242
Eval_StdReturn : 14.944108009338379
Eval_MaxReturn : -19.0
Eval_MinReturn : -82.0
Eval_AverageEpLen : 42.416666666666664
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -27.480000
best mean reward -26.590000
running time 1895.288423
At timestep:     27 / 1000At timestep:     77 / 1000At timestep:     94 / 1000At timestep:     129 / 1000At timestep:     205 / 1000At timestep:     278 / 1000At timestep:     326 / 1000At timestep:     425 / 1000At timestep:     469 / 1000At timestep:     515 / 1000At timestep:     562 / 1000At timestep:     599 / 1000At timestep:     649 / 1000At timestep:     708 / 1000At timestep:     759 / 1000At timestep:     816 / 1000At timestep:     861 / 1000At timestep:     909 / 1000At timestep:     943 / 1000At timestep:     970 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -27.479999542236328
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1895.2884232997894
Exploration Critic Loss : 13991422.0
Exploitation Critic Loss : 0.11508210748434067
Exploration Model Loss : 27.019317626953125
Actor Loss : 0.5564405918121338
Eval_AverageReturn : -46.761905670166016
Eval_StdReturn : 18.010326385498047
Eval_MaxReturn : -16.0
Eval_MinReturn : -98.0
Eval_AverageEpLen : 47.76190476190476
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -28.139999
best mean reward -26.590000
running time 1936.765961
At timestep:     32 / 1000At timestep:     77 / 1000At timestep:     148 / 1000At timestep:     170 / 1000At timestep:     226 / 1000At timestep:     284 / 1000At timestep:     308 / 1000At timestep:     352 / 1000At timestep:     396 / 1000At timestep:     437 / 1000At timestep:     464 / 1000At timestep:     496 / 1000At timestep:     532 / 1000At timestep:     568 / 1000At timestep:     625 / 1000At timestep:     678 / 1000At timestep:     731 / 1000At timestep:     764 / 1000At timestep:     806 / 1000At timestep:     850 / 1000At timestep:     883 / 1000At timestep:     935 / 1000At timestep:     973 / 1000At timestep:     1020 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_medium_unsupervised_lam2_PointmassMedium-v0_22-11-2022_22-30-02/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -28.139999389648438
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1936.765960931778
Exploration Critic Loss : 14989935.0
Exploitation Critic Loss : 0.11561616510152817
Exploration Model Loss : 35.83235168457031
Actor Loss : 0.5024091005325317
Eval_AverageReturn : -41.5
Eval_StdReturn : 11.726038932800293
Eval_MaxReturn : -21.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 42.5
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_unsupervised_lam10_PointmassMedium-v0_22-11-2022_23-03-08 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_unsupervised_lam10_PointmassMedium-v0_22-11-2022_23-03-08
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002466
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.002465963363647461
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 8.436761
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     666 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 8.436760902404785
Eval_AverageReturn : -139.375
Eval_StdReturn : 28.111106872558594
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.5
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -146.846161
best mean reward -inf
running time 16.986532
At timestep:     54 / 1000At timestep:     204 / 1000At timestep:     354 / 1000At timestep:     504 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -146.84616088867188
TimeSinceStart : 16.98653221130371
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -147.949997
best mean reward -inf
running time 56.305135
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -147.9499969482422
TimeSinceStart : 56.30513548851013
Exploration Critic Loss : 822037.5
Exploitation Critic Loss : 0.008650545962154865
Exploration Model Loss : 48.88676452636719
Actor Loss : 1.169165849685669
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -148.423080
best mean reward -inf
running time 91.804493
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -148.42308044433594
TimeSinceStart : 91.80449318885803
Exploration Critic Loss : 8694720.0
Exploitation Critic Loss : 0.016911257058382034
Exploration Model Loss : 39.46409225463867
Actor Loss : 0.9397300481796265
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -148.090912
best mean reward -inf
running time 129.449221
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -148.09091186523438
TimeSinceStart : 129.4492208957672
Exploration Critic Loss : 9892964.0
Exploitation Critic Loss : 0.026371795684099197
Exploration Model Loss : 53.92917251586914
Actor Loss : 0.7881793975830078
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -139.595245
best mean reward -inf
running time 167.123469
At timestep:     98 / 1000At timestep:     248 / 1000At timestep:     398 / 1000At timestep:     548 / 1000At timestep:     698 / 1000At timestep:     839 / 1000At timestep:     944 / 1000At timestep:     1075 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -139.59524536132812
TimeSinceStart : 167.12346935272217
Exploration Critic Loss : 9357956.0
Exploitation Critic Loss : 0.007234943564981222
Exploration Model Loss : 38.58817672729492
Actor Loss : 0.6426335573196411
Eval_AverageReturn : -133.875
Eval_StdReturn : 20.447723388671875
Eval_MaxReturn : -97.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 134.375
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -130.339630
best mean reward -inf
running time 203.387212
At timestep:     82 / 1000At timestep:     232 / 1000At timestep:     382 / 1000At timestep:     532 / 1000At timestep:     682 / 1000At timestep:     832 / 1000At timestep:     913 / 1000At timestep:     1063 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -130.33963012695312
TimeSinceStart : 203.387211561203
Exploration Critic Loss : 9457411.0
Exploitation Critic Loss : 0.056383807212114334
Exploration Model Loss : 26.168670654296875
Actor Loss : 0.552720308303833
Eval_AverageReturn : -132.625
Eval_StdReturn : 30.095420837402344
Eval_MaxReturn : -80.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 132.875
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -126.619049
best mean reward -inf
running time 241.781166
At timestep:     121 / 1000At timestep:     181 / 1000At timestep:     331 / 1000At timestep:     406 / 1000At timestep:     548 / 1000At timestep:     632 / 1000At timestep:     763 / 1000At timestep:     894 / 1000At timestep:     999 / 1000At timestep:     1149 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -126.61904907226562
TimeSinceStart : 241.78116607666016
Exploration Critic Loss : 14227858.0
Exploitation Critic Loss : 0.06582769751548767
Exploration Model Loss : 23.798416137695312
Actor Loss : 0.5415157079696655
Eval_AverageReturn : -114.0999984741211
Eval_StdReturn : 30.911001205444336
Eval_MaxReturn : -59.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 114.9
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -126.056335
best mean reward -inf
running time 279.333248
At timestep:     85 / 1000At timestep:     166 / 1000At timestep:     276 / 1000At timestep:     363 / 1000At timestep:     513 / 1000At timestep:     598 / 1000At timestep:     705 / 1000At timestep:     855 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -126.05633544921875
TimeSinceStart : 279.3332483768463
Exploration Critic Loss : 15490265.0
Exploitation Critic Loss : 0.08837500214576721
Exploration Model Loss : 23.09600830078125
Actor Loss : 0.4719940721988678
Eval_AverageReturn : -111.0
Eval_StdReturn : 29.128070831298828
Eval_MaxReturn : -80.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 111.66666666666667
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -118.392860
best mean reward -inf
running time 320.559078
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     536 / 1000At timestep:     686 / 1000At timestep:     833 / 1000At timestep:     884 / 1000At timestep:     1034 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -118.39286041259766
TimeSinceStart : 320.55907797813416
Exploration Critic Loss : 20231566.0
Exploitation Critic Loss : 0.09112642705440521
Exploration Model Loss : 22.854736328125
Actor Loss : 0.4741998016834259
Eval_AverageReturn : -128.75
Eval_StdReturn : 36.45116424560547
Eval_MaxReturn : -50.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 129.25
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -107.540001
best mean reward -107.540001
running time 360.064966
At timestep:     120 / 1000At timestep:     270 / 1000At timestep:     420 / 1000At timestep:     485 / 1000At timestep:     635 / 1000At timestep:     785 / 1000At timestep:     921 / 1000At timestep:     1071 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -107.54000091552734
Train_BestReturn : -107.54000091552734
TimeSinceStart : 360.0649664402008
Exploration Critic Loss : 24672520.0
Exploitation Critic Loss : 0.2015918791294098
Exploration Model Loss : 22.76898193359375
Actor Loss : 0.41088756918907166
Eval_AverageReturn : -133.5
Eval_StdReturn : 28.293107986450195
Eval_MaxReturn : -64.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 133.875
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -86.470001
best mean reward -86.470001
running time 406.604979
At timestep:     98 / 1000At timestep:     142 / 1000At timestep:     189 / 1000At timestep:     339 / 1000At timestep:     456 / 1000At timestep:     606 / 1000At timestep:     756 / 1000At timestep:     814 / 1000At timestep:     930 / 1000At timestep:     1080 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -86.47000122070312
Train_BestReturn : -86.47000122070312
TimeSinceStart : 406.6049792766571
Exploration Critic Loss : 22532964.0
Exploitation Critic Loss : 0.12635895609855652
Exploration Model Loss : 22.737899780273438
Actor Loss : 0.38861918449401855
Eval_AverageReturn : -107.4000015258789
Eval_StdReturn : 42.37499237060547
Eval_MaxReturn : -43.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 108.0
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -67.889999
best mean reward -67.889999
running time 450.453105
At timestep:     71 / 1000At timestep:     221 / 1000At timestep:     293 / 1000At timestep:     352 / 1000At timestep:     502 / 1000At timestep:     652 / 1000At timestep:     704 / 1000At timestep:     755 / 1000At timestep:     905 / 1000At timestep:     979 / 1000At timestep:     1037 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -67.88999938964844
Train_BestReturn : -67.88999938964844
TimeSinceStart : 450.4531047344208
Exploration Critic Loss : 26578650.0
Exploitation Critic Loss : 0.12359939515590668
Exploration Model Loss : 22.727279663085938
Actor Loss : 0.4167563319206238
Eval_AverageReturn : -93.63636016845703
Eval_StdReturn : 43.21042251586914
Eval_MaxReturn : -50.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 94.27272727272727
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -52.230000
best mean reward -52.230000
running time 493.230830
At timestep:     71 / 1000At timestep:     171 / 1000At timestep:     225 / 1000At timestep:     331 / 1000At timestep:     469 / 1000At timestep:     561 / 1000At timestep:     649 / 1000At timestep:     799 / 1000At timestep:     846 / 1000At timestep:     926 / 1000At timestep:     978 / 1000At timestep:     1123 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -52.22999954223633
Train_BestReturn : -52.22999954223633
TimeSinceStart : 493.23082995414734
Exploration Critic Loss : 16061727.0
Exploitation Critic Loss : 0.14899727702140808
Exploration Model Loss : 22.7230224609375
Actor Loss : 0.3904033899307251
Eval_AverageReturn : -92.66666412353516
Eval_StdReturn : 34.52374267578125
Eval_MaxReturn : -46.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 93.58333333333333
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -41.049999
best mean reward -41.049999
running time 535.575051
At timestep:     150 / 1000At timestep:     189 / 1000At timestep:     284 / 1000At timestep:     411 / 1000At timestep:     499 / 1000At timestep:     619 / 1000At timestep:     769 / 1000At timestep:     854 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -41.04999923706055
Train_BestReturn : -41.04999923706055
TimeSinceStart : 535.5750513076782
Exploration Critic Loss : 12604044.0
Exploitation Critic Loss : 0.12899917364120483
Exploration Model Loss : 22.721725463867188
Actor Loss : 0.4129147529602051
Eval_AverageReturn : -110.88888549804688
Eval_StdReturn : 36.170719146728516
Eval_MaxReturn : -38.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 111.55555555555556
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -37.259998
best mean reward -37.259998
running time 576.801951
At timestep:     136 / 1000At timestep:     163 / 1000At timestep:     266 / 1000At timestep:     305 / 1000At timestep:     357 / 1000At timestep:     402 / 1000At timestep:     459 / 1000At timestep:     609 / 1000At timestep:     697 / 1000At timestep:     734 / 1000At timestep:     806 / 1000At timestep:     906 / 1000At timestep:     965 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -37.2599983215332
Train_BestReturn : -37.2599983215332
TimeSinceStart : 576.8019506931305
Exploration Critic Loss : 17597236.0
Exploitation Critic Loss : 0.11820471286773682
Exploration Model Loss : 22.721038818359375
Actor Loss : 0.4348152279853821
Eval_AverageReturn : -72.14286041259766
Eval_StdReturn : 36.23112106323242
Eval_MaxReturn : -26.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 73.07142857142857
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -34.209999
best mean reward -34.209999
running time 622.626452
At timestep:     46 / 1000At timestep:     132 / 1000At timestep:     172 / 1000At timestep:     322 / 1000At timestep:     370 / 1000At timestep:     432 / 1000At timestep:     497 / 1000At timestep:     558 / 1000At timestep:     588 / 1000At timestep:     644 / 1000At timestep:     734 / 1000At timestep:     839 / 1000At timestep:     891 / 1000At timestep:     955 / 1000At timestep:     1045 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -34.209999084472656
Train_BestReturn : -34.209999084472656
TimeSinceStart : 622.6264522075653
Exploration Critic Loss : 14906079.0
Exploitation Critic Loss : 0.27234503626823425
Exploration Model Loss : 22.721115112304688
Actor Loss : 0.4390745460987091
Eval_AverageReturn : -68.73332977294922
Eval_StdReturn : 29.582578659057617
Eval_MaxReturn : -29.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 69.66666666666667
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -33.980000
best mean reward -33.980000
running time 671.901811
At timestep:     48 / 1000At timestep:     100 / 1000At timestep:     173 / 1000At timestep:     227 / 1000At timestep:     269 / 1000At timestep:     345 / 1000At timestep:     399 / 1000At timestep:     449 / 1000At timestep:     563 / 1000At timestep:     604 / 1000At timestep:     671 / 1000At timestep:     729 / 1000At timestep:     760 / 1000At timestep:     811 / 1000At timestep:     857 / 1000At timestep:     902 / 1000At timestep:     980 / 1000At timestep:     1033 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -33.97999954223633
Train_BestReturn : -33.97999954223633
TimeSinceStart : 671.9018106460571
Exploration Critic Loss : 18346024.0
Exploitation Critic Loss : 0.3017800748348236
Exploration Model Loss : 22.721237182617188
Actor Loss : 0.4076329469680786
Eval_AverageReturn : -56.38888931274414
Eval_StdReturn : 18.33678436279297
Eval_MaxReturn : -30.0
Eval_MinReturn : -113.0
Eval_AverageEpLen : 57.388888888888886
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -33.060001
best mean reward -33.060001
running time 723.610748
At timestep:     34 / 1000At timestep:     64 / 1000At timestep:     115 / 1000At timestep:     177 / 1000At timestep:     204 / 1000At timestep:     270 / 1000At timestep:     339 / 1000At timestep:     380 / 1000At timestep:     424 / 1000At timestep:     510 / 1000At timestep:     549 / 1000At timestep:     612 / 1000At timestep:     693 / 1000At timestep:     734 / 1000At timestep:     779 / 1000At timestep:     814 / 1000At timestep:     871 / 1000At timestep:     961 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -33.060001373291016
Train_BestReturn : -33.060001373291016
TimeSinceStart : 723.6107475757599
Exploration Critic Loss : 17576196.0
Exploitation Critic Loss : 0.14753997325897217
Exploration Model Loss : 22.721359252929688
Actor Loss : 0.49060875177383423
Eval_AverageReturn : -52.78947448730469
Eval_StdReturn : 18.409320831298828
Eval_MaxReturn : -26.0
Eval_MinReturn : -89.0
Eval_AverageEpLen : 53.78947368421053
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -32.189999
best mean reward -32.189999
running time 773.405146
At timestep:     82 / 1000At timestep:     232 / 1000At timestep:     271 / 1000At timestep:     357 / 1000At timestep:     459 / 1000At timestep:     533 / 1000At timestep:     683 / 1000At timestep:     747 / 1000At timestep:     819 / 1000At timestep:     855 / 1000At timestep:     930 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -32.189998626708984
Train_BestReturn : -32.189998626708984
TimeSinceStart : 773.4051461219788
Exploration Critic Loss : 19144912.0
Exploitation Critic Loss : 0.12792134284973145
Exploration Model Loss : 22.72149658203125
Actor Loss : 0.4515502154827118
Eval_AverageReturn : -85.0
Eval_StdReturn : 34.72751235961914
Eval_MaxReturn : -35.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 85.91666666666667
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -30.020000
best mean reward -30.020000
running time 823.542847
At timestep:     56 / 1000At timestep:     77 / 1000At timestep:     139 / 1000At timestep:     227 / 1000At timestep:     311 / 1000At timestep:     340 / 1000At timestep:     389 / 1000At timestep:     484 / 1000At timestep:     528 / 1000At timestep:     616 / 1000At timestep:     665 / 1000At timestep:     736 / 1000At timestep:     823 / 1000At timestep:     862 / 1000At timestep:     905 / 1000At timestep:     965 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -30.020000457763672
Train_BestReturn : -30.020000457763672
TimeSinceStart : 823.5428471565247
Exploration Critic Loss : 19264904.0
Exploitation Critic Loss : 0.10909484326839447
Exploration Model Loss : 22.721633911132812
Actor Loss : 0.5221875309944153
Eval_AverageReturn : -59.17647171020508
Eval_StdReturn : 21.628507614135742
Eval_MaxReturn : -20.0
Eval_MinReturn : -94.0
Eval_AverageEpLen : 60.1764705882353
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -29.290001
best mean reward -29.290001
running time 873.880676
At timestep:     79 / 1000At timestep:     112 / 1000At timestep:     138 / 1000At timestep:     175 / 1000At timestep:     251 / 1000At timestep:     281 / 1000At timestep:     314 / 1000At timestep:     356 / 1000At timestep:     382 / 1000At timestep:     473 / 1000At timestep:     515 / 1000At timestep:     545 / 1000At timestep:     584 / 1000At timestep:     648 / 1000At timestep:     724 / 1000At timestep:     756 / 1000At timestep:     827 / 1000At timestep:     876 / 1000At timestep:     935 / 1000At timestep:     973 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -29.290000915527344
Train_BestReturn : -29.290000915527344
TimeSinceStart : 873.8806757926941
Exploration Critic Loss : 16528342.0
Exploitation Critic Loss : 0.1481640636920929
Exploration Model Loss : 22.721786499023438
Actor Loss : 0.48357564210891724
Eval_AverageReturn : -47.42856979370117
Eval_StdReturn : 19.509634017944336
Eval_MaxReturn : -25.0
Eval_MinReturn : -90.0
Eval_AverageEpLen : 48.42857142857143
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -27.930000
best mean reward -27.930000
running time 923.918408
At timestep:     58 / 1000At timestep:     154 / 1000At timestep:     190 / 1000At timestep:     232 / 1000At timestep:     281 / 1000At timestep:     420 / 1000At timestep:     477 / 1000At timestep:     525 / 1000At timestep:     587 / 1000At timestep:     656 / 1000At timestep:     687 / 1000At timestep:     728 / 1000At timestep:     789 / 1000At timestep:     833 / 1000At timestep:     872 / 1000At timestep:     931 / 1000At timestep:     971 / 1000At timestep:     1092 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -27.93000030517578
Train_BestReturn : -27.93000030517578
TimeSinceStart : 923.9184083938599
Exploration Critic Loss : 16651298.0
Exploitation Critic Loss : 0.12651535868644714
Exploration Model Loss : 22.721908569335938
Actor Loss : 0.4596337676048279
Eval_AverageReturn : -59.66666793823242
Eval_StdReturn : 28.687976837158203
Eval_MaxReturn : -30.0
Eval_MinReturn : -138.0
Eval_AverageEpLen : 60.666666666666664
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -27.690001
best mean reward -27.690001
running time 974.354701
At timestep:     51 / 1000At timestep:     108 / 1000At timestep:     216 / 1000At timestep:     284 / 1000At timestep:     335 / 1000At timestep:     373 / 1000At timestep:     435 / 1000At timestep:     485 / 1000At timestep:     565 / 1000At timestep:     630 / 1000At timestep:     683 / 1000At timestep:     738 / 1000At timestep:     800 / 1000At timestep:     950 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -27.690000534057617
Train_BestReturn : -27.690000534057617
TimeSinceStart : 974.3547008037567
Exploration Critic Loss : 21212964.0
Exploitation Critic Loss : 0.09891791641712189
Exploration Model Loss : 22.722015380859375
Actor Loss : 0.49093806743621826
Eval_AverageReturn : -66.33333587646484
Eval_StdReturn : 27.209474563598633
Eval_MaxReturn : -37.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 67.26666666666667
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -27.889999
best mean reward -27.690001
running time 1024.293989
At timestep:     27 / 1000At timestep:     56 / 1000At timestep:     97 / 1000At timestep:     126 / 1000At timestep:     176 / 1000At timestep:     225 / 1000At timestep:     260 / 1000At timestep:     302 / 1000At timestep:     341 / 1000At timestep:     387 / 1000At timestep:     423 / 1000At timestep:     471 / 1000At timestep:     511 / 1000At timestep:     561 / 1000At timestep:     594 / 1000At timestep:     702 / 1000At timestep:     737 / 1000At timestep:     782 / 1000At timestep:     813 / 1000At timestep:     867 / 1000At timestep:     895 / 1000At timestep:     962 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -27.889999389648438
Train_BestReturn : -27.690000534057617
TimeSinceStart : 1024.2939887046814
Exploration Critic Loss : 18185868.0
Exploitation Critic Loss : 0.20318758487701416
Exploration Model Loss : 22.72216796875
Actor Loss : 0.5152838230133057
Eval_AverageReturn : -43.39130401611328
Eval_StdReturn : 16.97757911682129
Eval_MaxReturn : -26.0
Eval_MinReturn : -107.0
Eval_AverageEpLen : 44.391304347826086
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -28.150000
best mean reward -27.690001
running time 1076.272797
At timestep:     30 / 1000At timestep:     59 / 1000At timestep:     97 / 1000At timestep:     128 / 1000At timestep:     199 / 1000At timestep:     250 / 1000At timestep:     306 / 1000At timestep:     347 / 1000At timestep:     390 / 1000At timestep:     427 / 1000At timestep:     466 / 1000At timestep:     510 / 1000At timestep:     549 / 1000At timestep:     594 / 1000At timestep:     647 / 1000At timestep:     711 / 1000At timestep:     766 / 1000At timestep:     827 / 1000At timestep:     864 / 1000At timestep:     909 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -28.149999618530273
Train_BestReturn : -27.690000534057617
TimeSinceStart : 1076.2727966308594
Exploration Critic Loss : 15875194.0
Exploitation Critic Loss : 0.09024979174137115
Exploration Model Loss : 22.722320556640625
Actor Loss : 0.5550312995910645
Eval_AverageReturn : -46.71428680419922
Eval_StdReturn : 14.9766263961792
Eval_MaxReturn : -28.0
Eval_MinReturn : -92.0
Eval_AverageEpLen : 47.714285714285715
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -27.750000
best mean reward -27.690001
running time 1126.288922
At timestep:     36 / 1000At timestep:     83 / 1000At timestep:     161 / 1000At timestep:     203 / 1000At timestep:     232 / 1000At timestep:     278 / 1000At timestep:     325 / 1000At timestep:     386 / 1000At timestep:     425 / 1000At timestep:     460 / 1000At timestep:     513 / 1000At timestep:     547 / 1000At timestep:     612 / 1000At timestep:     648 / 1000At timestep:     683 / 1000At timestep:     721 / 1000At timestep:     768 / 1000At timestep:     805 / 1000At timestep:     861 / 1000At timestep:     891 / 1000At timestep:     944 / 1000At timestep:     979 / 1000At timestep:     999 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -27.75
Train_BestReturn : -27.690000534057617
TimeSinceStart : 1126.2889223098755
Exploration Critic Loss : 25406312.0
Exploitation Critic Loss : 0.09361158311367035
Exploration Model Loss : 22.722442626953125
Actor Loss : 0.5326208472251892
Eval_AverageReturn : -41.58333206176758
Eval_StdReturn : 13.22534465789795
Eval_MaxReturn : -19.0
Eval_MinReturn : -77.0
Eval_AverageEpLen : 42.583333333333336
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -27.430000
best mean reward -27.430000
running time 1178.744301
At timestep:     31 / 1000At timestep:     88 / 1000At timestep:     127 / 1000At timestep:     183 / 1000At timestep:     228 / 1000At timestep:     272 / 1000At timestep:     323 / 1000At timestep:     359 / 1000At timestep:     395 / 1000At timestep:     443 / 1000At timestep:     481 / 1000At timestep:     530 / 1000At timestep:     601 / 1000At timestep:     648 / 1000At timestep:     684 / 1000At timestep:     743 / 1000At timestep:     819 / 1000At timestep:     857 / 1000At timestep:     914 / 1000At timestep:     944 / 1000At timestep:     991 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -27.43000030517578
Train_BestReturn : -27.43000030517578
TimeSinceStart : 1178.7443010807037
Exploration Critic Loss : 15574059.0
Exploitation Critic Loss : 0.08625589311122894
Exploration Model Loss : 22.722564697265625
Actor Loss : 0.4926470220088959
Eval_AverageReturn : -45.5
Eval_StdReturn : 12.115917205810547
Eval_MaxReturn : -29.0
Eval_MinReturn : -75.0
Eval_AverageEpLen : 46.5
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -27.799999
best mean reward -27.430000
running time 1230.474894
At timestep:     34 / 1000At timestep:     66 / 1000At timestep:     111 / 1000At timestep:     145 / 1000At timestep:     210 / 1000At timestep:     245 / 1000At timestep:     275 / 1000At timestep:     311 / 1000At timestep:     345 / 1000At timestep:     386 / 1000At timestep:     427 / 1000At timestep:     494 / 1000At timestep:     562 / 1000At timestep:     620 / 1000At timestep:     708 / 1000At timestep:     754 / 1000At timestep:     780 / 1000At timestep:     823 / 1000At timestep:     850 / 1000At timestep:     886 / 1000At timestep:     913 / 1000At timestep:     948 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -27.799999237060547
Train_BestReturn : -27.43000030517578
TimeSinceStart : 1230.4748938083649
Exploration Critic Loss : 14385476.0
Exploitation Critic Loss : 0.137908935546875
Exploration Model Loss : 22.722732543945312
Actor Loss : 0.4827025532722473
Eval_AverageReturn : -43.173912048339844
Eval_StdReturn : 16.29791259765625
Eval_MaxReturn : -25.0
Eval_MinReturn : -87.0
Eval_AverageEpLen : 44.17391304347826
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -27.139999
best mean reward -27.139999
running time 1281.227533
At timestep:     32 / 1000At timestep:     56 / 1000At timestep:     103 / 1000At timestep:     199 / 1000At timestep:     276 / 1000At timestep:     354 / 1000At timestep:     392 / 1000At timestep:     435 / 1000At timestep:     471 / 1000At timestep:     505 / 1000At timestep:     532 / 1000At timestep:     571 / 1000At timestep:     602 / 1000At timestep:     649 / 1000At timestep:     722 / 1000At timestep:     767 / 1000At timestep:     838 / 1000At timestep:     868 / 1000At timestep:     896 / 1000At timestep:     951 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -27.139999389648438
Train_BestReturn : -27.139999389648438
TimeSinceStart : 1281.2275326251984
Exploration Critic Loss : 18591752.0
Exploitation Critic Loss : 0.11041286587715149
Exploration Model Loss : 22.722869873046875
Actor Loss : 0.5622513890266418
Eval_AverageReturn : -46.904762268066406
Eval_StdReturn : 19.68781280517578
Eval_MaxReturn : -23.0
Eval_MinReturn : -95.0
Eval_AverageEpLen : 47.904761904761905
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -26.950001
best mean reward -26.950001
running time 1330.823615
At timestep:     33 / 1000At timestep:     60 / 1000At timestep:     106 / 1000At timestep:     136 / 1000At timestep:     159 / 1000At timestep:     204 / 1000At timestep:     235 / 1000At timestep:     265 / 1000At timestep:     313 / 1000At timestep:     360 / 1000At timestep:     399 / 1000At timestep:     423 / 1000At timestep:     478 / 1000At timestep:     514 / 1000At timestep:     579 / 1000At timestep:     601 / 1000At timestep:     647 / 1000At timestep:     680 / 1000At timestep:     718 / 1000At timestep:     757 / 1000At timestep:     797 / 1000At timestep:     825 / 1000At timestep:     869 / 1000At timestep:     937 / 1000At timestep:     979 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -26.950000762939453
Train_BestReturn : -26.950000762939453
TimeSinceStart : 1330.8236145973206
Exploration Critic Loss : 26400728.0
Exploitation Critic Loss : 0.10654950141906738
Exploration Model Loss : 22.722946166992188
Actor Loss : 0.48079025745391846
Eval_AverageReturn : -38.03845977783203
Eval_StdReturn : 11.500772476196289
Eval_MaxReturn : -21.0
Eval_MinReturn : -67.0
Eval_AverageEpLen : 39.03846153846154
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -27.100000
best mean reward -26.950001
running time 1382.493352
At timestep:     40 / 1000At timestep:     114 / 1000At timestep:     149 / 1000At timestep:     169 / 1000At timestep:     212 / 1000At timestep:     249 / 1000At timestep:     276 / 1000At timestep:     299 / 1000At timestep:     333 / 1000At timestep:     369 / 1000At timestep:     402 / 1000At timestep:     437 / 1000At timestep:     468 / 1000At timestep:     501 / 1000At timestep:     550 / 1000At timestep:     584 / 1000At timestep:     616 / 1000At timestep:     645 / 1000At timestep:     666 / 1000At timestep:     716 / 1000At timestep:     755 / 1000At timestep:     803 / 1000At timestep:     863 / 1000At timestep:     910 / 1000At timestep:     954 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -27.100000381469727
Train_BestReturn : -26.950000762939453
TimeSinceStart : 1382.4933516979218
Exploration Critic Loss : 17719878.0
Exploitation Critic Loss : 0.07295326888561249
Exploration Model Loss : 22.722991943359375
Actor Loss : 0.48774638772010803
Eval_AverageReturn : -37.846153259277344
Eval_StdReturn : 12.132893562316895
Eval_MaxReturn : -19.0
Eval_MinReturn : -73.0
Eval_AverageEpLen : 38.84615384615385
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -27.590000
best mean reward -26.950001
running time 1433.165793
At timestep:     24 / 1000At timestep:     64 / 1000At timestep:     121 / 1000At timestep:     150 / 1000At timestep:     216 / 1000At timestep:     242 / 1000At timestep:     279 / 1000At timestep:     330 / 1000At timestep:     372 / 1000At timestep:     431 / 1000At timestep:     485 / 1000At timestep:     519 / 1000At timestep:     548 / 1000At timestep:     619 / 1000At timestep:     646 / 1000At timestep:     689 / 1000At timestep:     752 / 1000At timestep:     814 / 1000At timestep:     863 / 1000At timestep:     900 / 1000At timestep:     934 / 1000At timestep:     981 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -27.59000015258789
Train_BestReturn : -26.950000762939453
TimeSinceStart : 1433.1657934188843
Exploration Critic Loss : 18120038.0
Exploitation Critic Loss : 0.13878633081912994
Exploration Model Loss : 22.723052978515625
Actor Loss : 0.4708492159843445
Eval_AverageReturn : -43.565216064453125
Eval_StdReturn : 13.592920303344727
Eval_MaxReturn : -23.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 44.56521739130435
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -27.830000
best mean reward -26.950001
running time 1481.975240
At timestep:     35 / 1000At timestep:     78 / 1000At timestep:     115 / 1000At timestep:     161 / 1000At timestep:     194 / 1000At timestep:     242 / 1000At timestep:     275 / 1000At timestep:     312 / 1000At timestep:     374 / 1000At timestep:     425 / 1000At timestep:     478 / 1000At timestep:     550 / 1000At timestep:     595 / 1000At timestep:     638 / 1000At timestep:     673 / 1000At timestep:     730 / 1000At timestep:     764 / 1000At timestep:     797 / 1000At timestep:     841 / 1000At timestep:     872 / 1000At timestep:     897 / 1000At timestep:     937 / 1000At timestep:     965 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -27.829999923706055
Train_BestReturn : -26.950000762939453
TimeSinceStart : 1481.9752399921417
Exploration Critic Loss : 22410898.0
Exploitation Critic Loss : 0.10793249309062958
Exploration Model Loss : 22.72314453125
Actor Loss : 0.520881175994873
Eval_AverageReturn : -41.66666793823242
Eval_StdReturn : 11.433382034301758
Eval_MaxReturn : -24.0
Eval_MinReturn : -71.0
Eval_AverageEpLen : 42.666666666666664
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -27.510000
best mean reward -26.950001
running time 1532.414265
At timestep:     56 / 1000At timestep:     89 / 1000At timestep:     137 / 1000At timestep:     171 / 1000At timestep:     223 / 1000At timestep:     265 / 1000At timestep:     301 / 1000At timestep:     329 / 1000At timestep:     382 / 1000At timestep:     411 / 1000At timestep:     467 / 1000At timestep:     499 / 1000At timestep:     526 / 1000At timestep:     564 / 1000At timestep:     600 / 1000At timestep:     643 / 1000At timestep:     679 / 1000At timestep:     706 / 1000At timestep:     750 / 1000At timestep:     804 / 1000At timestep:     855 / 1000At timestep:     889 / 1000At timestep:     922 / 1000At timestep:     966 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -27.510000228881836
Train_BestReturn : -26.950000762939453
TimeSinceStart : 1532.4142649173737
Exploration Critic Loss : 21094504.0
Exploitation Critic Loss : 0.11113498359918594
Exploration Model Loss : 22.723220825195312
Actor Loss : 0.5131213068962097
Eval_AverageReturn : -39.400001525878906
Eval_StdReturn : 9.277931213378906
Eval_MaxReturn : -26.0
Eval_MinReturn : -55.0
Eval_AverageEpLen : 40.4
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -28.290001
best mean reward -26.950001
running time 1583.700121
At timestep:     32 / 1000At timestep:     59 / 1000At timestep:     100 / 1000At timestep:     129 / 1000At timestep:     157 / 1000At timestep:     192 / 1000At timestep:     219 / 1000At timestep:     262 / 1000At timestep:     305 / 1000At timestep:     337 / 1000At timestep:     380 / 1000At timestep:     407 / 1000At timestep:     461 / 1000At timestep:     495 / 1000At timestep:     529 / 1000At timestep:     575 / 1000At timestep:     611 / 1000At timestep:     644 / 1000At timestep:     684 / 1000At timestep:     728 / 1000At timestep:     768 / 1000At timestep:     818 / 1000At timestep:     840 / 1000At timestep:     862 / 1000At timestep:     901 / 1000At timestep:     932 / 1000At timestep:     984 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -28.290000915527344
Train_BestReturn : -26.950000762939453
TimeSinceStart : 1583.7001214027405
Exploration Critic Loss : 21712110.0
Exploitation Critic Loss : 0.21370245516300201
Exploration Model Loss : 22.7232666015625
Actor Loss : 0.5204936265945435
Eval_AverageReturn : -35.25
Eval_StdReturn : 8.449957847595215
Eval_MaxReturn : -21.0
Eval_MinReturn : -53.0
Eval_AverageEpLen : 36.25
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -27.360001
best mean reward -26.950001
running time 1635.496024
At timestep:     37 / 1000At timestep:     70 / 1000At timestep:     97 / 1000At timestep:     125 / 1000At timestep:     164 / 1000At timestep:     202 / 1000At timestep:     260 / 1000At timestep:     299 / 1000At timestep:     323 / 1000At timestep:     344 / 1000At timestep:     369 / 1000At timestep:     390 / 1000At timestep:     421 / 1000At timestep:     457 / 1000At timestep:     498 / 1000At timestep:     526 / 1000At timestep:     561 / 1000At timestep:     625 / 1000At timestep:     650 / 1000At timestep:     698 / 1000At timestep:     731 / 1000At timestep:     775 / 1000At timestep:     809 / 1000At timestep:     844 / 1000At timestep:     882 / 1000At timestep:     909 / 1000At timestep:     944 / 1000At timestep:     989 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -27.360000610351562
Train_BestReturn : -26.950000762939453
TimeSinceStart : 1635.496024131775
Exploration Critic Loss : 12081296.0
Exploitation Critic Loss : 0.07869446277618408
Exploration Model Loss : 22.723342895507812
Actor Loss : 0.4684738516807556
Eval_AverageReturn : -34.44827651977539
Eval_StdReturn : 9.80002212524414
Eval_MaxReturn : -20.0
Eval_MinReturn : -63.0
Eval_AverageEpLen : 35.44827586206897
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -27.049999
best mean reward -26.950001
running time 1686.090785
At timestep:     38 / 1000At timestep:     78 / 1000At timestep:     108 / 1000At timestep:     135 / 1000At timestep:     165 / 1000At timestep:     202 / 1000At timestep:     294 / 1000At timestep:     338 / 1000At timestep:     368 / 1000At timestep:     406 / 1000At timestep:     464 / 1000At timestep:     507 / 1000At timestep:     581 / 1000At timestep:     622 / 1000At timestep:     654 / 1000At timestep:     710 / 1000At timestep:     745 / 1000At timestep:     781 / 1000At timestep:     816 / 1000At timestep:     847 / 1000At timestep:     891 / 1000At timestep:     924 / 1000At timestep:     961 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -27.049999237060547
Train_BestReturn : -26.950000762939453
TimeSinceStart : 1686.0907852649689
Exploration Critic Loss : 12164710.0
Exploitation Critic Loss : 0.08953206241130829
Exploration Model Loss : 22.723403930664062
Actor Loss : 0.5071883201599121
Eval_AverageReturn : -40.70833206176758
Eval_StdReturn : 14.672760963439941
Eval_MaxReturn : -26.0
Eval_MinReturn : -91.0
Eval_AverageEpLen : 41.708333333333336
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -26.570000
best mean reward -26.570000
running time 1738.540348
At timestep:     48 / 1000At timestep:     83 / 1000At timestep:     110 / 1000At timestep:     151 / 1000At timestep:     201 / 1000At timestep:     239 / 1000At timestep:     264 / 1000At timestep:     291 / 1000At timestep:     319 / 1000At timestep:     371 / 1000At timestep:     397 / 1000At timestep:     419 / 1000At timestep:     456 / 1000At timestep:     491 / 1000At timestep:     527 / 1000At timestep:     548 / 1000At timestep:     585 / 1000At timestep:     619 / 1000At timestep:     660 / 1000At timestep:     700 / 1000At timestep:     737 / 1000At timestep:     769 / 1000At timestep:     794 / 1000At timestep:     821 / 1000At timestep:     852 / 1000At timestep:     887 / 1000At timestep:     914 / 1000At timestep:     955 / 1000At timestep:     989 / 1000At timestep:     1036 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -26.56999969482422
Train_BestReturn : -26.56999969482422
TimeSinceStart : 1738.5403475761414
Exploration Critic Loss : 17345900.0
Exploitation Critic Loss : 0.10203896462917328
Exploration Model Loss : 22.72344970703125
Actor Loss : 0.4604969620704651
Eval_AverageReturn : -33.53333282470703
Eval_StdReturn : 8.06942081451416
Eval_MaxReturn : -20.0
Eval_MinReturn : -51.0
Eval_AverageEpLen : 34.53333333333333
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -26.570000
best mean reward -26.570000
running time 1791.044849
At timestep:     29 / 1000At timestep:     102 / 1000At timestep:     129 / 1000At timestep:     175 / 1000At timestep:     203 / 1000At timestep:     239 / 1000At timestep:     268 / 1000At timestep:     324 / 1000At timestep:     378 / 1000At timestep:     404 / 1000At timestep:     456 / 1000At timestep:     486 / 1000At timestep:     513 / 1000At timestep:     551 / 1000At timestep:     618 / 1000At timestep:     641 / 1000At timestep:     691 / 1000At timestep:     726 / 1000At timestep:     759 / 1000At timestep:     791 / 1000At timestep:     811 / 1000At timestep:     845 / 1000At timestep:     888 / 1000At timestep:     919 / 1000At timestep:     944 / 1000At timestep:     981 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -26.56999969482422
Train_BestReturn : -26.56999969482422
TimeSinceStart : 1791.0448486804962
Exploration Critic Loss : 15199923.0
Exploitation Critic Loss : 0.11430874466896057
Exploration Model Loss : 22.7235107421875
Actor Loss : 0.48466601967811584
Eval_AverageReturn : -36.74074172973633
Eval_StdReturn : 13.113727569580078
Eval_MaxReturn : -19.0
Eval_MinReturn : -72.0
Eval_AverageEpLen : 37.74074074074074
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -26.870001
best mean reward -26.570000
running time 1839.747926
At timestep:     29 / 1000At timestep:     54 / 1000At timestep:     102 / 1000At timestep:     138 / 1000At timestep:     175 / 1000At timestep:     221 / 1000At timestep:     256 / 1000At timestep:     288 / 1000At timestep:     338 / 1000At timestep:     362 / 1000At timestep:     395 / 1000At timestep:     432 / 1000At timestep:     468 / 1000At timestep:     512 / 1000At timestep:     539 / 1000At timestep:     586 / 1000At timestep:     635 / 1000At timestep:     667 / 1000At timestep:     696 / 1000At timestep:     738 / 1000At timestep:     765 / 1000At timestep:     798 / 1000At timestep:     831 / 1000At timestep:     879 / 1000At timestep:     925 / 1000At timestep:     951 / 1000At timestep:     988 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -26.8700008392334
Train_BestReturn : -26.56999969482422
TimeSinceStart : 1839.747926235199
Exploration Critic Loss : 11310963.0
Exploitation Critic Loss : 0.12077111005783081
Exploration Model Loss : 22.723587036132812
Actor Loss : 0.4938966929912567
Eval_AverageReturn : -35.28571319580078
Eval_StdReturn : 8.070481300354004
Eval_MaxReturn : -23.0
Eval_MinReturn : -49.0
Eval_AverageEpLen : 36.285714285714285
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -27.840000
best mean reward -26.570000
running time 1890.860713
At timestep:     42 / 1000At timestep:     73 / 1000At timestep:     128 / 1000At timestep:     157 / 1000At timestep:     202 / 1000At timestep:     248 / 1000At timestep:     294 / 1000At timestep:     371 / 1000At timestep:     405 / 1000At timestep:     445 / 1000At timestep:     469 / 1000At timestep:     503 / 1000At timestep:     536 / 1000At timestep:     559 / 1000At timestep:     598 / 1000At timestep:     621 / 1000At timestep:     678 / 1000At timestep:     704 / 1000At timestep:     727 / 1000At timestep:     774 / 1000At timestep:     814 / 1000At timestep:     834 / 1000At timestep:     875 / 1000At timestep:     910 / 1000At timestep:     976 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -27.84000015258789
Train_BestReturn : -26.56999969482422
TimeSinceStart : 1890.8607132434845
Exploration Critic Loss : 15036380.0
Exploitation Critic Loss : 0.13595275580883026
Exploration Model Loss : 22.723648071289062
Actor Loss : 0.49540284276008606
Eval_AverageReturn : -37.96154022216797
Eval_StdReturn : 13.529115676879883
Eval_MaxReturn : -19.0
Eval_MinReturn : -76.0
Eval_AverageEpLen : 38.96153846153846
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -27.920000
best mean reward -26.570000
running time 1943.735631
At timestep:     52 / 1000At timestep:     90 / 1000At timestep:     114 / 1000At timestep:     143 / 1000At timestep:     177 / 1000At timestep:     217 / 1000At timestep:     263 / 1000At timestep:     293 / 1000At timestep:     336 / 1000At timestep:     363 / 1000At timestep:     394 / 1000At timestep:     438 / 1000At timestep:     475 / 1000At timestep:     528 / 1000At timestep:     565 / 1000At timestep:     602 / 1000At timestep:     646 / 1000At timestep:     710 / 1000At timestep:     738 / 1000At timestep:     774 / 1000At timestep:     805 / 1000At timestep:     831 / 1000At timestep:     869 / 1000At timestep:     908 / 1000At timestep:     932 / 1000At timestep:     958 / 1000At timestep:     991 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -27.920000076293945
Train_BestReturn : -26.56999969482422
TimeSinceStart : 1943.7356314659119
Exploration Critic Loss : 16219968.0
Exploitation Critic Loss : 0.10970397293567657
Exploration Model Loss : 22.723709106445312
Actor Loss : 0.4765159487724304
Eval_AverageReturn : -35.53571319580078
Eval_StdReturn : 9.317732810974121
Eval_MaxReturn : -23.0
Eval_MinReturn : -63.0
Eval_AverageEpLen : 36.535714285714285
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -27.889999
best mean reward -26.570000
running time 1994.505239
At timestep:     30 / 1000At timestep:     54 / 1000At timestep:     78 / 1000At timestep:     108 / 1000At timestep:     143 / 1000At timestep:     181 / 1000At timestep:     240 / 1000At timestep:     271 / 1000At timestep:     299 / 1000At timestep:     336 / 1000At timestep:     362 / 1000At timestep:     393 / 1000At timestep:     426 / 1000At timestep:     451 / 1000At timestep:     484 / 1000At timestep:     533 / 1000At timestep:     570 / 1000At timestep:     609 / 1000At timestep:     652 / 1000At timestep:     704 / 1000At timestep:     734 / 1000At timestep:     771 / 1000At timestep:     807 / 1000At timestep:     847 / 1000At timestep:     871 / 1000At timestep:     918 / 1000At timestep:     946 / 1000At timestep:     977 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -27.889999389648438
Train_BestReturn : -26.56999969482422
TimeSinceStart : 1994.5052387714386
Exploration Critic Loss : 17472984.0
Exploitation Critic Loss : 0.10435853898525238
Exploration Model Loss : 22.723800659179688
Actor Loss : 0.48546838760375977
Eval_AverageReturn : -34.82758712768555
Eval_StdReturn : 9.850849151611328
Eval_MaxReturn : -23.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 35.827586206896555
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -27.420000
best mean reward -26.570000
running time 2045.849708
At timestep:     45 / 1000At timestep:     104 / 1000At timestep:     131 / 1000At timestep:     184 / 1000At timestep:     214 / 1000At timestep:     259 / 1000At timestep:     283 / 1000At timestep:     329 / 1000At timestep:     368 / 1000At timestep:     400 / 1000At timestep:     456 / 1000At timestep:     496 / 1000At timestep:     521 / 1000At timestep:     569 / 1000At timestep:     599 / 1000At timestep:     637 / 1000At timestep:     670 / 1000At timestep:     699 / 1000At timestep:     739 / 1000At timestep:     758 / 1000At timestep:     786 / 1000At timestep:     814 / 1000At timestep:     863 / 1000At timestep:     886 / 1000At timestep:     936 / 1000At timestep:     974 / 1000At timestep:     1068 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -27.420000076293945
Train_BestReturn : -26.56999969482422
TimeSinceStart : 2045.8497080802917
Exploration Critic Loss : 27746692.0
Exploitation Critic Loss : 0.10810209810733795
Exploration Model Loss : 22.723876953125
Actor Loss : 0.5153948068618774
Eval_AverageReturn : -38.55555725097656
Eval_StdReturn : 15.07839584350586
Eval_MaxReturn : -18.0
Eval_MinReturn : -93.0
Eval_AverageEpLen : 39.55555555555556
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -28.190001
best mean reward -26.570000
running time 2097.305016
At timestep:     34 / 1000At timestep:     74 / 1000At timestep:     110 / 1000At timestep:     168 / 1000At timestep:     192 / 1000At timestep:     220 / 1000At timestep:     267 / 1000At timestep:     298 / 1000At timestep:     339 / 1000At timestep:     395 / 1000At timestep:     434 / 1000At timestep:     475 / 1000At timestep:     518 / 1000At timestep:     559 / 1000At timestep:     604 / 1000At timestep:     636 / 1000At timestep:     661 / 1000At timestep:     697 / 1000At timestep:     759 / 1000At timestep:     788 / 1000At timestep:     825 / 1000At timestep:     852 / 1000At timestep:     891 / 1000At timestep:     932 / 1000At timestep:     965 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -28.190000534057617
Train_BestReturn : -26.56999969482422
TimeSinceStart : 2097.3050162792206
Exploration Critic Loss : 23237236.0
Exploitation Critic Loss : 0.09287218004465103
Exploration Model Loss : 22.72393798828125
Actor Loss : 0.5018143653869629
Eval_AverageReturn : -37.5
Eval_StdReturn : 9.398240089416504
Eval_MaxReturn : -23.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 38.5
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -28.010000
best mean reward -26.570000
running time 2148.083634
At timestep:     50 / 1000At timestep:     79 / 1000At timestep:     117 / 1000At timestep:     140 / 1000At timestep:     168 / 1000At timestep:     192 / 1000At timestep:     217 / 1000At timestep:     240 / 1000At timestep:     263 / 1000At timestep:     293 / 1000At timestep:     314 / 1000At timestep:     353 / 1000At timestep:     378 / 1000At timestep:     431 / 1000At timestep:     463 / 1000At timestep:     510 / 1000At timestep:     544 / 1000At timestep:     573 / 1000At timestep:     606 / 1000At timestep:     636 / 1000At timestep:     687 / 1000At timestep:     717 / 1000At timestep:     774 / 1000At timestep:     804 / 1000At timestep:     842 / 1000At timestep:     865 / 1000At timestep:     889 / 1000At timestep:     922 / 1000At timestep:     950 / 1000At timestep:     997 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -28.010000228881836
Train_BestReturn : -26.56999969482422
TimeSinceStart : 2148.0836338996887
Exploration Critic Loss : 19781592.0
Exploitation Critic Loss : 0.13580721616744995
Exploration Model Loss : 22.724014282226562
Actor Loss : 0.4161766767501831
Eval_AverageReturn : -32.25806427001953
Eval_StdReturn : 9.87001895904541
Eval_MaxReturn : -20.0
Eval_MinReturn : -56.0
Eval_AverageEpLen : 33.25806451612903
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -27.629999
best mean reward -26.570000
running time 2198.118751
At timestep:     27 / 1000At timestep:     64 / 1000At timestep:     104 / 1000At timestep:     126 / 1000At timestep:     176 / 1000At timestep:     214 / 1000At timestep:     255 / 1000At timestep:     292 / 1000At timestep:     335 / 1000At timestep:     370 / 1000At timestep:     400 / 1000At timestep:     436 / 1000At timestep:     482 / 1000At timestep:     519 / 1000At timestep:     551 / 1000At timestep:     592 / 1000At timestep:     612 / 1000At timestep:     647 / 1000At timestep:     675 / 1000At timestep:     711 / 1000At timestep:     757 / 1000At timestep:     798 / 1000At timestep:     838 / 1000At timestep:     892 / 1000At timestep:     919 / 1000At timestep:     947 / 1000At timestep:     990 / 1000At timestep:     1033 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -27.6299991607666
Train_BestReturn : -26.56999969482422
TimeSinceStart : 2198.1187505722046
Exploration Critic Loss : 20875192.0
Exploitation Critic Loss : 0.12297618389129639
Exploration Model Loss : 22.724075317382812
Actor Loss : 0.5040229558944702
Eval_AverageReturn : -35.89285659790039
Eval_StdReturn : 7.916254997253418
Eval_MaxReturn : -19.0
Eval_MinReturn : -53.0
Eval_AverageEpLen : 36.892857142857146
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -27.770000
best mean reward -26.570000
running time 2249.522319
At timestep:     23 / 1000At timestep:     64 / 1000At timestep:     100 / 1000At timestep:     142 / 1000At timestep:     164 / 1000At timestep:     186 / 1000At timestep:     239 / 1000At timestep:     274 / 1000At timestep:     300 / 1000At timestep:     325 / 1000At timestep:     359 / 1000At timestep:     410 / 1000At timestep:     438 / 1000At timestep:     463 / 1000At timestep:     510 / 1000At timestep:     542 / 1000At timestep:     565 / 1000At timestep:     593 / 1000At timestep:     620 / 1000At timestep:     680 / 1000At timestep:     708 / 1000At timestep:     738 / 1000At timestep:     767 / 1000At timestep:     803 / 1000At timestep:     840 / 1000At timestep:     873 / 1000At timestep:     914 / 1000At timestep:     948 / 1000At timestep:     1008 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_medium_unsupervised_lam10_PointmassMedium-v0_22-11-2022_23-03-08/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -27.770000457763672
Train_BestReturn : -26.56999969482422
TimeSinceStart : 2249.5223186016083
Exploration Critic Loss : 19998224.0
Exploitation Critic Loss : 0.10946404188871384
Exploration Model Loss : 22.72412109375
Actor Loss : 0.4676348865032196
Eval_AverageReturn : -33.75862121582031
Eval_StdReturn : 10.656566619873047
Eval_MaxReturn : -21.0
Eval_MinReturn : -59.0
Eval_AverageEpLen : 34.758620689655174
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_unsupervised_lam20_PointmassMedium-v0_22-11-2022_23-41-35 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_unsupervised_lam20_PointmassMedium-v0_22-11-2022_23-41-35
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002500
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.002499818801879883
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 10.205913
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     666 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 10.205912590026855
Eval_AverageReturn : -139.375
Eval_StdReturn : 28.111106872558594
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.5
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -146.769226
best mean reward -inf
running time 21.023817
At timestep:     54 / 1000At timestep:     204 / 1000At timestep:     354 / 1000At timestep:     504 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -146.76922607421875
TimeSinceStart : 21.02381682395935
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -147.899994
best mean reward -inf
running time 67.594393
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -147.89999389648438
TimeSinceStart : 67.59439277648926
Exploration Critic Loss : 2261579.5
Exploitation Critic Loss : 0.03152179345488548
Exploration Model Loss : 130.72509765625
Actor Loss : 1.1679457426071167
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -148.384613
best mean reward -inf
running time 110.636626
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     373 / 1000At timestep:     523 / 1000At timestep:     673 / 1000At timestep:     823 / 1000At timestep:     973 / 1000At timestep:     1123 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -148.38461303710938
TimeSinceStart : 110.63662648200989
Exploration Critic Loss : 9073320.0
Exploitation Critic Loss : 0.03341522812843323
Exploration Model Loss : 38.10673522949219
Actor Loss : 0.971356213092804
Eval_AverageReturn : -140.25
Eval_StdReturn : 25.79607582092285
Eval_MaxReturn : -72.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 140.375
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -148.727280
best mean reward -inf
running time 155.141651
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -148.72727966308594
TimeSinceStart : 155.14165115356445
Exploration Critic Loss : 5407460.5
Exploitation Critic Loss : 0.023883264511823654
Exploration Model Loss : 32.59965515136719
Actor Loss : 0.7668870091438293
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -148.949997
best mean reward -inf
running time 199.831050
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -148.9499969482422
TimeSinceStart : 199.831050157547
Exploration Critic Loss : 10251316.0
Exploitation Critic Loss : 0.09785211831331253
Exploration Model Loss : 63.22117614746094
Actor Loss : 0.6280640959739685
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -149.086960
best mean reward -inf
running time 246.745678
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -149.0869598388672
TimeSinceStart : 246.7456784248352
Exploration Critic Loss : 8958778.0
Exploitation Critic Loss : 0.1150197684764862
Exploration Model Loss : 27.38930320739746
Actor Loss : 0.5660591125488281
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -149.207550
best mean reward -inf
running time 299.478259
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -149.20755004882812
TimeSinceStart : 299.47825932502747
Exploration Critic Loss : 11790140.0
Exploitation Critic Loss : 0.042609766125679016
Exploration Model Loss : 20.90802001953125
Actor Loss : 0.5077724456787109
Eval_AverageReturn : -145.42857360839844
Eval_StdReturn : 11.197667121887207
Eval_MaxReturn : -118.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 145.57142857142858
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -149.300003
best mean reward -inf
running time 350.087409
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -149.3000030517578
TimeSinceStart : 350.0874094963074
Exploration Critic Loss : 13790108.0
Exploitation Critic Loss : 0.0969935953617096
Exploration Model Loss : 19.363983154296875
Actor Loss : 0.47095221281051636
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -149.363632
best mean reward -inf
running time 399.532555
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -149.36363220214844
TimeSinceStart : 399.53255462646484
Exploration Critic Loss : 12205370.0
Exploitation Critic Loss : 0.25657978653907776
Exploration Model Loss : 18.872589111328125
Actor Loss : 0.4401620626449585
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -149.424652
best mean reward -inf
running time 452.149351
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -149.42465209960938
TimeSinceStart : 452.1493511199951
Exploration Critic Loss : 13310592.0
Exploitation Critic Loss : 0.10940507799386978
Exploration Model Loss : 18.701309204101562
Actor Loss : 0.40939587354660034
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -147.938278
best mean reward -inf
running time 504.797355
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -147.9382781982422
TimeSinceStart : 504.7973554134369
Exploration Critic Loss : 13360290.0
Exploitation Critic Loss : 0.23009705543518066
Exploration Model Loss : 18.639999389648438
Actor Loss : 0.40519529581069946
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -148.080460
best mean reward -inf
running time 557.798291
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -148.08045959472656
TimeSinceStart : 557.7982912063599
Exploration Critic Loss : 18072984.0
Exploitation Critic Loss : 0.00025459693279117346
Exploration Model Loss : 18.617523193359375
Actor Loss : 0.38177597522735596
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -147.252625
best mean reward -inf
running time 608.480819
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -147.25262451171875
TimeSinceStart : 608.4808194637299
Exploration Critic Loss : 10098643.0
Exploitation Critic Loss : 0.06147974729537964
Exploration Model Loss : 18.60968017578125
Actor Loss : 0.35547536611557007
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -146.880005
best mean reward -146.880005
running time 658.754178
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -146.8800048828125
Train_BestReturn : -146.8800048828125
TimeSinceStart : 658.754177570343
Exploration Critic Loss : 18113274.0
Exploitation Critic Loss : 0.1266646534204483
Exploration Model Loss : 18.6065673828125
Actor Loss : 0.3663044571876526
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -146.369995
best mean reward -146.369995
running time 710.391977
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -146.3699951171875
Train_BestReturn : -146.3699951171875
TimeSinceStart : 710.3919770717621
Exploration Critic Loss : 28740518.0
Exploitation Critic Loss : 0.12792783975601196
Exploration Model Loss : 18.605300903320312
Actor Loss : 0.34866827726364136
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -145.520004
best mean reward -145.520004
running time 762.301160
At timestep:     150 / 1000At timestep:     254 / 1000At timestep:     404 / 1000At timestep:     554 / 1000At timestep:     704 / 1000At timestep:     846 / 1000At timestep:     996 / 1000At timestep:     1146 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -145.52000427246094
Train_BestReturn : -145.52000427246094
TimeSinceStart : 762.301159620285
Exploration Critic Loss : 22005020.0
Exploitation Critic Loss : 0.19080975651741028
Exploration Model Loss : 18.60491943359375
Actor Loss : 0.3441457748413086
Eval_AverageReturn : -143.0
Eval_StdReturn : 15.402921676635742
Eval_MaxReturn : -103.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 143.25
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -144.940002
best mean reward -144.940002
running time 821.044971
At timestep:     96 / 1000At timestep:     192 / 1000At timestep:     342 / 1000At timestep:     492 / 1000At timestep:     642 / 1000At timestep:     792 / 1000At timestep:     942 / 1000At timestep:     1092 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -144.94000244140625
Train_BestReturn : -144.94000244140625
TimeSinceStart : 821.0449714660645
Exploration Critic Loss : 17614820.0
Exploitation Critic Loss : 0.19586235284805298
Exploration Model Loss : 18.60546875
Actor Loss : 0.3368149399757385
Eval_AverageReturn : -136.25
Eval_StdReturn : 23.815698623657227
Eval_MaxReturn : -95.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 136.5
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -142.820007
best mean reward -142.820007
running time 872.100504
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     541 / 1000At timestep:     691 / 1000At timestep:     841 / 1000At timestep:     956 / 1000At timestep:     1106 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -142.82000732421875
Train_BestReturn : -142.82000732421875
TimeSinceStart : 872.100504398346
Exploration Critic Loss : 19235438.0
Exploitation Critic Loss : 0.2061530500650406
Exploration Model Loss : 18.606048583984375
Actor Loss : 0.3314998745918274
Eval_AverageReturn : -138.0
Eval_StdReturn : 21.63330841064453
Eval_MaxReturn : -90.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.25
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -138.690002
best mean reward -138.690002
running time 924.289556
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000