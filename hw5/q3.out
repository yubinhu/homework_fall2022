


LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_medium_dqn_PointmassMedium-v0_21-11-2022_22-49-35 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_medium_dqn_PointmassMedium-v0_21-11-2022_22-49-35
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001963
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0019626617431640625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 5.805742
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 5.805742263793945
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 12.424855
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -150.0
TimeSinceStart : 12.424855470657349
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 28.441462
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -150.0
TimeSinceStart : 28.44146156311035
Exploitation Critic Loss : 2.629528898978606e-05
Exploration Critic Loss : 557693.375
Exploration Model Loss : 38.288970947265625
Exploitation Data q-values : 0.04346848279237747
Exploitation OOD q-values : 1.6531132459640503
Exploitation CQL Loss : 2.629528898978606e-05
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 44.624868
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -150.0
TimeSinceStart : 44.624868392944336
Exploitation Critic Loss : 1.1789817108365241e-05
Exploration Critic Loss : 8649828.0
Exploration Model Loss : 52.191749572753906
Exploitation Data q-values : 0.050308238714933395
Exploitation OOD q-values : 1.659192681312561
Exploitation CQL Loss : 1.1789817108365241e-05
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 61.545782
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -150.0
TimeSinceStart : 61.54578161239624
Exploitation Critic Loss : 1.103291469917167e-05
Exploration Critic Loss : 20203908.0
Exploration Model Loss : 150.7830810546875
Exploitation Data q-values : 0.04832678288221359
Exploitation OOD q-values : 1.6571773290634155
Exploitation CQL Loss : 1.103291469917167e-05
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 77.425808
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -150.0
TimeSinceStart : 77.42580795288086
Exploitation Critic Loss : 1.2824759323848411e-05
Exploration Critic Loss : 5139626.0
Exploration Model Loss : 35.08650207519531
Exploitation Data q-values : 0.04442773759365082
Exploitation OOD q-values : 1.6533176898956299
Exploitation CQL Loss : 1.2824759323848411e-05
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 93.593016
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -150.0
TimeSinceStart : 93.59301567077637
Exploitation Critic Loss : 3.3887320114445174e-06
Exploration Critic Loss : 7130588.5
Exploration Model Loss : 23.2208251953125
Exploitation Data q-values : 0.03834977000951767
Exploitation OOD q-values : 1.6474220752716064
Exploitation CQL Loss : 3.3887320114445174e-06
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 109.626933
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -150.0
TimeSinceStart : 109.62693309783936
Exploitation Critic Loss : 6.326627499220194e-06
Exploration Critic Loss : 8975240.0
Exploration Model Loss : 21.073013305664062
Exploitation Data q-values : 0.03388267755508423
Exploitation OOD q-values : 1.6434212923049927
Exploitation CQL Loss : 6.326627499220194e-06
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 126.149439
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -150.0
TimeSinceStart : 126.14943933486938
Exploitation Critic Loss : 0.0001450938289053738
Exploration Critic Loss : 7563267.0
Exploration Model Loss : 20.413665771484375
Exploitation Data q-values : 0.03121235966682434
Exploitation OOD q-values : 1.641286015510559
Exploitation CQL Loss : 0.0001450938289053738
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 141.930520
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -150.0
TimeSinceStart : 141.93052005767822
Exploitation Critic Loss : 6.334513727779267e-06
Exploration Critic Loss : 6985712.5
Exploration Model Loss : 20.185562133789062
Exploitation Data q-values : 0.021087583154439926
Exploitation OOD q-values : 1.6309571266174316
Exploitation CQL Loss : 6.334513727779267e-06
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 157.953516
At timestep:     86 / 1000At timestep:     236 / 1000At timestep:     386 / 1000At timestep:     536 / 1000At timestep:     686 / 1000At timestep:     836 / 1000At timestep:     986 / 1000At timestep:     1136 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -150.0
TimeSinceStart : 157.95351600646973
Exploitation Critic Loss : 3.0427406727540074e-07
Exploration Critic Loss : 5902928.0
Exploration Model Loss : 20.103424072265625
Exploitation Data q-values : 0.019547896459698677
Exploitation OOD q-values : 1.6289106607437134
Exploitation CQL Loss : 3.0427406727540074e-07
Eval_AverageReturn : -141.875
Eval_StdReturn : 21.496728897094727
Eval_MaxReturn : -85.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 142.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 174.457823
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -150.0
TimeSinceStart : 174.4578230381012
Exploitation Critic Loss : 1.772750835016268e-07
Exploration Critic Loss : 5567406.0
Exploration Model Loss : 20.074127197265625
Exploitation Data q-values : 0.016262294724583626
Exploitation OOD q-values : 1.625617265701294
Exploitation CQL Loss : 1.772750835016268e-07
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 190.583710
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -150.0
TimeSinceStart : 190.58371019363403
Exploitation Critic Loss : 1.0373661325502326e-06
Exploration Critic Loss : 5657777.5
Exploration Model Loss : 20.063507080078125
Exploitation Data q-values : 0.014338662847876549
Exploitation OOD q-values : 1.6240509748458862
Exploitation CQL Loss : 1.0373661325502326e-06
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 206.551177
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -150.0
TimeSinceStart : 206.5511772632599
Exploitation Critic Loss : 1.3661749562743353e-06
Exploration Critic Loss : 3350835.0
Exploration Model Loss : 20.058181762695312
Exploitation Data q-values : 0.010227521881461143
Exploitation OOD q-values : 1.6193925142288208
Exploitation CQL Loss : 1.3661749562743353e-06
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 222.413832
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -150.0
TimeSinceStart : 222.413831949234
Exploitation Critic Loss : 8.393689654440095e-07
Exploration Critic Loss : 2196535.25
Exploration Model Loss : 20.056442260742188
Exploitation Data q-values : 0.007602967321872711
Exploitation OOD q-values : 1.617002248764038
Exploitation CQL Loss : 8.393689654440095e-07
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 238.247499
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 238.24749875068665
Exploitation Critic Loss : 4.356328133781062e-07
Exploration Critic Loss : 1463603.75
Exploration Model Loss : 20.055511474609375
Exploitation Data q-values : 0.007333540823310614
Exploitation OOD q-values : 1.6167054176330566
Exploitation CQL Loss : 4.356328133781062e-07
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 254.285271
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 254.28527069091797
Exploitation Critic Loss : 3.633637675193313e-07
Exploration Critic Loss : 671166.9375
Exploration Model Loss : 20.05535888671875
Exploitation Data q-values : 0.006426272913813591
Exploitation OOD q-values : 1.6158037185668945
Exploitation CQL Loss : 3.633637675193313e-07
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 270.355379
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 270.35537934303284
Exploitation Critic Loss : 1.3928461157775018e-06
Exploration Critic Loss : 600973.25
Exploration Model Loss : 20.055068969726562
Exploitation Data q-values : 0.0033009867183864117
Exploitation OOD q-values : 1.6133153438568115
Exploitation CQL Loss : 1.3928461157775018e-06
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 286.691748
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 286.691748380661
Exploitation Critic Loss : 4.897274720860878e-06
Exploration Critic Loss : 487529.5625
Exploration Model Loss : 20.054794311523438
Exploitation Data q-values : 0.005745994858443737
Exploitation OOD q-values : 1.6150462627410889
Exploitation CQL Loss : 4.897274720860878e-06
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 303.053168
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 303.0531675815582
Exploitation Critic Loss : 1.4407930848392425e-07
Exploration Critic Loss : 551039.8125
Exploration Model Loss : 20.054428100585938
Exploitation Data q-values : 0.002425774000585079
Exploitation OOD q-values : 1.6118746995925903
Exploitation CQL Loss : 1.4407930848392425e-07
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 319.438198
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 319.43819785118103
Exploitation Critic Loss : 6.493505821936196e-08
Exploration Critic Loss : 384597.0625
Exploration Model Loss : 20.054092407226562
Exploitation Data q-values : 0.0019210109021514654
Exploitation OOD q-values : 1.611418604850769
Exploitation CQL Loss : 6.493505821936196e-08
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -149.929993
best mean reward -149.929993
running time 335.182322
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -149.92999267578125
Train_BestReturn : -149.92999267578125
TimeSinceStart : 335.1823220252991
Exploitation Critic Loss : 1.0349414480970154e-07
Exploration Critic Loss : 351431.71875
Exploration Model Loss : 20.053787231445312
Exploitation Data q-values : 0.001862152712419629
Exploitation OOD q-values : 1.61130690574646
Exploitation CQL Loss : 1.0349414480970154e-07
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -149.929993
best mean reward -149.929993
running time 351.304147
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -149.92999267578125
Train_BestReturn : -149.92999267578125
TimeSinceStart : 351.30414724349976
Exploitation Critic Loss : 0.0012624142691493034
Exploration Critic Loss : 333229.84375
Exploration Model Loss : 20.053497314453125
Exploitation Data q-values : 0.02781064435839653
Exploitation OOD q-values : 1.6422982215881348
Exploitation CQL Loss : 0.0012624142691493034
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -149.929993
best mean reward -149.929993
running time 367.644485
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -149.92999267578125
Train_BestReturn : -149.92999267578125
TimeSinceStart : 367.6444854736328
Exploitation Critic Loss : 0.00642537884414196
Exploration Critic Loss : 256626.0625
Exploration Model Loss : 20.053146362304688
Exploitation Data q-values : 0.12256083637475967
Exploitation OOD q-values : 1.7393279075622559
Exploitation CQL Loss : 0.00642537884414196
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -149.419998
best mean reward -149.419998
running time 383.880690
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -149.4199981689453
Train_BestReturn : -149.4199981689453
TimeSinceStart : 383.8806903362274
Exploitation Critic Loss : 0.015319007448852062
Exploration Critic Loss : 241302.09375
Exploration Model Loss : 20.052749633789062
Exploitation Data q-values : 0.30668434500694275
Exploitation OOD q-values : 1.9314601421356201
Exploitation CQL Loss : 0.015319007448852062
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -147.149994
best mean reward -147.149994
running time 400.247327
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -147.14999389648438
Train_BestReturn : -147.14999389648438
TimeSinceStart : 400.24732661247253
Exploitation Critic Loss : 0.05863106623291969
Exploration Critic Loss : 255350.765625
Exploration Model Loss : 20.052322387695312
Exploitation Data q-values : 0.5596672296524048
Exploitation OOD q-values : 2.172865629196167
Exploitation CQL Loss : 0.05863106623291969
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -144.720001
best mean reward -144.720001
running time 416.341793
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     685 / 1000At timestep:     835 / 1000At timestep:     889 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -144.72000122070312
Train_BestReturn : -144.72000122070312
TimeSinceStart : 416.3417925834656
Exploitation Critic Loss : 0.19987601041793823
Exploration Critic Loss : 217521.09375
Exploration Model Loss : 20.0518798828125
Exploitation Data q-values : 1.2104928493499756
Exploitation OOD q-values : 2.84456205368042
Exploitation CQL Loss : 0.19987601041793823
Eval_AverageReturn : -129.625
Eval_StdReturn : 36.13148880004883
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 129.875
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -141.979996
best mean reward -141.979996
running time 432.697287
At timestep:     150 / 1000At timestep:     192 / 1000At timestep:     342 / 1000At timestep:     492 / 1000At timestep:     642 / 1000At timestep:     792 / 1000At timestep:     942 / 1000At timestep:     1092 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -141.97999572753906
Train_BestReturn : -141.97999572753906
TimeSinceStart : 432.69728660583496
Exploitation Critic Loss : 0.9223589897155762
Exploration Critic Loss : 185582.296875
Exploration Model Loss : 20.051406860351562
Exploitation Data q-values : 2.4045138359069824
Exploitation OOD q-values : 4.133813858032227
Exploitation CQL Loss : 0.9223589897155762
Eval_AverageReturn : -136.375
Eval_StdReturn : 36.048362731933594
Eval_MaxReturn : -41.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 136.5
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -136.179993
best mean reward -136.179993
running time 448.914757
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     661 / 1000At timestep:     774 / 1000At timestep:     924 / 1000At timestep:     1074 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -136.17999267578125
Train_BestReturn : -136.17999267578125
TimeSinceStart : 448.91475677490234
Exploitation Critic Loss : 1.3345085382461548
Exploration Critic Loss : 178797.9375
Exploration Model Loss : 20.050888061523438
Exploitation Data q-values : 3.8387346267700195
Exploitation OOD q-values : 5.5633697509765625
Exploitation CQL Loss : 1.3345085382461548
Eval_AverageReturn : -134.0
Eval_StdReturn : 30.610456466674805
Eval_MaxReturn : -60.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 134.25
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -133.960007
best mean reward -133.960007
running time 465.757674
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     394 / 1000At timestep:     506 / 1000At timestep:     656 / 1000At timestep:     806 / 1000At timestep:     956 / 1000At timestep:     1106 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -133.9600067138672
Train_BestReturn : -133.9600067138672
TimeSinceStart : 465.75767374038696
Exploitation Critic Loss : 3.1899478435516357
Exploration Critic Loss : 160709.0625
Exploration Model Loss : 20.050460815429688
Exploitation Data q-values : 6.237099647521973
Exploitation OOD q-values : 8.114056587219238
Exploitation CQL Loss : 3.1899478435516357
Eval_AverageReturn : -138.0
Eval_StdReturn : 21.266170501708984
Eval_MaxReturn : -93.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.25
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -132.800003
best mean reward -132.800003
running time 482.338503
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -132.8000030517578
Train_BestReturn : -132.8000030517578
TimeSinceStart : 482.3385031223297
Exploitation Critic Loss : 4.126049995422363
Exploration Critic Loss : 102224.28125
Exploration Model Loss : 20.050064086914062
Exploitation Data q-values : 7.868668556213379
Exploitation OOD q-values : 9.991479873657227
Exploitation CQL Loss : 4.126049995422363
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -130.979996
best mean reward -130.979996
running time 498.037097
At timestep:     150 / 1000At timestep:     235 / 1000At timestep:     304 / 1000At timestep:     454 / 1000At timestep:     590 / 1000At timestep:     673 / 1000At timestep:     823 / 1000At timestep:     853 / 1000At timestep:     981 / 1000At timestep:     1131 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -130.97999572753906
Train_BestReturn : -130.97999572753906
TimeSinceStart : 498.03709745407104
Exploitation Critic Loss : 8.728413581848145
Exploration Critic Loss : 93249.796875
Exploration Model Loss : 20.049606323242188
Exploitation Data q-values : 11.96354866027832
Exploitation OOD q-values : 13.989802360534668
Exploitation CQL Loss : 8.728413581848145
Eval_AverageReturn : -112.5
Eval_StdReturn : 41.27529525756836
Eval_MaxReturn : -29.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 113.1
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -126.680000
best mean reward -126.680000
running time 513.327991
At timestep:     76 / 1000At timestep:     168 / 1000At timestep:     240 / 1000At timestep:     314 / 1000At timestep:     365 / 1000At timestep:     455 / 1000At timestep:     560 / 1000At timestep:     639 / 1000At timestep:     674 / 1000At timestep:     783 / 1000At timestep:     843 / 1000At timestep:     957 / 1000At timestep:     999 / 1000At timestep:     1089 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -126.68000030517578
Train_BestReturn : -126.68000030517578
TimeSinceStart : 513.3279914855957
Exploitation Critic Loss : 6.995427131652832
Exploration Critic Loss : 90710.5
Exploration Model Loss : 20.049148559570312
Exploitation Data q-values : 13.206927299499512
Exploitation OOD q-values : 15.15169906616211
Exploitation CQL Loss : 6.995427131652832
Eval_AverageReturn : -76.78571319580078
Eval_StdReturn : 23.4891414642334
Eval_MaxReturn : -34.0
Eval_MinReturn : -113.0
Eval_AverageEpLen : 77.78571428571429
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -115.169998
best mean reward -115.169998
running time 529.101465
At timestep:     150 / 1000At timestep:     228 / 1000At timestep:     378 / 1000At timestep:     520 / 1000At timestep:     566 / 1000At timestep:     716 / 1000At timestep:     794 / 1000At timestep:     944 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -115.16999816894531
Train_BestReturn : -115.16999816894531
TimeSinceStart : 529.1014652252197
Exploitation Critic Loss : 5.088159561157227
Exploration Critic Loss : 73956.6171875
Exploration Model Loss : 20.048751831054688
Exploitation Data q-values : 14.843174934387207
Exploitation OOD q-values : 16.531986236572266
Exploitation CQL Loss : 5.088159561157227
Eval_AverageReturn : -113.33333587646484
Eval_StdReturn : 40.227134704589844
Eval_MaxReturn : -45.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 113.88888888888889
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -100.709999
best mean reward -100.709999
running time 544.069851
At timestep:     65 / 1000At timestep:     105 / 1000At timestep:     207 / 1000At timestep:     242 / 1000At timestep:     288 / 1000At timestep:     364 / 1000At timestep:     513 / 1000At timestep:     622 / 1000At timestep:     683 / 1000At timestep:     726 / 1000At timestep:     762 / 1000At timestep:     873 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -100.70999908447266
Train_BestReturn : -100.70999908447266
TimeSinceStart : 544.0698511600494
Exploitation Critic Loss : 4.939969062805176
Exploration Critic Loss : 54317.703125
Exploration Model Loss : 20.048294067382812
Exploitation Data q-values : 16.327512741088867
Exploitation OOD q-values : 18.366119384765625
Exploitation CQL Loss : 4.939969062805176
Eval_AverageReturn : -76.38461303710938
Eval_StdReturn : 37.67786407470703
Eval_MaxReturn : -34.0
Eval_MinReturn : -148.0
Eval_AverageEpLen : 77.38461538461539
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -86.449997
best mean reward -86.449997
running time 560.062846
At timestep:     42 / 1000At timestep:     90 / 1000At timestep:     120 / 1000At timestep:     158 / 1000At timestep:     220 / 1000At timestep:     269 / 1000At timestep:     321 / 1000At timestep:     365 / 1000At timestep:     413 / 1000At timestep:     439 / 1000At timestep:     478 / 1000At timestep:     538 / 1000At timestep:     567 / 1000At timestep:     622 / 1000At timestep:     664 / 1000At timestep:     701 / 1000At timestep:     730 / 1000At timestep:     786 / 1000At timestep:     826 / 1000At timestep:     879 / 1000At timestep:     909 / 1000At timestep:     946 / 1000At timestep:     993 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -86.44999694824219
Train_BestReturn : -86.44999694824219
TimeSinceStart : 560.0628461837769
Exploitation Critic Loss : 12.01986026763916
Exploration Critic Loss : 71962.328125
Exploration Model Loss : 20.047866821289062
Exploitation Data q-values : 16.308246612548828
Exploitation OOD q-values : 18.148387908935547
Exploitation CQL Loss : 12.01986026763916
Eval_AverageReturn : -41.45833206176758
Eval_StdReturn : 10.551379203796387
Eval_MaxReturn : -25.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 42.458333333333336
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -75.879997
best mean reward -75.879997
running time 576.763200
At timestep:     47 / 1000At timestep:     69 / 1000At timestep:     110 / 1000At timestep:     174 / 1000At timestep:     208 / 1000At timestep:     265 / 1000At timestep:     314 / 1000At timestep:     358 / 1000At timestep:     392 / 1000At timestep:     421 / 1000At timestep:     486 / 1000At timestep:     531 / 1000At timestep:     553 / 1000At timestep:     596 / 1000At timestep:     645 / 1000At timestep:     710 / 1000At timestep:     800 / 1000At timestep:     829 / 1000At timestep:     875 / 1000At timestep:     949 / 1000At timestep:     984 / 1000At timestep:     1055 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -75.87999725341797
Train_BestReturn : -75.87999725341797
TimeSinceStart : 576.7631995677948
Exploitation Critic Loss : 10.73092269897461
Exploration Critic Loss : 32102.958984375
Exploration Model Loss : 20.04736328125
Exploitation Data q-values : 18.990631103515625
Exploitation OOD q-values : 21.093311309814453
Exploitation CQL Loss : 10.73092269897461
Eval_AverageReturn : -46.95454406738281
Eval_StdReturn : 17.353221893310547
Eval_MaxReturn : -21.0
Eval_MinReturn : -89.0
Eval_AverageEpLen : 47.95454545454545
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -57.930000
best mean reward -57.930000
running time 593.725759
At timestep:     26 / 1000At timestep:     54 / 1000At timestep:     97 / 1000At timestep:     155 / 1000At timestep:     209 / 1000At timestep:     251 / 1000At timestep:     288 / 1000At timestep:     333 / 1000At timestep:     379 / 1000At timestep:     419 / 1000At timestep:     480 / 1000At timestep:     553 / 1000At timestep:     610 / 1000At timestep:     672 / 1000At timestep:     708 / 1000At timestep:     733 / 1000At timestep:     791 / 1000At timestep:     869 / 1000At timestep:     898 / 1000At timestep:     949 / 1000At timestep:     986 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -57.93000030517578
Train_BestReturn : -57.93000030517578
TimeSinceStart : 593.7257590293884
Exploitation Critic Loss : 12.068611145019531
Exploration Critic Loss : 30256.9609375
Exploration Model Loss : 20.04693603515625
Exploitation Data q-values : 21.890195846557617
Exploitation OOD q-values : 23.971904754638672
Exploitation CQL Loss : 12.068611145019531
Eval_AverageReturn : -45.54545593261719
Eval_StdReturn : 14.408732414245605
Eval_MaxReturn : -24.0
Eval_MinReturn : -77.0
Eval_AverageEpLen : 46.54545454545455
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -50.520000
best mean reward -50.520000
running time 610.540252
At timestep:     27 / 1000At timestep:     49 / 1000At timestep:     94 / 1000At timestep:     119 / 1000At timestep:     157 / 1000At timestep:     199 / 1000At timestep:     231 / 1000At timestep:     292 / 1000At timestep:     325 / 1000At timestep:     371 / 1000At timestep:     426 / 1000At timestep:     487 / 1000At timestep:     546 / 1000At timestep:     579 / 1000At timestep:     618 / 1000At timestep:     688 / 1000At timestep:     724 / 1000At timestep:     759 / 1000At timestep:     801 / 1000At timestep:     858 / 1000At timestep:     912 / 1000At timestep:     960 / 1000At timestep:     993 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -50.52000045776367
Train_BestReturn : -50.52000045776367
TimeSinceStart : 610.5402524471283
Exploitation Critic Loss : 7.589878082275391
Exploration Critic Loss : 19094.33203125
Exploration Model Loss : 20.046554565429688
Exploitation Data q-values : 25.29859161376953
Exploitation OOD q-values : 27.835100173950195
Exploitation CQL Loss : 7.589878082275391
Eval_AverageReturn : -41.625
Eval_StdReturn : 12.740396499633789
Eval_MaxReturn : -21.0
Eval_MinReturn : -69.0
Eval_AverageEpLen : 42.625
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -49.430000
best mean reward -49.430000
running time 626.899628
At timestep:     46 / 1000At timestep:     72 / 1000At timestep:     121 / 1000At timestep:     175 / 1000At timestep:     233 / 1000At timestep:     315 / 1000At timestep:     344 / 1000At timestep:     406 / 1000At timestep:     453 / 1000At timestep:     501 / 1000At timestep:     551 / 1000At timestep:     579 / 1000At timestep:     617 / 1000At timestep:     648 / 1000At timestep:     687 / 1000At timestep:     720 / 1000At timestep:     785 / 1000At timestep:     845 / 1000At timestep:     914 / 1000At timestep:     958 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -49.43000030517578
Train_BestReturn : -49.43000030517578
TimeSinceStart : 626.8996279239655
Exploitation Critic Loss : 19.362998962402344
Exploration Critic Loss : 27725.203125
Exploration Model Loss : 20.04608154296875
Exploitation Data q-values : 26.60793685913086
Exploitation OOD q-values : 29.11426544189453
Exploitation CQL Loss : 19.362998962402344
Eval_AverageReturn : -48.14285659790039
Eval_StdReturn : 15.394140243530273
Eval_MaxReturn : -25.0
Eval_MinReturn : -81.0
Eval_AverageEpLen : 49.142857142857146
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -50.740002
best mean reward -49.430000
running time 643.309976
At timestep:     39 / 1000At timestep:     84 / 1000At timestep:     136 / 1000At timestep:     175 / 1000At timestep:     203 / 1000At timestep:     242 / 1000At timestep:     288 / 1000At timestep:     335 / 1000At timestep:     378 / 1000At timestep:     410 / 1000At timestep:     454 / 1000At timestep:     489 / 1000At timestep:     528 / 1000At timestep:     556 / 1000At timestep:     593 / 1000At timestep:     628 / 1000At timestep:     682 / 1000At timestep:     726 / 1000At timestep:     758 / 1000At timestep:     803 / 1000At timestep:     842 / 1000At timestep:     891 / 1000At timestep:     911 / 1000At timestep:     961 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -50.7400016784668
Train_BestReturn : -49.43000030517578
TimeSinceStart : 643.309975862503
Exploitation Critic Loss : 27.967079162597656
Exploration Critic Loss : 23518.25
Exploration Model Loss : 20.045730590820312
Exploitation Data q-values : 28.044492721557617
Exploitation OOD q-values : 30.21122169494629
Exploitation CQL Loss : 27.967079162597656
Eval_AverageReturn : -39.279998779296875
Eval_StdReturn : 8.027552604675293
Eval_MaxReturn : -19.0
Eval_MinReturn : -53.0
Eval_AverageEpLen : 40.28
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -49.689999
best mean reward -49.430000
running time 660.301756
At timestep:     32 / 1000At timestep:     52 / 1000At timestep:     80 / 1000At timestep:     125 / 1000At timestep:     162 / 1000At timestep:     212 / 1000At timestep:     252 / 1000At timestep:     293 / 1000At timestep:     339 / 1000At timestep:     369 / 1000At timestep:     407 / 1000At timestep:     442 / 1000At timestep:     471 / 1000At timestep:     509 / 1000At timestep:     552 / 1000At timestep:     620 / 1000At timestep:     661 / 1000At timestep:     693 / 1000At timestep:     742 / 1000At timestep:     824 / 1000At timestep:     868 / 1000At timestep:     925 / 1000At timestep:     950 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -49.689998626708984
Train_BestReturn : -49.43000030517578
TimeSinceStart : 660.30175614357
Exploitation Critic Loss : 10.664972305297852
Exploration Critic Loss : 17662.05859375
Exploration Model Loss : 20.045318603515625
Exploitation Data q-values : 34.19647216796875
Exploitation OOD q-values : 36.49183654785156
Exploitation CQL Loss : 10.664972305297852
Eval_AverageReturn : -41.25
Eval_StdReturn : 14.024532318115234
Eval_MaxReturn : -19.0
Eval_MinReturn : -81.0
Eval_AverageEpLen : 42.25
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -48.990002
best mean reward -48.990002
running time 677.332220
At timestep:     62 / 1000At timestep:     84 / 1000At timestep:     147 / 1000At timestep:     174 / 1000At timestep:     220 / 1000At timestep:     363 / 1000At timestep:     439 / 1000At timestep:     488 / 1000At timestep:     610 / 1000At timestep:     684 / 1000At timestep:     772 / 1000At timestep:     863 / 1000At timestep:     947 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -48.9900016784668
Train_BestReturn : -48.9900016784668
TimeSinceStart : 677.3322200775146
Exploitation Critic Loss : 7.574008464813232
Exploration Critic Loss : 10280.2734375
Exploration Model Loss : 20.044876098632812
Exploitation Data q-values : 34.66303253173828
Exploitation OOD q-values : 36.9078369140625
Exploitation CQL Loss : 7.574008464813232
Eval_AverageReturn : -71.64286041259766
Eval_StdReturn : 31.882835388183594
Eval_MaxReturn : -21.0
Eval_MinReturn : -142.0
Eval_AverageEpLen : 72.64285714285714
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -49.709999
best mean reward -48.990002
running time 693.008734
At timestep:     31 / 1000At timestep:     60 / 1000At timestep:     81 / 1000At timestep:     103 / 1000At timestep:     137 / 1000At timestep:     192 / 1000At timestep:     240 / 1000At timestep:     272 / 1000At timestep:     308 / 1000At timestep:     350 / 1000At timestep:     380 / 1000At timestep:     411 / 1000At timestep:     481 / 1000At timestep:     508 / 1000At timestep:     546 / 1000At timestep:     592 / 1000At timestep:     664 / 1000At timestep:     690 / 1000At timestep:     718 / 1000At timestep:     749 / 1000At timestep:     776 / 1000At timestep:     804 / 1000At timestep:     858 / 1000At timestep:     904 / 1000At timestep:     958 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -49.709999084472656
Train_BestReturn : -48.9900016784668
TimeSinceStart : 693.0087335109711
Exploitation Critic Loss : 13.747221946716309
Exploration Critic Loss : 6691.4833984375
Exploration Model Loss : 20.044387817382812
Exploitation Data q-values : 34.84412384033203
Exploitation OOD q-values : 37.09278869628906
Exploitation CQL Loss : 13.747221946716309
Eval_AverageReturn : -37.57692337036133
Eval_StdReturn : 13.50832462310791
Eval_MaxReturn : -20.0
Eval_MinReturn : -71.0
Eval_AverageEpLen : 38.57692307692308
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -47.419998
best mean reward -47.419998
running time 709.975237
At timestep:     38 / 1000At timestep:     82 / 1000At timestep:     115 / 1000At timestep:     169 / 1000At timestep:     220 / 1000At timestep:     244 / 1000At timestep:     294 / 1000At timestep:     323 / 1000At timestep:     373 / 1000At timestep:     404 / 1000At timestep:     441 / 1000At timestep:     462 / 1000At timestep:     501 / 1000At timestep:     538 / 1000At timestep:     569 / 1000At timestep:     598 / 1000At timestep:     625 / 1000At timestep:     649 / 1000At timestep:     707 / 1000At timestep:     776 / 1000At timestep:     807 / 1000At timestep:     844 / 1000At timestep:     889 / 1000At timestep:     929 / 1000At timestep:     961 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -47.41999816894531
Train_BestReturn : -47.41999816894531
TimeSinceStart : 709.9752366542816
Exploitation Critic Loss : 4.847434997558594
Exploration Critic Loss : 4739.0087890625
Exploration Model Loss : 20.043930053710938
Exploitation Data q-values : 34.459434509277344
Exploitation OOD q-values : 36.491451263427734
Exploitation CQL Loss : 4.847434997558594
Eval_AverageReturn : -38.42307662963867
Eval_StdReturn : 12.344955444335938
Eval_MaxReturn : -20.0
Eval_MinReturn : -68.0
Eval_AverageEpLen : 39.42307692307692
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -44.020000
best mean reward -44.020000
running time 727.264302
At timestep:     24 / 1000At timestep:     46 / 1000At timestep:     67 / 1000At timestep:     85 / 1000At timestep:     102 / 1000At timestep:     138 / 1000At timestep:     170 / 1000At timestep:     199 / 1000At timestep:     225 / 1000At timestep:     263 / 1000At timestep:     295 / 1000At timestep:     329 / 1000At timestep:     347 / 1000At timestep:     373 / 1000At timestep:     406 / 1000At timestep:     448 / 1000At timestep:     480 / 1000At timestep:     501 / 1000At timestep:     535 / 1000At timestep:     564 / 1000At timestep:     595 / 1000At timestep:     612 / 1000At timestep:     651 / 1000At timestep:     681 / 1000At timestep:     706 / 1000At timestep:     736 / 1000At timestep:     772 / 1000At timestep:     808 / 1000At timestep:     848 / 1000At timestep:     881 / 1000At timestep:     934 / 1000At timestep:     975 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -44.02000045776367
Train_BestReturn : -44.02000045776367
TimeSinceStart : 727.2643024921417
Exploitation Critic Loss : 13.595946311950684
Exploration Critic Loss : 5050.9365234375
Exploration Model Loss : 20.043411254882812
Exploitation Data q-values : 37.950714111328125
Exploitation OOD q-values : 40.012123107910156
Exploitation CQL Loss : 13.595946311950684
Eval_AverageReturn : -29.303030014038086
Eval_StdReturn : 8.125846862792969
Eval_MaxReturn : -16.0
Eval_MinReturn : -52.0
Eval_AverageEpLen : 30.303030303030305
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -44.759998
best mean reward -44.020000
running time 745.338984
At timestep:     28 / 1000At timestep:     55 / 1000At timestep:     78 / 1000At timestep:     107 / 1000At timestep:     149 / 1000At timestep:     179 / 1000At timestep:     208 / 1000At timestep:     231 / 1000At timestep:     262 / 1000At timestep:     282 / 1000At timestep:     323 / 1000At timestep:     362 / 1000At timestep:     393 / 1000At timestep:     423 / 1000At timestep:     456 / 1000At timestep:     483 / 1000At timestep:     511 / 1000At timestep:     545 / 1000At timestep:     581 / 1000At timestep:     608 / 1000At timestep:     640 / 1000At timestep:     682 / 1000At timestep:     718 / 1000At timestep:     744 / 1000At timestep:     777 / 1000At timestep:     806 / 1000At timestep:     831 / 1000At timestep:     880 / 1000At timestep:     900 / 1000At timestep:     928 / 1000At timestep:     968 / 1000At timestep:     995 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -44.7599983215332
Train_BestReturn : -44.02000045776367
TimeSinceStart : 745.3389840126038
Exploitation Critic Loss : 19.88337516784668
Exploration Critic Loss : 6754.64501953125
Exploration Model Loss : 20.04296875
Exploitation Data q-values : 36.35540008544922
Exploitation OOD q-values : 38.269081115722656
Exploitation CQL Loss : 19.88337516784668
Eval_AverageReturn : -30.060606002807617
Eval_StdReturn : 6.550222873687744
Eval_MaxReturn : -19.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 31.060606060606062
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -42.849998
best mean reward -42.849998
running time 762.544748
At timestep:     37 / 1000At timestep:     90 / 1000At timestep:     108 / 1000At timestep:     143 / 1000At timestep:     174 / 1000At timestep:     212 / 1000At timestep:     252 / 1000At timestep:     300 / 1000At timestep:     339 / 1000At timestep:     376 / 1000At timestep:     413 / 1000At timestep:     460 / 1000At timestep:     500 / 1000At timestep:     553 / 1000At timestep:     593 / 1000At timestep:     657 / 1000At timestep:     693 / 1000At timestep:     714 / 1000At timestep:     739 / 1000At timestep:     779 / 1000At timestep:     838 / 1000At timestep:     864 / 1000At timestep:     898 / 1000At timestep:     946 / 1000At timestep:     991 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -42.849998474121094
Train_BestReturn : -42.849998474121094
TimeSinceStart : 762.5447483062744
Exploitation Critic Loss : 16.165416717529297
Exploration Critic Loss : 5311.2900390625
Exploration Model Loss : 20.042556762695312
Exploitation Data q-values : 37.53221130371094
Exploitation OOD q-values : 39.44685363769531
Exploitation CQL Loss : 16.165416717529297
Eval_AverageReturn : -38.269229888916016
Eval_StdReturn : 10.775891304016113
Eval_MaxReturn : -17.0
Eval_MinReturn : -63.0
Eval_AverageEpLen : 39.26923076923077
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -40.709999
best mean reward -40.709999
running time 778.020331
At timestep:     38 / 1000At timestep:     68 / 1000At timestep:     111 / 1000At timestep:     152 / 1000At timestep:     179 / 1000At timestep:     219 / 1000At timestep:     263 / 1000At timestep:     307 / 1000At timestep:     388 / 1000At timestep:     433 / 1000At timestep:     478 / 1000At timestep:     525 / 1000At timestep:     546 / 1000At timestep:     597 / 1000At timestep:     636 / 1000At timestep:     678 / 1000At timestep:     700 / 1000At timestep:     746 / 1000At timestep:     789 / 1000At timestep:     826 / 1000At timestep:     854 / 1000At timestep:     879 / 1000At timestep:     922 / 1000At timestep:     976 / 1000At timestep:     1010 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q3_medium_dqn_PointmassMedium-v0_21-11-2022_22-49-35/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -40.709999084472656
Train_BestReturn : -40.709999084472656
TimeSinceStart : 778.0203306674957
Exploitation Critic Loss : 11.555279731750488
Exploration Critic Loss : 3567.021484375
Exploration Model Loss : 20.04217529296875
Exploitation Data q-values : 37.79274368286133
Exploitation OOD q-values : 39.67484664916992
Exploitation CQL Loss : 11.555279731750488
Eval_AverageReturn : -39.400001525878906
Eval_StdReturn : 11.943199157714844
Eval_MaxReturn : -20.0
Eval_MinReturn : -80.0
Eval_AverageEpLen : 40.4
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_medium_cql_PointmassMedium-v0_21-11-2022_23-02-53 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_medium_cql_PointmassMedium-v0_21-11-2022_23-02-53
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001994
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0019941329956054688
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -137.857147
best mean reward -inf
running time 4.843678
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -137.85714721679688
TimeSinceStart : 4.84367823600769
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -143.461533
best mean reward -inf
running time 9.560052
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -143.46153259277344
TimeSinceStart : 9.560051918029785
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -145.750000
best mean reward -inf
running time 21.098094
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -145.75
TimeSinceStart : 21.098093509674072
Exploitation Critic Loss : 0.14688456058502197
Exploration Critic Loss : 834171.5
Exploration Model Loss : 55.050113677978516
Exploitation Data q-values : 0.5518813133239746
Exploitation OOD q-values : 2.108457565307617
Exploitation CQL Loss : 1.7034610509872437
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -146.851852
best mean reward -inf
running time 32.254242
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -146.8518524169922
TimeSinceStart : 32.25424242019653
Exploitation Critic Loss : 0.14481985569000244
Exploration Critic Loss : 5789817.0
Exploration Model Loss : 54.920345306396484
Exploitation Data q-values : 0.8295589089393616
Exploitation OOD q-values : 2.279047727584839
Exploitation CQL Loss : 1.594308614730835
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -147.424240
best mean reward -inf
running time 43.372702
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -147.4242401123047
TimeSinceStart : 43.37270188331604
Exploitation Critic Loss : 1.4253278970718384
Exploration Critic Loss : 4603852.0
Exploration Model Loss : 42.3551025390625
Exploitation Data q-values : 1.5916807651519775
Exploitation OOD q-values : 3.0752995014190674
Exploitation CQL Loss : 2.9089465141296387
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -147.875000
best mean reward -inf
running time 54.562921
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -147.875
TimeSinceStart : 54.562920570373535
Exploitation Critic Loss : 0.8608835339546204
Exploration Critic Loss : 6467366.0
Exploration Model Loss : 42.69654846191406
Exploitation Data q-values : 2.3230040073394775
Exploitation OOD q-values : 3.838253974914551
Exploitation CQL Loss : 2.376133680343628
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -148.191483
best mean reward -inf
running time 65.880719
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -148.1914825439453
TimeSinceStart : 65.88071918487549
Exploitation Critic Loss : 64.95237731933594
Exploration Critic Loss : 5589999.0
Exploration Model Loss : 34.2049560546875
Exploitation Data q-values : 4.455516815185547
Exploitation OOD q-values : 6.8747148513793945
Exploitation CQL Loss : 67.37157440185547
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -148.396225
best mean reward -inf
running time 76.838545
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -148.39622497558594
TimeSinceStart : 76.83854484558105
Exploitation Critic Loss : 3.523134469985962
Exploration Critic Loss : 6235837.0
Exploration Model Loss : 32.37019348144531
Exploitation Data q-values : 5.915395736694336
Exploitation OOD q-values : 8.023820877075195
Exploitation CQL Loss : 5.631558895111084
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -148.583328
best mean reward -inf
running time 88.199561
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -148.5833282470703
TimeSinceStart : 88.19956111907959
Exploitation Critic Loss : 34.72038269042969
Exploration Critic Loss : 7300267.0
Exploration Model Loss : 25.555862426757812
Exploitation Data q-values : 13.28524398803711
Exploitation OOD q-values : 19.006561279296875
Exploitation CQL Loss : 40.44169998168945
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -148.731339
best mean reward -inf
running time 99.309900
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -148.73133850097656
TimeSinceStart : 99.30989980697632
Exploitation Critic Loss : 218.29510498046875
Exploration Critic Loss : 6438813.0
Exploration Model Loss : 23.977920532226562
Exploitation Data q-values : 32.11073303222656
Exploitation OOD q-values : 46.529380798339844
Exploitation CQL Loss : 232.7137451171875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -148.835617
best mean reward -inf
running time 110.504822
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -148.8356170654297
TimeSinceStart : 110.50482249259949
Exploitation Critic Loss : 268.7257080078125
Exploration Critic Loss : 6926634.5
Exploration Model Loss : 23.469894409179688
Exploitation Data q-values : 62.021507263183594
Exploitation OOD q-values : 85.2273178100586
Exploitation CQL Loss : 291.9315185546875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -148.937500
best mean reward -inf
running time 121.465513
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -148.9375
TimeSinceStart : 121.4655134677887
Exploitation Critic Loss : 1039.65966796875
Exploration Critic Loss : 5814698.0
Exploration Model Loss : 23.291427612304688
Exploitation Data q-values : 140.13743591308594
Exploitation OOD q-values : 187.86004638671875
Exploitation CQL Loss : 1087.38232421875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -149.022995
best mean reward -inf
running time 132.779643
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -149.0229949951172
TimeSinceStart : 132.7796425819397
Exploitation Critic Loss : 4026.41064453125
Exploration Critic Loss : 7038135.0
Exploration Model Loss : 23.226104736328125
Exploitation Data q-values : 286.64617919921875
Exploitation OOD q-values : 351.43890380859375
Exploitation CQL Loss : 4091.203369140625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -149.086029
best mean reward -inf
running time 143.762524
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -149.08602905273438
TimeSinceStart : 143.7625241279602
Exploitation Critic Loss : 5857.734375
Exploration Critic Loss : 6070637.0
Exploration Model Loss : 23.201644897460938
Exploitation Data q-values : 433.51617431640625
Exploitation OOD q-values : 520.0198364257812
Exploitation CQL Loss : 5944.23779296875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -149.149994
best mean reward -inf
running time 154.504685
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -149.14999389648438
TimeSinceStart : 154.50468516349792
Exploitation Critic Loss : 9693.5224609375
Exploration Critic Loss : 2798960.5
Exploration Model Loss : 23.192413330078125
Exploitation Data q-values : 685.138916015625
Exploitation OOD q-values : 792.4815673828125
Exploitation CQL Loss : 9800.865234375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 165.252328
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 165.25232815742493
Exploitation Critic Loss : 14687.740234375
Exploration Critic Loss : 5210292.0
Exploration Model Loss : 23.189422607421875
Exploitation Data q-values : 809.3952026367188
Exploitation OOD q-values : 908.4976806640625
Exploitation CQL Loss : 14786.8427734375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 175.893247
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 175.89324712753296
Exploitation Critic Loss : 19328.779296875
Exploration Critic Loss : 2039574.625
Exploration Model Loss : 23.18841552734375
Exploitation Data q-values : 992.0108642578125
Exploitation OOD q-values : 1121.53271484375
Exploitation CQL Loss : 19458.30078125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 186.709410
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 186.70941019058228
Exploitation Critic Loss : 63634.42578125
Exploration Critic Loss : 2618735.0
Exploration Model Loss : 23.1883544921875
Exploitation Data q-values : 1340.578125
Exploitation OOD q-values : 1554.5367431640625
Exploitation CQL Loss : 63848.3828125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 197.522138
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 197.52213764190674
Exploitation Critic Loss : 85994.2109375
Exploration Critic Loss : 679217.75
Exploration Model Loss : 23.188507080078125
Exploitation Data q-values : 1775.177978515625
Exploitation OOD q-values : 2043.507080078125
Exploitation CQL Loss : 86262.5390625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 208.259888
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 208.2598876953125
Exploitation Critic Loss : 127444.1015625
Exploration Critic Loss : 704978.25
Exploration Model Loss : 23.188507080078125
Exploitation Data q-values : 2277.2861328125
Exploitation OOD q-values : 2569.734375
Exploitation CQL Loss : 127736.546875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 218.890574
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 218.8905735015869
Exploitation Critic Loss : 76382.328125
Exploration Critic Loss : 426731.5625
Exploration Model Loss : 23.188491821289062
Exploitation Data q-values : 2738.180419921875
Exploitation OOD q-values : 2927.256103515625
Exploitation CQL Loss : 76571.40625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 229.550076
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 229.5500762462616
Exploitation Critic Loss : 33296.6015625
Exploration Critic Loss : 486829.875
Exploration Model Loss : 23.188751220703125
Exploitation Data q-values : 2652.10595703125
Exploitation OOD q-values : 2762.143798828125
Exploitation CQL Loss : 33406.640625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -150.000000
best mean reward -150.000000
running time 240.054849
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -150.0
Train_BestReturn : -150.0
TimeSinceStart : 240.05484867095947
Exploitation Critic Loss : 6577.603515625
Exploration Critic Loss : 359022.84375
Exploration Model Loss : 23.18902587890625
Exploitation Data q-values : 2495.49658203125
Exploitation OOD q-values : 2559.0
Exploitation CQL Loss : 6641.10693359375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -149.110001
best mean reward -149.110001
running time 250.834946
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -149.11000061035156
Train_BestReturn : -149.11000061035156
TimeSinceStart : 250.83494591712952
Exploitation Critic Loss : 21319.25390625
Exploration Critic Loss : 394759.0625
Exploration Model Loss : 23.1893310546875
Exploitation Data q-values : 2120.5537109375
Exploitation OOD q-values : 2156.04541015625
Exploitation CQL Loss : 21354.74609375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -149.110001
best mean reward -149.110001
running time 261.623227
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -149.11000061035156
Train_BestReturn : -149.11000061035156
TimeSinceStart : 261.62322664260864
Exploitation Critic Loss : 19872.162109375
Exploration Critic Loss : 357734.0
Exploration Model Loss : 23.1895751953125
Exploitation Data q-values : 1854.18115234375
Exploitation OOD q-values : 1877.4112548828125
Exploitation CQL Loss : 19895.392578125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -148.250000
best mean reward -148.250000
running time 272.336026
At timestep:     150 / 1000At timestep:     211 / 1000At timestep:     361 / 1000At timestep:     511 / 1000At timestep:     661 / 1000At timestep:     811 / 1000At timestep:     938 / 1000At timestep:     1088 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -148.25
Train_BestReturn : -148.25
TimeSinceStart : 272.33602595329285
Exploitation Critic Loss : 2398.603759765625
Exploration Critic Loss : 252825.84375
Exploration Model Loss : 23.18988037109375
Exploitation Data q-values : 1607.109619140625
Exploitation OOD q-values : 1631.3968505859375
Exploitation CQL Loss : 2422.89111328125
Eval_AverageReturn : -135.75
Eval_StdReturn : 29.68901252746582
Eval_MaxReturn : -60.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 136.0
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -147.580002
best mean reward -147.580002
running time 283.070489
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     439 / 1000At timestep:     589 / 1000At timestep:     686 / 1000At timestep:     757 / 1000At timestep:     907 / 1000At timestep:     975 / 1000At timestep:     1125 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -147.5800018310547
Train_BestReturn : -147.5800018310547
TimeSinceStart : 283.0704891681671
Exploitation Critic Loss : 93757.0078125
Exploration Critic Loss : 344093.4375
Exploration Model Loss : 23.190200805664062
Exploitation Data q-values : 1361.76708984375
Exploitation OOD q-values : 1378.28271484375
Exploitation CQL Loss : 93773.5234375
Eval_AverageReturn : -124.55555725097656
Eval_StdReturn : 34.192626953125
Eval_MaxReturn : -67.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 125.0
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -147.580002
best mean reward -147.580002
running time 293.993194
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -147.5800018310547
Train_BestReturn : -147.5800018310547
TimeSinceStart : 293.9931938648224
Exploitation Critic Loss : 1555.634765625
Exploration Critic Loss : 218718.46875
Exploration Model Loss : 23.1904296875
Exploitation Data q-values : 1182.8397216796875
Exploitation OOD q-values : 1200.1220703125
Exploitation CQL Loss : 1572.9171142578125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -144.520004
best mean reward -144.520004
running time 304.796501
At timestep:     97 / 1000At timestep:     202 / 1000At timestep:     352 / 1000At timestep:     502 / 1000At timestep:     627 / 1000At timestep:     743 / 1000At timestep:     893 / 1000At timestep:     948 / 1000At timestep:     1098 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -144.52000427246094
Train_BestReturn : -144.52000427246094
TimeSinceStart : 304.7965006828308
Exploitation Critic Loss : 15345.271484375
Exploration Critic Loss : 276927.25
Exploration Model Loss : 23.190444946289062
Exploitation Data q-values : 1036.0589599609375
Exploitation OOD q-values : 1051.57958984375
Exploitation CQL Loss : 15360.7919921875
Eval_AverageReturn : -121.44444274902344
Eval_StdReturn : 31.287593841552734
Eval_MaxReturn : -54.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 122.0
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -143.470001
best mean reward -143.470001
running time 315.735826
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -143.47000122070312
Train_BestReturn : -143.47000122070312
TimeSinceStart : 315.735826253891
Exploitation Critic Loss : 5838.482421875
Exploration Critic Loss : 204588.609375
Exploration Model Loss : 23.190750122070312
Exploitation Data q-values : 891.5478515625
Exploitation OOD q-values : 915.6494140625
Exploitation CQL Loss : 5862.583984375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -141.559998
best mean reward -141.559998
running time 326.526548
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -141.55999755859375
Train_BestReturn : -141.55999755859375
TimeSinceStart : 326.52654790878296
Exploitation Critic Loss : 20188.505859375
Exploration Critic Loss : 181304.0
Exploration Model Loss : 23.191070556640625
Exploitation Data q-values : 828.3818359375
Exploitation OOD q-values : 859.0982666015625
Exploitation CQL Loss : 20219.22265625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -140.080002
best mean reward -140.080002
running time 337.312953
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -140.0800018310547
Train_BestReturn : -140.0800018310547
TimeSinceStart : 337.3129529953003
Exploitation Critic Loss : 5692.2373046875
Exploration Critic Loss : 125939.7578125
Exploration Model Loss : 23.19140625
Exploitation Data q-values : 814.0800170898438
Exploitation OOD q-values : 839.7882080078125
Exploitation CQL Loss : 5717.94580078125
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -139.600006
best mean reward -139.600006
running time 347.991080
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -139.60000610351562
Train_BestReturn : -139.60000610351562
TimeSinceStart : 347.9910800457001
Exploitation Critic Loss : 3425.846435546875
Exploration Critic Loss : 134534.421875
Exploration Model Loss : 23.19171142578125
Exploitation Data q-values : 729.691650390625
Exploitation OOD q-values : 759.30615234375
Exploitation CQL Loss : 3455.4609375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -134.130005
best mean reward -134.130005
running time 358.907977
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -134.1300048828125
Train_BestReturn : -134.1300048828125
TimeSinceStart : 358.9079773426056
Exploitation Critic Loss : 11440.6259765625
Exploration Critic Loss : 125663.4375
Exploration Model Loss : 23.192047119140625
Exploitation Data q-values : 791.3385009765625
Exploitation OOD q-values : 824.5009765625
Exploitation CQL Loss : 11473.7880859375
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -126.809998
best mean reward -126.809998
running time 370.033724
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -126.80999755859375
Train_BestReturn : -126.80999755859375
TimeSinceStart : 370.0337243080139
Exploitation Critic Loss : 17178.333984375
Exploration Critic Loss : 104457.9375
Exploration Model Loss : 23.192398071289062
Exploitation Data q-values : 725.37646484375
Exploitation OOD q-values : 754.7924194335938
Exploitation CQL Loss : 17207.75
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -117.809998
best mean reward -117.809998
running time 381.245247
At timestep:     26 / 1000At timestep:     129 / 1000At timestep:     279 / 1000At timestep:     336 / 1000At timestep:     445 / 1000At timestep:     496 / 1000At timestep:     646 / 1000At timestep:     690 / 1000At timestep:     712 / 1000At timestep:     862 / 1000At timestep:     896 / 1000At timestep:     934 / 1000At timestep:     962 / 1000At timestep:     1053 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -117.80999755859375
Train_BestReturn : -117.80999755859375
TimeSinceStart : 381.2452473640442
Exploitation Critic Loss : 4086.81787109375
Exploration Critic Loss : 80625.28125
Exploration Model Loss : 23.19268798828125
Exploitation Data q-values : 680.875732421875
Exploitation OOD q-values : 701.4744262695312
Exploitation CQL Loss : 4107.41650390625
Eval_AverageReturn : -74.42857360839844
Eval_StdReturn : 47.698028564453125
Eval_MaxReturn : -21.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 75.21428571428571
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -103.269997
best mean reward -103.269997
running time 393.644483
At timestep:     144 / 1000At timestep:     271 / 1000At timestep:     311 / 1000At timestep:     461 / 1000At timestep:     581 / 1000At timestep:     731 / 1000At timestep:     881 / 1000At timestep:     922 / 1000At timestep:     1072 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -103.2699966430664
Train_BestReturn : -103.2699966430664
TimeSinceStart : 393.644483089447
Exploitation Critic Loss : 7754.0556640625
Exploration Critic Loss : 84585.9921875
Exploration Model Loss : 23.1929931640625
Exploitation Data q-values : 650.180419921875
Exploitation OOD q-values : 670.96875
Exploitation CQL Loss : 7774.84423828125
Eval_AverageReturn : -118.55555725097656
Eval_StdReturn : 43.59946823120117
Eval_MaxReturn : -39.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 119.11111111111111
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -92.769997
best mean reward -92.769997
running time 405.157974
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     344 / 1000At timestep:     375 / 1000At timestep:     525 / 1000At timestep:     674 / 1000At timestep:     824 / 1000At timestep:     849 / 1000At timestep:     918 / 1000At timestep:     981 / 1000At timestep:     1131 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -92.7699966430664
Train_BestReturn : -92.7699966430664
TimeSinceStart : 405.1579740047455
Exploitation Critic Loss : 1004.0689086914062
Exploration Critic Loss : 42413.03515625
Exploration Model Loss : 23.193328857421875
Exploitation Data q-values : 636.952392578125
Exploitation OOD q-values : 655.2978515625
Exploitation CQL Loss : 1022.414306640625
Eval_AverageReturn : -102.2727279663086
Eval_StdReturn : 53.206443786621094
Eval_MaxReturn : -24.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 102.81818181818181
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -81.800003
best mean reward -81.800003
running time 416.936760
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -81.80000305175781
Train_BestReturn : -81.80000305175781
TimeSinceStart : 416.9367604255676
Exploitation Critic Loss : 3112.921142578125
Exploration Critic Loss : 56026.96875
Exploration Model Loss : 23.1934814453125
Exploitation Data q-values : 534.2083740234375
Exploitation OOD q-values : 552.4064331054688
Exploitation CQL Loss : 3131.119140625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -73.779999
best mean reward -73.779999
running time 428.253306
At timestep:     150 / 1000At timestep:     266 / 1000At timestep:     416 / 1000At timestep:     566 / 1000At timestep:     716 / 1000At timestep:     866 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -73.77999877929688
Train_BestReturn : -73.77999877929688
TimeSinceStart : 428.25330567359924
Exploitation Critic Loss : 4451.046875
Exploration Critic Loss : 38732.63671875
Exploration Model Loss : 23.193817138671875
Exploitation Data q-values : 516.529052734375
Exploitation OOD q-values : 530.054443359375
Exploitation CQL Loss : 4464.572265625
Eval_AverageReturn : -145.0
Eval_StdReturn : 12.247448921203613
Eval_MaxReturn : -115.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 145.14285714285714
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -69.559998
best mean reward -69.559998
running time 439.778038
At timestep:     91 / 1000At timestep:     241 / 1000At timestep:     391 / 1000At timestep:     541 / 1000At timestep:     653 / 1000At timestep:     803 / 1000At timestep:     953 / 1000At timestep:     999 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -69.55999755859375
Train_BestReturn : -69.55999755859375
TimeSinceStart : 439.7780375480652
Exploitation Critic Loss : 2834.605224609375
Exploration Critic Loss : 31144.296875
Exploration Model Loss : 23.194122314453125
Exploitation Data q-values : 487.46075439453125
Exploitation OOD q-values : 500.6812744140625
Exploitation CQL Loss : 2847.82568359375
Eval_AverageReturn : -113.11111450195312
Eval_StdReturn : 47.47267150878906
Eval_MaxReturn : -22.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 113.55555555555556
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -68.139999
best mean reward -68.139999
running time 451.316568
At timestep:     83 / 1000At timestep:     233 / 1000At timestep:     383 / 1000At timestep:     438 / 1000At timestep:     588 / 1000At timestep:     611 / 1000At timestep:     761 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -68.13999938964844
Train_BestReturn : -68.13999938964844
TimeSinceStart : 451.31656765937805
Exploitation Critic Loss : 1178.41015625
Exploration Critic Loss : 28665.412109375
Exploration Model Loss : 23.19439697265625
Exploitation Data q-values : 409.6105041503906
Exploitation OOD q-values : 422.79583740234375
Exploitation CQL Loss : 1191.595458984375
Eval_AverageReturn : -111.19999694824219
Eval_StdReturn : 49.38177490234375
Eval_MaxReturn : -22.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 111.6
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -72.250000
best mean reward -68.139999
running time 462.925666
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     673 / 1000At timestep:     745 / 1000At timestep:     798 / 1000At timestep:     948 / 1000At timestep:     1098 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -72.25
Train_BestReturn : -68.13999938964844
TimeSinceStart : 462.92566561698914
Exploitation Critic Loss : 1481.83447265625
Exploration Critic Loss : 22659.486328125
Exploration Model Loss : 23.1947021484375
Exploitation Data q-values : 378.3994140625
Exploitation OOD q-values : 387.14959716796875
Exploitation CQL Loss : 1490.5845947265625
Eval_AverageReturn : -121.66666412353516
Eval_StdReturn : 40.42001724243164
Eval_MaxReturn : -52.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 122.0
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -74.910004
best mean reward -68.139999
running time 475.049378
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -74.91000366210938
Train_BestReturn : -68.13999938964844
TimeSinceStart : 475.04937839508057
Exploitation Critic Loss : 1074.2774658203125
Exploration Critic Loss : 17868.775390625
Exploration Model Loss : 23.195022583007812
Exploitation Data q-values : 340.780029296875
Exploitation OOD q-values : 349.16748046875
Exploitation CQL Loss : 1082.6649169921875
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -79.709999
best mean reward -68.139999
running time 486.058913
At timestep:     150 / 1000At timestep:     234 / 1000At timestep:     384 / 1000At timestep:     527 / 1000At timestep:     677 / 1000At timestep:     827 / 1000At timestep:     879 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -79.70999908447266
Train_BestReturn : -68.13999938964844
TimeSinceStart : 486.05891275405884
Exploitation Critic Loss : 441.3273620605469
Exploration Critic Loss : 14857.7666015625
Exploration Model Loss : 23.195327758789062
Exploitation Data q-values : 306.2944641113281
Exploitation OOD q-values : 311.90576171875
Exploitation CQL Loss : 446.9386291503906
Eval_AverageReturn : -128.125
Eval_StdReturn : 36.274776458740234
Eval_MaxReturn : -51.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 128.625
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -85.800003
best mean reward -68.139999
running time 497.429716
At timestep:     44 / 1000At timestep:     194 / 1000At timestep:     344 / 1000At timestep:     493 / 1000At timestep:     643 / 1000At timestep:     793 / 1000At timestep:     871 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -85.80000305175781
Train_BestReturn : -68.13999938964844
TimeSinceStart : 497.4297161102295
Exploitation Critic Loss : 72.47269439697266
Exploration Critic Loss : 4784.9228515625
Exploration Model Loss : 23.195648193359375
Exploitation Data q-values : 267.3175964355469
Exploitation OOD q-values : 271.8120422363281
Exploitation CQL Loss : 76.96714782714844
Eval_AverageReturn : -127.25
Eval_StdReturn : 39.75157165527344
Eval_MaxReturn : -43.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 127.625
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -89.599998
best mean reward -68.139999
running time 508.675083
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     974 / 1000At timestep:     1124 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -89.5999984741211
Train_BestReturn : -68.13999938964844
TimeSinceStart : 508.67508339881897
Exploitation Critic Loss : 187.6997528076172
Exploration Critic Loss : 9149.134765625
Exploration Model Loss : 23.195877075195312
Exploitation Data q-values : 238.7003173828125
Exploitation OOD q-values : 243.36245727539062
Exploitation CQL Loss : 192.3618927001953
Eval_AverageReturn : -140.375
Eval_StdReturn : 25.465356826782227
Eval_MaxReturn : -73.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 140.5
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -92.120003
best mean reward -68.139999
running time 519.918222
At timestep:     150 / 1000At timestep:     267 / 1000At timestep:     361 / 1000At timestep:     511 / 1000At timestep:     661 / 1000At timestep:     811 / 1000At timestep:     961 / 1000At timestep:     1111 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -92.12000274658203
Train_BestReturn : -68.13999938964844
TimeSinceStart : 519.918221950531
Exploitation Critic Loss : 30.46895408630371
Exploration Critic Loss : 3082.94970703125
Exploration Model Loss : 23.19610595703125
Exploitation Data q-values : 204.7763671875
Exploitation OOD q-values : 208.58901977539062
Exploitation CQL Loss : 34.2816162109375
Eval_AverageReturn : -138.625
Eval_StdReturn : 20.52399444580078
Eval_MaxReturn : -93.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.875
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -98.610001
best mean reward -68.139999
running time 530.967086
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q3_medium_cql_PointmassMedium-v0_21-11-2022_23-02-53/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -98.61000061035156
Train_BestReturn : -68.13999938964844
TimeSinceStart : 530.967086315155
Exploitation Critic Loss : 166.9937744140625
Exploration Critic Loss : 7846.33984375
Exploration Model Loss : 23.196212768554688
Exploitation Data q-values : 187.1941680908203
Exploitation OOD q-values : 190.72970581054688
Exploitation CQL Loss : 170.52931213378906
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_hard_dqn_PointmassHard-v0_21-11-2022_23-11-57 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_hard_dqn_PointmassHard-v0_21-11-2022_23-11-57
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001752
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0017516613006591797
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 6.603366
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -100.0
TimeSinceStart : 6.603366136550903
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 13.136327
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -100.0
TimeSinceStart : 13.136326789855957
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 25.560251
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -100.0
TimeSinceStart : 25.560250759124756
Exploitation Critic Loss : 6.062593456590548e-06
Exploration Critic Loss : 755517.8125
Exploration Model Loss : 51.01533126831055
Exploitation Data q-values : -0.01721690222620964
Exploitation OOD q-values : 1.5923762321472168
Exploitation CQL Loss : 6.062593456590548e-06
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 38.105984
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -100.0
TimeSinceStart : 38.10598397254944
Exploitation Critic Loss : 7.662670782337955e-07
Exploration Critic Loss : 11863918.0
Exploration Model Loss : 59.077545166015625
Exploitation Data q-values : -0.011102497577667236
Exploitation OOD q-values : 1.5982651710510254
Exploitation CQL Loss : 7.662670782337955e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 50.679300
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -100.0
TimeSinceStart : 50.67929983139038
Exploitation Critic Loss : 4.698580369222327e-07
Exploration Critic Loss : 6735681.5
Exploration Model Loss : 37.79481506347656
Exploitation Data q-values : -0.00561639666557312
Exploitation OOD q-values : 1.60378098487854
Exploitation CQL Loss : 4.698580369222327e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 63.154588
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -100.0
TimeSinceStart : 63.15458846092224
Exploitation Critic Loss : 8.20182208371989e-07
Exploration Critic Loss : 5208145.0
Exploration Model Loss : 37.94730758666992
Exploitation Data q-values : -0.0006131939589977264
Exploitation OOD q-values : 1.6088440418243408
Exploitation CQL Loss : 8.20182208371989e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 75.696791
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -100.0
TimeSinceStart : 75.69679141044617
Exploitation Critic Loss : 4.090534844181093e-07
Exploration Critic Loss : 4397392.5
Exploration Model Loss : 31.002445220947266
Exploitation Data q-values : 0.0022268020547926426
Exploitation OOD q-values : 1.6116111278533936
Exploitation CQL Loss : 4.090534844181093e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 88.150679
At timestep:     100 / 1000At timestep:     181 / 1000At timestep:     281 / 1000At timestep:     381 / 1000At timestep:     481 / 1000At timestep:     581 / 1000At timestep:     681 / 1000At timestep:     781 / 1000At timestep:     881 / 1000At timestep:     981 / 1000At timestep:     1081 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -100.0
TimeSinceStart : 88.15067863464355
Exploitation Critic Loss : 7.037503451101657e-07
Exploration Critic Loss : 4432584.5
Exploration Model Loss : 34.1064453125
Exploitation Data q-values : 0.004451124928891659
Exploitation OOD q-values : 1.613882064819336
Exploitation CQL Loss : 7.037503451101657e-07
Eval_AverageReturn : -98.18181610107422
Eval_StdReturn : 5.7495951652526855
Eval_MaxReturn : -80.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 98.27272727272727
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 100.780577
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -100.0
TimeSinceStart : 100.78057742118835
Exploitation Critic Loss : 1.9083477127423976e-06
Exploration Critic Loss : 3633254.5
Exploration Model Loss : 51.31813049316406
Exploitation Data q-values : 0.005880165379494429
Exploitation OOD q-values : 1.6156350374221802
Exploitation CQL Loss : 1.9083477127423976e-06
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 113.250682
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -100.0
TimeSinceStart : 113.25068187713623
Exploitation Critic Loss : 9.661767762736417e-06
Exploration Critic Loss : 2918886.25
Exploration Model Loss : 41.37420654296875
Exploitation Data q-values : 0.007082940079271793
Exploitation OOD q-values : 1.614664077758789
Exploitation CQL Loss : 9.661767762736417e-06
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 125.678554
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 125.67855405807495
Exploitation Critic Loss : 6.318840860330965e-06
Exploration Critic Loss : 4205524.5
Exploration Model Loss : 54.86323547363281
Exploitation Data q-values : 0.010734048672020435
Exploitation OOD q-values : 1.6205031871795654
Exploitation CQL Loss : 6.318840860330965e-06
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 138.084768
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 138.08476781845093
Exploitation Critic Loss : 2.6807824724528473e-06
Exploration Critic Loss : 1945975.875
Exploration Model Loss : 27.503707885742188
Exploitation Data q-values : 0.009621034376323223
Exploitation OOD q-values : 1.6190366744995117
Exploitation CQL Loss : 2.6807824724528473e-06
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 150.532095
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 150.53209519386292
Exploitation Critic Loss : 2.1364671738410834e-06
Exploration Critic Loss : 1971929.75
Exploration Model Loss : 36.073822021484375
Exploitation Data q-values : 0.00780150480568409
Exploitation OOD q-values : 1.6174567937850952
Exploitation CQL Loss : 2.1364671738410834e-06
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 163.007777
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 163.00777745246887
Exploitation Critic Loss : 3.3351673778270197e-07
Exploration Critic Loss : 2478870.5
Exploration Model Loss : 52.40211486816406
Exploitation Data q-values : 0.006806280463933945
Exploitation OOD q-values : 1.6161179542541504
Exploitation CQL Loss : 3.3351673778270197e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 175.336422
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 175.33642172813416
Exploitation Critic Loss : 1.4885883956594625e-06
Exploration Critic Loss : 1034800.375
Exploration Model Loss : 27.057952880859375
Exploitation Data q-values : 0.00481342151761055
Exploitation OOD q-values : 1.61503267288208
Exploitation CQL Loss : 1.4885883956594625e-06
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 187.788454
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 187.78845405578613
Exploitation Critic Loss : 4.455605903785909e-06
Exploration Critic Loss : 841140.3125
Exploration Model Loss : 35.85978698730469
Exploitation Data q-values : 0.0030015369411557913
Exploitation OOD q-values : 1.613612413406372
Exploitation CQL Loss : 4.455605903785909e-06
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 200.215753
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 200.2157530784607
Exploitation Critic Loss : 6.936939911383888e-08
Exploration Critic Loss : 875387.625
Exploration Model Loss : 52.29005432128906
Exploitation Data q-values : 0.003332919906824827
Exploitation OOD q-values : 1.612764596939087
Exploitation CQL Loss : 6.936939911383888e-08
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 212.684583
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 212.6845829486847
Exploitation Critic Loss : 1.2799392834494938e-07
Exploration Critic Loss : 246297.3125
Exploration Model Loss : 27.035629272460938
Exploitation Data q-values : 0.0031885586213320494
Exploitation OOD q-values : 1.6126163005828857
Exploitation CQL Loss : 1.2799392834494938e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 225.143190
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     282 / 1000At timestep:     382 / 1000At timestep:     482 / 1000At timestep:     582 / 1000At timestep:     682 / 1000At timestep:     782 / 1000At timestep:     882 / 1000At timestep:     982 / 1000At timestep:     1082 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 225.14319014549255
Exploitation Critic Loss : 8.741957202573758e-08
Exploration Critic Loss : 227032.75
Exploration Model Loss : 35.848419189453125
Exploitation Data q-values : 0.002946465043351054
Exploitation OOD q-values : 1.6123418807983398
Exploitation CQL Loss : 8.741957202573758e-08
Eval_AverageReturn : -98.2727279663086
Eval_StdReturn : 5.46211576461792
Eval_MaxReturn : -81.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 98.36363636363636
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 237.703705
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 237.70370483398438
Exploitation Critic Loss : 7.860380719648674e-07
Exploration Critic Loss : 208485.71875
Exploration Model Loss : 52.28471374511719
Exploitation Data q-values : 0.003019344061613083
Exploitation OOD q-values : 1.6124564409255981
Exploitation CQL Loss : 7.860380719648674e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 250.088397
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 250.08839654922485
Exploitation Critic Loss : 1.39041716806787e-07
Exploration Critic Loss : 224092.046875
Exploration Model Loss : 27.035140991210938
Exploitation Data q-values : 0.001652248902246356
Exploitation OOD q-values : 1.6110379695892334
Exploitation CQL Loss : 1.39041716806787e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 262.428267
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 262.4282670021057
Exploitation Critic Loss : 1.8763151388156984e-07
Exploration Critic Loss : 202476.28125
Exploration Model Loss : 35.848480224609375
Exploitation Data q-values : 0.0019391553942114115
Exploitation OOD q-values : 1.6112501621246338
Exploitation CQL Loss : 1.8763151388156984e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 274.783359
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 274.7833592891693
Exploitation Critic Loss : 8.880817858880619e-07
Exploration Critic Loss : 133735.671875
Exploration Model Loss : 52.28498840332031
Exploitation Data q-values : 0.0008417092030867934
Exploitation OOD q-values : 1.610236406326294
Exploitation CQL Loss : 8.880817858880619e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 287.205246
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 287.20524644851685
Exploitation Critic Loss : 1.215115162267466e-06
Exploration Critic Loss : 186866.125
Exploration Model Loss : 27.034881591796875
Exploitation Data q-values : 0.0017331823473796248
Exploitation OOD q-values : 1.610917568206787
Exploitation CQL Loss : 1.215115162267466e-06
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 299.545290
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 299.54528975486755
Exploitation Critic Loss : 6.774587291147327e-08
Exploration Critic Loss : 101871.7265625
Exploration Model Loss : 35.84822082519531
Exploitation Data q-values : 0.0008919788524508476
Exploitation OOD q-values : 1.6102899312973022
Exploitation CQL Loss : 6.774587291147327e-08
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 312.039563
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 312.0395631790161
Exploitation Critic Loss : 1.6645688560856797e-07
Exploration Critic Loss : 119632.2578125
Exploration Model Loss : 52.28492736816406
Exploitation Data q-values : 0.000980256823822856
Exploitation OOD q-values : 1.6103509664535522
Exploitation CQL Loss : 1.6645688560856797e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 324.560203
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 324.5602025985718
Exploitation Critic Loss : 8.265221396186462e-08
Exploration Critic Loss : 91005.453125
Exploration Model Loss : 27.034942626953125
Exploitation Data q-values : 0.001129303127527237
Exploitation OOD q-values : 1.6105234622955322
Exploitation CQL Loss : 8.265221396186462e-08
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 336.842813
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     560 / 1000At timestep:     660 / 1000At timestep:     760 / 1000At timestep:     860 / 1000At timestep:     960 / 1000At timestep:     1060 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 336.84281301498413
Exploitation Critic Loss : 8.363048920045912e-08
Exploration Critic Loss : 83241.46875
Exploration Model Loss : 35.84828186035156
Exploitation Data q-values : 0.001316640991717577
Exploitation OOD q-values : 1.6107139587402344
Exploitation CQL Loss : 8.363048920045912e-08
Eval_AverageReturn : -96.2727279663086
Eval_StdReturn : 11.78667163848877
Eval_MaxReturn : -59.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 96.36363636363636
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 349.548147
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 349.5481467247009
Exploitation Critic Loss : 1.6417152437497862e-07
Exploration Critic Loss : 66887.625
Exploration Model Loss : 52.28489685058594
Exploitation Data q-values : 0.0010940753854811192
Exploitation OOD q-values : 1.610445261001587
Exploitation CQL Loss : 1.6417152437497862e-07
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -99.720001
best mean reward -99.720001
running time 362.020823
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -99.72000122070312
Train_BestReturn : -99.72000122070312
TimeSinceStart : 362.02082347869873
Exploitation Critic Loss : 0.00045016728108748794
Exploration Critic Loss : 71484.6328125
Exploration Model Loss : 27.034988403320312
Exploitation Data q-values : 0.0301472470164299
Exploitation OOD q-values : 1.6386380195617676
Exploitation CQL Loss : 0.00045016728108748794
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -98.949997
best mean reward -98.949997
running time 374.758492
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -98.94999694824219
Train_BestReturn : -98.94999694824219
TimeSinceStart : 374.75849175453186
Exploitation Critic Loss : 7.01296521583572e-05
Exploration Critic Loss : 30164.234375
Exploration Model Loss : 35.848297119140625
Exploitation Data q-values : 0.05849505215883255
Exploitation OOD q-values : 1.665396809577942
Exploitation CQL Loss : 7.01296521583572e-05
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -98.610001
best mean reward -98.610001
running time 387.317877
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -98.61000061035156
Train_BestReturn : -98.61000061035156
TimeSinceStart : 387.3178765773773
Exploitation Critic Loss : 0.008979760110378265
Exploration Critic Loss : 32920.53515625
Exploration Model Loss : 52.28486633300781
Exploitation Data q-values : 0.1736534684896469
Exploitation OOD q-values : 1.7740415334701538
Exploitation CQL Loss : 0.008979760110378265
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -98.610001
best mean reward -98.610001
running time 399.894902
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -98.61000061035156
Train_BestReturn : -98.61000061035156
TimeSinceStart : 399.8949017524719
Exploitation Critic Loss : 0.0326588898897171
Exploration Critic Loss : 32406.349609375
Exploration Model Loss : 27.034988403320312
Exploitation Data q-values : 0.30875635147094727
Exploitation OOD q-values : 1.9121408462524414
Exploitation CQL Loss : 0.0326588898897171
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -98.610001
best mean reward -98.610001
running time 412.520840
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -98.61000061035156
Train_BestReturn : -98.61000061035156
TimeSinceStart : 412.5208396911621
Exploitation Critic Loss : 0.01904565468430519
Exploration Critic Loss : 21106.140625
Exploration Model Loss : 35.84831237792969
Exploitation Data q-values : 0.5467557907104492
Exploitation OOD q-values : 2.1477291584014893
Exploitation CQL Loss : 0.01904565468430519
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -98.610001
best mean reward -98.610001
running time 425.026503
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -98.61000061035156
Train_BestReturn : -98.61000061035156
TimeSinceStart : 425.02650260925293
Exploitation Critic Loss : 0.1801319122314453
Exploration Critic Loss : 13673.8212890625
Exploration Model Loss : 52.28486633300781
Exploitation Data q-values : 0.7476717829704285
Exploitation OOD q-values : 2.349125862121582
Exploitation CQL Loss : 0.1801319122314453
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -98.610001
best mean reward -98.610001
running time 437.643479
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -98.61000061035156
Train_BestReturn : -98.61000061035156
TimeSinceStart : 437.643479347229
Exploitation Critic Loss : 0.15569862723350525
Exploration Critic Loss : 10030.982421875
Exploration Model Loss : 27.034988403320312
Exploitation Data q-values : 1.8247427940368652
Exploitation OOD q-values : 3.480034351348877
Exploitation CQL Loss : 0.15569862723350525
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -98.610001
best mean reward -98.610001
running time 450.262161
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -98.61000061035156
Train_BestReturn : -98.61000061035156
TimeSinceStart : 450.2621605396271
Exploitation Critic Loss : 0.12913495302200317
Exploration Critic Loss : 12237.5234375
Exploration Model Loss : 35.848297119140625
Exploitation Data q-values : 1.4474067687988281
Exploitation OOD q-values : 3.0536932945251465
Exploitation CQL Loss : 0.12913495302200317
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -98.610001
best mean reward -98.610001
running time 462.968158
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -98.61000061035156
Train_BestReturn : -98.61000061035156
TimeSinceStart : 462.96815824508667
Exploitation Critic Loss : 0.18295788764953613
Exploration Critic Loss : 6983.43017578125
Exploration Model Loss : 52.28486633300781
Exploitation Data q-values : 1.7527925968170166
Exploitation OOD q-values : 3.355041265487671
Exploitation CQL Loss : 0.18295788764953613
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -98.610001
best mean reward -98.610001
running time 475.760692
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -98.61000061035156
Train_BestReturn : -98.61000061035156
TimeSinceStart : 475.76069235801697
Exploitation Critic Loss : 0.13447625935077667
Exploration Critic Loss : 3615.857421875
Exploration Model Loss : 27.034988403320312
Exploitation Data q-values : 2.1098415851593018
Exploitation OOD q-values : 3.6948235034942627
Exploitation CQL Loss : 0.13447625935077667
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -98.739998
best mean reward -98.610001
running time 488.333671
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     642 / 1000At timestep:     676 / 1000At timestep:     731 / 1000At timestep:     831 / 1000At timestep:     931 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -98.73999786376953
Train_BestReturn : -98.61000061035156
TimeSinceStart : 488.33367109298706
Exploitation Critic Loss : 22.351947784423828
Exploration Critic Loss : 13616.2919921875
Exploration Model Loss : 35.848297119140625
Exploitation Data q-values : 3.2719671726226807
Exploitation OOD q-values : 4.902461528778076
Exploitation CQL Loss : 22.351947784423828
Eval_AverageReturn : -85.66666412353516
Eval_StdReturn : 25.200307846069336
Eval_MaxReturn : -33.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 85.91666666666667
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -98.820000
best mean reward -98.610001
running time 501.393175
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     380 / 1000At timestep:     480 / 1000At timestep:     580 / 1000At timestep:     680 / 1000At timestep:     780 / 1000At timestep:     880 / 1000At timestep:     980 / 1000At timestep:     1080 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -98.81999969482422
Train_BestReturn : -98.61000061035156
TimeSinceStart : 501.39317512512207
Exploitation Critic Loss : 0.4475080668926239
Exploration Critic Loss : 4023.4677734375
Exploration Model Loss : 52.28486633300781
Exploitation Data q-values : 3.5355725288391113
Exploitation OOD q-values : 5.137114524841309
Exploitation CQL Loss : 0.4475080668926239
Eval_AverageReturn : -98.09091186523438
Eval_StdReturn : 6.037075996398926
Eval_MaxReturn : -79.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 98.18181818181819
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -96.989998
best mean reward -96.989998
running time 514.402190
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     676 / 1000At timestep:     776 / 1000At timestep:     876 / 1000At timestep:     921 / 1000At timestep:     964 / 1000At timestep:     1064 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -96.98999786376953
Train_BestReturn : -96.98999786376953
TimeSinceStart : 514.4021902084351
Exploitation Critic Loss : 0.22769421339035034
Exploration Critic Loss : 4419.60302734375
Exploration Model Loss : 27.03497314453125
Exploitation Data q-values : 3.583303689956665
Exploitation OOD q-values : 5.182633399963379
Exploitation CQL Loss : 0.22769421339035034
Eval_AverageReturn : -88.41666412353516
Eval_StdReturn : 21.437732696533203
Eval_MaxReturn : -42.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 88.66666666666667
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -96.180000
best mean reward -96.180000
running time 527.330285
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -96.18000030517578
Train_BestReturn : -96.18000030517578
TimeSinceStart : 527.3302845954895
Exploitation Critic Loss : 0.23536604642868042
Exploration Critic Loss : 1316.081787109375
Exploration Model Loss : 35.848297119140625
Exploitation Data q-values : 4.171280860900879
Exploitation OOD q-values : 5.744150161743164
Exploitation CQL Loss : 0.23536604642868042
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -95.089996
best mean reward -95.089996
running time 540.068157
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -95.08999633789062
Train_BestReturn : -95.08999633789062
TimeSinceStart : 540.0681571960449
Exploitation Critic Loss : 1.4214457273483276
Exploration Critic Loss : 4489.529296875
Exploration Model Loss : 52.28486633300781
Exploitation Data q-values : 4.80068302154541
Exploitation OOD q-values : 6.413552284240723
Exploitation CQL Loss : 1.4214457273483276
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -94.790001
best mean reward -94.790001
running time 552.911703
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -94.79000091552734
Train_BestReturn : -94.79000091552734
TimeSinceStart : 552.9117028713226
Exploitation Critic Loss : 16.303306579589844
Exploration Critic Loss : 2675.62646484375
Exploration Model Loss : 27.03497314453125
Exploitation Data q-values : 5.349850177764893
Exploitation OOD q-values : 6.980958461761475
Exploitation CQL Loss : 16.303306579589844
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -93.320000
best mean reward -93.320000
running time 565.951817
At timestep:     39 / 1000At timestep:     95 / 1000At timestep:     177 / 1000At timestep:     277 / 1000At timestep:     350 / 1000At timestep:     424 / 1000At timestep:     480 / 1000At timestep:     514 / 1000At timestep:     552 / 1000At timestep:     643 / 1000At timestep:     723 / 1000At timestep:     790 / 1000At timestep:     824 / 1000At timestep:     896 / 1000At timestep:     954 / 1000At timestep:     1052 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -93.31999969482422
Train_BestReturn : -93.31999969482422
TimeSinceStart : 565.9518167972565
Exploitation Critic Loss : 0.5074254870414734
Exploration Critic Loss : 1469.7196044921875
Exploration Model Loss : 35.848297119140625
Exploitation Data q-values : 5.8992767333984375
Exploitation OOD q-values : 7.419137954711914
Exploitation CQL Loss : 0.5074254870414734
Eval_AverageReturn : -64.8125
Eval_StdReturn : 21.36064910888672
Eval_MaxReturn : -33.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 65.75
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -91.010002
best mean reward -91.010002
running time 580.326479
At timestep:     73 / 1000At timestep:     119 / 1000At timestep:     163 / 1000At timestep:     263 / 1000At timestep:     308 / 1000At timestep:     408 / 1000At timestep:     508 / 1000At timestep:     556 / 1000At timestep:     656 / 1000At timestep:     756 / 1000At timestep:     838 / 1000At timestep:     905 / 1000At timestep:     985 / 1000At timestep:     1061 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -91.01000213623047
Train_BestReturn : -91.01000213623047
TimeSinceStart : 580.3264787197113
Exploitation Critic Loss : 4.93843412399292
Exploration Critic Loss : 4090.773193359375
Exploration Model Loss : 52.28486633300781
Exploitation Data q-values : 6.40247917175293
Exploitation OOD q-values : 7.9962615966796875
Exploitation CQL Loss : 4.93843412399292
Eval_AverageReturn : -75.14286041259766
Eval_StdReturn : 22.257766723632812
Eval_MaxReturn : -43.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 75.78571428571429
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -86.720001
best mean reward -86.720001
running time 594.402408
At timestep:     69 / 1000At timestep:     127 / 1000At timestep:     227 / 1000At timestep:     277 / 1000At timestep:     326 / 1000At timestep:     426 / 1000At timestep:     526 / 1000At timestep:     626 / 1000At timestep:     726 / 1000At timestep:     794 / 1000At timestep:     894 / 1000At timestep:     985 / 1000At timestep:     1043 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -86.72000122070312
Train_BestReturn : -86.72000122070312
TimeSinceStart : 594.4024078845978
Exploitation Critic Loss : 0.8707940578460693
Exploration Critic Loss : 898.4173583984375
Exploration Model Loss : 27.03497314453125
Exploitation Data q-values : 6.920537948608398
Exploitation OOD q-values : 8.468050003051758
Exploitation CQL Loss : 0.8707940578460693
Eval_AverageReturn : -79.69230651855469
Eval_StdReturn : 21.22184944152832
Eval_MaxReturn : -48.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 80.23076923076923
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -83.169998
best mean reward -83.169998
running time 608.203440
At timestep:     32 / 1000At timestep:     66 / 1000At timestep:     132 / 1000At timestep:     191 / 1000At timestep:     242 / 1000At timestep:     284 / 1000At timestep:     323 / 1000At timestep:     361 / 1000At timestep:     412 / 1000At timestep:     442 / 1000At timestep:     478 / 1000At timestep:     522 / 1000At timestep:     552 / 1000At timestep:     603 / 1000At timestep:     642 / 1000At timestep:     696 / 1000At timestep:     745 / 1000At timestep:     782 / 1000At timestep:     829 / 1000At timestep:     864 / 1000At timestep:     902 / 1000At timestep:     944 / 1000At timestep:     983 / 1000At timestep:     1083 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q3_hard_dqn_PointmassHard-v0_21-11-2022_23-11-57/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -83.16999816894531
Train_BestReturn : -83.16999816894531
TimeSinceStart : 608.2034397125244
Exploitation Critic Loss : 0.6510818004608154
Exploration Critic Loss : 1284.729248046875
Exploration Model Loss : 35.848297119140625
Exploitation Data q-values : 6.380678176879883
Exploitation OOD q-values : 7.941084861755371
Exploitation CQL Loss : 0.6510818004608154
Eval_AverageReturn : -44.16666793823242
Eval_StdReturn : 14.707329750061035
Eval_MaxReturn : -29.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 45.125
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_hard_cql_PointmassHard-v0_21-11-2022_23-22-26 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_hard_cql_PointmassHard-v0_21-11-2022_23-22-26
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001943
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0019428730010986328
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 6.709788
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -100.0
TimeSinceStart : 6.709787607192993
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 13.365487
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -100.0
TimeSinceStart : 13.365487098693848
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 25.941685
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -100.0
TimeSinceStart : 25.94168496131897
Exploitation Critic Loss : 0.06829869747161865
Exploration Critic Loss : 634729.8125
Exploration Model Loss : 46.135169982910156
Exploitation Data q-values : 0.19198748469352722
Exploitation OOD q-values : 1.655924677848816
Exploitation CQL Loss : 1.5322359800338745
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 38.899231
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -100.0
TimeSinceStart : 38.89923119544983
Exploitation Critic Loss : 0.09794794023036957
Exploration Critic Loss : 8696710.0
Exploration Model Loss : 41.87111282348633
Exploitation Data q-values : 0.5809135437011719
Exploitation OOD q-values : 1.8996437788009644
Exploitation CQL Loss : 1.4166781902313232
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 51.551334
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -100.0
TimeSinceStart : 51.551334381103516
Exploitation Critic Loss : 0.10199029743671417
Exploration Critic Loss : 6088242.0
Exploration Model Loss : 36.64076232910156
Exploitation Data q-values : 0.8689600825309753
Exploitation OOD q-values : 2.239902973175049
Exploitation CQL Loss : 1.472933292388916
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 64.217708
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -100.0
TimeSinceStart : 64.2177083492279
Exploitation Critic Loss : 0.08746407926082611
Exploration Critic Loss : 6230325.5
Exploration Model Loss : 20.933029174804688
Exploitation Data q-values : 1.3803563117980957
Exploitation OOD q-values : 2.7413604259490967
Exploitation CQL Loss : 1.4484680891036987
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 77.012927
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -100.0
TimeSinceStart : 77.01292657852173
Exploitation Critic Loss : 0.126495361328125
Exploration Critic Loss : 8413772.0
Exploration Model Loss : 18.678237915039062
Exploitation Data q-values : 1.7642103433609009
Exploitation OOD q-values : 3.0824358463287354
Exploitation CQL Loss : 1.4447212219238281
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 89.850354
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -100.0
TimeSinceStart : 89.85035371780396
Exploitation Critic Loss : 0.17319604754447937
Exploration Critic Loss : 7807718.0
Exploration Model Loss : 18.009613037109375
Exploitation Data q-values : 2.0231242179870605
Exploitation OOD q-values : 3.3335437774658203
Exploitation CQL Loss : 1.4836158752441406
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 102.664206
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -100.0
TimeSinceStart : 102.6642062664032
Exploitation Critic Loss : 0.13807643949985504
Exploration Critic Loss : 6628658.0
Exploration Model Loss : 17.781402587890625
Exploitation Data q-values : 2.3803653717041016
Exploitation OOD q-values : 3.65561580657959
Exploitation CQL Loss : 1.413326621055603
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -100.000000
best mean reward -inf
running time 115.370559
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -100.0
TimeSinceStart : 115.37055945396423
Exploitation Critic Loss : 0.19868692755699158
Exploration Critic Loss : 6738328.0
Exploration Model Loss : 17.698562622070312
Exploitation Data q-values : 2.509215831756592
Exploitation OOD q-values : 3.77412748336792
Exploitation CQL Loss : 1.4635988473892212
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 128.221034
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 128.22103357315063
Exploitation Critic Loss : 0.1482563316822052
Exploration Critic Loss : 6276417.0
Exploration Model Loss : 17.668441772460938
Exploitation Data q-values : 2.655812978744507
Exploitation OOD q-values : 3.92307186126709
Exploitation CQL Loss : 1.4155150651931763
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 141.204696
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 141.20469570159912
Exploitation Critic Loss : 0.1692172735929489
Exploration Critic Loss : 6839772.0
Exploration Model Loss : 17.657150268554688
Exploitation Data q-values : 2.949122905731201
Exploitation OOD q-values : 4.150812149047852
Exploitation CQL Loss : 1.3709064722061157
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 154.335116
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 154.33511567115784
Exploitation Critic Loss : 0.22007186710834503
Exploration Critic Loss : 2688193.0
Exploration Model Loss : 17.652862548828125
Exploitation Data q-values : 2.9205281734466553
Exploitation OOD q-values : 4.153722763061523
Exploitation CQL Loss : 1.4532665014266968
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 167.567425
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 167.56742477416992
Exploitation Critic Loss : 0.28490760922431946
Exploration Critic Loss : 2374101.75
Exploration Model Loss : 17.651229858398438
Exploitation Data q-values : 2.9712235927581787
Exploitation OOD q-values : 4.185348987579346
Exploitation CQL Loss : 1.4990330934524536
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 180.740413
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 180.74041271209717
Exploitation Critic Loss : 0.28676944971084595
Exploration Critic Loss : 1740122.25
Exploration Model Loss : 17.65087890625
Exploitation Data q-values : 3.2238926887512207
Exploitation OOD q-values : 4.406082630157471
Exploitation CQL Loss : 1.4689598083496094
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 193.778141
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 193.7781412601471
Exploitation Critic Loss : 0.2281804084777832
Exploration Critic Loss : 3330913.25
Exploration Model Loss : 17.65087890625
Exploitation Data q-values : 3.3085720539093018
Exploitation OOD q-values : 4.485629558563232
Exploitation CQL Loss : 1.4052377939224243
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 207.105469
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 207.10546851158142
Exploitation Critic Loss : 0.32393020391464233
Exploration Critic Loss : 915693.125
Exploration Model Loss : 17.651138305664062
Exploitation Data q-values : 3.328537940979004
Exploitation OOD q-values : 4.525887489318848
Exploitation CQL Loss : 1.5212798118591309
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 220.314156
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 220.31415605545044
Exploitation Critic Loss : 0.14918114244937897
Exploration Critic Loss : 577047.8125
Exploration Model Loss : 17.651504516601562
Exploitation Data q-values : 3.457515239715576
Exploitation OOD q-values : 4.596685409545898
Exploitation CQL Loss : 1.28835129737854
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 233.091626
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 233.09162640571594
Exploitation Critic Loss : 0.2791832685470581
Exploration Critic Loss : 244924.59375
Exploration Model Loss : 17.651596069335938
Exploitation Data q-values : 3.4124932289123535
Exploitation OOD q-values : 4.558417320251465
Exploitation CQL Loss : 1.4251079559326172
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 245.892491
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 245.89249086380005
Exploitation Critic Loss : 0.3545316457748413
Exploration Critic Loss : 282823.8125
Exploration Model Loss : 17.651992797851562
Exploitation Data q-values : 3.3334150314331055
Exploitation OOD q-values : 4.508944511413574
Exploitation CQL Loss : 1.5300612449645996
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 258.675190
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 258.67518973350525
Exploitation Critic Loss : 0.2833288311958313
Exploration Critic Loss : 270240.0
Exploration Model Loss : 17.652145385742188
Exploitation Data q-values : 3.265347480773926
Exploitation OOD q-values : 4.433246612548828
Exploitation CQL Loss : 1.451228380203247
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 271.469294
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 271.4692943096161
Exploitation Critic Loss : 0.24325013160705566
Exploration Critic Loss : 169774.65625
Exploration Model Loss : 17.652511596679688
Exploitation Data q-values : 3.373623847961426
Exploitation OOD q-values : 4.482475757598877
Exploitation CQL Loss : 1.3521020412445068
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 284.232594
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 284.2325940132141
Exploitation Critic Loss : 0.2564214766025543
Exploration Critic Loss : 132613.921875
Exploration Model Loss : 17.652862548828125
Exploitation Data q-values : 3.424405813217163
Exploitation OOD q-values : 4.5803422927856445
Exploitation CQL Loss : 1.4123581647872925
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 296.937100
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 296.93709993362427
Exploitation Critic Loss : 0.27435925602912903
Exploration Critic Loss : 166971.90625
Exploration Model Loss : 17.653182983398438
Exploitation Data q-values : 3.4416468143463135
Exploitation OOD q-values : 4.596463203430176
Exploitation CQL Loss : 1.429175615310669
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 309.677205
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 309.677205324173
Exploitation Critic Loss : 0.17357754707336426
Exploration Critic Loss : 74240.578125
Exploration Model Loss : 17.653549194335938
Exploitation Data q-values : 3.4815773963928223
Exploitation OOD q-values : 4.642696380615234
Exploitation CQL Loss : 1.3346965312957764
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 322.440827
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 322.4408268928528
Exploitation Critic Loss : 0.3315666913986206
Exploration Critic Loss : 116403.53125
Exploration Model Loss : 17.653610229492188
Exploitation Data q-values : 3.5611138343811035
Exploitation OOD q-values : 4.712467193603516
Exploitation CQL Loss : 1.4829200506210327
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 335.180370
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 335.18037033081055
Exploitation Critic Loss : 0.26108837127685547
Exploration Critic Loss : 108624.859375
Exploration Model Loss : 17.653976440429688
Exploitation Data q-values : 3.5959486961364746
Exploitation OOD q-values : 4.75390625
Exploitation CQL Loss : 1.4190462827682495
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 347.641340
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 347.64133977890015
Exploitation Critic Loss : 0.3078659176826477
Exploration Critic Loss : 79478.1796875
Exploration Model Loss : 17.654281616210938
Exploitation Data q-values : 3.6400341987609863
Exploitation OOD q-values : 4.78780460357666
Exploitation CQL Loss : 1.4556360244750977
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 360.059766
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 360.0597655773163
Exploitation Critic Loss : 0.3064364194869995
Exploration Critic Loss : 77247.875
Exploration Model Loss : 17.654617309570312
Exploitation Data q-values : 3.625889301300049
Exploitation OOD q-values : 4.72330379486084
Exploitation CQL Loss : 1.4038509130477905
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 372.534866
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 372.5348656177521
Exploitation Critic Loss : 0.3832942247390747
Exploration Critic Loss : 80906.25
Exploration Model Loss : 17.654647827148438
Exploitation Data q-values : 3.5140228271484375
Exploitation OOD q-values : 4.674857139587402
Exploitation CQL Loss : 1.5441287755966187
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 384.991339
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 384.991338968277
Exploitation Critic Loss : 0.44726037979125977
Exploration Critic Loss : 64087.75390625
Exploration Model Loss : 17.654647827148438
Exploitation Data q-values : 3.5781712532043457
Exploitation OOD q-values : 4.699321269989014
Exploitation CQL Loss : 1.5684107542037964
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 397.454719
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 397.45471930503845
Exploitation Critic Loss : 0.24090054631233215
Exploration Critic Loss : 33878.65234375
Exploration Model Loss : 17.654647827148438
Exploitation Data q-values : 3.622910499572754
Exploitation OOD q-values : 4.7418622970581055
Exploitation CQL Loss : 1.35985267162323
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 409.875822
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 409.87582206726074
Exploitation Critic Loss : 0.23843249678611755
Exploration Critic Loss : 35116.98828125
Exploration Model Loss : 17.654769897460938
Exploitation Data q-values : 3.5640153884887695
Exploitation OOD q-values : 4.660380840301514
Exploitation CQL Loss : 1.3347980976104736
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 422.346541
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 422.3465414047241
Exploitation Critic Loss : 0.3925294280052185
Exploration Critic Loss : 43530.44140625
Exploration Model Loss : 17.655120849609375
Exploitation Data q-values : 3.5421361923217773
Exploitation OOD q-values : 4.605873107910156
Exploitation CQL Loss : 1.456265926361084
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 434.853087
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 434.8530869483948
Exploitation Critic Loss : 0.2952887713909149
Exploration Critic Loss : 27142.328125
Exploration Model Loss : 17.655471801757812
Exploitation Data q-values : 3.6122870445251465
Exploitation OOD q-values : 4.691228866577148
Exploitation CQL Loss : 1.3742303848266602
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 447.580146
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 447.58014607429504
Exploitation Critic Loss : 0.33859115839004517
Exploration Critic Loss : 18690.09765625
Exploration Model Loss : 17.65570068359375
Exploitation Data q-values : 3.502122402191162
Exploitation OOD q-values : 4.596909046173096
Exploitation CQL Loss : 1.433377742767334
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 460.086879
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 460.0868790149689
Exploitation Critic Loss : 0.3938627243041992
Exploration Critic Loss : 18427.12109375
Exploration Model Loss : 17.655929565429688
Exploitation Data q-values : 3.5057458877563477
Exploitation OOD q-values : 4.5946784019470215
Exploitation CQL Loss : 1.4827954769134521
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 472.621105
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 472.6211049556732
Exploitation Critic Loss : 0.2834644913673401
Exploration Critic Loss : 12719.197265625
Exploration Model Loss : 17.656051635742188
Exploitation Data q-values : 3.5742454528808594
Exploitation OOD q-values : 4.606759548187256
Exploitation CQL Loss : 1.315978765487671
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 485.333459
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 485.3334586620331
Exploitation Critic Loss : 0.37934601306915283
Exploration Critic Loss : 16268.60546875
Exploration Model Loss : 17.656417846679688
Exploitation Data q-values : 3.55578875541687
Exploitation OOD q-values : 4.636351585388184
Exploitation CQL Loss : 1.459908366203308
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 498.044352
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 498.0443522930145
Exploitation Critic Loss : 0.22885121405124664
Exploration Critic Loss : 6411.48828125
Exploration Model Loss : 17.656570434570312
Exploitation Data q-values : 3.6003880500793457
Exploitation OOD q-values : 4.65988826751709
Exploitation CQL Loss : 1.2883514165878296
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 510.732622
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 510.7326216697693
Exploitation Critic Loss : 0.37256598472595215
Exploration Critic Loss : 14754.55859375
Exploration Model Loss : 17.656600952148438
Exploitation Data q-values : 3.6010260581970215
Exploitation OOD q-values : 4.654581546783447
Exploitation CQL Loss : 1.4261219501495361
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 523.403315
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 523.4033150672913
Exploitation Critic Loss : 0.18935588002204895
Exploration Critic Loss : 2597.03369140625
Exploration Model Loss : 17.656692504882812
Exploitation Data q-values : 3.5236501693725586
Exploitation OOD q-values : 4.598669052124023
Exploitation CQL Loss : 1.2643746137619019
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 536.096088
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 536.0960881710052
Exploitation Critic Loss : 0.3407445549964905
Exploration Critic Loss : 5139.4462890625
Exploration Model Loss : 17.656875610351562
Exploitation Data q-values : 3.405301332473755
Exploitation OOD q-values : 4.478599548339844
Exploitation CQL Loss : 1.4140429496765137
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 548.797321
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 548.7973206043243
Exploitation Critic Loss : 0.25383055210113525
Exploration Critic Loss : 3218.95556640625
Exploration Model Loss : 17.657211303710938
Exploitation Data q-values : 3.4457623958587646
Exploitation OOD q-values : 4.562337398529053
Exploitation CQL Loss : 1.3704054355621338
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 561.484620
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 561.4846203327179
Exploitation Critic Loss : 0.3897727131843567
Exploration Critic Loss : 5955.2734375
Exploration Model Loss : 17.657608032226562
Exploitation Data q-values : 3.5642335414886475
Exploitation OOD q-values : 4.571920394897461
Exploitation CQL Loss : 1.3974597454071045
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 574.140976
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 574.1409757137299
Exploitation Critic Loss : 0.3519354462623596
Exploration Critic Loss : 2939.306884765625
Exploration Model Loss : 17.657760620117188
Exploitation Data q-values : 3.5375542640686035
Exploitation OOD q-values : 4.641671180725098
Exploitation CQL Loss : 1.456052303314209
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 586.844255
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 586.844254732132
Exploitation Critic Loss : 0.21733632683753967
Exploration Critic Loss : 763.9254150390625
Exploration Model Loss : 17.65802001953125
Exploitation Data q-values : 3.544818878173828
Exploitation OOD q-values : 4.646998405456543
Exploitation CQL Loss : 1.3195158243179321
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 599.526644
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 599.5266435146332
Exploitation Critic Loss : 0.23031896352767944
Exploration Critic Loss : 1326.5831298828125
Exploration Model Loss : 17.658309936523438
Exploitation Data q-values : 3.555234670639038
Exploitation OOD q-values : 4.644687652587891
Exploitation CQL Loss : 1.3197715282440186
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -100.000000
best mean reward -100.000000
running time 612.205622
At timestep:     100 / 1000At timestep:     200 / 1000At timestep:     300 / 1000At timestep:     400 / 1000At timestep:     500 / 1000At timestep:     600 / 1000At timestep:     700 / 1000At timestep:     800 / 1000At timestep:     900 / 1000At timestep:     1000 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q3_hard_cql_PointmassHard-v0_21-11-2022_23-22-26/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -100.0
Train_BestReturn : -100.0
TimeSinceStart : 612.2056219577789
Exploitation Critic Loss : 0.3614305257797241
Exploration Critic Loss : 2294.730224609375
Exploration Model Loss : 17.658538818359375
Exploitation Data q-values : 3.5519213676452637
Exploitation OOD q-values : 4.615926265716553
Exploitation CQL Loss : 1.4254356622695923
Eval_AverageReturn : -100.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -100.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 100.0
Buffer size : 49001
Done logging...


