


LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam0.1_PointmassMedium-v0_22-11-2022_21-24-30 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam0.1_PointmassMedium-v0_22-11-2022_21-24-30
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002457
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0024566650390625
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -149.500000
best mean reward -inf
running time 8.214197
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     666 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -149.5
TimeSinceStart : 8.214196681976318
Eval_AverageReturn : -139.375
Eval_StdReturn : 28.111106872558594
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.5
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -149.692307
best mean reward -inf
running time 17.406145
At timestep:     54 / 1000At timestep:     204 / 1000At timestep:     354 / 1000At timestep:     504 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -149.6923065185547
TimeSinceStart : 17.406145095825195
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -149.800003
best mean reward -inf
running time 56.969489
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -149.8000030517578
TimeSinceStart : 56.969489097595215
Exploration Critic Loss : 693047.25
Exploitation Critic Loss : 0.00832095555961132
Exploration Model Loss : 45.21950149536133
Actor Loss : 1.1846492290496826
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -149.846161
best mean reward -inf
running time 95.878943
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -149.84616088867188
TimeSinceStart : 95.87894296646118
Exploration Critic Loss : 5497118.5
Exploitation Critic Loss : 0.01733039878308773
Exploration Model Loss : 41.70564270019531
Actor Loss : 0.9580307006835938
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -148.090912
best mean reward -inf
running time 133.142598
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -148.09091186523438
TimeSinceStart : 133.14259839057922
Exploration Critic Loss : 8313851.5
Exploitation Critic Loss : 0.05009579285979271
Exploration Model Loss : 87.22187805175781
Actor Loss : 0.8301112651824951
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -148.425003
best mean reward -inf
running time 172.093337
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -148.4250030517578
TimeSinceStart : 172.09333682060242
Exploration Critic Loss : 6612324.5
Exploitation Critic Loss : 0.03390850871801376
Exploration Model Loss : 51.754730224609375
Actor Loss : 0.6530923843383789
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -145.872345
best mean reward -inf
running time 209.907541
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -145.87234497070312
TimeSinceStart : 209.90754055976868
Exploration Critic Loss : 9306175.0
Exploitation Critic Loss : 0.11885898560285568
Exploration Model Loss : 61.54863357543945
Actor Loss : 0.594617486000061
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -142.678574
best mean reward -inf
running time 248.358398
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     446 / 1000At timestep:     564 / 1000At timestep:     714 / 1000At timestep:     864 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -142.67857360839844
TimeSinceStart : 248.35839796066284
Exploration Critic Loss : 7435307.0
Exploitation Critic Loss : 0.04401592165231705
Exploration Model Loss : 23.30157470703125
Actor Loss : 0.5101239085197449
Eval_AverageReturn : -144.57142639160156
Eval_StdReturn : 11.387426376342773
Eval_MaxReturn : -117.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 144.85714285714286
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -137.646149
best mean reward -inf
running time 287.323885
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     705 / 1000At timestep:     840 / 1000At timestep:     990 / 1000At timestep:     1140 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -137.64614868164062
TimeSinceStart : 287.32388496398926
Exploration Critic Loss : 6331121.0
Exploitation Critic Loss : 0.06640034914016724
Exploration Model Loss : 20.672027587890625
Actor Loss : 0.45765310525894165
Eval_AverageReturn : -142.25
Eval_StdReturn : 15.376523971557617
Eval_MaxReturn : -104.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 142.5
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -128.545456
best mean reward -inf
running time 325.584805
At timestep:     150 / 1000At timestep:     278 / 1000At timestep:     428 / 1000At timestep:     532 / 1000At timestep:     618 / 1000At timestep:     768 / 1000At timestep:     834 / 1000At timestep:     984 / 1000At timestep:     1114 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -128.5454559326172
TimeSinceStart : 325.58480525016785
Exploration Critic Loss : 5246886.0
Exploitation Critic Loss : 0.0808449313044548
Exploration Model Loss : 19.904983520507812
Actor Loss : 0.44246312975883484
Eval_AverageReturn : -123.22222137451172
Eval_StdReturn : 30.13959312438965
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 123.77777777777777
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -117.118279
best mean reward -inf
running time 366.113436
At timestep:     150 / 1000At timestep:     253 / 1000At timestep:     365 / 1000At timestep:     510 / 1000At timestep:     613 / 1000At timestep:     704 / 1000At timestep:     801 / 1000At timestep:     951 / 1000At timestep:     972 / 1000At timestep:     1122 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -117.11827850341797
TimeSinceStart : 366.1134355068207
Exploration Critic Loss : 5688076.0
Exploitation Critic Loss : 0.19572070240974426
Exploration Model Loss : 19.642349243164062
Actor Loss : 0.4381971061229706
Eval_AverageReturn : -111.5
Eval_StdReturn : 38.41679382324219
Eval_MaxReturn : -20.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 112.2
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -101.239998
best mean reward -101.239998
running time 406.764125
At timestep:     104 / 1000At timestep:     240 / 1000At timestep:     351 / 1000At timestep:     501 / 1000At timestep:     603 / 1000At timestep:     680 / 1000At timestep:     807 / 1000At timestep:     886 / 1000At timestep:     964 / 1000At timestep:     1101 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -101.23999786376953
Train_BestReturn : -101.23999786376953
TimeSinceStart : 406.7641246318817
Exploration Critic Loss : 4706289.0
Exploitation Critic Loss : 0.23660466074943542
Exploration Model Loss : 19.54803466796875
Actor Loss : 0.40605974197387695
Eval_AverageReturn : -109.19999694824219
Eval_StdReturn : 25.592185974121094
Eval_MaxReturn : -76.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 110.1
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -85.599998
best mean reward -85.599998
running time 448.238364
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     352 / 1000At timestep:     502 / 1000At timestep:     604 / 1000At timestep:     651 / 1000At timestep:     769 / 1000At timestep:     869 / 1000At timestep:     925 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -85.5999984741211
Train_BestReturn : -85.5999984741211
TimeSinceStart : 448.2383635044098
Exploration Critic Loss : 4187727.5
Exploitation Critic Loss : 0.14248928427696228
Exploration Model Loss : 19.512359619140625
Actor Loss : 0.4079393744468689
Eval_AverageReturn : -101.69999694824219
Eval_StdReturn : 38.869136810302734
Eval_MaxReturn : -46.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 102.4
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -58.820000
best mean reward -58.820000
running time 492.262990
At timestep:     143 / 1000At timestep:     293 / 1000At timestep:     443 / 1000At timestep:     550 / 1000At timestep:     693 / 1000At timestep:     781 / 1000At timestep:     861 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -58.81999969482422
Train_BestReturn : -58.81999969482422
TimeSinceStart : 492.2629897594452
Exploration Critic Loss : 6845509.5
Exploitation Critic Loss : 0.07460575550794601
Exploration Model Loss : 19.498947143554688
Actor Loss : 0.37943726778030396
Eval_AverageReturn : -124.875
Eval_StdReturn : 27.551939010620117
Eval_MaxReturn : -79.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 125.625
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -48.509998
best mean reward -48.509998
running time 533.656372
At timestep:     23 / 1000At timestep:     173 / 1000At timestep:     312 / 1000At timestep:     387 / 1000At timestep:     433 / 1000At timestep:     504 / 1000At timestep:     580 / 1000At timestep:     650 / 1000At timestep:     745 / 1000At timestep:     797 / 1000At timestep:     947 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -48.5099983215332
Train_BestReturn : -48.5099983215332
TimeSinceStart : 533.6563723087311
Exploration Critic Loss : 4249772.5
Exploitation Critic Loss : 0.3021537661552429
Exploration Model Loss : 19.49420166015625
Actor Loss : 0.41405218839645386
Eval_AverageReturn : -82.5
Eval_StdReturn : 40.6006965637207
Eval_MaxReturn : -22.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 83.33333333333333
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -42.139999
best mean reward -42.139999
running time 576.616869
At timestep:     150 / 1000At timestep:     192 / 1000At timestep:     341 / 1000At timestep:     419 / 1000At timestep:     471 / 1000At timestep:     621 / 1000At timestep:     742 / 1000At timestep:     801 / 1000At timestep:     898 / 1000At timestep:     927 / 1000At timestep:     978 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -42.13999938964844
Train_BestReturn : -42.13999938964844
TimeSinceStart : 576.6168687343597
Exploration Critic Loss : 1635111.0
Exploitation Critic Loss : 0.19074639678001404
Exploration Model Loss : 19.492263793945312
Actor Loss : 0.3960576355457306
Eval_AverageReturn : -84.5
Eval_StdReturn : 44.460655212402344
Eval_MaxReturn : -28.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 85.33333333333333
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -38.240002
best mean reward -38.240002
running time 618.965265
At timestep:     99 / 1000At timestep:     136 / 1000At timestep:     221 / 1000At timestep:     360 / 1000At timestep:     423 / 1000At timestep:     472 / 1000At timestep:     559 / 1000At timestep:     709 / 1000At timestep:     754 / 1000At timestep:     783 / 1000At timestep:     901 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -38.2400016784668
Train_BestReturn : -38.2400016784668
TimeSinceStart : 618.9652647972107
Exploration Critic Loss : 1917238.75
Exploitation Critic Loss : 0.15810470283031464
Exploration Model Loss : 19.491607666015625
Actor Loss : 0.40234750509262085
Eval_AverageReturn : -84.33333587646484
Eval_StdReturn : 39.52706527709961
Eval_MaxReturn : -28.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 85.25
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -38.700001
best mean reward -38.240002
running time 659.414716
At timestep:     42 / 1000At timestep:     103 / 1000At timestep:     144 / 1000At timestep:     265 / 1000At timestep:     365 / 1000At timestep:     406 / 1000At timestep:     464 / 1000At timestep:     556 / 1000At timestep:     641 / 1000At timestep:     724 / 1000At timestep:     845 / 1000At timestep:     964 / 1000At timestep:     1082 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -38.70000076293945
Train_BestReturn : -38.2400016784668
TimeSinceStart : 659.4147162437439
Exploration Critic Loss : 1316530.5
Exploitation Critic Loss : 0.1133325845003128
Exploration Model Loss : 19.491241455078125
Actor Loss : 0.41470813751220703
Eval_AverageReturn : -82.23076629638672
Eval_StdReturn : 30.508899688720703
Eval_MaxReturn : -40.0
Eval_MinReturn : -120.0
Eval_AverageEpLen : 83.23076923076923
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -35.590000
best mean reward -35.590000
running time 702.641720
At timestep:     85 / 1000At timestep:     183 / 1000At timestep:     333 / 1000At timestep:     379 / 1000At timestep:     509 / 1000At timestep:     546 / 1000At timestep:     586 / 1000At timestep:     654 / 1000At timestep:     743 / 1000At timestep:     799 / 1000At timestep:     847 / 1000At timestep:     952 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -35.59000015258789
Train_BestReturn : -35.59000015258789
TimeSinceStart : 702.6417195796967
Exploration Critic Loss : 694219.8125
Exploitation Critic Loss : 0.13886825740337372
Exploration Model Loss : 19.490753173828125
Actor Loss : 0.48483604192733765
Eval_AverageReturn : -76.69230651855469
Eval_StdReturn : 34.42356872558594
Eval_MaxReturn : -36.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 77.61538461538461
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -35.290001
best mean reward -35.290001
running time 745.605382
At timestep:     102 / 1000At timestep:     156 / 1000At timestep:     193 / 1000At timestep:     286 / 1000At timestep:     357 / 1000At timestep:     407 / 1000At timestep:     494 / 1000At timestep:     554 / 1000At timestep:     704 / 1000At timestep:     747 / 1000At timestep:     872 / 1000At timestep:     907 / 1000At timestep:     976 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -35.290000915527344
Train_BestReturn : -35.290000915527344
TimeSinceStart : 745.6053822040558
Exploration Critic Loss : 698412.3125
Exploitation Critic Loss : 0.17866191267967224
Exploration Model Loss : 19.490493774414062
Actor Loss : 0.44908565282821655
Eval_AverageReturn : -72.57142639160156
Eval_StdReturn : 33.10095977783203
Eval_MaxReturn : -34.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 73.5
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -35.430000
best mean reward -35.290001
running time 786.463216
At timestep:     150 / 1000At timestep:     209 / 1000At timestep:     326 / 1000At timestep:     379 / 1000At timestep:     420 / 1000At timestep:     485 / 1000At timestep:     622 / 1000At timestep:     737 / 1000At timestep:     832 / 1000At timestep:     913 / 1000At timestep:     967 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -35.43000030517578
Train_BestReturn : -35.290000915527344
TimeSinceStart : 786.4632158279419
Exploration Critic Loss : 717933.875
Exploitation Critic Loss : 0.11911121755838394
Exploration Model Loss : 19.490066528320312
Actor Loss : 0.4541918635368347
Eval_AverageReturn : -85.66666412353516
Eval_StdReturn : 34.43915557861328
Eval_MaxReturn : -40.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 86.58333333333333
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -35.919998
best mean reward -35.290001
running time 828.629023
At timestep:     87 / 1000At timestep:     130 / 1000At timestep:     243 / 1000At timestep:     294 / 1000At timestep:     365 / 1000At timestep:     433 / 1000At timestep:     490 / 1000At timestep:     543 / 1000At timestep:     612 / 1000At timestep:     680 / 1000At timestep:     812 / 1000At timestep:     867 / 1000At timestep:     900 / 1000At timestep:     976 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -35.91999816894531
Train_BestReturn : -35.290000915527344
TimeSinceStart : 828.6290230751038
Exploration Critic Loss : 640718.8125
Exploitation Critic Loss : 0.17491833865642548
Exploration Model Loss : 19.489501953125
Actor Loss : 0.45093482732772827
Eval_AverageReturn : -67.0
Eval_StdReturn : 25.54212188720703
Eval_MaxReturn : -32.0
Eval_MinReturn : -131.0
Eval_AverageEpLen : 68.0
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -36.919998
best mean reward -35.290001
running time 871.941357
At timestep:     22 / 1000At timestep:     93 / 1000At timestep:     133 / 1000At timestep:     183 / 1000At timestep:     226 / 1000At timestep:     253 / 1000At timestep:     299 / 1000At timestep:     399 / 1000At timestep:     466 / 1000At timestep:     516 / 1000At timestep:     612 / 1000At timestep:     676 / 1000At timestep:     715 / 1000At timestep:     769 / 1000At timestep:     812 / 1000At timestep:     932 / 1000At timestep:     974 / 1000At timestep:     1048 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -36.91999816894531
Train_BestReturn : -35.290000915527344
TimeSinceStart : 871.9413566589355
Exploration Critic Loss : 565320.875
Exploitation Critic Loss : 0.12696513533592224
Exploration Model Loss : 19.48907470703125
Actor Loss : 0.49152106046676636
Eval_AverageReturn : -57.22222137451172
Eval_StdReturn : 25.308223724365234
Eval_MaxReturn : -21.0
Eval_MinReturn : -119.0
Eval_AverageEpLen : 58.22222222222222
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -34.900002
best mean reward -34.900002
running time 916.667634
At timestep:     49 / 1000At timestep:     107 / 1000At timestep:     164 / 1000At timestep:     202 / 1000At timestep:     243 / 1000At timestep:     266 / 1000At timestep:     337 / 1000At timestep:     363 / 1000At timestep:     414 / 1000At timestep:     445 / 1000At timestep:     475 / 1000At timestep:     538 / 1000At timestep:     590 / 1000At timestep:     619 / 1000At timestep:     652 / 1000At timestep:     766 / 1000At timestep:     835 / 1000At timestep:     962 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -34.900001525878906
Train_BestReturn : -34.900001525878906
TimeSinceStart : 916.6676342487335
Exploration Critic Loss : 474285.53125
Exploitation Critic Loss : 0.10364926606416702
Exploration Model Loss : 19.488525390625
Actor Loss : 0.4608471691608429
Eval_AverageReturn : -51.894737243652344
Eval_StdReturn : 27.16982650756836
Eval_MaxReturn : -22.0
Eval_MinReturn : -126.0
Eval_AverageEpLen : 52.89473684210526
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -33.480000
best mean reward -33.480000
running time 961.012442
At timestep:     105 / 1000At timestep:     183 / 1000At timestep:     289 / 1000At timestep:     369 / 1000At timestep:     470 / 1000At timestep:     534 / 1000At timestep:     555 / 1000At timestep:     599 / 1000At timestep:     644 / 1000At timestep:     729 / 1000At timestep:     779 / 1000At timestep:     835 / 1000At timestep:     911 / 1000At timestep:     998 / 1000At timestep:     1096 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -33.47999954223633
Train_BestReturn : -33.47999954223633
TimeSinceStart : 961.0124423503876
Exploration Critic Loss : 555383.0
Exploitation Critic Loss : 0.11705296486616135
Exploration Model Loss : 19.488067626953125
Actor Loss : 0.5105908513069153
Eval_AverageReturn : -72.06666564941406
Eval_StdReturn : 24.729110717773438
Eval_MaxReturn : -20.0
Eval_MinReturn : -105.0
Eval_AverageEpLen : 73.06666666666666
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -30.639999
best mean reward -30.639999
running time 1004.520374
At timestep:     40 / 1000At timestep:     73 / 1000At timestep:     109 / 1000At timestep:     148 / 1000At timestep:     223 / 1000At timestep:     269 / 1000At timestep:     332 / 1000At timestep:     359 / 1000At timestep:     405 / 1000At timestep:     505 / 1000At timestep:     540 / 1000At timestep:     578 / 1000At timestep:     644 / 1000At timestep:     721 / 1000At timestep:     769 / 1000At timestep:     829 / 1000At timestep:     865 / 1000At timestep:     913 / 1000At timestep:     992 / 1000At timestep:     1052 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -30.639999389648438
Train_BestReturn : -30.639999389648438
TimeSinceStart : 1004.5203738212585
Exploration Critic Loss : 506456.75
Exploitation Critic Loss : 0.12794512510299683
Exploration Model Loss : 19.4876708984375
Actor Loss : 0.4710233509540558
Eval_AverageReturn : -51.599998474121094
Eval_StdReturn : 18.74139976501465
Eval_MaxReturn : -26.0
Eval_MinReturn : -99.0
Eval_AverageEpLen : 52.6
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -29.070000
best mean reward -29.070000
running time 1050.156597
At timestep:     38 / 1000At timestep:     79 / 1000At timestep:     106 / 1000At timestep:     154 / 1000At timestep:     193 / 1000At timestep:     246 / 1000At timestep:     328 / 1000At timestep:     377 / 1000At timestep:     449 / 1000At timestep:     521 / 1000At timestep:     568 / 1000At timestep:     614 / 1000At timestep:     657 / 1000At timestep:     730 / 1000At timestep:     868 / 1000At timestep:     942 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -29.06999969482422
Train_BestReturn : -29.06999969482422
TimeSinceStart : 1050.156596660614
Exploration Critic Loss : 460389.875
Exploitation Critic Loss : 0.11608685553073883
Exploration Model Loss : 19.487274169921875
Actor Loss : 0.4501606822013855
Eval_AverageReturn : -58.29411697387695
Eval_StdReturn : 24.96059226989746
Eval_MaxReturn : -26.0
Eval_MinReturn : -137.0
Eval_AverageEpLen : 59.294117647058826
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -27.760000
best mean reward -27.760000
running time 1094.307169
At timestep:     54 / 1000At timestep:     162 / 1000At timestep:     192 / 1000At timestep:     252 / 1000At timestep:     283 / 1000At timestep:     321 / 1000At timestep:     362 / 1000At timestep:     405 / 1000At timestep:     457 / 1000At timestep:     552 / 1000At timestep:     583 / 1000At timestep:     704 / 1000At timestep:     737 / 1000At timestep:     789 / 1000At timestep:     860 / 1000At timestep:     892 / 1000At timestep:     971 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -27.760000228881836
Train_BestReturn : -27.760000228881836
TimeSinceStart : 1094.3071694374084
Exploration Critic Loss : 380231.8125
Exploitation Critic Loss : 0.11875277757644653
Exploration Model Loss : 19.486663818359375
Actor Loss : 0.5106390714645386
Eval_AverageReturn : -55.16666793823242
Eval_StdReturn : 27.14211654663086
Eval_MaxReturn : -29.0
Eval_MinReturn : -120.0
Eval_AverageEpLen : 56.166666666666664
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -27.209999
best mean reward -27.209999
running time 1137.360441
At timestep:     82 / 1000At timestep:     140 / 1000At timestep:     185 / 1000At timestep:     246 / 1000At timestep:     296 / 1000At timestep:     331 / 1000At timestep:     364 / 1000At timestep:     396 / 1000At timestep:     430 / 1000At timestep:     452 / 1000At timestep:     505 / 1000At timestep:     575 / 1000At timestep:     619 / 1000At timestep:     678 / 1000At timestep:     724 / 1000At timestep:     792 / 1000At timestep:     855 / 1000At timestep:     925 / 1000At timestep:     965 / 1000At timestep:     1036 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -27.209999084472656
Train_BestReturn : -27.209999084472656
TimeSinceStart : 1137.3604407310486
Exploration Critic Loss : 406280.78125
Exploitation Critic Loss : 0.11618086695671082
Exploration Model Loss : 19.486160278320312
Actor Loss : 0.5458037853240967
Eval_AverageReturn : -50.79999923706055
Eval_StdReturn : 15.848028182983398
Eval_MaxReturn : -21.0
Eval_MinReturn : -81.0
Eval_AverageEpLen : 51.8
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -27.700001
best mean reward -27.209999
running time 1181.323943
At timestep:     39 / 1000At timestep:     119 / 1000At timestep:     202 / 1000At timestep:     255 / 1000At timestep:     310 / 1000At timestep:     354 / 1000At timestep:     391 / 1000At timestep:     441 / 1000At timestep:     499 / 1000At timestep:     522 / 1000At timestep:     565 / 1000At timestep:     617 / 1000At timestep:     662 / 1000At timestep:     729 / 1000At timestep:     784 / 1000At timestep:     833 / 1000At timestep:     859 / 1000At timestep:     915 / 1000At timestep:     961 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -27.700000762939453
Train_BestReturn : -27.209999084472656
TimeSinceStart : 1181.3239433765411
Exploration Critic Loss : 453865.25
Exploitation Critic Loss : 0.09788448363542557
Exploration Model Loss : 19.48577880859375
Actor Loss : 0.49803972244262695
Eval_AverageReturn : -49.75
Eval_StdReturn : 14.435632705688477
Eval_MaxReturn : -22.0
Eval_MinReturn : -82.0
Eval_AverageEpLen : 50.75
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -27.629999
best mean reward -27.209999
running time 1226.752296
At timestep:     76 / 1000At timestep:     106 / 1000At timestep:     146 / 1000At timestep:     170 / 1000At timestep:     233 / 1000At timestep:     301 / 1000At timestep:     347 / 1000At timestep:     390 / 1000At timestep:     441 / 1000At timestep:     484 / 1000At timestep:     536 / 1000At timestep:     563 / 1000At timestep:     628 / 1000At timestep:     670 / 1000At timestep:     730 / 1000At timestep:     788 / 1000At timestep:     872 / 1000At timestep:     934 / 1000At timestep:     986 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -27.6299991607666
Train_BestReturn : -27.209999084472656
TimeSinceStart : 1226.752296447754
Exploration Critic Loss : 525082.5
Exploitation Critic Loss : 0.14160487055778503
Exploration Model Loss : 19.485153198242188
Actor Loss : 0.4575690031051636
Eval_AverageReturn : -50.400001525878906
Eval_StdReturn : 15.451212882995605
Eval_MaxReturn : -23.0
Eval_MinReturn : -83.0
Eval_AverageEpLen : 51.4
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -27.500000
best mean reward -27.209999
running time 1270.184294
At timestep:     39 / 1000At timestep:     137 / 1000At timestep:     192 / 1000At timestep:     251 / 1000At timestep:     333 / 1000At timestep:     360 / 1000At timestep:     445 / 1000At timestep:     480 / 1000At timestep:     537 / 1000At timestep:     565 / 1000At timestep:     611 / 1000At timestep:     657 / 1000At timestep:     695 / 1000At timestep:     756 / 1000At timestep:     802 / 1000At timestep:     850 / 1000At timestep:     889 / 1000At timestep:     919 / 1000At timestep:     967 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -27.5
Train_BestReturn : -27.209999084472656
TimeSinceStart : 1270.1842939853668
Exploration Critic Loss : 383631.5
Exploitation Critic Loss : 0.08723330497741699
Exploration Model Loss : 19.484649658203125
Actor Loss : 0.5545452237129211
Eval_AverageReturn : -50.20000076293945
Eval_StdReturn : 18.613435745239258
Eval_MaxReturn : -26.0
Eval_MinReturn : -97.0
Eval_AverageEpLen : 51.2
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -26.969999
best mean reward -26.969999
running time 1313.947004
At timestep:     57 / 1000At timestep:     77 / 1000At timestep:     117 / 1000At timestep:     152 / 1000At timestep:     219 / 1000At timestep:     280 / 1000At timestep:     324 / 1000At timestep:     377 / 1000At timestep:     411 / 1000At timestep:     446 / 1000At timestep:     481 / 1000At timestep:     528 / 1000At timestep:     553 / 1000At timestep:     599 / 1000At timestep:     637 / 1000At timestep:     698 / 1000At timestep:     728 / 1000At timestep:     767 / 1000At timestep:     804 / 1000At timestep:     842 / 1000At timestep:     886 / 1000At timestep:     935 / 1000At timestep:     969 / 1000At timestep:     999 / 1000At timestep:     1087 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -26.969999313354492
Train_BestReturn : -26.969999313354492
TimeSinceStart : 1313.9470038414001
Exploration Critic Loss : 362812.4375
Exploitation Critic Loss : 0.10445782542228699
Exploration Model Loss : 19.4840087890625
Actor Loss : 0.5133681297302246
Eval_AverageReturn : -42.47999954223633
Eval_StdReturn : 14.502744674682617
Eval_MaxReturn : -19.0
Eval_MinReturn : -87.0
Eval_AverageEpLen : 43.48
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -27.120001
best mean reward -26.969999
running time 1358.504376
At timestep:     29 / 1000At timestep:     70 / 1000At timestep:     144 / 1000At timestep:     175 / 1000At timestep:     200 / 1000At timestep:     244 / 1000At timestep:     306 / 1000At timestep:     366 / 1000At timestep:     425 / 1000At timestep:     457 / 1000At timestep:     508 / 1000At timestep:     552 / 1000At timestep:     619 / 1000At timestep:     668 / 1000At timestep:     690 / 1000At timestep:     725 / 1000At timestep:     789 / 1000At timestep:     841 / 1000At timestep:     888 / 1000At timestep:     917 / 1000At timestep:     954 / 1000At timestep:     978 / 1000At timestep:     1036 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -27.1200008392334
Train_BestReturn : -26.969999313354492
TimeSinceStart : 1358.5043761730194
Exploration Critic Loss : 281517.3125
Exploitation Critic Loss : 0.11334151774644852
Exploration Model Loss : 19.483489990234375
Actor Loss : 0.5301638841629028
Eval_AverageReturn : -44.043479919433594
Eval_StdReturn : 14.889385223388672
Eval_MaxReturn : -21.0
Eval_MinReturn : -73.0
Eval_AverageEpLen : 45.04347826086956
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -27.299999
best mean reward -26.969999
running time 1405.077839
At timestep:     36 / 1000At timestep:     136 / 1000At timestep:     166 / 1000At timestep:     217 / 1000At timestep:     272 / 1000At timestep:     309 / 1000At timestep:     380 / 1000At timestep:     437 / 1000At timestep:     506 / 1000At timestep:     549 / 1000At timestep:     592 / 1000At timestep:     614 / 1000At timestep:     643 / 1000At timestep:     669 / 1000At timestep:     732 / 1000At timestep:     796 / 1000At timestep:     833 / 1000At timestep:     875 / 1000At timestep:     928 / 1000At timestep:     965 / 1000At timestep:     983 / 1000At timestep:     1069 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -27.299999237060547
Train_BestReturn : -26.969999313354492
TimeSinceStart : 1405.0778393745422
Exploration Critic Loss : 307016.78125
Exploitation Critic Loss : 0.09466727077960968
Exploration Model Loss : 19.4830322265625
Actor Loss : 0.5655737519264221
Eval_AverageReturn : -47.59090805053711
Eval_StdReturn : 20.330583572387695
Eval_MaxReturn : -17.0
Eval_MinReturn : -99.0
Eval_AverageEpLen : 48.59090909090909
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -27.100000
best mean reward -26.969999
running time 1448.500947
At timestep:     43 / 1000At timestep:     74 / 1000At timestep:     102 / 1000At timestep:     156 / 1000At timestep:     179 / 1000At timestep:     210 / 1000At timestep:     242 / 1000At timestep:     287 / 1000At timestep:     328 / 1000At timestep:     387 / 1000At timestep:     417 / 1000At timestep:     472 / 1000At timestep:     514 / 1000At timestep:     565 / 1000At timestep:     602 / 1000At timestep:     657 / 1000At timestep:     690 / 1000At timestep:     784 / 1000At timestep:     831 / 1000At timestep:     882 / 1000At timestep:     913 / 1000At timestep:     949 / 1000At timestep:     985 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -27.100000381469727
Train_BestReturn : -26.969999313354492
TimeSinceStart : 1448.5009472370148
Exploration Critic Loss : 203289.5
Exploitation Critic Loss : 0.13960880041122437
Exploration Model Loss : 19.4825439453125
Actor Loss : 0.5766225457191467
Eval_AverageReturn : -41.33333206176758
Eval_StdReturn : 14.624939918518066
Eval_MaxReturn : -22.0
Eval_MinReturn : -93.0
Eval_AverageEpLen : 42.333333333333336
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -25.750000
best mean reward -25.750000
running time 1493.973567
At timestep:     36 / 1000At timestep:     84 / 1000At timestep:     131 / 1000At timestep:     159 / 1000At timestep:     191 / 1000At timestep:     222 / 1000At timestep:     272 / 1000At timestep:     299 / 1000At timestep:     341 / 1000At timestep:     385 / 1000At timestep:     452 / 1000At timestep:     505 / 1000At timestep:     559 / 1000At timestep:     606 / 1000At timestep:     650 / 1000At timestep:     726 / 1000At timestep:     759 / 1000At timestep:     808 / 1000At timestep:     872 / 1000At timestep:     918 / 1000At timestep:     952 / 1000At timestep:     979 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -25.75
Train_BestReturn : -25.75
TimeSinceStart : 1493.9735667705536
Exploration Critic Loss : 255193.4375
Exploitation Critic Loss : 0.13060268759727478
Exploration Model Loss : 19.482009887695312
Actor Loss : 0.5251532793045044
Eval_AverageReturn : -42.60869598388672
Eval_StdReturn : 13.295108795166016
Eval_MaxReturn : -23.0
Eval_MinReturn : -75.0
Eval_AverageEpLen : 43.608695652173914
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -25.639999
best mean reward -25.639999
running time 1539.760402
At timestep:     44 / 1000At timestep:     68 / 1000At timestep:     128 / 1000At timestep:     159 / 1000At timestep:     221 / 1000At timestep:     243 / 1000At timestep:     285 / 1000At timestep:     356 / 1000At timestep:     386 / 1000At timestep:     432 / 1000At timestep:     479 / 1000At timestep:     513 / 1000At timestep:     589 / 1000At timestep:     654 / 1000At timestep:     682 / 1000At timestep:     728 / 1000At timestep:     754 / 1000At timestep:     796 / 1000At timestep:     846 / 1000At timestep:     883 / 1000At timestep:     915 / 1000At timestep:     959 / 1000At timestep:     982 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -25.639999389648438
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1539.760401725769
Exploration Critic Loss : 294160.0
Exploitation Critic Loss : 0.15891264379024506
Exploration Model Loss : 19.481399536132812
Actor Loss : 0.5604918003082275
Eval_AverageReturn : -40.79166793823242
Eval_StdReturn : 15.558971405029297
Eval_MaxReturn : -20.0
Eval_MinReturn : -75.0
Eval_AverageEpLen : 41.791666666666664
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -26.330000
best mean reward -25.639999
running time 1585.929358
At timestep:     35 / 1000At timestep:     72 / 1000At timestep:     127 / 1000At timestep:     166 / 1000At timestep:     232 / 1000At timestep:     291 / 1000At timestep:     364 / 1000At timestep:     392 / 1000At timestep:     433 / 1000At timestep:     493 / 1000At timestep:     532 / 1000At timestep:     590 / 1000At timestep:     625 / 1000At timestep:     673 / 1000At timestep:     721 / 1000At timestep:     771 / 1000At timestep:     806 / 1000At timestep:     869 / 1000At timestep:     927 / 1000At timestep:     966 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -26.329999923706055
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1585.9293575286865
Exploration Critic Loss : 307631.96875
Exploitation Critic Loss : 0.12219467759132385
Exploration Model Loss : 19.480880737304688
Actor Loss : 0.5127905607223511
Eval_AverageReturn : -47.619049072265625
Eval_StdReturn : 12.0573091506958
Eval_MaxReturn : -27.0
Eval_MinReturn : -72.0
Eval_AverageEpLen : 48.61904761904762
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -25.719999
best mean reward -25.639999
running time 1630.796432
At timestep:     50 / 1000At timestep:     92 / 1000At timestep:     122 / 1000At timestep:     181 / 1000At timestep:     224 / 1000At timestep:     255 / 1000At timestep:     292 / 1000At timestep:     326 / 1000At timestep:     419 / 1000At timestep:     448 / 1000At timestep:     481 / 1000At timestep:     515 / 1000At timestep:     573 / 1000At timestep:     600 / 1000At timestep:     629 / 1000At timestep:     657 / 1000At timestep:     687 / 1000At timestep:     723 / 1000At timestep:     767 / 1000At timestep:     812 / 1000At timestep:     836 / 1000At timestep:     878 / 1000At timestep:     932 / 1000At timestep:     962 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -25.719999313354492
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1630.7964322566986
Exploration Critic Loss : 216376.84375
Exploitation Critic Loss : 0.08836191892623901
Exploration Model Loss : 19.4803466796875
Actor Loss : 0.48378312587738037
Eval_AverageReturn : -39.040000915527344
Eval_StdReturn : 14.404109954833984
Eval_MaxReturn : -23.0
Eval_MinReturn : -92.0
Eval_AverageEpLen : 40.04
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -25.870001
best mean reward -25.639999
running time 1676.263506
At timestep:     28 / 1000At timestep:     69 / 1000At timestep:     105 / 1000At timestep:     152 / 1000At timestep:     196 / 1000At timestep:     220 / 1000At timestep:     328 / 1000At timestep:     395 / 1000At timestep:     424 / 1000At timestep:     459 / 1000At timestep:     498 / 1000At timestep:     557 / 1000At timestep:     593 / 1000At timestep:     655 / 1000At timestep:     720 / 1000At timestep:     758 / 1000At timestep:     803 / 1000At timestep:     842 / 1000At timestep:     867 / 1000At timestep:     907 / 1000At timestep:     949 / 1000At timestep:     987 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -25.8700008392334
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1676.2635061740875
Exploration Critic Loss : 281701.84375
Exploitation Critic Loss : 0.13170044124126434
Exploration Model Loss : 19.480026245117188
Actor Loss : 0.4722678065299988
Eval_AverageReturn : -44.130435943603516
Eval_StdReturn : 17.75020980834961
Eval_MaxReturn : -23.0
Eval_MinReturn : -107.0
Eval_AverageEpLen : 45.130434782608695
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -26.670000
best mean reward -25.639999
running time 1721.272622
At timestep:     42 / 1000At timestep:     87 / 1000At timestep:     122 / 1000At timestep:     148 / 1000At timestep:     203 / 1000At timestep:     238 / 1000At timestep:     251 / 1000At timestep:     290 / 1000At timestep:     319 / 1000At timestep:     343 / 1000At timestep:     381 / 1000At timestep:     421 / 1000At timestep:     454 / 1000At timestep:     484 / 1000At timestep:     505 / 1000At timestep:     546 / 1000At timestep:     588 / 1000At timestep:     630 / 1000At timestep:     666 / 1000At timestep:     701 / 1000At timestep:     758 / 1000At timestep:     788 / 1000At timestep:     847 / 1000At timestep:     881 / 1000At timestep:     917 / 1000At timestep:     939 / 1000At timestep:     960 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -26.670000076293945
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1721.272621870041
Exploration Critic Loss : 236636.875
Exploitation Critic Loss : 0.09297981858253479
Exploration Model Loss : 19.479507446289062
Actor Loss : 0.48925647139549255
Eval_AverageReturn : -34.75
Eval_StdReturn : 10.612071990966797
Eval_MaxReturn : -12.0
Eval_MinReturn : -58.0
Eval_AverageEpLen : 35.75
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -26.580000
best mean reward -25.639999
running time 1767.049703
At timestep:     32 / 1000At timestep:     96 / 1000At timestep:     132 / 1000At timestep:     176 / 1000At timestep:     240 / 1000At timestep:     284 / 1000At timestep:     320 / 1000At timestep:     360 / 1000At timestep:     392 / 1000At timestep:     447 / 1000At timestep:     495 / 1000At timestep:     537 / 1000At timestep:     572 / 1000At timestep:     600 / 1000At timestep:     630 / 1000At timestep:     660 / 1000At timestep:     722 / 1000At timestep:     750 / 1000At timestep:     779 / 1000At timestep:     818 / 1000At timestep:     867 / 1000At timestep:     906 / 1000At timestep:     932 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -26.579999923706055
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1767.0497026443481
Exploration Critic Loss : 226249.96875
Exploitation Critic Loss : 0.07657225430011749
Exploration Model Loss : 19.47900390625
Actor Loss : 0.49323803186416626
Eval_AverageReturn : -40.79166793823242
Eval_StdReturn : 12.776865005493164
Eval_MaxReturn : -25.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 41.791666666666664
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -27.250000
best mean reward -25.639999
running time 1811.080988
At timestep:     36 / 1000At timestep:     71 / 1000At timestep:     92 / 1000At timestep:     118 / 1000At timestep:     168 / 1000At timestep:     211 / 1000At timestep:     251 / 1000At timestep:     294 / 1000At timestep:     364 / 1000At timestep:     402 / 1000At timestep:     434 / 1000At timestep:     495 / 1000At timestep:     537 / 1000At timestep:     596 / 1000At timestep:     622 / 1000At timestep:     660 / 1000At timestep:     691 / 1000At timestep:     724 / 1000At timestep:     760 / 1000At timestep:     799 / 1000At timestep:     838 / 1000At timestep:     867 / 1000At timestep:     896 / 1000At timestep:     951 / 1000At timestep:     992 / 1000At timestep:     1041 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -27.25
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1811.0809876918793
Exploration Critic Loss : 236834.515625
Exploitation Critic Loss : 0.1692952811717987
Exploration Model Loss : 19.47900390625
Actor Loss : 0.5024560689926147
Eval_AverageReturn : -39.03845977783203
Eval_StdReturn : 11.41348934173584
Eval_MaxReturn : -20.0
Eval_MinReturn : -69.0
Eval_AverageEpLen : 40.03846153846154
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -26.020000
best mean reward -25.639999
running time 1856.143915
At timestep:     37 / 1000At timestep:     70 / 1000At timestep:     94 / 1000At timestep:     130 / 1000At timestep:     168 / 1000At timestep:     200 / 1000At timestep:     277 / 1000At timestep:     337 / 1000At timestep:     367 / 1000At timestep:     393 / 1000At timestep:     445 / 1000At timestep:     491 / 1000At timestep:     534 / 1000At timestep:     574 / 1000At timestep:     600 / 1000At timestep:     654 / 1000At timestep:     697 / 1000At timestep:     738 / 1000At timestep:     768 / 1000At timestep:     799 / 1000At timestep:     830 / 1000At timestep:     880 / 1000At timestep:     927 / 1000At timestep:     954 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -26.020000457763672
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1856.143914937973
Exploration Critic Loss : 194948.8125
Exploitation Critic Loss : 0.09958240389823914
Exploration Model Loss : 19.47900390625
Actor Loss : 0.5500608086585999
Eval_AverageReturn : -39.20000076293945
Eval_StdReturn : 12.253978729248047
Eval_MaxReturn : -23.0
Eval_MinReturn : -76.0
Eval_AverageEpLen : 40.2
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -26.370001
best mean reward -25.639999
running time 1901.780390
At timestep:     75 / 1000At timestep:     99 / 1000At timestep:     135 / 1000At timestep:     174 / 1000At timestep:     203 / 1000At timestep:     295 / 1000At timestep:     363 / 1000At timestep:     401 / 1000At timestep:     447 / 1000At timestep:     481 / 1000At timestep:     515 / 1000At timestep:     538 / 1000At timestep:     577 / 1000At timestep:     634 / 1000At timestep:     670 / 1000At timestep:     718 / 1000At timestep:     769 / 1000At timestep:     809 / 1000At timestep:     853 / 1000At timestep:     913 / 1000At timestep:     945 / 1000At timestep:     981 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -26.3700008392334
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1901.780389547348
Exploration Critic Loss : 173451.03125
Exploitation Critic Loss : 0.08637303858995438
Exploration Model Loss : 19.478988647460938
Actor Loss : 0.4860406219959259
Eval_AverageReturn : -43.08695602416992
Eval_StdReturn : 16.365161895751953
Eval_MaxReturn : -22.0
Eval_MinReturn : -91.0
Eval_AverageEpLen : 44.08695652173913
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -26.309999
best mean reward -25.639999
running time 1946.707165
At timestep:     28 / 1000At timestep:     69 / 1000At timestep:     98 / 1000At timestep:     121 / 1000At timestep:     139 / 1000At timestep:     200 / 1000At timestep:     246 / 1000At timestep:     287 / 1000At timestep:     324 / 1000At timestep:     401 / 1000At timestep:     442 / 1000At timestep:     480 / 1000At timestep:     513 / 1000At timestep:     536 / 1000At timestep:     574 / 1000At timestep:     607 / 1000At timestep:     655 / 1000At timestep:     690 / 1000At timestep:     739 / 1000At timestep:     761 / 1000At timestep:     804 / 1000At timestep:     838 / 1000At timestep:     874 / 1000At timestep:     907 / 1000At timestep:     936 / 1000At timestep:     968 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -26.309999465942383
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1946.7071647644043
Exploration Critic Loss : 159776.234375
Exploitation Critic Loss : 0.0971129983663559
Exploration Model Loss : 19.478775024414062
Actor Loss : 0.4892946481704712
Eval_AverageReturn : -36.25925827026367
Eval_StdReturn : 11.98947811126709
Eval_MaxReturn : -17.0
Eval_MinReturn : -76.0
Eval_AverageEpLen : 37.25925925925926
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -27.250000
best mean reward -25.639999
running time 1991.636012
At timestep:     39 / 1000At timestep:     90 / 1000At timestep:     128 / 1000At timestep:     157 / 1000At timestep:     183 / 1000At timestep:     230 / 1000At timestep:     275 / 1000At timestep:     294 / 1000At timestep:     331 / 1000At timestep:     371 / 1000At timestep:     402 / 1000At timestep:     454 / 1000At timestep:     485 / 1000At timestep:     518 / 1000At timestep:     545 / 1000At timestep:     580 / 1000At timestep:     627 / 1000At timestep:     660 / 1000At timestep:     700 / 1000At timestep:     733 / 1000At timestep:     780 / 1000At timestep:     800 / 1000At timestep:     838 / 1000At timestep:     890 / 1000At timestep:     920 / 1000At timestep:     959 / 1000At timestep:     989 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -27.25
Train_BestReturn : -25.639999389648438
TimeSinceStart : 1991.6360120773315
Exploration Critic Loss : 148729.3125
Exploitation Critic Loss : 0.07606498897075653
Exploration Model Loss : 19.47808837890625
Actor Loss : 0.5237749814987183
Eval_AverageReturn : -35.25
Eval_StdReturn : 8.990571022033691
Eval_MaxReturn : -18.0
Eval_MinReturn : -51.0
Eval_AverageEpLen : 36.25
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -27.570000
best mean reward -25.639999
running time 2037.629918
At timestep:     33 / 1000At timestep:     58 / 1000At timestep:     85 / 1000At timestep:     147 / 1000At timestep:     182 / 1000At timestep:     234 / 1000At timestep:     275 / 1000At timestep:     307 / 1000At timestep:     336 / 1000At timestep:     364 / 1000At timestep:     423 / 1000At timestep:     471 / 1000At timestep:     499 / 1000At timestep:     554 / 1000At timestep:     591 / 1000At timestep:     612 / 1000At timestep:     648 / 1000At timestep:     676 / 1000At timestep:     703 / 1000At timestep:     733 / 1000At timestep:     762 / 1000At timestep:     794 / 1000At timestep:     838 / 1000At timestep:     900 / 1000At timestep:     943 / 1000At timestep:     970 / 1000At timestep:     1033 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_medium_supervised_lam0.1_PointmassMedium-v0_22-11-2022_21-24-30/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -27.56999969482422
Train_BestReturn : -25.639999389648438
TimeSinceStart : 2037.6299178600311
Exploration Critic Loss : 144945.765625
Exploitation Critic Loss : 0.10811864584684372
Exploration Model Loss : 19.477554321289062
Actor Loss : 0.463872492313385
Eval_AverageReturn : -37.25925827026367
Eval_StdReturn : 12.662551879882812
Eval_MaxReturn : -20.0
Eval_MinReturn : -62.0
Eval_AverageEpLen : 38.25925925925926
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam1_PointmassMedium-v0_22-11-2022_21-59-20 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam1_PointmassMedium-v0_22-11-2022_21-59-20
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002506
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0025060176849365234
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -143.666672
best mean reward -inf
running time 8.785604
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     666 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -143.6666717529297
TimeSinceStart : 8.785604476928711
Eval_AverageReturn : -139.375
Eval_StdReturn : 28.111106872558594
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.5
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -144.923080
best mean reward -inf
running time 18.005877
At timestep:     54 / 1000At timestep:     204 / 1000At timestep:     354 / 1000At timestep:     504 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -144.92308044433594
TimeSinceStart : 18.00587749481201
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -146.699997
best mean reward -inf
running time 56.431269
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -146.6999969482422
TimeSinceStart : 56.43126893043518
Exploration Critic Loss : 1125630.375
Exploitation Critic Loss : 0.00883611012250185
Exploration Model Loss : 76.40471649169922
Actor Loss : 1.1781630516052246
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -147.555557
best mean reward -inf
running time 94.008948
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -147.55555725097656
TimeSinceStart : 94.00894808769226
Exploration Critic Loss : 6889873.0
Exploitation Critic Loss : 0.009223864413797855
Exploration Model Loss : 47.61787033081055
Actor Loss : 0.9320536255836487
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -142.558823
best mean reward -inf
running time 132.566249
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -142.55882263183594
TimeSinceStart : 132.56624937057495
Exploration Critic Loss : 5960214.0
Exploitation Critic Loss : 0.011621411889791489
Exploration Model Loss : 25.387908935546875
Actor Loss : 0.8032994866371155
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -137.860458
best mean reward -inf
running time 169.291178
At timestep:     91 / 1000At timestep:     241 / 1000At timestep:     391 / 1000At timestep:     541 / 1000At timestep:     691 / 1000At timestep:     841 / 1000At timestep:     968 / 1000At timestep:     1118 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -137.86045837402344
TimeSinceStart : 169.29117846488953
Exploration Critic Loss : 7276175.0
Exploitation Critic Loss : 0.08822576701641083
Exploration Model Loss : 20.098098754882812
Actor Loss : 0.6843711733818054
Eval_AverageReturn : -139.5
Eval_StdReturn : 20.291624069213867
Eval_MaxReturn : -90.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.75
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -122.333336
best mean reward -inf
running time 208.842768
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     383 / 1000At timestep:     430 / 1000At timestep:     562 / 1000At timestep:     712 / 1000At timestep:     839 / 1000At timestep:     959 / 1000At timestep:     1074 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -122.33333587646484
TimeSinceStart : 208.84276795387268
Exploration Critic Loss : 7586995.5
Exploitation Critic Loss : 0.07096488028764725
Exploration Model Loss : 18.861190795898438
Actor Loss : 0.6189525723457336
Eval_AverageReturn : -118.66666412353516
Eval_StdReturn : 32.99494934082031
Eval_MaxReturn : -46.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 119.33333333333333
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -96.243904
best mean reward -inf
running time 249.437014
At timestep:     104 / 1000At timestep:     188 / 1000At timestep:     281 / 1000At timestep:     375 / 1000At timestep:     397 / 1000At timestep:     476 / 1000At timestep:     579 / 1000At timestep:     711 / 1000At timestep:     861 / 1000At timestep:     893 / 1000At timestep:     945 / 1000At timestep:     1063 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -96.24390411376953
TimeSinceStart : 249.4370138645172
Exploration Critic Loss : 8107862.0
Exploitation Critic Loss : 0.12144596129655838
Exploration Model Loss : 18.462799072265625
Actor Loss : 0.5556932687759399
Eval_AverageReturn : -87.66666412353516
Eval_StdReturn : 36.77257537841797
Eval_MaxReturn : -21.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 88.58333333333333
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -81.959999
best mean reward -81.959999
running time 289.845605
At timestep:     58 / 1000At timestep:     102 / 1000At timestep:     198 / 1000At timestep:     348 / 1000At timestep:     424 / 1000At timestep:     518 / 1000At timestep:     591 / 1000At timestep:     664 / 1000At timestep:     737 / 1000At timestep:     887 / 1000At timestep:     982 / 1000At timestep:     1048 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -81.95999908447266
Train_BestReturn : -81.95999908447266
TimeSinceStart : 289.845605134964
Exploration Critic Loss : 7001392.0
Exploitation Critic Loss : 0.09163003414869308
Exploration Model Loss : 18.322616577148438
Actor Loss : 0.5301411151885986
Eval_AverageReturn : -86.5
Eval_StdReturn : 31.915252685546875
Eval_MaxReturn : -43.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 87.33333333333333
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -60.349998
best mean reward -60.349998
running time 331.451848
At timestep:     98 / 1000At timestep:     158 / 1000At timestep:     308 / 1000At timestep:     382 / 1000At timestep:     411 / 1000At timestep:     468 / 1000At timestep:     618 / 1000At timestep:     724 / 1000At timestep:     874 / 1000At timestep:     932 / 1000At timestep:     995 / 1000At timestep:     1073 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -60.349998474121094
Train_BestReturn : -60.349998474121094
TimeSinceStart : 331.45184779167175
Exploration Critic Loss : 6572229.0
Exploitation Critic Loss : 0.13777759671211243
Exploration Model Loss : 18.271347045898438
Actor Loss : 0.5273153781890869
Eval_AverageReturn : -88.66666412353516
Eval_StdReturn : 40.15040969848633
Eval_MaxReturn : -28.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 89.41666666666667
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -40.130001
best mean reward -40.130001
running time 373.077690
At timestep:     64 / 1000At timestep:     214 / 1000At timestep:     287 / 1000At timestep:     437 / 1000At timestep:     515 / 1000At timestep:     543 / 1000At timestep:     601 / 1000At timestep:     668 / 1000At timestep:     818 / 1000At timestep:     968 / 1000At timestep:     1118 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -40.130001068115234
Train_BestReturn : -40.130001068115234
TimeSinceStart : 373.0776901245117
Exploration Critic Loss : 4480626.0
Exploitation Critic Loss : 0.0727669820189476
Exploration Model Loss : 18.25177001953125
Actor Loss : 0.4813554883003235
Eval_AverageReturn : -101.09091186523438
Eval_StdReturn : 46.22367477416992
Eval_MaxReturn : -27.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 101.63636363636364
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -39.470001
best mean reward -39.470001
running time 413.780769
At timestep:     47 / 1000At timestep:     117 / 1000At timestep:     198 / 1000At timestep:     224 / 1000At timestep:     340 / 1000At timestep:     394 / 1000At timestep:     424 / 1000At timestep:     505 / 1000At timestep:     607 / 1000At timestep:     661 / 1000At timestep:     721 / 1000At timestep:     778 / 1000At timestep:     852 / 1000At timestep:     930 / 1000At timestep:     965 / 1000At timestep:     1043 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -39.470001220703125
Train_BestReturn : -39.470001220703125
TimeSinceStart : 413.78076910972595
Exploration Critic Loss : 4995903.0
Exploitation Critic Loss : 0.11230741441249847
Exploration Model Loss : 18.244400024414062
Actor Loss : 0.5052772760391235
Eval_AverageReturn : -64.1875
Eval_StdReturn : 24.013586044311523
Eval_MaxReturn : -25.0
Eval_MinReturn : -115.0
Eval_AverageEpLen : 65.1875
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -38.799999
best mean reward -38.799999
running time 456.737543
At timestep:     65 / 1000At timestep:     111 / 1000At timestep:     171 / 1000At timestep:     277 / 1000At timestep:     405 / 1000At timestep:     436 / 1000At timestep:     529 / 1000At timestep:     608 / 1000At timestep:     650 / 1000At timestep:     692 / 1000At timestep:     762 / 1000At timestep:     857 / 1000At timestep:     891 / 1000At timestep:     985 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -38.79999923706055
Train_BestReturn : -38.79999923706055
TimeSinceStart : 456.73754262924194
Exploration Critic Loss : 8929684.0
Exploitation Critic Loss : 0.10215432941913605
Exploration Model Loss : 18.241851806640625
Actor Loss : 0.4996289610862732
Eval_AverageReturn : -66.86666870117188
Eval_StdReturn : 29.27767562866211
Eval_MaxReturn : -30.0
Eval_MinReturn : -127.0
Eval_AverageEpLen : 67.86666666666666
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -36.090000
best mean reward -36.090000
running time 498.648568
At timestep:     53 / 1000At timestep:     187 / 1000At timestep:     290 / 1000At timestep:     377 / 1000At timestep:     434 / 1000At timestep:     464 / 1000At timestep:     525 / 1000At timestep:     573 / 1000At timestep:     663 / 1000At timestep:     749 / 1000At timestep:     791 / 1000At timestep:     830 / 1000At timestep:     864 / 1000At timestep:     912 / 1000At timestep:     1048 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -36.09000015258789
Train_BestReturn : -36.09000015258789
TimeSinceStart : 498.64856791496277
Exploration Critic Loss : 6307861.0
Exploitation Critic Loss : 0.11277129501104355
Exploration Model Loss : 18.240997314453125
Actor Loss : 0.4953831732273102
Eval_AverageReturn : -68.86666870117188
Eval_StdReturn : 33.300384521484375
Eval_MaxReturn : -29.0
Eval_MinReturn : -135.0
Eval_AverageEpLen : 69.86666666666666
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -36.099998
best mean reward -36.090000
running time 540.031508
At timestep:     70 / 1000At timestep:     108 / 1000At timestep:     140 / 1000At timestep:     177 / 1000At timestep:     309 / 1000At timestep:     377 / 1000At timestep:     402 / 1000At timestep:     437 / 1000At timestep:     471 / 1000At timestep:     509 / 1000At timestep:     585 / 1000At timestep:     630 / 1000At timestep:     662 / 1000At timestep:     756 / 1000At timestep:     868 / 1000At timestep:     900 / 1000At timestep:     974 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -36.099998474121094
Train_BestReturn : -36.09000015258789
TimeSinceStart : 540.0315082073212
Exploration Critic Loss : 3925707.5
Exploitation Critic Loss : 0.1287386566400528
Exploration Model Loss : 18.24078369140625
Actor Loss : 0.45185351371765137
Eval_AverageReturn : -56.16666793823242
Eval_StdReturn : 29.944856643676758
Eval_MaxReturn : -24.0
Eval_MinReturn : -131.0
Eval_AverageEpLen : 57.166666666666664
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -32.049999
best mean reward -32.049999
running time 584.230936
At timestep:     72 / 1000At timestep:     111 / 1000At timestep:     162 / 1000At timestep:     221 / 1000At timestep:     277 / 1000At timestep:     325 / 1000At timestep:     383 / 1000At timestep:     445 / 1000At timestep:     547 / 1000At timestep:     636 / 1000At timestep:     676 / 1000At timestep:     801 / 1000At timestep:     866 / 1000At timestep:     962 / 1000At timestep:     996 / 1000At timestep:     1071 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -32.04999923706055
Train_BestReturn : -32.04999923706055
TimeSinceStart : 584.2309358119965
Exploration Critic Loss : 2464445.0
Exploitation Critic Loss : 0.22931788861751556
Exploration Model Loss : 18.240798950195312
Actor Loss : 0.5245606303215027
Eval_AverageReturn : -65.9375
Eval_StdReturn : 24.38869857788086
Eval_MaxReturn : -33.0
Eval_MinReturn : -124.0
Eval_AverageEpLen : 66.9375
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -28.940001
best mean reward -28.940001
running time 627.168180
At timestep:     83 / 1000At timestep:     117 / 1000At timestep:     149 / 1000At timestep:     190 / 1000At timestep:     230 / 1000At timestep:     276 / 1000At timestep:     319 / 1000At timestep:     357 / 1000At timestep:     406 / 1000At timestep:     474 / 1000At timestep:     506 / 1000At timestep:     557 / 1000At timestep:     630 / 1000At timestep:     662 / 1000At timestep:     700 / 1000At timestep:     734 / 1000At timestep:     774 / 1000At timestep:     812 / 1000At timestep:     862 / 1000At timestep:     899 / 1000At timestep:     924 / 1000At timestep:     982 / 1000At timestep:     1035 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -28.940000534057617
Train_BestReturn : -28.940000534057617
TimeSinceStart : 627.1681804656982
Exploration Critic Loss : 2310161.75
Exploitation Critic Loss : 0.16713815927505493
Exploration Model Loss : 18.2408447265625
Actor Loss : 0.5178537964820862
Eval_AverageReturn : -44.0
Eval_StdReturn : 13.984463691711426
Eval_MaxReturn : -24.0
Eval_MinReturn : -82.0
Eval_AverageEpLen : 45.0
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -26.850000
best mean reward -26.850000
running time 670.399375
At timestep:     44 / 1000At timestep:     85 / 1000At timestep:     128 / 1000At timestep:     199 / 1000At timestep:     241 / 1000At timestep:     304 / 1000At timestep:     370 / 1000At timestep:     402 / 1000At timestep:     437 / 1000At timestep:     487 / 1000At timestep:     562 / 1000At timestep:     596 / 1000At timestep:     627 / 1000At timestep:     688 / 1000At timestep:     740 / 1000At timestep:     773 / 1000At timestep:     800 / 1000At timestep:     864 / 1000At timestep:     923 / 1000At timestep:     987 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -26.850000381469727
Train_BestReturn : -26.850000381469727
TimeSinceStart : 670.3993747234344
Exploration Critic Loss : 1413251.75
Exploitation Critic Loss : 0.11515316367149353
Exploration Model Loss : 18.2410888671875
Actor Loss : 0.49295198917388916
Eval_AverageReturn : -47.85714340209961
Eval_StdReturn : 14.469295501708984
Eval_MaxReturn : -26.0
Eval_MinReturn : -74.0
Eval_AverageEpLen : 48.857142857142854
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -26.910000
best mean reward -26.850000
running time 714.780663
At timestep:     49 / 1000At timestep:     84 / 1000At timestep:     140 / 1000At timestep:     242 / 1000At timestep:     298 / 1000At timestep:     367 / 1000At timestep:     405 / 1000At timestep:     444 / 1000At timestep:     483 / 1000At timestep:     589 / 1000At timestep:     631 / 1000At timestep:     671 / 1000At timestep:     709 / 1000At timestep:     750 / 1000At timestep:     792 / 1000At timestep:     849 / 1000At timestep:     880 / 1000At timestep:     919 / 1000At timestep:     941 / 1000At timestep:     979 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -26.90999984741211
Train_BestReturn : -26.850000381469727
TimeSinceStart : 714.7806630134583
Exploration Critic Loss : 1016053.125
Exploitation Critic Loss : 0.09360954910516739
Exploration Model Loss : 18.241378784179688
Actor Loss : 0.4797006845474243
Eval_AverageReturn : -47.28571319580078
Eval_StdReturn : 20.656240463256836
Eval_MaxReturn : -21.0
Eval_MinReturn : -105.0
Eval_AverageEpLen : 48.285714285714285
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -27.010000
best mean reward -26.850000
running time 759.046864
At timestep:     38 / 1000At timestep:     68 / 1000At timestep:     151 / 1000At timestep:     216 / 1000At timestep:     241 / 1000At timestep:     292 / 1000At timestep:     347 / 1000At timestep:     427 / 1000At timestep:     467 / 1000At timestep:     518 / 1000At timestep:     553 / 1000At timestep:     653 / 1000At timestep:     715 / 1000At timestep:     785 / 1000At timestep:     857 / 1000At timestep:     891 / 1000At timestep:     923 / 1000At timestep:     975 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -27.010000228881836
Train_BestReturn : -26.850000381469727
TimeSinceStart : 759.0468642711639
Exploration Critic Loss : 673501.0
Exploitation Critic Loss : 0.09191809594631195
Exploration Model Loss : 18.241683959960938
Actor Loss : 0.48341476917266846
Eval_AverageReturn : -52.47368240356445
Eval_StdReturn : 20.129497528076172
Eval_MaxReturn : -24.0
Eval_MinReturn : -99.0
Eval_AverageEpLen : 53.473684210526315
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -27.120001
best mean reward -26.850000
running time 801.817100
At timestep:     57 / 1000At timestep:     107 / 1000At timestep:     146 / 1000At timestep:     196 / 1000At timestep:     234 / 1000At timestep:     270 / 1000At timestep:     311 / 1000At timestep:     340 / 1000At timestep:     380 / 1000At timestep:     460 / 1000At timestep:     489 / 1000At timestep:     528 / 1000At timestep:     566 / 1000At timestep:     599 / 1000At timestep:     634 / 1000At timestep:     693 / 1000At timestep:     719 / 1000At timestep:     750 / 1000At timestep:     808 / 1000At timestep:     857 / 1000At timestep:     895 / 1000At timestep:     946 / 1000At timestep:     981 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -27.1200008392334
Train_BestReturn : -26.850000381469727
TimeSinceStart : 801.817099571228
Exploration Critic Loss : 686527.75
Exploitation Critic Loss : 0.08961760252714157
Exploration Model Loss : 18.241943359375
Actor Loss : 0.5351279973983765
Eval_AverageReturn : -41.16666793823242
Eval_StdReturn : 12.236103057861328
Eval_MaxReturn : -25.0
Eval_MinReturn : -79.0
Eval_AverageEpLen : 42.166666666666664
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -27.420000
best mean reward -26.850000
running time 846.577373
At timestep:     45 / 1000At timestep:     88 / 1000At timestep:     124 / 1000At timestep:     163 / 1000At timestep:     264 / 1000At timestep:     296 / 1000At timestep:     321 / 1000At timestep:     366 / 1000At timestep:     401 / 1000At timestep:     445 / 1000At timestep:     497 / 1000At timestep:     549 / 1000At timestep:     606 / 1000At timestep:     651 / 1000At timestep:     696 / 1000At timestep:     748 / 1000At timestep:     785 / 1000At timestep:     835 / 1000At timestep:     881 / 1000At timestep:     930 / 1000At timestep:     980 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -27.420000076293945
Train_BestReturn : -26.850000381469727
TimeSinceStart : 846.5773727893829
Exploration Critic Loss : 849103.0
Exploitation Critic Loss : 0.09368569403886795
Exploration Model Loss : 18.242233276367188
Actor Loss : 0.45074814558029175
Eval_AverageReturn : -45.681819915771484
Eval_StdReturn : 13.965497016906738
Eval_MaxReturn : -24.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 46.68181818181818
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -27.330000
best mean reward -26.850000
running time 890.777610
At timestep:     46 / 1000At timestep:     77 / 1000At timestep:     128 / 1000At timestep:     163 / 1000At timestep:     211 / 1000At timestep:     255 / 1000At timestep:     292 / 1000At timestep:     324 / 1000At timestep:     369 / 1000At timestep:     420 / 1000At timestep:     466 / 1000At timestep:     513 / 1000At timestep:     548 / 1000At timestep:     581 / 1000At timestep:     657 / 1000At timestep:     687 / 1000At timestep:     728 / 1000At timestep:     760 / 1000At timestep:     810 / 1000At timestep:     843 / 1000At timestep:     868 / 1000At timestep:     939 / 1000At timestep:     974 / 1000At timestep:     1071 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -27.329999923706055
Train_BestReturn : -26.850000381469727
TimeSinceStart : 890.7776098251343
Exploration Critic Loss : 870646.0
Exploitation Critic Loss : 0.10985986888408661
Exploration Model Loss : 18.242523193359375
Actor Loss : 0.5299819707870483
Eval_AverageReturn : -43.625
Eval_StdReturn : 16.17305564880371
Eval_MaxReturn : -24.0
Eval_MinReturn : -96.0
Eval_AverageEpLen : 44.625
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -27.799999
best mean reward -26.850000
running time 933.669866
At timestep:     61 / 1000At timestep:     79 / 1000At timestep:     121 / 1000At timestep:     166 / 1000At timestep:     207 / 1000At timestep:     238 / 1000At timestep:     279 / 1000At timestep:     335 / 1000At timestep:     358 / 1000At timestep:     396 / 1000At timestep:     448 / 1000At timestep:     479 / 1000At timestep:     516 / 1000At timestep:     579 / 1000At timestep:     611 / 1000At timestep:     649 / 1000At timestep:     697 / 1000At timestep:     760 / 1000At timestep:     794 / 1000At timestep:     851 / 1000At timestep:     890 / 1000At timestep:     943 / 1000At timestep:     980 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -27.799999237060547
Train_BestReturn : -26.850000381469727
TimeSinceStart : 933.6698656082153
Exploration Critic Loss : 785655.3125
Exploitation Critic Loss : 0.11838672310113907
Exploration Model Loss : 18.242843627929688
Actor Loss : 0.5352329611778259
Eval_AverageReturn : -41.875
Eval_StdReturn : 11.896471977233887
Eval_MaxReturn : -17.0
Eval_MinReturn : -62.0
Eval_AverageEpLen : 42.875
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -27.650000
best mean reward -26.850000
running time 977.407260
At timestep:     28 / 1000At timestep:     58 / 1000At timestep:     100 / 1000At timestep:     129 / 1000At timestep:     156 / 1000At timestep:     178 / 1000At timestep:     225 / 1000At timestep:     270 / 1000At timestep:     321 / 1000At timestep:     345 / 1000At timestep:     409 / 1000At timestep:     465 / 1000At timestep:     513 / 1000At timestep:     562 / 1000At timestep:     596 / 1000At timestep:     636 / 1000At timestep:     701 / 1000At timestep:     733 / 1000At timestep:     773 / 1000At timestep:     808 / 1000At timestep:     847 / 1000At timestep:     884 / 1000At timestep:     914 / 1000At timestep:     959 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -27.649999618530273
Train_BestReturn : -26.850000381469727
TimeSinceStart : 977.4072597026825
Exploration Critic Loss : 717039.4375
Exploitation Critic Loss : 0.1049845814704895
Exploration Model Loss : 18.2431640625
Actor Loss : 0.47067153453826904
Eval_AverageReturn : -39.720001220703125
Eval_StdReturn : 11.871040344238281
Eval_MaxReturn : -21.0
Eval_MinReturn : -64.0
Eval_AverageEpLen : 40.72
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -27.469999
best mean reward -26.850000
running time 1022.261701
At timestep:     65 / 1000At timestep:     101 / 1000At timestep:     124 / 1000At timestep:     164 / 1000At timestep:     204 / 1000At timestep:     270 / 1000At timestep:     312 / 1000At timestep:     356 / 1000At timestep:     376 / 1000At timestep:     399 / 1000At timestep:     419 / 1000At timestep:     467 / 1000At timestep:     504 / 1000At timestep:     525 / 1000At timestep:     552 / 1000At timestep:     590 / 1000At timestep:     617 / 1000At timestep:     664 / 1000At timestep:     715 / 1000At timestep:     758 / 1000At timestep:     788 / 1000At timestep:     827 / 1000At timestep:     865 / 1000At timestep:     904 / 1000At timestep:     964 / 1000At timestep:     995 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -27.469999313354492
Train_BestReturn : -26.850000381469727
TimeSinceStart : 1022.2617008686066
Exploration Critic Loss : 807651.375
Exploitation Critic Loss : 0.102887362241745
Exploration Model Loss : 18.243453979492188
Actor Loss : 0.5014153122901917
Eval_AverageReturn : -36.96296310424805
Eval_StdReturn : 12.538773536682129
Eval_MaxReturn : -19.0
Eval_MinReturn : -65.0
Eval_AverageEpLen : 37.96296296296296
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -26.969999
best mean reward -26.850000
running time 1065.533667
At timestep:     24 / 1000At timestep:     58 / 1000At timestep:     102 / 1000At timestep:     152 / 1000At timestep:     182 / 1000At timestep:     238 / 1000At timestep:     264 / 1000At timestep:     301 / 1000At timestep:     365 / 1000At timestep:     407 / 1000At timestep:     430 / 1000At timestep:     470 / 1000At timestep:     491 / 1000At timestep:     526 / 1000At timestep:     595 / 1000At timestep:     625 / 1000At timestep:     677 / 1000At timestep:     717 / 1000At timestep:     762 / 1000At timestep:     788 / 1000At timestep:     825 / 1000At timestep:     868 / 1000At timestep:     908 / 1000At timestep:     942 / 1000At timestep:     989 / 1000At timestep:     1044 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -26.969999313354492
Train_BestReturn : -26.850000381469727
TimeSinceStart : 1065.533667087555
Exploration Critic Loss : 673232.125
Exploitation Critic Loss : 0.0759369283914566
Exploration Model Loss : 18.243759155273438
Actor Loss : 0.5411820411682129
Eval_AverageReturn : -39.153846740722656
Eval_StdReturn : 12.180351257324219
Eval_MaxReturn : -20.0
Eval_MinReturn : -68.0
Eval_AverageEpLen : 40.15384615384615
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -26.760000
best mean reward -26.760000
running time 1108.273728
At timestep:     34 / 1000At timestep:     64 / 1000At timestep:     98 / 1000At timestep:     130 / 1000At timestep:     153 / 1000At timestep:     193 / 1000At timestep:     257 / 1000At timestep:     284 / 1000At timestep:     318 / 1000At timestep:     361 / 1000At timestep:     429 / 1000At timestep:     465 / 1000At timestep:     499 / 1000At timestep:     524 / 1000At timestep:     570 / 1000At timestep:     612 / 1000At timestep:     649 / 1000At timestep:     682 / 1000At timestep:     728 / 1000At timestep:     747 / 1000At timestep:     771 / 1000At timestep:     829 / 1000At timestep:     874 / 1000At timestep:     907 / 1000At timestep:     950 / 1000At timestep:     997 / 1000At timestep:     1063 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -26.760000228881836
Train_BestReturn : -26.760000228881836
TimeSinceStart : 1108.2737278938293
Exploration Critic Loss : 724159.4375
Exploitation Critic Loss : 0.1299898326396942
Exploration Model Loss : 18.244064331054688
Actor Loss : 0.48493027687072754
Eval_AverageReturn : -38.37036895751953
Eval_StdReturn : 12.661250114440918
Eval_MaxReturn : -18.0
Eval_MinReturn : -67.0
Eval_AverageEpLen : 39.370370370370374
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -26.090000
best mean reward -26.090000
running time 1154.511085
At timestep:     62 / 1000At timestep:     109 / 1000At timestep:     143 / 1000At timestep:     189 / 1000At timestep:     234 / 1000At timestep:     275 / 1000At timestep:     331 / 1000At timestep:     370 / 1000At timestep:     414 / 1000At timestep:     454 / 1000At timestep:     506 / 1000At timestep:     558 / 1000At timestep:     587 / 1000At timestep:     615 / 1000At timestep:     648 / 1000At timestep:     679 / 1000At timestep:     716 / 1000At timestep:     747 / 1000At timestep:     779 / 1000At timestep:     814 / 1000At timestep:     839 / 1000At timestep:     876 / 1000At timestep:     917 / 1000At timestep:     941 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -26.09000015258789
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1154.5110850334167
Exploration Critic Loss : 769574.6875
Exploitation Critic Loss : 0.11895641684532166
Exploration Model Loss : 18.24432373046875
Actor Loss : 0.46969515085220337
Eval_AverageReturn : -40.08000183105469
Eval_StdReturn : 13.181563377380371
Eval_MaxReturn : -23.0
Eval_MinReturn : -85.0
Eval_AverageEpLen : 41.08
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -27.100000
best mean reward -26.090000
running time 1196.258771
At timestep:     42 / 1000At timestep:     78 / 1000At timestep:     118 / 1000At timestep:     155 / 1000At timestep:     214 / 1000At timestep:     257 / 1000At timestep:     292 / 1000At timestep:     333 / 1000At timestep:     371 / 1000At timestep:     405 / 1000At timestep:     440 / 1000At timestep:     482 / 1000At timestep:     510 / 1000At timestep:     536 / 1000At timestep:     589 / 1000At timestep:     609 / 1000At timestep:     645 / 1000At timestep:     691 / 1000At timestep:     722 / 1000At timestep:     757 / 1000At timestep:     807 / 1000At timestep:     844 / 1000At timestep:     875 / 1000At timestep:     938 / 1000At timestep:     991 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -27.100000381469727
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1196.2587714195251
Exploration Critic Loss : 792831.75
Exploitation Critic Loss : 0.11544467508792877
Exploration Model Loss : 18.244552612304688
Actor Loss : 0.5218684673309326
Eval_AverageReturn : -38.5
Eval_StdReturn : 9.724078178405762
Eval_MaxReturn : -19.0
Eval_MinReturn : -62.0
Eval_AverageEpLen : 39.5
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -26.950001
best mean reward -26.090000
running time 1240.183093
At timestep:     62 / 1000At timestep:     142 / 1000At timestep:     189 / 1000At timestep:     222 / 1000At timestep:     261 / 1000At timestep:     291 / 1000At timestep:     317 / 1000At timestep:     356 / 1000At timestep:     392 / 1000At timestep:     431 / 1000At timestep:     459 / 1000At timestep:     487 / 1000At timestep:     535 / 1000At timestep:     571 / 1000At timestep:     607 / 1000At timestep:     645 / 1000At timestep:     679 / 1000At timestep:     722 / 1000At timestep:     753 / 1000At timestep:     777 / 1000At timestep:     843 / 1000At timestep:     869 / 1000At timestep:     903 / 1000At timestep:     953 / 1000At timestep:     977 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -26.950000762939453
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1240.1830925941467
Exploration Critic Loss : 793909.3125
Exploitation Critic Loss : 0.13697534799575806
Exploration Model Loss : 18.244857788085938
Actor Loss : 0.5482605695724487
Eval_AverageReturn : -38.11538314819336
Eval_StdReturn : 13.17143440246582
Eval_MaxReturn : -23.0
Eval_MinReturn : -79.0
Eval_AverageEpLen : 39.11538461538461
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -26.969999
best mean reward -26.090000
running time 1285.051368
At timestep:     46 / 1000At timestep:     79 / 1000At timestep:     118 / 1000At timestep:     149 / 1000At timestep:     183 / 1000At timestep:     223 / 1000At timestep:     264 / 1000At timestep:     293 / 1000At timestep:     330 / 1000At timestep:     363 / 1000At timestep:     395 / 1000At timestep:     440 / 1000At timestep:     475 / 1000At timestep:     516 / 1000At timestep:     547 / 1000At timestep:     587 / 1000At timestep:     622 / 1000At timestep:     658 / 1000At timestep:     690 / 1000At timestep:     731 / 1000At timestep:     771 / 1000At timestep:     795 / 1000At timestep:     828 / 1000At timestep:     856 / 1000At timestep:     879 / 1000At timestep:     900 / 1000At timestep:     932 / 1000At timestep:     955 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -26.969999313354492
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1285.051367521286
Exploration Critic Loss : 662748.25
Exploitation Critic Loss : 0.1062568947672844
Exploration Model Loss : 18.2451171875
Actor Loss : 0.47345614433288574
Eval_AverageReturn : -34.379310607910156
Eval_StdReturn : 9.245367050170898
Eval_MaxReturn : -20.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 35.37931034482759
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -26.940001
best mean reward -26.090000
running time 1329.868955
At timestep:     31 / 1000At timestep:     71 / 1000At timestep:     112 / 1000At timestep:     136 / 1000At timestep:     198 / 1000At timestep:     226 / 1000At timestep:     282 / 1000At timestep:     314 / 1000At timestep:     356 / 1000At timestep:     403 / 1000At timestep:     435 / 1000At timestep:     470 / 1000At timestep:     506 / 1000At timestep:     547 / 1000At timestep:     608 / 1000At timestep:     674 / 1000At timestep:     733 / 1000At timestep:     771 / 1000At timestep:     815 / 1000At timestep:     840 / 1000At timestep:     879 / 1000At timestep:     898 / 1000At timestep:     937 / 1000At timestep:     983 / 1000At timestep:     1037 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -26.940000534057617
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1329.8689546585083
Exploration Critic Loss : 849245.25
Exploitation Critic Loss : 0.11664585769176483
Exploration Model Loss : 18.245376586914062
Actor Loss : 0.4834422767162323
Eval_AverageReturn : -40.47999954223633
Eval_StdReturn : 12.309736251831055
Eval_MaxReturn : -18.0
Eval_MinReturn : -65.0
Eval_AverageEpLen : 41.48
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -27.770000
best mean reward -26.090000
running time 1373.522274
At timestep:     30 / 1000At timestep:     53 / 1000At timestep:     74 / 1000At timestep:     100 / 1000At timestep:     124 / 1000At timestep:     152 / 1000At timestep:     191 / 1000At timestep:     212 / 1000At timestep:     242 / 1000At timestep:     286 / 1000At timestep:     339 / 1000At timestep:     377 / 1000At timestep:     408 / 1000At timestep:     439 / 1000At timestep:     474 / 1000At timestep:     501 / 1000At timestep:     547 / 1000At timestep:     579 / 1000At timestep:     615 / 1000At timestep:     672 / 1000At timestep:     700 / 1000At timestep:     733 / 1000At timestep:     769 / 1000At timestep:     795 / 1000At timestep:     824 / 1000At timestep:     881 / 1000At timestep:     930 / 1000At timestep:     981 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -27.770000457763672
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1373.5222735404968
Exploration Critic Loss : 684568.125
Exploitation Critic Loss : 0.09609110653400421
Exploration Model Loss : 18.24566650390625
Actor Loss : 0.5188440084457397
Eval_AverageReturn : -34.17241287231445
Eval_StdReturn : 10.339308738708496
Eval_MaxReturn : -20.0
Eval_MinReturn : -56.0
Eval_AverageEpLen : 35.172413793103445
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -28.170000
best mean reward -26.090000
running time 1419.139346
At timestep:     21 / 1000At timestep:     64 / 1000At timestep:     100 / 1000At timestep:     130 / 1000At timestep:     157 / 1000At timestep:     195 / 1000At timestep:     234 / 1000At timestep:     261 / 1000At timestep:     293 / 1000At timestep:     327 / 1000At timestep:     371 / 1000At timestep:     408 / 1000At timestep:     444 / 1000At timestep:     478 / 1000At timestep:     513 / 1000At timestep:     550 / 1000At timestep:     582 / 1000At timestep:     633 / 1000At timestep:     670 / 1000At timestep:     698 / 1000At timestep:     722 / 1000At timestep:     753 / 1000At timestep:     786 / 1000At timestep:     815 / 1000At timestep:     850 / 1000At timestep:     880 / 1000At timestep:     927 / 1000At timestep:     953 / 1000At timestep:     990 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -28.170000076293945
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1419.1393461227417
Exploration Critic Loss : 660783.25
Exploitation Critic Loss : 0.09552648663520813
Exploration Model Loss : 18.2459716796875
Actor Loss : 0.5093324184417725
Eval_AverageReturn : -32.86666488647461
Eval_StdReturn : 6.6619977951049805
Eval_MaxReturn : -20.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 33.86666666666667
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -27.490000
best mean reward -26.090000
running time 1465.729227
At timestep:     23 / 1000At timestep:     50 / 1000At timestep:     90 / 1000At timestep:     123 / 1000At timestep:     160 / 1000At timestep:     187 / 1000At timestep:     213 / 1000At timestep:     256 / 1000At timestep:     289 / 1000At timestep:     320 / 1000At timestep:     354 / 1000At timestep:     398 / 1000At timestep:     433 / 1000At timestep:     467 / 1000At timestep:     498 / 1000At timestep:     532 / 1000At timestep:     566 / 1000At timestep:     596 / 1000At timestep:     642 / 1000At timestep:     693 / 1000At timestep:     732 / 1000At timestep:     765 / 1000At timestep:     793 / 1000At timestep:     840 / 1000At timestep:     891 / 1000At timestep:     919 / 1000At timestep:     946 / 1000At timestep:     984 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -27.489999771118164
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1465.7292265892029
Exploration Critic Loss : 546946.25
Exploitation Critic Loss : 0.10947020351886749
Exploration Model Loss : 18.24627685546875
Actor Loss : 0.47166168689727783
Eval_AverageReturn : -33.931034088134766
Eval_StdReturn : 7.427493095397949
Eval_MaxReturn : -22.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 34.93103448275862
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -27.160000
best mean reward -26.090000
running time 1510.067205
At timestep:     29 / 1000At timestep:     59 / 1000At timestep:     83 / 1000At timestep:     115 / 1000At timestep:     155 / 1000At timestep:     188 / 1000At timestep:     232 / 1000At timestep:     255 / 1000At timestep:     284 / 1000At timestep:     321 / 1000At timestep:     352 / 1000At timestep:     385 / 1000At timestep:     419 / 1000At timestep:     467 / 1000At timestep:     498 / 1000At timestep:     525 / 1000At timestep:     584 / 1000At timestep:     620 / 1000At timestep:     640 / 1000At timestep:     670 / 1000At timestep:     709 / 1000At timestep:     751 / 1000At timestep:     803 / 1000At timestep:     852 / 1000At timestep:     879 / 1000At timestep:     923 / 1000At timestep:     953 / 1000At timestep:     997 / 1000At timestep:     1030 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -27.15999984741211
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1510.06720495224
Exploration Critic Loss : 740687.375
Exploitation Critic Loss : 0.15513169765472412
Exploration Model Loss : 18.246353149414062
Actor Loss : 0.4903143644332886
Eval_AverageReturn : -34.517242431640625
Eval_StdReturn : 9.057748794555664
Eval_MaxReturn : -19.0
Eval_MinReturn : -58.0
Eval_AverageEpLen : 35.51724137931034
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -26.879999
best mean reward -26.090000
running time 1554.064273
At timestep:     32 / 1000At timestep:     58 / 1000At timestep:     96 / 1000At timestep:     132 / 1000At timestep:     164 / 1000At timestep:     218 / 1000At timestep:     246 / 1000At timestep:     272 / 1000At timestep:     316 / 1000At timestep:     350 / 1000At timestep:     378 / 1000At timestep:     408 / 1000At timestep:     444 / 1000At timestep:     470 / 1000At timestep:     505 / 1000At timestep:     552 / 1000At timestep:     583 / 1000At timestep:     604 / 1000At timestep:     632 / 1000At timestep:     661 / 1000At timestep:     685 / 1000At timestep:     724 / 1000At timestep:     752 / 1000At timestep:     785 / 1000At timestep:     811 / 1000At timestep:     834 / 1000At timestep:     899 / 1000At timestep:     933 / 1000At timestep:     974 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -26.8799991607666
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1554.0642733573914
Exploration Critic Loss : 668262.0625
Exploitation Critic Loss : 0.09707731008529663
Exploration Model Loss : 18.246444702148438
Actor Loss : 0.4731180667877197
Eval_AverageReturn : -33.0
Eval_StdReturn : 9.52540397644043
Eval_MaxReturn : -20.0
Eval_MinReturn : -64.0
Eval_AverageEpLen : 34.0
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -27.090000
best mean reward -26.090000
running time 1599.664788
At timestep:     25 / 1000At timestep:     63 / 1000At timestep:     124 / 1000At timestep:     172 / 1000At timestep:     201 / 1000At timestep:     233 / 1000At timestep:     262 / 1000At timestep:     302 / 1000At timestep:     327 / 1000At timestep:     362 / 1000At timestep:     413 / 1000At timestep:     444 / 1000At timestep:     469 / 1000At timestep:     491 / 1000At timestep:     528 / 1000At timestep:     565 / 1000At timestep:     602 / 1000At timestep:     632 / 1000At timestep:     676 / 1000At timestep:     714 / 1000At timestep:     748 / 1000At timestep:     789 / 1000At timestep:     819 / 1000At timestep:     849 / 1000At timestep:     877 / 1000At timestep:     902 / 1000At timestep:     930 / 1000At timestep:     998 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -27.09000015258789
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1599.664787530899
Exploration Critic Loss : 639062.25
Exploitation Critic Loss : 0.09233111143112183
Exploration Model Loss : 18.246597290039062
Actor Loss : 0.5045714974403381
Eval_AverageReturn : -34.379310607910156
Eval_StdReturn : 10.581094741821289
Eval_MaxReturn : -21.0
Eval_MinReturn : -67.0
Eval_AverageEpLen : 35.37931034482759
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -26.730000
best mean reward -26.090000
running time 1644.071731
At timestep:     27 / 1000At timestep:     81 / 1000At timestep:     108 / 1000At timestep:     139 / 1000At timestep:     171 / 1000At timestep:     200 / 1000At timestep:     234 / 1000At timestep:     281 / 1000At timestep:     308 / 1000At timestep:     337 / 1000At timestep:     366 / 1000At timestep:     395 / 1000At timestep:     447 / 1000At timestep:     476 / 1000At timestep:     518 / 1000At timestep:     540 / 1000At timestep:     602 / 1000At timestep:     639 / 1000At timestep:     672 / 1000At timestep:     711 / 1000At timestep:     740 / 1000At timestep:     761 / 1000At timestep:     802 / 1000At timestep:     837 / 1000At timestep:     865 / 1000At timestep:     895 / 1000At timestep:     926 / 1000At timestep:     961 / 1000At timestep:     994 / 1000At timestep:     1045 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -26.729999542236328
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1644.0717310905457
Exploration Critic Loss : 459189.0625
Exploitation Critic Loss : 0.12935982644557953
Exploration Model Loss : 18.24688720703125
Actor Loss : 0.5290811061859131
Eval_AverageReturn : -33.83333206176758
Eval_StdReturn : 9.626640319824219
Eval_MaxReturn : -20.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 34.833333333333336
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -26.549999
best mean reward -26.090000
running time 1687.732761
At timestep:     52 / 1000At timestep:     89 / 1000At timestep:     114 / 1000At timestep:     155 / 1000At timestep:     184 / 1000At timestep:     214 / 1000At timestep:     247 / 1000At timestep:     272 / 1000At timestep:     333 / 1000At timestep:     366 / 1000At timestep:     403 / 1000At timestep:     444 / 1000At timestep:     478 / 1000At timestep:     525 / 1000At timestep:     554 / 1000At timestep:     581 / 1000At timestep:     639 / 1000At timestep:     683 / 1000At timestep:     730 / 1000At timestep:     767 / 1000At timestep:     800 / 1000At timestep:     853 / 1000At timestep:     896 / 1000At timestep:     919 / 1000At timestep:     957 / 1000At timestep:     994 / 1000At timestep:     1046 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -26.549999237060547
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1687.732760667801
Exploration Critic Loss : 692409.875
Exploitation Critic Loss : 0.10139733552932739
Exploration Model Loss : 18.2470703125
Actor Loss : 0.44102758169174194
Eval_AverageReturn : -37.74074172973633
Eval_StdReturn : 10.141927719116211
Eval_MaxReturn : -22.0
Eval_MinReturn : -60.0
Eval_AverageEpLen : 38.74074074074074
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -26.570000
best mean reward -26.090000
running time 1731.815387
At timestep:     45 / 1000At timestep:     90 / 1000At timestep:     133 / 1000At timestep:     162 / 1000At timestep:     205 / 1000At timestep:     235 / 1000At timestep:     255 / 1000At timestep:     296 / 1000At timestep:     330 / 1000At timestep:     370 / 1000At timestep:     406 / 1000At timestep:     433 / 1000At timestep:     455 / 1000At timestep:     498 / 1000At timestep:     530 / 1000At timestep:     574 / 1000At timestep:     610 / 1000At timestep:     654 / 1000At timestep:     683 / 1000At timestep:     726 / 1000At timestep:     761 / 1000At timestep:     791 / 1000At timestep:     834 / 1000At timestep:     884 / 1000At timestep:     914 / 1000At timestep:     943 / 1000At timestep:     987 / 1000At timestep:     1036 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -26.56999969482422
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1731.815387248993
Exploration Critic Loss : 492737.75
Exploitation Critic Loss : 0.12366722524166107
Exploration Model Loss : 18.24737548828125
Actor Loss : 0.4142128825187683
Eval_AverageReturn : -36.0
Eval_StdReturn : 7.986595630645752
Eval_MaxReturn : -19.0
Eval_MinReturn : -49.0
Eval_AverageEpLen : 37.0
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -26.770000
best mean reward -26.090000
running time 1777.561498
At timestep:     28 / 1000At timestep:     71 / 1000At timestep:     108 / 1000At timestep:     169 / 1000At timestep:     193 / 1000At timestep:     227 / 1000At timestep:     266 / 1000At timestep:     300 / 1000At timestep:     335 / 1000At timestep:     371 / 1000At timestep:     404 / 1000At timestep:     453 / 1000At timestep:     484 / 1000At timestep:     528 / 1000At timestep:     568 / 1000At timestep:     600 / 1000At timestep:     632 / 1000At timestep:     665 / 1000At timestep:     697 / 1000At timestep:     728 / 1000At timestep:     758 / 1000At timestep:     784 / 1000At timestep:     810 / 1000At timestep:     843 / 1000At timestep:     880 / 1000At timestep:     929 / 1000At timestep:     958 / 1000At timestep:     993 / 1000At timestep:     1034 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -26.770000457763672
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1777.5614984035492
Exploration Critic Loss : 774993.5
Exploitation Critic Loss : 0.08883817493915558
Exploration Model Loss : 18.24761962890625
Actor Loss : 0.45156532526016235
Eval_AverageReturn : -34.655174255371094
Eval_StdReturn : 7.773854732513428
Eval_MaxReturn : -23.0
Eval_MinReturn : -60.0
Eval_AverageEpLen : 35.6551724137931
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -27.760000
best mean reward -26.090000
running time 1822.096556
At timestep:     34 / 1000At timestep:     63 / 1000At timestep:     99 / 1000At timestep:     122 / 1000At timestep:     155 / 1000At timestep:     184 / 1000At timestep:     216 / 1000At timestep:     238 / 1000At timestep:     291 / 1000At timestep:     322 / 1000At timestep:     357 / 1000At timestep:     410 / 1000At timestep:     441 / 1000At timestep:     503 / 1000At timestep:     539 / 1000At timestep:     565 / 1000At timestep:     605 / 1000At timestep:     631 / 1000At timestep:     666 / 1000At timestep:     707 / 1000At timestep:     743 / 1000At timestep:     789 / 1000At timestep:     823 / 1000At timestep:     864 / 1000At timestep:     893 / 1000At timestep:     915 / 1000At timestep:     955 / 1000At timestep:     980 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -27.760000228881836
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1822.0965559482574
Exploration Critic Loss : 689914.125
Exploitation Critic Loss : 0.08323528617620468
Exploration Model Loss : 18.247756958007812
Actor Loss : 0.48369449377059937
Eval_AverageReturn : -33.72413635253906
Eval_StdReturn : 9.43908405303955
Eval_MaxReturn : -21.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 34.724137931034484
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -26.900000
best mean reward -26.090000
running time 1864.415468
At timestep:     20 / 1000At timestep:     46 / 1000At timestep:     66 / 1000At timestep:     88 / 1000At timestep:     136 / 1000At timestep:     163 / 1000At timestep:     191 / 1000At timestep:     223 / 1000At timestep:     257 / 1000At timestep:     297 / 1000At timestep:     331 / 1000At timestep:     370 / 1000At timestep:     405 / 1000At timestep:     464 / 1000At timestep:     501 / 1000At timestep:     526 / 1000At timestep:     553 / 1000At timestep:     577 / 1000At timestep:     613 / 1000At timestep:     637 / 1000At timestep:     682 / 1000At timestep:     723 / 1000At timestep:     755 / 1000At timestep:     783 / 1000At timestep:     828 / 1000At timestep:     868 / 1000At timestep:     908 / 1000At timestep:     935 / 1000At timestep:     969 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -26.899999618530273
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1864.4154677391052
Exploration Critic Loss : 519914.8125
Exploitation Critic Loss : 0.09147804230451584
Exploration Model Loss : 18.248046875
Actor Loss : 0.6335967779159546
Eval_AverageReturn : -32.96666717529297
Eval_StdReturn : 9.350520133972168
Eval_MaxReturn : -19.0
Eval_MinReturn : -58.0
Eval_AverageEpLen : 33.96666666666667
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -26.360001
best mean reward -26.090000
running time 1910.218295
At timestep:     47 / 1000At timestep:     83 / 1000At timestep:     131 / 1000At timestep:     160 / 1000At timestep:     189 / 1000At timestep:     222 / 1000At timestep:     252 / 1000At timestep:     282 / 1000At timestep:     326 / 1000At timestep:     360 / 1000At timestep:     405 / 1000At timestep:     424 / 1000At timestep:     471 / 1000At timestep:     493 / 1000At timestep:     523 / 1000At timestep:     550 / 1000At timestep:     587 / 1000At timestep:     624 / 1000At timestep:     657 / 1000At timestep:     690 / 1000At timestep:     729 / 1000At timestep:     771 / 1000At timestep:     802 / 1000At timestep:     833 / 1000At timestep:     882 / 1000At timestep:     914 / 1000At timestep:     940 / 1000At timestep:     981 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -26.360000610351562
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1910.2182953357697
Exploration Critic Loss : 685266.9375
Exploitation Critic Loss : 0.12194880843162537
Exploration Model Loss : 18.24835205078125
Actor Loss : 0.48382389545440674
Eval_AverageReturn : -33.82758712768555
Eval_StdReturn : 7.768499374389648
Eval_MaxReturn : -18.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 34.827586206896555
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -26.840000
best mean reward -26.090000
running time 1956.139844
At timestep:     40 / 1000At timestep:     69 / 1000At timestep:     111 / 1000At timestep:     171 / 1000At timestep:     211 / 1000At timestep:     238 / 1000At timestep:     286 / 1000At timestep:     335 / 1000At timestep:     359 / 1000At timestep:     407 / 1000At timestep:     437 / 1000At timestep:     464 / 1000At timestep:     491 / 1000At timestep:     526 / 1000At timestep:     551 / 1000At timestep:     578 / 1000At timestep:     615 / 1000At timestep:     644 / 1000At timestep:     675 / 1000At timestep:     726 / 1000At timestep:     769 / 1000At timestep:     806 / 1000At timestep:     844 / 1000At timestep:     870 / 1000At timestep:     896 / 1000At timestep:     922 / 1000At timestep:     949 / 1000At timestep:     980 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -26.84000015258789
Train_BestReturn : -26.09000015258789
TimeSinceStart : 1956.1398437023163
Exploration Critic Loss : 548477.0
Exploitation Critic Loss : 0.10168682038784027
Exploration Model Loss : 18.24853515625
Actor Loss : 0.5334291458129883
Eval_AverageReturn : -33.931034088134766
Eval_StdReturn : 9.306768417358398
Eval_MaxReturn : -23.0
Eval_MinReturn : -59.0
Eval_AverageEpLen : 34.93103448275862
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -27.299999
best mean reward -26.090000
running time 2001.102268
At timestep:     21 / 1000At timestep:     68 / 1000At timestep:     118 / 1000At timestep:     143 / 1000At timestep:     169 / 1000At timestep:     211 / 1000At timestep:     238 / 1000At timestep:     270 / 1000At timestep:     294 / 1000At timestep:     342 / 1000At timestep:     379 / 1000At timestep:     405 / 1000At timestep:     449 / 1000At timestep:     480 / 1000At timestep:     504 / 1000At timestep:     541 / 1000At timestep:     572 / 1000At timestep:     612 / 1000At timestep:     649 / 1000At timestep:     680 / 1000At timestep:     729 / 1000At timestep:     752 / 1000At timestep:     786 / 1000At timestep:     812 / 1000At timestep:     852 / 1000At timestep:     893 / 1000At timestep:     934 / 1000At timestep:     969 / 1000At timestep:     991 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -27.299999237060547
Train_BestReturn : -26.09000015258789
TimeSinceStart : 2001.1022675037384
Exploration Critic Loss : 678480.75
Exploitation Critic Loss : 0.12882038950920105
Exploration Model Loss : 18.248687744140625
Actor Loss : 0.4788667857646942
Eval_AverageReturn : -33.233333587646484
Eval_StdReturn : 8.558361053466797
Eval_MaxReturn : -20.0
Eval_MinReturn : -49.0
Eval_AverageEpLen : 34.233333333333334
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -27.309999
best mean reward -26.090000
running time 2045.577592
At timestep:     18 / 1000At timestep:     59 / 1000At timestep:     95 / 1000At timestep:     125 / 1000At timestep:     148 / 1000At timestep:     175 / 1000At timestep:     204 / 1000At timestep:     226 / 1000At timestep:     266 / 1000At timestep:     296 / 1000At timestep:     317 / 1000At timestep:     336 / 1000At timestep:     371 / 1000At timestep:     395 / 1000At timestep:     436 / 1000At timestep:     462 / 1000At timestep:     482 / 1000At timestep:     531 / 1000At timestep:     555 / 1000At timestep:     585 / 1000At timestep:     616 / 1000At timestep:     664 / 1000At timestep:     692 / 1000At timestep:     730 / 1000At timestep:     778 / 1000At timestep:     808 / 1000At timestep:     837 / 1000At timestep:     874 / 1000At timestep:     893 / 1000At timestep:     915 / 1000At timestep:     952 / 1000At timestep:     981 / 1000At timestep:     1022 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_medium_supervised_lam1_PointmassMedium-v0_22-11-2022_21-59-20/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -27.309999465942383
Train_BestReturn : -26.09000015258789
TimeSinceStart : 2045.5775921344757
Exploration Critic Loss : 446431.75
Exploitation Critic Loss : 0.1046423614025116
Exploration Model Loss : 18.248992919921875
Actor Loss : 0.4735490083694458
Eval_AverageReturn : -29.969696044921875
Eval_StdReturn : 8.726430892944336
Eval_MaxReturn : -17.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 30.96969696969697
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam2_PointmassMedium-v0_22-11-2022_22-34-17 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam2_PointmassMedium-v0_22-11-2022_22-34-17
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002397
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0023965835571289062
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 8.788003
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     666 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 8.788002967834473
Eval_AverageReturn : -139.375
Eval_StdReturn : 28.111106872558594
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.5
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -147.384613
best mean reward -inf
running time 17.992405
At timestep:     54 / 1000At timestep:     204 / 1000At timestep:     354 / 1000At timestep:     504 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -147.38461303710938
TimeSinceStart : 17.99240517616272
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -148.300003
best mean reward -inf
running time 56.806821
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     589 / 1000At timestep:     739 / 1000At timestep:     889 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -148.3000030517578
TimeSinceStart : 56.80682063102722
Exploration Critic Loss : 719703.75
Exploitation Critic Loss : 0.008307132869958878
Exploration Model Loss : 49.939430236816406
Actor Loss : 1.2010751962661743
Eval_AverageReturn : -148.2857208251953
Eval_StdReturn : 4.199125289916992
Eval_MaxReturn : -138.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 148.42857142857142
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -148.692307
best mean reward -inf
running time 93.824608
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -148.6923065185547
TimeSinceStart : 93.82460761070251
Exploration Critic Loss : 9150650.0
Exploitation Critic Loss : 0.04584725946187973
Exploration Model Loss : 70.83719635009766
Actor Loss : 0.9558336138725281
Eval_AverageReturn : -143.57142639160156
Eval_StdReturn : 15.746719360351562
Eval_MaxReturn : -105.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 143.71428571428572
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -147.484848
best mean reward -inf
running time 130.764977
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -147.48484802246094
TimeSinceStart : 130.76497650146484
Exploration Critic Loss : 5318670.5
Exploitation Critic Loss : 0.044292937964200974
Exploration Model Loss : 45.96406936645508
Actor Loss : 0.819824755191803
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -145.609756
best mean reward -inf
running time 168.859844
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     412 / 1000At timestep:     562 / 1000At timestep:     712 / 1000At timestep:     862 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -145.60975646972656
TimeSinceStart : 168.8598437309265
Exploration Critic Loss : 5465514.0
Exploitation Critic Loss : 0.07299423217773438
Exploration Model Loss : 33.816131591796875
Actor Loss : 0.6640200614929199
Eval_AverageReturn : -144.42857360839844
Eval_StdReturn : 13.64715576171875
Eval_MaxReturn : -111.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 144.57142857142858
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -141.061218
best mean reward -inf
running time 206.189906
At timestep:     114 / 1000At timestep:     191 / 1000At timestep:     264 / 1000At timestep:     414 / 1000At timestep:     548 / 1000At timestep:     698 / 1000At timestep:     848 / 1000At timestep:     998 / 1000At timestep:     1148 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -141.06121826171875
TimeSinceStart : 206.18990635871887
Exploration Critic Loss : 9028944.0
Exploitation Critic Loss : 0.01187334954738617
Exploration Model Loss : 68.50358581542969
Actor Loss : 0.5775783061981201
Eval_AverageReturn : -127.11111450195312
Eval_StdReturn : 30.737035751342773
Eval_MaxReturn : -72.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 127.55555555555556
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -136.258621
best mean reward -inf
running time 245.135416
At timestep:     150 / 1000At timestep:     275 / 1000At timestep:     425 / 1000At timestep:     575 / 1000At timestep:     725 / 1000At timestep:     875 / 1000At timestep:     997 / 1000At timestep:     1147 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -136.2586212158203
TimeSinceStart : 245.13541555404663
Exploration Critic Loss : 5022641.0
Exploitation Critic Loss : 0.11057212948799133
Exploration Model Loss : 31.70888328552246
Actor Loss : 0.5092288851737976
Eval_AverageReturn : -143.0
Eval_StdReturn : 11.863810539245605
Eval_MaxReturn : -121.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 143.375
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -122.739723
best mean reward -inf
running time 284.367154
At timestep:     150 / 1000At timestep:     237 / 1000At timestep:     387 / 1000At timestep:     493 / 1000At timestep:     643 / 1000At timestep:     693 / 1000At timestep:     843 / 1000At timestep:     993 / 1000At timestep:     1143 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -122.7397232055664
TimeSinceStart : 284.3671543598175
Exploration Critic Loss : 5442950.0
Exploitation Critic Loss : 0.08582627773284912
Exploration Model Loss : 63.251678466796875
Actor Loss : 0.48107317090034485
Eval_AverageReturn : -126.66666412353516
Eval_StdReturn : 35.62458419799805
Eval_MaxReturn : -49.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 127.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -112.159088
best mean reward -inf
running time 322.396638
At timestep:     109 / 1000At timestep:     220 / 1000At timestep:     367 / 1000At timestep:     517 / 1000At timestep:     636 / 1000At timestep:     786 / 1000At timestep:     885 / 1000At timestep:     949 / 1000At timestep:     1043 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -112.15908813476562
TimeSinceStart : 322.3966381549835
Exploration Critic Loss : 4619972.0
Exploitation Critic Loss : 0.0973275825381279
Exploration Model Loss : 29.095230102539062
Actor Loss : 0.47415488958358765
Eval_AverageReturn : -115.11111450195312
Eval_StdReturn : 27.830615997314453
Eval_MaxReturn : -63.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 115.88888888888889
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -100.360001
best mean reward -100.360001
running time 361.009015
At timestep:     99 / 1000At timestep:     165 / 1000At timestep:     275 / 1000At timestep:     425 / 1000At timestep:     533 / 1000At timestep:     683 / 1000At timestep:     789 / 1000At timestep:     939 / 1000At timestep:     1089 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -100.36000061035156
Train_BestReturn : -100.36000061035156
TimeSinceStart : 361.009015083313
Exploration Critic Loss : 5802478.0
Exploitation Critic Loss : 0.17689889669418335
Exploration Model Loss : 32.72343444824219
Actor Loss : 0.41399991512298584
Eval_AverageReturn : -120.44444274902344
Eval_StdReturn : 29.101778030395508
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 121.0
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -78.830002
best mean reward -78.830002
running time 399.294656
At timestep:     134 / 1000At timestep:     197 / 1000At timestep:     314 / 1000At timestep:     404 / 1000At timestep:     455 / 1000At timestep:     502 / 1000At timestep:     644 / 1000At timestep:     677 / 1000At timestep:     806 / 1000At timestep:     947 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -78.83000183105469
Train_BestReturn : -78.83000183105469
TimeSinceStart : 399.2946557998657
Exploration Critic Loss : 6927858.0
Exploitation Critic Loss : 0.15229670703411102
Exploration Model Loss : 28.482574462890625
Actor Loss : 0.40475624799728394
Eval_AverageReturn : -94.45454406738281
Eval_StdReturn : 38.94136428833008
Eval_MaxReturn : -32.0
Eval_MinReturn : -141.0
Eval_AverageEpLen : 95.45454545454545
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -56.290001
best mean reward -56.290001
running time 440.416106
At timestep:     65 / 1000At timestep:     196 / 1000At timestep:     249 / 1000At timestep:     360 / 1000At timestep:     510 / 1000At timestep:     568 / 1000At timestep:     599 / 1000At timestep:     749 / 1000At timestep:     824 / 1000At timestep:     934 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -56.290000915527344
Train_BestReturn : -56.290000915527344
TimeSinceStart : 440.41610646247864
Exploration Critic Loss : 5349590.0
Exploitation Critic Loss : 0.1459069848060608
Exploration Model Loss : 27.376800537109375
Actor Loss : 0.4063602387905121
Eval_AverageReturn : -90.36363983154297
Eval_StdReturn : 39.43757247924805
Eval_MaxReturn : -30.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 91.18181818181819
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -43.770000
best mean reward -43.770000
running time 480.732480
At timestep:     116 / 1000At timestep:     175 / 1000At timestep:     250 / 1000At timestep:     336 / 1000At timestep:     413 / 1000At timestep:     538 / 1000At timestep:     592 / 1000At timestep:     634 / 1000At timestep:     711 / 1000At timestep:     754 / 1000At timestep:     799 / 1000At timestep:     823 / 1000At timestep:     924 / 1000At timestep:     967 / 1000At timestep:     1082 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -43.77000045776367
Train_BestReturn : -43.77000045776367
TimeSinceStart : 480.7324798107147
Exploration Critic Loss : 5164065.0
Exploitation Critic Loss : 0.05067237466573715
Exploration Model Loss : 27.010238647460938
Actor Loss : 0.38657432794570923
Eval_AverageReturn : -71.13333129882812
Eval_StdReturn : 30.329010009765625
Eval_MaxReturn : -23.0
Eval_MinReturn : -124.0
Eval_AverageEpLen : 72.13333333333334
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -37.660000
best mean reward -37.660000
running time 522.094084
At timestep:     64 / 1000At timestep:     116 / 1000At timestep:     246 / 1000At timestep:     327 / 1000At timestep:     358 / 1000At timestep:     420 / 1000At timestep:     515 / 1000At timestep:     578 / 1000At timestep:     660 / 1000At timestep:     726 / 1000At timestep:     822 / 1000At timestep:     939 / 1000At timestep:     978 / 1000At timestep:     1037 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -37.65999984741211
Train_BestReturn : -37.65999984741211
TimeSinceStart : 522.0940837860107
Exploration Critic Loss : 4632680.0
Exploitation Critic Loss : 0.10155785083770752
Exploration Model Loss : 26.880615234375
Actor Loss : 0.42789149284362793
Eval_AverageReturn : -73.07142639160156
Eval_StdReturn : 27.038238525390625
Eval_MaxReturn : -30.0
Eval_MinReturn : -129.0
Eval_AverageEpLen : 74.07142857142857
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -33.020000
best mean reward -33.020000
running time 564.457988
At timestep:     79 / 1000At timestep:     208 / 1000At timestep:     259 / 1000At timestep:     353 / 1000At timestep:     444 / 1000At timestep:     503 / 1000At timestep:     641 / 1000At timestep:     734 / 1000At timestep:     800 / 1000At timestep:     871 / 1000At timestep:     931 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -33.02000045776367
Train_BestReturn : -33.02000045776367
TimeSinceStart : 564.4579877853394
Exploration Critic Loss : 2502487.5
Exploitation Critic Loss : 0.17839020490646362
Exploration Model Loss : 26.83331298828125
Actor Loss : 0.4352976083755493
Eval_AverageReturn : -84.5
Eval_StdReturn : 25.93742561340332
Eval_MaxReturn : -50.0
Eval_MinReturn : -137.0
Eval_AverageEpLen : 85.5
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -32.349998
best mean reward -32.349998
running time 605.271859
At timestep:     70 / 1000At timestep:     165 / 1000At timestep:     267 / 1000At timestep:     317 / 1000At timestep:     384 / 1000At timestep:     432 / 1000At timestep:     499 / 1000At timestep:     535 / 1000At timestep:     583 / 1000At timestep:     648 / 1000At timestep:     683 / 1000At timestep:     730 / 1000At timestep:     763 / 1000At timestep:     802 / 1000At timestep:     851 / 1000At timestep:     974 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -32.349998474121094
Train_BestReturn : -32.349998474121094
TimeSinceStart : 605.2718586921692
Exploration Critic Loss : 1865937.375
Exploitation Critic Loss : 0.14605149626731873
Exploration Model Loss : 26.815597534179688
Actor Loss : 0.4793629050254822
Eval_AverageReturn : -59.47058868408203
Eval_StdReturn : 24.56951141357422
Eval_MaxReturn : -32.0
Eval_MinReturn : -122.0
Eval_AverageEpLen : 60.470588235294116
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -31.219999
best mean reward -31.219999
running time 647.725190
At timestep:     30 / 1000At timestep:     71 / 1000At timestep:     130 / 1000At timestep:     163 / 1000At timestep:     204 / 1000At timestep:     243 / 1000At timestep:     290 / 1000At timestep:     355 / 1000At timestep:     384 / 1000At timestep:     444 / 1000At timestep:     487 / 1000At timestep:     535 / 1000At timestep:     588 / 1000At timestep:     640 / 1000At timestep:     716 / 1000At timestep:     759 / 1000At timestep:     806 / 1000At timestep:     858 / 1000At timestep:     961 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -31.219999313354492
Train_BestReturn : -31.219999313354492
TimeSinceStart : 647.7251901626587
Exploration Critic Loss : 2768332.25
Exploitation Critic Loss : 0.12124864757061005
Exploration Model Loss : 26.80792236328125
Actor Loss : 0.4398428797721863
Eval_AverageReturn : -49.849998474121094
Eval_StdReturn : 16.526569366455078
Eval_MaxReturn : -28.0
Eval_MinReturn : -102.0
Eval_AverageEpLen : 50.85
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -30.340000
best mean reward -30.340000
running time 691.135171
At timestep:     49 / 1000At timestep:     109 / 1000At timestep:     149 / 1000At timestep:     198 / 1000At timestep:     250 / 1000At timestep:     316 / 1000At timestep:     339 / 1000At timestep:     402 / 1000At timestep:     518 / 1000At timestep:     600 / 1000At timestep:     635 / 1000At timestep:     707 / 1000At timestep:     817 / 1000At timestep:     912 / 1000At timestep:     970 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -30.34000015258789
Train_BestReturn : -30.34000015258789
TimeSinceStart : 691.1351714134216
Exploration Critic Loss : 771206.0
Exploitation Critic Loss : 0.11479563266038895
Exploration Model Loss : 26.804595947265625
Actor Loss : 0.48563694953918457
Eval_AverageReturn : -61.875
Eval_StdReturn : 25.838138580322266
Eval_MaxReturn : -22.0
Eval_MinReturn : -115.0
Eval_AverageEpLen : 62.875
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -30.219999
best mean reward -30.219999
running time 732.142566
At timestep:     60 / 1000At timestep:     100 / 1000At timestep:     133 / 1000At timestep:     165 / 1000At timestep:     223 / 1000At timestep:     268 / 1000At timestep:     344 / 1000At timestep:     364 / 1000At timestep:     423 / 1000At timestep:     456 / 1000At timestep:     501 / 1000At timestep:     557 / 1000At timestep:     578 / 1000At timestep:     635 / 1000At timestep:     679 / 1000At timestep:     710 / 1000At timestep:     739 / 1000At timestep:     783 / 1000At timestep:     849 / 1000At timestep:     930 / 1000At timestep:     977 / 1000At timestep:     1044 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -30.219999313354492
Train_BestReturn : -30.219999313354492
TimeSinceStart : 732.1425659656525
Exploration Critic Loss : 727880.625
Exploitation Critic Loss : 0.13428616523742676
Exploration Model Loss : 26.803802490234375
Actor Loss : 0.4722929000854492
Eval_AverageReturn : -46.45454406738281
Eval_StdReturn : 16.576894760131836
Eval_MaxReturn : -19.0
Eval_MinReturn : -80.0
Eval_AverageEpLen : 47.45454545454545
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -28.790001
best mean reward -28.790001
running time 775.820677
At timestep:     37 / 1000At timestep:     76 / 1000At timestep:     137 / 1000At timestep:     173 / 1000At timestep:     206 / 1000At timestep:     286 / 1000At timestep:     380 / 1000At timestep:     432 / 1000At timestep:     474 / 1000At timestep:     523 / 1000At timestep:     607 / 1000At timestep:     639 / 1000At timestep:     678 / 1000At timestep:     727 / 1000At timestep:     754 / 1000At timestep:     784 / 1000At timestep:     842 / 1000At timestep:     892 / 1000At timestep:     928 / 1000At timestep:     987 / 1000At timestep:     1070 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -28.790000915527344
Train_BestReturn : -28.790000915527344
TimeSinceStart : 775.820677280426
Exploration Critic Loss : 655932.1875
Exploitation Critic Loss : 0.2009788602590561
Exploration Model Loss : 26.803298950195312
Actor Loss : 0.5131264925003052
Eval_AverageReturn : -49.9523811340332
Eval_StdReturn : 19.201875686645508
Eval_MaxReturn : -26.0
Eval_MinReturn : -93.0
Eval_AverageEpLen : 50.95238095238095
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -28.799999
best mean reward -28.790001
running time 821.164003
At timestep:     50 / 1000At timestep:     112 / 1000At timestep:     145 / 1000At timestep:     178 / 1000At timestep:     226 / 1000At timestep:     268 / 1000At timestep:     305 / 1000At timestep:     336 / 1000At timestep:     381 / 1000At timestep:     432 / 1000At timestep:     481 / 1000At timestep:     522 / 1000At timestep:     576 / 1000At timestep:     622 / 1000At timestep:     661 / 1000At timestep:     697 / 1000At timestep:     734 / 1000At timestep:     790 / 1000At timestep:     848 / 1000At timestep:     916 / 1000At timestep:     948 / 1000At timestep:     967 / 1000At timestep:     995 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -28.799999237060547
Train_BestReturn : -28.790000915527344
TimeSinceStart : 821.1640028953552
Exploration Critic Loss : 693663.125
Exploitation Critic Loss : 0.1254982352256775
Exploration Model Loss : 26.802902221679688
Actor Loss : 0.49975964426994324
Eval_AverageReturn : -42.0
Eval_StdReturn : 11.423659324645996
Eval_MaxReturn : -18.0
Eval_MinReturn : -67.0
Eval_AverageEpLen : 43.0
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -28.530001
best mean reward -28.530001
running time 864.661648
At timestep:     29 / 1000At timestep:     55 / 1000At timestep:     89 / 1000At timestep:     124 / 1000At timestep:     159 / 1000At timestep:     200 / 1000At timestep:     227 / 1000At timestep:     270 / 1000At timestep:     318 / 1000At timestep:     368 / 1000At timestep:     418 / 1000At timestep:     465 / 1000At timestep:     537 / 1000At timestep:     572 / 1000At timestep:     604 / 1000At timestep:     631 / 1000At timestep:     683 / 1000At timestep:     730 / 1000At timestep:     782 / 1000At timestep:     830 / 1000At timestep:     864 / 1000At timestep:     892 / 1000At timestep:     959 / 1000At timestep:     999 / 1000At timestep:     1058 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -28.530000686645508
Train_BestReturn : -28.530000686645508
TimeSinceStart : 864.6616480350494
Exploration Critic Loss : 709218.125
Exploitation Critic Loss : 0.10798308253288269
Exploration Model Loss : 26.802413940429688
Actor Loss : 0.5252317190170288
Eval_AverageReturn : -41.31999969482422
Eval_StdReturn : 12.187600135803223
Eval_MaxReturn : -25.0
Eval_MinReturn : -71.0
Eval_AverageEpLen : 42.32
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -28.299999
best mean reward -28.299999
running time 909.124728
At timestep:     55 / 1000At timestep:     125 / 1000At timestep:     160 / 1000At timestep:     194 / 1000At timestep:     259 / 1000At timestep:     321 / 1000At timestep:     368 / 1000At timestep:     407 / 1000At timestep:     487 / 1000At timestep:     524 / 1000At timestep:     558 / 1000At timestep:     621 / 1000At timestep:     666 / 1000At timestep:     707 / 1000At timestep:     741 / 1000At timestep:     792 / 1000At timestep:     881 / 1000At timestep:     918 / 1000At timestep:     950 / 1000At timestep:     993 / 1000At timestep:     1058 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -28.299999237060547
Train_BestReturn : -28.299999237060547
TimeSinceStart : 909.1247282028198
Exploration Critic Loss : 875470.3125
Exploitation Critic Loss : 0.13174739480018616
Exploration Model Loss : 26.802169799804688
Actor Loss : 0.47822993993759155
Eval_AverageReturn : -49.380950927734375
Eval_StdReturn : 16.25240135192871
Eval_MaxReturn : -31.0
Eval_MinReturn : -88.0
Eval_AverageEpLen : 50.38095238095238
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -27.990000
best mean reward -27.990000
running time 953.464733
At timestep:     47 / 1000At timestep:     99 / 1000At timestep:     126 / 1000At timestep:     176 / 1000At timestep:     233 / 1000At timestep:     268 / 1000At timestep:     351 / 1000At timestep:     425 / 1000At timestep:     474 / 1000At timestep:     550 / 1000At timestep:     613 / 1000At timestep:     670 / 1000At timestep:     694 / 1000At timestep:     744 / 1000At timestep:     794 / 1000At timestep:     830 / 1000At timestep:     901 / 1000At timestep:     924 / 1000At timestep:     955 / 1000At timestep:     987 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -27.989999771118164
Train_BestReturn : -27.989999771118164
TimeSinceStart : 953.4647331237793
Exploration Critic Loss : 535048.4375
Exploitation Critic Loss : 0.08147411048412323
Exploration Model Loss : 26.801864624023438
Actor Loss : 0.5498892068862915
Eval_AverageReturn : -47.095237731933594
Eval_StdReturn : 17.936141967773438
Eval_MaxReturn : -22.0
Eval_MinReturn : -82.0
Eval_AverageEpLen : 48.095238095238095
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -27.750000
best mean reward -27.750000
running time 995.856293
At timestep:     46 / 1000At timestep:     85 / 1000At timestep:     135 / 1000At timestep:     169 / 1000At timestep:     210 / 1000At timestep:     247 / 1000At timestep:     276 / 1000At timestep:     321 / 1000At timestep:     422 / 1000At timestep:     469 / 1000At timestep:     528 / 1000At timestep:     552 / 1000At timestep:     601 / 1000At timestep:     654 / 1000At timestep:     703 / 1000At timestep:     737 / 1000At timestep:     775 / 1000At timestep:     824 / 1000At timestep:     862 / 1000At timestep:     919 / 1000At timestep:     942 / 1000At timestep:     982 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -27.75
Train_BestReturn : -27.75
TimeSinceStart : 995.8562927246094
Exploration Critic Loss : 601623.9375
Exploitation Critic Loss : 0.1094423234462738
Exploration Model Loss : 26.801361083984375
Actor Loss : 0.4867885708808899
Eval_AverageReturn : -43.826087951660156
Eval_StdReturn : 15.173352241516113
Eval_MaxReturn : -22.0
Eval_MinReturn : -100.0
Eval_AverageEpLen : 44.82608695652174
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -27.200001
best mean reward -27.200001
running time 1039.260451
At timestep:     53 / 1000At timestep:     89 / 1000At timestep:     130 / 1000At timestep:     172 / 1000At timestep:     215 / 1000At timestep:     267 / 1000At timestep:     307 / 1000At timestep:     376 / 1000At timestep:     432 / 1000At timestep:     486 / 1000At timestep:     508 / 1000At timestep:     566 / 1000At timestep:     600 / 1000At timestep:     622 / 1000At timestep:     656 / 1000At timestep:     678 / 1000At timestep:     712 / 1000At timestep:     755 / 1000At timestep:     786 / 1000At timestep:     859 / 1000At timestep:     882 / 1000At timestep:     912 / 1000At timestep:     988 / 1000At timestep:     1026 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -27.200000762939453
Train_BestReturn : -27.200000762939453
TimeSinceStart : 1039.260451078415
Exploration Critic Loss : 586041.0625
Exploitation Critic Loss : 0.09708915650844574
Exploration Model Loss : 26.800949096679688
Actor Loss : 0.4978058934211731
Eval_AverageReturn : -41.75
Eval_StdReturn : 15.446817398071289
Eval_MaxReturn : -21.0
Eval_MinReturn : -75.0
Eval_AverageEpLen : 42.75
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -27.170000
best mean reward -27.170000
running time 1083.669603
At timestep:     31 / 1000At timestep:     78 / 1000At timestep:     120 / 1000At timestep:     156 / 1000At timestep:     238 / 1000At timestep:     270 / 1000At timestep:     326 / 1000At timestep:     353 / 1000At timestep:     406 / 1000At timestep:     478 / 1000At timestep:     520 / 1000At timestep:     568 / 1000At timestep:     593 / 1000At timestep:     647 / 1000At timestep:     700 / 1000At timestep:     744 / 1000At timestep:     784 / 1000At timestep:     836 / 1000At timestep:     858 / 1000At timestep:     889 / 1000At timestep:     983 / 1000At timestep:     1037 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -27.170000076293945
Train_BestReturn : -27.170000076293945
TimeSinceStart : 1083.6696028709412
Exploration Critic Loss : 601922.375
Exploitation Critic Loss : 0.13549548387527466
Exploration Model Loss : 26.800521850585938
Actor Loss : 0.5070005655288696
Eval_AverageReturn : -46.1363639831543
Eval_StdReturn : 17.594667434692383
Eval_MaxReturn : -21.0
Eval_MinReturn : -93.0
Eval_AverageEpLen : 47.13636363636363
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -26.969999
best mean reward -26.969999
running time 1126.367712
At timestep:     28 / 1000At timestep:     73 / 1000At timestep:     111 / 1000At timestep:     178 / 1000At timestep:     211 / 1000At timestep:     248 / 1000At timestep:     283 / 1000At timestep:     325 / 1000At timestep:     420 / 1000At timestep:     453 / 1000At timestep:     517 / 1000At timestep:     572 / 1000At timestep:     603 / 1000At timestep:     641 / 1000At timestep:     667 / 1000At timestep:     700 / 1000At timestep:     738 / 1000At timestep:     791 / 1000At timestep:     823 / 1000At timestep:     864 / 1000At timestep:     924 / 1000At timestep:     946 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -26.969999313354492
Train_BestReturn : -26.969999313354492
TimeSinceStart : 1126.3677122592926
Exploration Critic Loss : 536573.75
Exploitation Critic Loss : 0.1481567621231079
Exploration Model Loss : 26.80023193359375
Actor Loss : 0.5346888303756714
Eval_AverageReturn : -42.91304397583008
Eval_StdReturn : 16.65484046936035
Eval_MaxReturn : -21.0
Eval_MinReturn : -94.0
Eval_AverageEpLen : 43.91304347826087
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -26.820000
best mean reward -26.820000
running time 1169.878763
At timestep:     67 / 1000At timestep:     109 / 1000At timestep:     143 / 1000At timestep:     170 / 1000At timestep:     224 / 1000At timestep:     278 / 1000At timestep:     309 / 1000At timestep:     342 / 1000At timestep:     398 / 1000At timestep:     447 / 1000At timestep:     491 / 1000At timestep:     541 / 1000At timestep:     582 / 1000At timestep:     615 / 1000At timestep:     657 / 1000At timestep:     707 / 1000At timestep:     768 / 1000At timestep:     809 / 1000At timestep:     905 / 1000At timestep:     938 / 1000At timestep:     974 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -26.81999969482422
Train_BestReturn : -26.81999969482422
TimeSinceStart : 1169.8787634372711
Exploration Critic Loss : 694821.875
Exploitation Critic Loss : 0.08832962065935135
Exploration Model Loss : 26.799789428710938
Actor Loss : 0.5450985431671143
Eval_AverageReturn : -44.6363639831543
Eval_StdReturn : 15.242122650146484
Eval_MaxReturn : -26.0
Eval_MinReturn : -95.0
Eval_AverageEpLen : 45.63636363636363
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -27.150000
best mean reward -26.820000
running time 1214.380375
At timestep:     88 / 1000At timestep:     130 / 1000At timestep:     167 / 1000At timestep:     204 / 1000At timestep:     227 / 1000At timestep:     267 / 1000At timestep:     313 / 1000At timestep:     368 / 1000At timestep:     414 / 1000At timestep:     447 / 1000At timestep:     480 / 1000At timestep:     513 / 1000At timestep:     565 / 1000At timestep:     601 / 1000At timestep:     637 / 1000At timestep:     687 / 1000At timestep:     712 / 1000At timestep:     796 / 1000At timestep:     847 / 1000At timestep:     883 / 1000At timestep:     939 / 1000At timestep:     959 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -27.149999618530273
Train_BestReturn : -26.81999969482422
TimeSinceStart : 1214.3803746700287
Exploration Critic Loss : 458296.96875
Exploitation Critic Loss : 0.09197655320167542
Exploration Model Loss : 26.799575805664062
Actor Loss : 0.5182480812072754
Eval_AverageReturn : -42.65217208862305
Eval_StdReturn : 16.161169052124023
Eval_MaxReturn : -19.0
Eval_MinReturn : -87.0
Eval_AverageEpLen : 43.65217391304348
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -27.370001
best mean reward -26.820000
running time 1256.600463
At timestep:     32 / 1000At timestep:     64 / 1000At timestep:     86 / 1000At timestep:     110 / 1000At timestep:     157 / 1000At timestep:     230 / 1000At timestep:     259 / 1000At timestep:     309 / 1000At timestep:     352 / 1000At timestep:     383 / 1000At timestep:     407 / 1000At timestep:     440 / 1000At timestep:     473 / 1000At timestep:     515 / 1000At timestep:     562 / 1000At timestep:     594 / 1000At timestep:     620 / 1000At timestep:     643 / 1000At timestep:     688 / 1000At timestep:     732 / 1000At timestep:     755 / 1000At timestep:     782 / 1000At timestep:     814 / 1000At timestep:     862 / 1000At timestep:     893 / 1000At timestep:     993 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -27.3700008392334
Train_BestReturn : -26.81999969482422
TimeSinceStart : 1256.6004631519318
Exploration Critic Loss : 501184.15625
Exploitation Critic Loss : 0.08833080530166626
Exploration Model Loss : 26.799270629882812
Actor Loss : 0.5540447235107422
Eval_AverageReturn : -36.814815521240234
Eval_StdReturn : 16.59548568725586
Eval_MaxReturn : -21.0
Eval_MinReturn : -99.0
Eval_AverageEpLen : 37.81481481481482
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -26.570000
best mean reward -26.570000
running time 1299.652454
At timestep:     36 / 1000At timestep:     84 / 1000At timestep:     105 / 1000At timestep:     131 / 1000At timestep:     193 / 1000At timestep:     242 / 1000At timestep:     283 / 1000At timestep:     321 / 1000At timestep:     361 / 1000At timestep:     412 / 1000At timestep:     477 / 1000At timestep:     510 / 1000At timestep:     572 / 1000At timestep:     605 / 1000At timestep:     635 / 1000At timestep:     703 / 1000At timestep:     748 / 1000At timestep:     784 / 1000At timestep:     817 / 1000At timestep:     849 / 1000At timestep:     890 / 1000At timestep:     922 / 1000At timestep:     951 / 1000At timestep:     989 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -26.56999969482422
Train_BestReturn : -26.56999969482422
TimeSinceStart : 1299.6524543762207
Exploration Critic Loss : 480779.75
Exploitation Critic Loss : 0.09842206537723541
Exploration Model Loss : 26.798828125
Actor Loss : 0.4688902795314789
Eval_AverageReturn : -40.0
Eval_StdReturn : 12.253978729248047
Eval_MaxReturn : -20.0
Eval_MinReturn : -67.0
Eval_AverageEpLen : 41.0
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -26.250000
best mean reward -26.250000
running time 1344.711326
At timestep:     34 / 1000At timestep:     91 / 1000At timestep:     115 / 1000At timestep:     183 / 1000At timestep:     214 / 1000At timestep:     256 / 1000At timestep:     300 / 1000At timestep:     336 / 1000At timestep:     374 / 1000At timestep:     408 / 1000At timestep:     456 / 1000At timestep:     503 / 1000At timestep:     530 / 1000At timestep:     566 / 1000At timestep:     600 / 1000At timestep:     620 / 1000At timestep:     660 / 1000At timestep:     685 / 1000At timestep:     748 / 1000At timestep:     774 / 1000At timestep:     804 / 1000At timestep:     856 / 1000At timestep:     890 / 1000At timestep:     933 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -26.25
Train_BestReturn : -26.25
TimeSinceStart : 1344.7113258838654
Exploration Critic Loss : 398675.6875
Exploitation Critic Loss : 0.14728419482707977
Exploration Model Loss : 26.798416137695312
Actor Loss : 0.5064607858657837
Eval_AverageReturn : -39.20000076293945
Eval_StdReturn : 13.443214416503906
Eval_MaxReturn : -19.0
Eval_MinReturn : -71.0
Eval_AverageEpLen : 40.2
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -25.680000
best mean reward -25.680000
running time 1387.702549
At timestep:     21 / 1000At timestep:     52 / 1000At timestep:     84 / 1000At timestep:     114 / 1000At timestep:     146 / 1000At timestep:     175 / 1000At timestep:     201 / 1000At timestep:     275 / 1000At timestep:     318 / 1000At timestep:     358 / 1000At timestep:     388 / 1000At timestep:     414 / 1000At timestep:     444 / 1000At timestep:     511 / 1000At timestep:     536 / 1000At timestep:     565 / 1000At timestep:     625 / 1000At timestep:     661 / 1000At timestep:     695 / 1000At timestep:     726 / 1000At timestep:     751 / 1000At timestep:     776 / 1000At timestep:     826 / 1000At timestep:     876 / 1000At timestep:     916 / 1000At timestep:     964 / 1000At timestep:     991 / 1000At timestep:     1032 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -25.68000030517578
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1387.7025492191315
Exploration Critic Loss : 593640.625
Exploitation Critic Loss : 0.10370098054409027
Exploration Model Loss : 26.797988891601562
Actor Loss : 0.4755111038684845
Eval_AverageReturn : -35.85714340209961
Eval_StdReturn : 13.043099403381348
Eval_MaxReturn : -20.0
Eval_MinReturn : -73.0
Eval_AverageEpLen : 36.857142857142854
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -26.840000
best mean reward -25.680000
running time 1431.553389
At timestep:     18 / 1000At timestep:     69 / 1000At timestep:     103 / 1000At timestep:     142 / 1000At timestep:     164 / 1000At timestep:     192 / 1000At timestep:     235 / 1000At timestep:     268 / 1000At timestep:     306 / 1000At timestep:     377 / 1000At timestep:     411 / 1000At timestep:     441 / 1000At timestep:     494 / 1000At timestep:     529 / 1000At timestep:     578 / 1000At timestep:     627 / 1000At timestep:     667 / 1000At timestep:     701 / 1000At timestep:     736 / 1000At timestep:     778 / 1000At timestep:     800 / 1000At timestep:     828 / 1000At timestep:     849 / 1000At timestep:     875 / 1000At timestep:     898 / 1000At timestep:     925 / 1000At timestep:     952 / 1000At timestep:     997 / 1000At timestep:     1054 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -26.84000015258789
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1431.553388595581
Exploration Critic Loss : 386486.375
Exploitation Critic Loss : 0.09237971156835556
Exploration Model Loss : 26.7978515625
Actor Loss : 0.5028827786445618
Eval_AverageReturn : -35.344825744628906
Eval_StdReturn : 12.132247924804688
Eval_MaxReturn : -17.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 36.3448275862069
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -26.740000
best mean reward -25.680000
running time 1477.101019
At timestep:     66 / 1000At timestep:     92 / 1000At timestep:     138 / 1000At timestep:     167 / 1000At timestep:     195 / 1000At timestep:     231 / 1000At timestep:     259 / 1000At timestep:     302 / 1000At timestep:     341 / 1000At timestep:     373 / 1000At timestep:     420 / 1000At timestep:     473 / 1000At timestep:     536 / 1000At timestep:     565 / 1000At timestep:     615 / 1000At timestep:     664 / 1000At timestep:     692 / 1000At timestep:     720 / 1000At timestep:     758 / 1000At timestep:     797 / 1000At timestep:     896 / 1000At timestep:     952 / 1000At timestep:     996 / 1000At timestep:     1048 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -26.739999771118164
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1477.101018667221
Exploration Critic Loss : 519019.0
Exploitation Critic Loss : 0.09182395040988922
Exploration Model Loss : 26.797454833984375
Actor Loss : 0.4771261215209961
Eval_AverageReturn : -42.66666793823242
Eval_StdReturn : 16.231826782226562
Eval_MaxReturn : -25.0
Eval_MinReturn : -98.0
Eval_AverageEpLen : 43.666666666666664
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -26.510000
best mean reward -25.680000
running time 1520.951959
At timestep:     46 / 1000At timestep:     103 / 1000At timestep:     145 / 1000At timestep:     175 / 1000At timestep:     242 / 1000At timestep:     275 / 1000At timestep:     306 / 1000At timestep:     341 / 1000At timestep:     400 / 1000At timestep:     438 / 1000At timestep:     470 / 1000At timestep:     498 / 1000At timestep:     533 / 1000At timestep:     558 / 1000At timestep:     602 / 1000At timestep:     628 / 1000At timestep:     673 / 1000At timestep:     699 / 1000At timestep:     767 / 1000At timestep:     799 / 1000At timestep:     826 / 1000At timestep:     856 / 1000At timestep:     877 / 1000At timestep:     907 / 1000At timestep:     950 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -26.510000228881836
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1520.9519593715668
Exploration Critic Loss : 386902.40625
Exploitation Critic Loss : 0.09087686240673065
Exploration Model Loss : 26.797042846679688
Actor Loss : 0.46086007356643677
Eval_AverageReturn : -37.53845977783203
Eval_StdReturn : 12.776874542236328
Eval_MaxReturn : -20.0
Eval_MinReturn : -67.0
Eval_AverageEpLen : 38.53846153846154
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -26.879999
best mean reward -25.680000
running time 1564.164513
At timestep:     40 / 1000At timestep:     81 / 1000At timestep:     126 / 1000At timestep:     169 / 1000At timestep:     202 / 1000At timestep:     227 / 1000At timestep:     260 / 1000At timestep:     299 / 1000At timestep:     335 / 1000At timestep:     396 / 1000At timestep:     432 / 1000At timestep:     467 / 1000At timestep:     508 / 1000At timestep:     545 / 1000At timestep:     592 / 1000At timestep:     633 / 1000At timestep:     691 / 1000At timestep:     728 / 1000At timestep:     765 / 1000At timestep:     796 / 1000At timestep:     835 / 1000At timestep:     869 / 1000At timestep:     896 / 1000At timestep:     920 / 1000At timestep:     947 / 1000At timestep:     995 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -26.8799991607666
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1564.1645131111145
Exploration Critic Loss : 306920.5625
Exploitation Critic Loss : 0.07589796185493469
Exploration Model Loss : 26.796768188476562
Actor Loss : 0.4918593764305115
Eval_AverageReturn : -37.11111068725586
Eval_StdReturn : 8.551947593688965
Eval_MaxReturn : -23.0
Eval_MinReturn : -60.0
Eval_AverageEpLen : 38.111111111111114
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -26.340000
best mean reward -25.680000
running time 1609.373484
At timestep:     39 / 1000At timestep:     74 / 1000At timestep:     116 / 1000At timestep:     148 / 1000At timestep:     180 / 1000At timestep:     234 / 1000At timestep:     276 / 1000At timestep:     318 / 1000At timestep:     361 / 1000At timestep:     394 / 1000At timestep:     417 / 1000At timestep:     461 / 1000At timestep:     507 / 1000At timestep:     532 / 1000At timestep:     587 / 1000At timestep:     624 / 1000At timestep:     652 / 1000At timestep:     692 / 1000At timestep:     719 / 1000At timestep:     749 / 1000At timestep:     780 / 1000At timestep:     806 / 1000At timestep:     849 / 1000At timestep:     877 / 1000At timestep:     935 / 1000At timestep:     979 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -26.34000015258789
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1609.3734838962555
Exploration Critic Loss : 294072.65625
Exploitation Critic Loss : 0.09246125817298889
Exploration Model Loss : 26.796600341796875
Actor Loss : 0.5098831653594971
Eval_AverageReturn : -37.074073791503906
Eval_StdReturn : 9.396319389343262
Eval_MaxReturn : -22.0
Eval_MinReturn : -57.0
Eval_AverageEpLen : 38.074074074074076
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -26.520000
best mean reward -25.680000
running time 1654.156272
At timestep:     17 / 1000At timestep:     55 / 1000At timestep:     101 / 1000At timestep:     137 / 1000At timestep:     166 / 1000At timestep:     199 / 1000At timestep:     227 / 1000At timestep:     248 / 1000At timestep:     277 / 1000At timestep:     306 / 1000At timestep:     349 / 1000At timestep:     383 / 1000At timestep:     418 / 1000At timestep:     453 / 1000At timestep:     510 / 1000At timestep:     546 / 1000At timestep:     590 / 1000At timestep:     640 / 1000At timestep:     682 / 1000At timestep:     719 / 1000At timestep:     757 / 1000At timestep:     783 / 1000At timestep:     816 / 1000At timestep:     846 / 1000At timestep:     871 / 1000At timestep:     903 / 1000At timestep:     941 / 1000At timestep:     979 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -26.520000457763672
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1654.1562724113464
Exploration Critic Loss : 276059.375
Exploitation Critic Loss : 0.10713648796081543
Exploration Model Loss : 26.796493530273438
Actor Loss : 0.5027201771736145
Eval_AverageReturn : -34.41379165649414
Eval_StdReturn : 8.572278022766113
Eval_MaxReturn : -16.0
Eval_MinReturn : -56.0
Eval_AverageEpLen : 35.41379310344828
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -26.700001
best mean reward -25.680000
running time 1696.019119
At timestep:     27 / 1000At timestep:     80 / 1000At timestep:     103 / 1000At timestep:     145 / 1000At timestep:     177 / 1000At timestep:     217 / 1000At timestep:     253 / 1000At timestep:     280 / 1000At timestep:     314 / 1000At timestep:     351 / 1000At timestep:     374 / 1000At timestep:     414 / 1000At timestep:     448 / 1000At timestep:     475 / 1000At timestep:     517 / 1000At timestep:     563 / 1000At timestep:     615 / 1000At timestep:     667 / 1000At timestep:     696 / 1000At timestep:     732 / 1000At timestep:     769 / 1000At timestep:     798 / 1000At timestep:     822 / 1000At timestep:     854 / 1000At timestep:     885 / 1000At timestep:     912 / 1000At timestep:     969 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -26.700000762939453
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1696.0191185474396
Exploration Critic Loss : 291157.25
Exploitation Critic Loss : 0.12720811367034912
Exploration Model Loss : 26.796356201171875
Actor Loss : 0.4738432765007019
Eval_AverageReturn : -34.92856979370117
Eval_StdReturn : 9.292579650878906
Eval_MaxReturn : -22.0
Eval_MinReturn : -56.0
Eval_AverageEpLen : 35.92857142857143
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -27.150000
best mean reward -25.680000
running time 1739.520595
At timestep:     27 / 1000At timestep:     58 / 1000At timestep:     85 / 1000At timestep:     135 / 1000At timestep:     191 / 1000At timestep:     222 / 1000At timestep:     250 / 1000At timestep:     287 / 1000At timestep:     332 / 1000At timestep:     368 / 1000At timestep:     402 / 1000At timestep:     436 / 1000At timestep:     459 / 1000At timestep:     490 / 1000At timestep:     518 / 1000At timestep:     545 / 1000At timestep:     581 / 1000At timestep:     607 / 1000At timestep:     657 / 1000At timestep:     686 / 1000At timestep:     717 / 1000At timestep:     784 / 1000At timestep:     817 / 1000At timestep:     844 / 1000At timestep:     888 / 1000At timestep:     913 / 1000At timestep:     941 / 1000At timestep:     983 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -27.149999618530273
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1739.5205945968628
Exploration Critic Loss : 215469.96875
Exploitation Critic Loss : 0.10250326991081238
Exploration Model Loss : 26.796249389648438
Actor Loss : 0.49422845244407654
Eval_AverageReturn : -34.0
Eval_StdReturn : 10.160776138305664
Eval_MaxReturn : -22.0
Eval_MinReturn : -66.0
Eval_AverageEpLen : 35.0
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -27.270000
best mean reward -25.680000
running time 1785.396252
At timestep:     32 / 1000At timestep:     78 / 1000At timestep:     127 / 1000At timestep:     154 / 1000At timestep:     184 / 1000At timestep:     216 / 1000At timestep:     253 / 1000At timestep:     284 / 1000At timestep:     308 / 1000At timestep:     344 / 1000At timestep:     395 / 1000At timestep:     439 / 1000At timestep:     481 / 1000At timestep:     517 / 1000At timestep:     553 / 1000At timestep:     578 / 1000At timestep:     604 / 1000At timestep:     628 / 1000At timestep:     649 / 1000At timestep:     675 / 1000At timestep:     699 / 1000At timestep:     729 / 1000At timestep:     777 / 1000At timestep:     805 / 1000At timestep:     846 / 1000At timestep:     931 / 1000At timestep:     962 / 1000At timestep:     992 / 1000At timestep:     1041 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -27.270000457763672
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1785.3962516784668
Exploration Critic Loss : 160014.609375
Exploitation Critic Loss : 0.09725800901651382
Exploration Model Loss : 26.796051025390625
Actor Loss : 0.5280653834342957
Eval_AverageReturn : -34.89655303955078
Eval_StdReturn : 12.658225059509277
Eval_MaxReturn : -20.0
Eval_MinReturn : -84.0
Eval_AverageEpLen : 35.89655172413793
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -26.139999
best mean reward -25.680000
running time 1830.509072
At timestep:     27 / 1000At timestep:     52 / 1000At timestep:     93 / 1000At timestep:     133 / 1000At timestep:     160 / 1000At timestep:     189 / 1000At timestep:     220 / 1000At timestep:     246 / 1000At timestep:     274 / 1000At timestep:     309 / 1000At timestep:     352 / 1000At timestep:     378 / 1000At timestep:     418 / 1000At timestep:     466 / 1000At timestep:     534 / 1000At timestep:     567 / 1000At timestep:     599 / 1000At timestep:     621 / 1000At timestep:     683 / 1000At timestep:     720 / 1000At timestep:     769 / 1000At timestep:     793 / 1000At timestep:     836 / 1000At timestep:     861 / 1000At timestep:     898 / 1000At timestep:     923 / 1000At timestep:     953 / 1000At timestep:     984 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -26.139999389648438
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1830.5090715885162
Exploration Critic Loss : 236002.296875
Exploitation Critic Loss : 0.08500956743955612
Exploration Model Loss : 26.795822143554688
Actor Loss : 0.4925186038017273
Eval_AverageReturn : -33.965518951416016
Eval_StdReturn : 10.930763244628906
Eval_MaxReturn : -21.0
Eval_MinReturn : -67.0
Eval_AverageEpLen : 34.96551724137931
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -26.150000
best mean reward -25.680000
running time 1875.999366
At timestep:     34 / 1000At timestep:     68 / 1000At timestep:     94 / 1000At timestep:     150 / 1000At timestep:     178 / 1000At timestep:     207 / 1000At timestep:     232 / 1000At timestep:     261 / 1000At timestep:     295 / 1000At timestep:     332 / 1000At timestep:     359 / 1000At timestep:     391 / 1000At timestep:     420 / 1000At timestep:     456 / 1000At timestep:     488 / 1000At timestep:     528 / 1000At timestep:     569 / 1000At timestep:     612 / 1000At timestep:     637 / 1000At timestep:     669 / 1000At timestep:     695 / 1000At timestep:     719 / 1000At timestep:     756 / 1000At timestep:     795 / 1000At timestep:     811 / 1000At timestep:     838 / 1000At timestep:     875 / 1000At timestep:     919 / 1000At timestep:     967 / 1000At timestep:     996 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -26.149999618530273
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1875.9993662834167
Exploration Critic Loss : 277579.1875
Exploitation Critic Loss : 0.08519504964351654
Exploration Model Loss : 26.795822143554688
Actor Loss : 0.49089986085891724
Eval_AverageReturn : -32.0
Eval_StdReturn : 7.991931438446045
Eval_MaxReturn : -15.0
Eval_MinReturn : -55.0
Eval_AverageEpLen : 33.0
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -26.389999
best mean reward -25.680000
running time 1922.663158
At timestep:     35 / 1000At timestep:     57 / 1000At timestep:     81 / 1000At timestep:     114 / 1000At timestep:     140 / 1000At timestep:     181 / 1000At timestep:     232 / 1000At timestep:     258 / 1000At timestep:     305 / 1000At timestep:     331 / 1000At timestep:     358 / 1000At timestep:     392 / 1000At timestep:     426 / 1000At timestep:     462 / 1000At timestep:     514 / 1000At timestep:     556 / 1000At timestep:     592 / 1000At timestep:     623 / 1000At timestep:     651 / 1000At timestep:     680 / 1000At timestep:     728 / 1000At timestep:     760 / 1000At timestep:     803 / 1000At timestep:     839 / 1000At timestep:     877 / 1000At timestep:     899 / 1000At timestep:     924 / 1000At timestep:     958 / 1000At timestep:     995 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -26.389999389648438
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1922.663157939911
Exploration Critic Loss : 202410.53125
Exploitation Critic Loss : 0.10674446821212769
Exploration Model Loss : 26.795791625976562
Actor Loss : 0.4596233069896698
Eval_AverageReturn : -33.099998474121094
Eval_StdReturn : 8.22739315032959
Eval_MaxReturn : -21.0
Eval_MinReturn : -51.0
Eval_AverageEpLen : 34.1
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -26.850000
best mean reward -25.680000
running time 1968.588749
At timestep:     33 / 1000At timestep:     57 / 1000At timestep:     91 / 1000At timestep:     126 / 1000At timestep:     165 / 1000At timestep:     211 / 1000At timestep:     236 / 1000At timestep:     261 / 1000At timestep:     292 / 1000At timestep:     323 / 1000At timestep:     361 / 1000At timestep:     386 / 1000At timestep:     423 / 1000At timestep:     460 / 1000At timestep:     492 / 1000At timestep:     527 / 1000At timestep:     556 / 1000At timestep:     589 / 1000At timestep:     626 / 1000At timestep:     688 / 1000At timestep:     728 / 1000At timestep:     755 / 1000At timestep:     793 / 1000At timestep:     831 / 1000At timestep:     867 / 1000At timestep:     894 / 1000At timestep:     914 / 1000At timestep:     939 / 1000At timestep:     968 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -26.850000381469727
Train_BestReturn : -25.68000030517578
TimeSinceStart : 1968.5887486934662
Exploration Critic Loss : 254994.40625
Exploitation Critic Loss : 0.10825575143098831
Exploration Model Loss : 26.795700073242188
Actor Loss : 0.4607802927494049
Eval_AverageReturn : -32.86666488647461
Eval_StdReturn : 8.281438827514648
Eval_MaxReturn : -19.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 33.86666666666667
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -27.590000
best mean reward -25.680000
running time 2013.322597
At timestep:     57 / 1000At timestep:     93 / 1000At timestep:     125 / 1000At timestep:     160 / 1000At timestep:     191 / 1000At timestep:     219 / 1000At timestep:     314 / 1000At timestep:     368 / 1000At timestep:     393 / 1000At timestep:     442 / 1000At timestep:     482 / 1000At timestep:     524 / 1000At timestep:     555 / 1000At timestep:     611 / 1000At timestep:     670 / 1000At timestep:     723 / 1000At timestep:     777 / 1000At timestep:     803 / 1000At timestep:     835 / 1000At timestep:     867 / 1000At timestep:     897 / 1000At timestep:     939 / 1000At timestep:     971 / 1000At timestep:     1014 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_medium_supervised_lam2_PointmassMedium-v0_22-11-2022_22-34-17/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -27.59000015258789
Train_BestReturn : -25.68000030517578
TimeSinceStart : 2013.322597026825
Exploration Critic Loss : 278727.46875
Exploitation Critic Loss : 0.13127781450748444
Exploration Model Loss : 26.795578002929688
Actor Loss : 0.5473428964614868
Eval_AverageReturn : -41.25
Eval_StdReturn : 15.265019416809082
Eval_MaxReturn : -24.0
Eval_MinReturn : -94.0
Eval_AverageEpLen : 42.25
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam10_PointmassMedium-v0_22-11-2022_23-08-43 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam10_PointmassMedium-v0_22-11-2022_23-08-43
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002438
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0024378299713134766
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -150.000000
best mean reward -inf
running time 9.656759
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     666 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -150.0
TimeSinceStart : 9.656759023666382
Eval_AverageReturn : -139.375
Eval_StdReturn : 28.111106872558594
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.5
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -146.538467
best mean reward -inf
running time 19.278815
At timestep:     54 / 1000At timestep:     204 / 1000At timestep:     354 / 1000At timestep:     504 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -146.53846740722656
TimeSinceStart : 19.278815031051636
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -147.750000
best mean reward -inf
running time 62.305004
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -147.75
TimeSinceStart : 62.30500388145447
Exploration Critic Loss : 1103228.0
Exploitation Critic Loss : 0.017837176099419594
Exploration Model Loss : 80.48444366455078
Actor Loss : 1.1699774265289307
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -148.269226
best mean reward -inf
running time 102.734609
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -148.26922607421875
TimeSinceStart : 102.73460936546326
Exploration Critic Loss : 8906906.0
Exploitation Critic Loss : 0.053395938128232956
Exploration Model Loss : 66.17023468017578
Actor Loss : 0.9286506175994873
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -148.636368
best mean reward -inf
running time 141.809766
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1039 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -148.63636779785156
TimeSinceStart : 141.80976605415344
Exploration Critic Loss : 5489849.0
Exploitation Critic Loss : 0.0001242784783244133
Exploration Model Loss : 43.407711029052734
Actor Loss : 0.7789067625999451
Eval_AverageReturn : -148.2857208251953
Eval_StdReturn : 4.199125289916992
Eval_MaxReturn : -138.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 148.42857142857142
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -148.350006
best mean reward -inf
running time 181.558621
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -148.35000610351562
TimeSinceStart : 181.5586211681366
Exploration Critic Loss : 7583733.0
Exploitation Critic Loss : 0.00020607755868695676
Exploration Model Loss : 54.96708297729492
Actor Loss : 0.6469718217849731
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -145.312500
best mean reward -inf
running time 219.234929
At timestep:     98 / 1000At timestep:     196 / 1000At timestep:     346 / 1000At timestep:     496 / 1000At timestep:     646 / 1000At timestep:     796 / 1000At timestep:     946 / 1000At timestep:     1096 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -145.3125
TimeSinceStart : 219.23492860794067
Exploration Critic Loss : 6128364.0
Exploitation Critic Loss : 0.07865028083324432
Exploration Model Loss : 35.406890869140625
Actor Loss : 0.5846289396286011
Eval_AverageReturn : -136.75
Eval_StdReturn : 22.94967269897461
Eval_MaxReturn : -97.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 137.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -141.535721
best mean reward -inf
running time 260.976171
At timestep:     87 / 1000At timestep:     237 / 1000At timestep:     387 / 1000At timestep:     537 / 1000At timestep:     651 / 1000At timestep:     801 / 1000At timestep:     951 / 1000At timestep:     1101 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -141.5357208251953
TimeSinceStart : 260.9761712551117
Exploration Critic Loss : 6593249.0
Exploitation Critic Loss : 0.004792934749275446
Exploration Model Loss : 65.10519409179688
Actor Loss : 0.5330647826194763
Eval_AverageReturn : -137.375
Eval_StdReturn : 22.885244369506836
Eval_MaxReturn : -86.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 137.625
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -140.234375
best mean reward -inf
running time 301.650150
At timestep:     79 / 1000At timestep:     229 / 1000At timestep:     379 / 1000At timestep:     529 / 1000At timestep:     679 / 1000At timestep:     829 / 1000At timestep:     979 / 1000At timestep:     1129 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -140.234375
TimeSinceStart : 301.65015029907227
Exploration Critic Loss : 3204556.0
Exploitation Critic Loss : 0.001737190643325448
Exploration Model Loss : 28.377288818359375
Actor Loss : 0.47277331352233887
Eval_AverageReturn : -141.0
Eval_StdReturn : 23.8117618560791
Eval_MaxReturn : -78.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 141.125
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -137.638885
best mean reward -inf
running time 345.683952
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -137.63888549804688
TimeSinceStart : 345.68395161628723
Exploration Critic Loss : 4369495.0
Exploitation Critic Loss : 0.05997656658291817
Exploration Model Loss : 36.16053771972656
Actor Loss : 0.44472455978393555
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -128.423523
best mean reward -inf
running time 392.976524
At timestep:     31 / 1000At timestep:     181 / 1000At timestep:     258 / 1000At timestep:     408 / 1000At timestep:     555 / 1000At timestep:     705 / 1000At timestep:     855 / 1000At timestep:     980 / 1000At timestep:     1129 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -128.42352294921875
TimeSinceStart : 392.9765238761902
Exploration Critic Loss : 6040761.5
Exploitation Critic Loss : 0.1454034447669983
Exploration Model Loss : 53.282684326171875
Actor Loss : 0.42735522985458374
Eval_AverageReturn : -124.88888549804688
Eval_StdReturn : 40.68427276611328
Eval_MaxReturn : -30.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 125.44444444444444
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -123.614586
best mean reward -inf
running time 438.364584
At timestep:     150 / 1000At timestep:     293 / 1000At timestep:     443 / 1000At timestep:     593 / 1000At timestep:     743 / 1000At timestep:     893 / 1000At timestep:     1043 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -123.61458587646484
TimeSinceStart : 438.36458444595337
Exploration Critic Loss : 2898219.5
Exploitation Critic Loss : 0.1033513993024826
Exploration Model Loss : 26.572311401367188
Actor Loss : 0.3909374475479126
Eval_AverageReturn : -148.85714721679688
Eval_StdReturn : 2.799417018890381
Eval_MaxReturn : -142.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 149.0
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -120.440002
best mean reward -120.440002
running time 483.316927
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     369 / 1000At timestep:     519 / 1000At timestep:     612 / 1000At timestep:     741 / 1000At timestep:     881 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -120.44000244140625
Train_BestReturn : -120.44000244140625
TimeSinceStart : 483.31692695617676
Exploration Critic Loss : 2849977.5
Exploitation Critic Loss : 0.2750636041164398
Exploration Model Loss : 35.342437744140625
Actor Loss : 0.37508052587509155
Eval_AverageReturn : -128.375
Eval_StdReturn : 29.478540420532227
Eval_MaxReturn : -68.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 128.875
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -108.080002
best mean reward -108.080002
running time 527.954287
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     714 / 1000At timestep:     810 / 1000At timestep:     943 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -108.08000183105469
Train_BestReturn : -108.08000183105469
TimeSinceStart : 527.9542872905731
Exploration Critic Loss : 3385723.0
Exploitation Critic Loss : 0.10852210968732834
Exploration Model Loss : 52.8531494140625
Actor Loss : 0.3931632936000824
Eval_AverageReturn : -130.75
Eval_StdReturn : 21.475276947021484
Eval_MaxReturn : -95.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 131.25
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -83.500000
best mean reward -83.500000
running time 574.649104
At timestep:     80 / 1000At timestep:     230 / 1000At timestep:     350 / 1000At timestep:     414 / 1000At timestep:     539 / 1000At timestep:     689 / 1000At timestep:     809 / 1000At timestep:     959 / 1000At timestep:     1082 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -83.5
Train_BestReturn : -83.5
TimeSinceStart : 574.6491038799286
Exploration Critic Loss : 1488595.5
Exploitation Critic Loss : 0.04117850959300995
Exploration Model Loss : 26.491256713867188
Actor Loss : 0.36835870146751404
Eval_AverageReturn : -119.55555725097656
Eval_StdReturn : 29.155181884765625
Eval_MaxReturn : -63.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 120.22222222222223
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -63.650002
best mean reward -63.650002
running time 620.966417
At timestep:     64 / 1000At timestep:     180 / 1000At timestep:     330 / 1000At timestep:     480 / 1000At timestep:     630 / 1000At timestep:     780 / 1000At timestep:     841 / 1000At timestep:     900 / 1000At timestep:     978 / 1000At timestep:     1128 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -63.650001525878906
Train_BestReturn : -63.650001525878906
TimeSinceStart : 620.9664170742035
Exploration Critic Loss : 1089389.5
Exploitation Critic Loss : 0.0900864452123642
Exploration Model Loss : 35.30183410644531
Actor Loss : 0.40031805634498596
Eval_AverageReturn : -112.30000305175781
Eval_StdReturn : 40.588294982910156
Eval_MaxReturn : -58.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 112.8
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -52.270000
best mean reward -52.270000
running time 669.385061
At timestep:     132 / 1000At timestep:     219 / 1000At timestep:     369 / 1000At timestep:     455 / 1000At timestep:     571 / 1000At timestep:     604 / 1000At timestep:     657 / 1000At timestep:     748 / 1000At timestep:     896 / 1000At timestep:     1046 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -52.27000045776367
Train_BestReturn : -52.27000045776367
TimeSinceStart : 669.3850612640381
Exploration Critic Loss : 1143983.75
Exploitation Critic Loss : 0.09361100196838379
Exploration Model Loss : 52.833343505859375
Actor Loss : 0.38784635066986084
Eval_AverageReturn : -103.80000305175781
Eval_StdReturn : 39.622718811035156
Eval_MaxReturn : -32.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 104.6
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -43.630001
best mean reward -43.630001
running time 715.945610
At timestep:     60 / 1000At timestep:     144 / 1000At timestep:     228 / 1000At timestep:     369 / 1000At timestep:     447 / 1000At timestep:     544 / 1000At timestep:     603 / 1000At timestep:     753 / 1000At timestep:     792 / 1000At timestep:     863 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -43.630001068115234
Train_BestReturn : -43.630001068115234
TimeSinceStart : 715.9456102848053
Exploration Critic Loss : 601436.125
Exploitation Critic Loss : 0.2243138551712036
Exploration Model Loss : 26.486557006835938
Actor Loss : 0.38841742277145386
Eval_AverageReturn : -91.2727279663086
Eval_StdReturn : 37.018646240234375
Eval_MaxReturn : -38.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 92.0909090909091
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -41.610001
best mean reward -41.610001
running time 763.425448
At timestep:     47 / 1000At timestep:     186 / 1000At timestep:     336 / 1000At timestep:     427 / 1000At timestep:     466 / 1000At timestep:     542 / 1000At timestep:     616 / 1000At timestep:     675 / 1000At timestep:     819 / 1000At timestep:     903 / 1000At timestep:     972 / 1000At timestep:     1069 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -41.61000061035156
Train_BestReturn : -41.61000061035156
TimeSinceStart : 763.4254479408264
Exploration Critic Loss : 772598.75
Exploitation Critic Loss : 0.13305281102657318
Exploration Model Loss : 35.29997253417969
Actor Loss : 0.4424300491809845
Eval_AverageReturn : -88.16666412353516
Eval_StdReturn : 35.84650802612305
Eval_MaxReturn : -38.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 89.08333333333333
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -40.930000
best mean reward -40.930000
running time 810.314935
At timestep:     140 / 1000At timestep:     216 / 1000At timestep:     239 / 1000At timestep:     311 / 1000At timestep:     461 / 1000At timestep:     554 / 1000At timestep:     672 / 1000At timestep:     772 / 1000At timestep:     922 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -40.93000030517578
Train_BestReturn : -40.93000030517578
TimeSinceStart : 810.3149349689484
Exploration Critic Loss : 616080.3125
Exploitation Critic Loss : 0.23753319680690765
Exploration Model Loss : 52.8331298828125
Actor Loss : 0.3911978006362915
Eval_AverageReturn : -102.0
Eval_StdReturn : 37.960506439208984
Eval_MaxReturn : -22.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 102.8
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -39.270000
best mean reward -39.270000
running time 856.993102
At timestep:     56 / 1000At timestep:     127 / 1000At timestep:     261 / 1000At timestep:     334 / 1000At timestep:     425 / 1000At timestep:     491 / 1000At timestep:     583 / 1000At timestep:     682 / 1000At timestep:     751 / 1000At timestep:     804 / 1000At timestep:     839 / 1000At timestep:     957 / 1000At timestep:     993 / 1000At timestep:     1099 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -39.27000045776367
Train_BestReturn : -39.27000045776367
TimeSinceStart : 856.993102312088
Exploration Critic Loss : 794026.625
Exploitation Critic Loss : 0.2677958607673645
Exploration Model Loss : 26.486862182617188
Actor Loss : 0.513954758644104
Eval_AverageReturn : -77.5
Eval_StdReturn : 28.46489143371582
Eval_MaxReturn : -34.0
Eval_MinReturn : -133.0
Eval_AverageEpLen : 78.5
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -38.980000
best mean reward -38.980000
running time 906.434114
At timestep:     61 / 1000At timestep:     189 / 1000At timestep:     238 / 1000At timestep:     299 / 1000At timestep:     349 / 1000At timestep:     437 / 1000At timestep:     494 / 1000At timestep:     574 / 1000At timestep:     643 / 1000At timestep:     699 / 1000At timestep:     765 / 1000At timestep:     803 / 1000At timestep:     866 / 1000At timestep:     948 / 1000At timestep:     992 / 1000At timestep:     1142 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -38.97999954223633
Train_BestReturn : -38.97999954223633
TimeSinceStart : 906.4341142177582
Exploration Critic Loss : 590877.3125
Exploitation Critic Loss : 0.11866097152233124
Exploration Model Loss : 35.30027770996094
Actor Loss : 0.45669424533843994
Eval_AverageReturn : -70.4375
Eval_StdReturn : 29.14826011657715
Eval_MaxReturn : -37.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 71.375
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -39.369999
best mean reward -38.980000
running time 954.677919
At timestep:     47 / 1000At timestep:     147 / 1000At timestep:     257 / 1000At timestep:     317 / 1000At timestep:     350 / 1000At timestep:     393 / 1000At timestep:     432 / 1000At timestep:     522 / 1000At timestep:     663 / 1000At timestep:     725 / 1000At timestep:     817 / 1000At timestep:     868 / 1000At timestep:     985 / 1000At timestep:     1033 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -39.369998931884766
Train_BestReturn : -38.97999954223633
TimeSinceStart : 954.6779193878174
Exploration Critic Loss : 619602.75
Exploitation Critic Loss : 0.15966159105300903
Exploration Model Loss : 52.83282470703125
Actor Loss : 0.47929880023002625
Eval_AverageReturn : -72.78571319580078
Eval_StdReturn : 32.76578903198242
Eval_MaxReturn : -32.0
Eval_MinReturn : -140.0
Eval_AverageEpLen : 73.78571428571429
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -37.279999
best mean reward -37.279999
running time 1002.648920
At timestep:     90 / 1000At timestep:     182 / 1000At timestep:     224 / 1000At timestep:     288 / 1000At timestep:     421 / 1000At timestep:     571 / 1000At timestep:     633 / 1000At timestep:     719 / 1000At timestep:     825 / 1000At timestep:     874 / 1000At timestep:     948 / 1000At timestep:     983 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -37.279998779296875
Train_BestReturn : -37.279998779296875
TimeSinceStart : 1002.6489200592041
Exploration Critic Loss : 443735.75
Exploitation Critic Loss : 0.11127375811338425
Exploration Model Loss : 26.487152099609375
Actor Loss : 0.4458886981010437
Eval_AverageReturn : -79.84615325927734
Eval_StdReturn : 32.83020782470703
Eval_MaxReturn : -34.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 80.76923076923077
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -36.020000
best mean reward -36.020000
running time 1051.631845
At timestep:     30 / 1000At timestep:     141 / 1000At timestep:     193 / 1000At timestep:     246 / 1000At timestep:     298 / 1000At timestep:     338 / 1000At timestep:     406 / 1000At timestep:     452 / 1000At timestep:     537 / 1000At timestep:     639 / 1000At timestep:     689 / 1000At timestep:     748 / 1000At timestep:     793 / 1000At timestep:     855 / 1000At timestep:     894 / 1000At timestep:     980 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -36.02000045776367
Train_BestReturn : -36.02000045776367
TimeSinceStart : 1051.6318454742432
Exploration Critic Loss : 404455.6875
Exploitation Critic Loss : 0.13276918232440948
Exploration Model Loss : 35.30055236816406
Actor Loss : 0.467290461063385
Eval_AverageReturn : -59.411766052246094
Eval_StdReturn : 22.149381637573242
Eval_MaxReturn : -29.0
Eval_MinReturn : -110.0
Eval_AverageEpLen : 60.411764705882355
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -33.430000
best mean reward -33.430000
running time 1101.704758
At timestep:     63 / 1000At timestep:     153 / 1000At timestep:     203 / 1000At timestep:     277 / 1000At timestep:     328 / 1000At timestep:     373 / 1000At timestep:     449 / 1000At timestep:     486 / 1000At timestep:     581 / 1000At timestep:     636 / 1000At timestep:     677 / 1000At timestep:     746 / 1000At timestep:     795 / 1000At timestep:     879 / 1000At timestep:     933 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -33.43000030517578
Train_BestReturn : -33.43000030517578
TimeSinceStart : 1101.7047579288483
Exploration Critic Loss : 412872.75
Exploitation Critic Loss : 0.12183630466461182
Exploration Model Loss : 52.83251953125
Actor Loss : 0.4643624424934387
Eval_AverageReturn : -61.625
Eval_StdReturn : 17.124088287353516
Eval_MaxReturn : -36.0
Eval_MinReturn : -94.0
Eval_AverageEpLen : 62.625
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -32.220001
best mean reward -32.220001
running time 1150.373468
At timestep:     82 / 1000At timestep:     141 / 1000At timestep:     178 / 1000At timestep:     271 / 1000At timestep:     362 / 1000At timestep:     412 / 1000At timestep:     497 / 1000At timestep:     557 / 1000At timestep:     696 / 1000At timestep:     736 / 1000At timestep:     804 / 1000At timestep:     886 / 1000At timestep:     950 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -32.220001220703125
Train_BestReturn : -32.220001220703125
TimeSinceStart : 1150.3734681606293
Exploration Critic Loss : 387928.8125
Exploitation Critic Loss : 0.14140623807907104
Exploration Model Loss : 26.487457275390625
Actor Loss : 0.5235780477523804
Eval_AverageReturn : -74.0
Eval_StdReturn : 25.97251319885254
Eval_MaxReturn : -36.0
Eval_MinReturn : -138.0
Eval_AverageEpLen : 75.0
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -29.559999
best mean reward -29.559999
running time 1199.566180
At timestep:     87 / 1000At timestep:     133 / 1000At timestep:     195 / 1000At timestep:     238 / 1000At timestep:     375 / 1000At timestep:     434 / 1000At timestep:     496 / 1000At timestep:     529 / 1000At timestep:     617 / 1000At timestep:     658 / 1000At timestep:     734 / 1000At timestep:     830 / 1000At timestep:     880 / 1000At timestep:     926 / 1000At timestep:     954 / 1000At timestep:     985 / 1000At timestep:     1048 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -29.559999465942383
Train_BestReturn : -29.559999465942383
TimeSinceStart : 1199.5661804676056
Exploration Critic Loss : 353718.0625
Exploitation Critic Loss : 0.0989052951335907
Exploration Model Loss : 35.30085754394531
Actor Loss : 0.5175380706787109
Eval_AverageReturn : -60.64706039428711
Eval_StdReturn : 27.334400177001953
Eval_MaxReturn : -27.0
Eval_MinReturn : -136.0
Eval_AverageEpLen : 61.64705882352941
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -27.680000
best mean reward -27.680000
running time 1250.897321
At timestep:     92 / 1000At timestep:     117 / 1000At timestep:     182 / 1000At timestep:     239 / 1000At timestep:     264 / 1000At timestep:     328 / 1000At timestep:     397 / 1000At timestep:     446 / 1000At timestep:     483 / 1000At timestep:     524 / 1000At timestep:     621 / 1000At timestep:     734 / 1000At timestep:     771 / 1000At timestep:     818 / 1000At timestep:     862 / 1000At timestep:     911 / 1000At timestep:     943 / 1000At timestep:     991 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -27.68000030517578
Train_BestReturn : -27.68000030517578
TimeSinceStart : 1250.897320985794
Exploration Critic Loss : 358682.25
Exploitation Critic Loss : 0.13396356999874115
Exploration Model Loss : 52.83221435546875
Actor Loss : 0.49202051758766174
Eval_AverageReturn : -52.894737243652344
Eval_StdReturn : 23.861209869384766
Eval_MaxReturn : -24.0
Eval_MinReturn : -112.0
Eval_AverageEpLen : 53.89473684210526
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -27.280001
best mean reward -27.280001
running time 1299.901590
At timestep:     26 / 1000At timestep:     61 / 1000At timestep:     99 / 1000At timestep:     183 / 1000At timestep:     225 / 1000At timestep:     274 / 1000At timestep:     310 / 1000At timestep:     378 / 1000At timestep:     429 / 1000At timestep:     524 / 1000At timestep:     575 / 1000At timestep:     630 / 1000At timestep:     692 / 1000At timestep:     749 / 1000At timestep:     778 / 1000At timestep:     817 / 1000At timestep:     861 / 1000At timestep:     922 / 1000At timestep:     1055 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -27.280000686645508
Train_BestReturn : -27.280000686645508
TimeSinceStart : 1299.9015901088715
Exploration Critic Loss : 286337.53125
Exploitation Critic Loss : 0.07591824233531952
Exploration Model Loss : 26.487747192382812
Actor Loss : 0.5144729614257812
Eval_AverageReturn : -54.52631759643555
Eval_StdReturn : 25.087987899780273
Eval_MaxReturn : -25.0
Eval_MinReturn : -132.0
Eval_AverageEpLen : 55.526315789473685
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -27.219999
best mean reward -27.219999
running time 1349.069304
At timestep:     44 / 1000At timestep:     105 / 1000At timestep:     157 / 1000At timestep:     219 / 1000At timestep:     275 / 1000At timestep:     316 / 1000At timestep:     374 / 1000At timestep:     405 / 1000At timestep:     463 / 1000At timestep:     511 / 1000At timestep:     552 / 1000At timestep:     589 / 1000At timestep:     634 / 1000At timestep:     673 / 1000At timestep:     704 / 1000At timestep:     740 / 1000At timestep:     771 / 1000At timestep:     811 / 1000At timestep:     867 / 1000At timestep:     948 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -27.219999313354492
Train_BestReturn : -27.219999313354492
TimeSinceStart : 1349.0693037509918
Exploration Critic Loss : 198310.109375
Exploitation Critic Loss : 0.06909100711345673
Exploration Model Loss : 35.30091857910156
Actor Loss : 0.5253959894180298
Eval_AverageReturn : -46.71428680419922
Eval_StdReturn : 12.406384468078613
Eval_MaxReturn : -30.0
Eval_MinReturn : -80.0
Eval_AverageEpLen : 47.714285714285715
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -27.660000
best mean reward -27.219999
running time 1399.340876
At timestep:     58 / 1000At timestep:     128 / 1000At timestep:     172 / 1000At timestep:     216 / 1000At timestep:     275 / 1000At timestep:     338 / 1000At timestep:     382 / 1000At timestep:     448 / 1000At timestep:     478 / 1000At timestep:     585 / 1000At timestep:     615 / 1000At timestep:     702 / 1000At timestep:     727 / 1000At timestep:     761 / 1000At timestep:     843 / 1000At timestep:     926 / 1000At timestep:     975 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -27.65999984741211
Train_BestReturn : -27.219999313354492
TimeSinceStart : 1399.3408756256104
Exploration Critic Loss : 263047.15625
Exploitation Critic Loss : 0.09006600826978683
Exploration Model Loss : 52.832183837890625
Actor Loss : 0.4821467995643616
Eval_AverageReturn : -55.27777862548828
Eval_StdReturn : 22.268077850341797
Eval_MaxReturn : -24.0
Eval_MinReturn : -106.0
Eval_AverageEpLen : 56.27777777777778
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -27.030001
best mean reward -27.030001
running time 1448.680881
At timestep:     39 / 1000At timestep:     96 / 1000At timestep:     144 / 1000At timestep:     208 / 1000At timestep:     244 / 1000At timestep:     343 / 1000At timestep:     398 / 1000At timestep:     437 / 1000At timestep:     486 / 1000At timestep:     536 / 1000At timestep:     579 / 1000At timestep:     634 / 1000At timestep:     694 / 1000At timestep:     744 / 1000At timestep:     807 / 1000At timestep:     861 / 1000At timestep:     891 / 1000At timestep:     950 / 1000At timestep:     979 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -27.030000686645508
Train_BestReturn : -27.030000686645508
TimeSinceStart : 1448.6808805465698
Exploration Critic Loss : 241163.3125
Exploitation Critic Loss : 0.14874807000160217
Exploration Model Loss : 26.48779296875
Actor Loss : 0.4983750879764557
Eval_AverageReturn : -49.599998474121094
Eval_StdReturn : 15.291828155517578
Eval_MaxReturn : -28.0
Eval_MinReturn : -98.0
Eval_AverageEpLen : 50.6
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -27.459999
best mean reward -27.030001
running time 1498.018739
At timestep:     32 / 1000At timestep:     95 / 1000At timestep:     130 / 1000At timestep:     219 / 1000At timestep:     261 / 1000At timestep:     316 / 1000At timestep:     354 / 1000At timestep:     388 / 1000At timestep:     432 / 1000At timestep:     480 / 1000At timestep:     512 / 1000At timestep:     543 / 1000At timestep:     634 / 1000At timestep:     767 / 1000At timestep:     798 / 1000At timestep:     838 / 1000At timestep:     877 / 1000At timestep:     957 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -27.459999084472656
Train_BestReturn : -27.030000686645508
TimeSinceStart : 1498.0187389850616
Exploration Critic Loss : 218867.84375
Exploitation Critic Loss : 0.0926039069890976
Exploration Model Loss : 35.301361083984375
Actor Loss : 0.5059939622879028
Eval_AverageReturn : -51.68421173095703
Eval_StdReturn : 26.56085968017578
Eval_MaxReturn : -30.0
Eval_MinReturn : -132.0
Eval_AverageEpLen : 52.68421052631579
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -27.280001
best mean reward -27.030001
running time 1548.561066
At timestep:     105 / 1000At timestep:     155 / 1000At timestep:     186 / 1000At timestep:     232 / 1000At timestep:     264 / 1000At timestep:     301 / 1000At timestep:     336 / 1000At timestep:     367 / 1000At timestep:     401 / 1000At timestep:     481 / 1000At timestep:     565 / 1000At timestep:     605 / 1000At timestep:     648 / 1000At timestep:     683 / 1000At timestep:     744 / 1000At timestep:     775 / 1000At timestep:     849 / 1000At timestep:     894 / 1000At timestep:     930 / 1000At timestep:     981 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -27.280000686645508
Train_BestReturn : -27.030000686645508
TimeSinceStart : 1548.5610659122467
Exploration Critic Loss : 173993.1875
Exploitation Critic Loss : 0.12125129997730255
Exploration Model Loss : 52.83172607421875
Actor Loss : 0.536239504814148
Eval_AverageReturn : -46.904762268066406
Eval_StdReturn : 20.67873191833496
Eval_MaxReturn : -24.0
Eval_MinReturn : -104.0
Eval_AverageEpLen : 47.904761904761905
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -27.570000
best mean reward -27.030001
running time 1599.334529
At timestep:     64 / 1000At timestep:     92 / 1000At timestep:     152 / 1000At timestep:     180 / 1000At timestep:     224 / 1000At timestep:     317 / 1000At timestep:     353 / 1000At timestep:     402 / 1000At timestep:     434 / 1000At timestep:     490 / 1000At timestep:     532 / 1000At timestep:     617 / 1000At timestep:     680 / 1000At timestep:     712 / 1000At timestep:     766 / 1000At timestep:     814 / 1000At timestep:     863 / 1000At timestep:     899 / 1000At timestep:     953 / 1000At timestep:     987 / 1000At timestep:     1035 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -27.56999969482422
Train_BestReturn : -27.030000686645508
TimeSinceStart : 1599.3345293998718
Exploration Critic Loss : 154324.40625
Exploitation Critic Loss : 0.1084011122584343
Exploration Model Loss : 26.488250732421875
Actor Loss : 0.5346876978874207
Eval_AverageReturn : -48.28571319580078
Eval_StdReturn : 16.861177444458008
Eval_MaxReturn : -27.0
Eval_MinReturn : -92.0
Eval_AverageEpLen : 49.285714285714285
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -26.830000
best mean reward -26.830000
running time 1649.158197
At timestep:     46 / 1000At timestep:     114 / 1000At timestep:     143 / 1000At timestep:     183 / 1000At timestep:     235 / 1000At timestep:     312 / 1000At timestep:     387 / 1000At timestep:     437 / 1000At timestep:     537 / 1000At timestep:     563 / 1000At timestep:     617 / 1000At timestep:     664 / 1000At timestep:     733 / 1000At timestep:     775 / 1000At timestep:     818 / 1000At timestep:     857 / 1000At timestep:     912 / 1000At timestep:     949 / 1000At timestep:     985 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -26.829999923706055
Train_BestReturn : -26.829999923706055
TimeSinceStart : 1649.158197402954
Exploration Critic Loss : 157018.625
Exploitation Critic Loss : 0.13978444039821625
Exploration Model Loss : 35.30168151855469
Actor Loss : 0.5027847290039062
Eval_AverageReturn : -49.54999923706055
Eval_StdReturn : 18.567378997802734
Eval_MaxReturn : -25.0
Eval_MinReturn : -99.0
Eval_AverageEpLen : 50.55
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -27.070000
best mean reward -26.830000
running time 1699.225173
At timestep:     80 / 1000At timestep:     107 / 1000At timestep:     141 / 1000At timestep:     185 / 1000At timestep:     219 / 1000At timestep:     267 / 1000At timestep:     304 / 1000At timestep:     339 / 1000At timestep:     365 / 1000At timestep:     396 / 1000At timestep:     468 / 1000At timestep:     515 / 1000At timestep:     553 / 1000At timestep:     595 / 1000At timestep:     652 / 1000At timestep:     696 / 1000At timestep:     734 / 1000At timestep:     761 / 1000At timestep:     807 / 1000At timestep:     846 / 1000At timestep:     918 / 1000At timestep:     951 / 1000At timestep:     989 / 1000At timestep:     1035 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -27.06999969482422
Train_BestReturn : -26.829999923706055
TimeSinceStart : 1699.2251727581024
Exploration Critic Loss : 116886.546875
Exploitation Critic Loss : 0.10140307247638702
Exploration Model Loss : 52.8314208984375
Actor Loss : 0.47662824392318726
Eval_AverageReturn : -42.125
Eval_StdReturn : 14.003905296325684
Eval_MaxReturn : -25.0
Eval_MinReturn : -79.0
Eval_AverageEpLen : 43.125
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -27.129999
best mean reward -26.830000
running time 1751.013347
At timestep:     80 / 1000At timestep:     106 / 1000At timestep:     143 / 1000At timestep:     181 / 1000At timestep:     236 / 1000At timestep:     299 / 1000At timestep:     365 / 1000At timestep:     416 / 1000At timestep:     466 / 1000At timestep:     514 / 1000At timestep:     558 / 1000At timestep:     607 / 1000At timestep:     672 / 1000At timestep:     727 / 1000At timestep:     765 / 1000At timestep:     807 / 1000At timestep:     849 / 1000At timestep:     890 / 1000At timestep:     950 / 1000At timestep:     983 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -27.1299991607666
Train_BestReturn : -26.829999923706055
TimeSinceStart : 1751.0133471488953
Exploration Critic Loss : 131041.9375
Exploitation Critic Loss : 0.12834472954273224
Exploration Model Loss : 26.488540649414062
Actor Loss : 0.5472686290740967
Eval_AverageReturn : -47.9523811340332
Eval_StdReturn : 12.487998962402344
Eval_MaxReturn : -25.0
Eval_MinReturn : -79.0
Eval_AverageEpLen : 48.95238095238095
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -26.590000
best mean reward -26.590000
running time 1800.533944
At timestep:     51 / 1000At timestep:     88 / 1000At timestep:     120 / 1000At timestep:     165 / 1000At timestep:     195 / 1000At timestep:     240 / 1000At timestep:     297 / 1000At timestep:     341 / 1000At timestep:     388 / 1000At timestep:     425 / 1000At timestep:     490 / 1000At timestep:     510 / 1000At timestep:     553 / 1000At timestep:     587 / 1000At timestep:     618 / 1000At timestep:     653 / 1000At timestep:     695 / 1000At timestep:     724 / 1000At timestep:     752 / 1000At timestep:     835 / 1000At timestep:     884 / 1000At timestep:     936 / 1000At timestep:     972 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -26.59000015258789
Train_BestReturn : -26.59000015258789
TimeSinceStart : 1800.5339443683624
Exploration Critic Loss : 100438.78125
Exploitation Critic Loss : 0.15764814615249634
Exploration Model Loss : 35.301971435546875
Actor Loss : 0.4866616129875183
Eval_AverageReturn : -42.25
Eval_StdReturn : 13.941096305847168
Eval_MaxReturn : -19.0
Eval_MinReturn : -82.0
Eval_AverageEpLen : 43.25
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -26.350000
best mean reward -26.350000
running time 1849.902733
At timestep:     53 / 1000At timestep:     90 / 1000At timestep:     125 / 1000At timestep:     162 / 1000At timestep:     226 / 1000At timestep:     253 / 1000At timestep:     298 / 1000At timestep:     379 / 1000At timestep:     413 / 1000At timestep:     459 / 1000At timestep:     508 / 1000At timestep:     547 / 1000At timestep:     569 / 1000At timestep:     604 / 1000At timestep:     641 / 1000At timestep:     664 / 1000At timestep:     702 / 1000At timestep:     726 / 1000At timestep:     756 / 1000At timestep:     821 / 1000At timestep:     887 / 1000At timestep:     917 / 1000At timestep:     945 / 1000At timestep:     983 / 1000At timestep:     1048 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -26.350000381469727
Train_BestReturn : -26.350000381469727
TimeSinceStart : 1849.9027330875397
Exploration Critic Loss : 66938.9765625
Exploitation Critic Loss : 0.11176600307226181
Exploration Model Loss : 52.83111572265625
Actor Loss : 0.5097113847732544
Eval_AverageReturn : -40.91999816894531
Eval_StdReturn : 15.369892120361328
Eval_MaxReturn : -21.0
Eval_MinReturn : -80.0
Eval_AverageEpLen : 41.92
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -27.209999
best mean reward -26.350000
running time 1901.621655
At timestep:     38 / 1000At timestep:     76 / 1000At timestep:     117 / 1000At timestep:     174 / 1000At timestep:     213 / 1000At timestep:     249 / 1000At timestep:     289 / 1000At timestep:     337 / 1000At timestep:     370 / 1000At timestep:     419 / 1000At timestep:     451 / 1000At timestep:     486 / 1000At timestep:     532 / 1000At timestep:     557 / 1000At timestep:     620 / 1000At timestep:     667 / 1000At timestep:     717 / 1000At timestep:     749 / 1000At timestep:     770 / 1000At timestep:     831 / 1000At timestep:     875 / 1000At timestep:     910 / 1000At timestep:     957 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -27.209999084472656
Train_BestReturn : -26.350000381469727
TimeSinceStart : 1901.6216547489166
Exploration Critic Loss : 43554.74609375
Exploitation Critic Loss : 0.07093429565429688
Exploration Model Loss : 26.488845825195312
Actor Loss : 0.48328834772109985
Eval_AverageReturn : -41.5
Eval_StdReturn : 10.939226150512695
Eval_MaxReturn : -20.0
Eval_MinReturn : -62.0
Eval_AverageEpLen : 42.5
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -26.570000
best mean reward -26.350000
running time 1952.704839
At timestep:     29 / 1000At timestep:     69 / 1000At timestep:     140 / 1000At timestep:     206 / 1000At timestep:     267 / 1000At timestep:     322 / 1000At timestep:     381 / 1000At timestep:     417 / 1000At timestep:     465 / 1000At timestep:     499 / 1000At timestep:     547 / 1000At timestep:     573 / 1000At timestep:     619 / 1000At timestep:     663 / 1000At timestep:     689 / 1000At timestep:     724 / 1000At timestep:     759 / 1000At timestep:     816 / 1000At timestep:     864 / 1000At timestep:     920 / 1000At timestep:     945 / 1000At timestep:     978 / 1000At timestep:     1040 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -26.56999969482422
Train_BestReturn : -26.350000381469727
TimeSinceStart : 1952.7048394680023
Exploration Critic Loss : 71903.609375
Exploitation Critic Loss : 0.0943792387843132
Exploration Model Loss : 35.3018798828125
Actor Loss : 0.46288245916366577
Eval_AverageReturn : -44.21739196777344
Eval_StdReturn : 13.513138771057129
Eval_MaxReturn : -24.0
Eval_MinReturn : -70.0
Eval_AverageEpLen : 45.21739130434783
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -26.730000
best mean reward -26.350000
running time 2002.335711
At timestep:     70 / 1000At timestep:     102 / 1000At timestep:     133 / 1000At timestep:     169 / 1000At timestep:     204 / 1000At timestep:     260 / 1000At timestep:     299 / 1000At timestep:     334 / 1000At timestep:     372 / 1000At timestep:     405 / 1000At timestep:     441 / 1000At timestep:     527 / 1000At timestep:     568 / 1000At timestep:     598 / 1000At timestep:     633 / 1000At timestep:     680 / 1000At timestep:     726 / 1000At timestep:     772 / 1000At timestep:     797 / 1000At timestep:     881 / 1000At timestep:     920 / 1000At timestep:     964 / 1000At timestep:     997 / 1000At timestep:     1030 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -26.729999542236328
Train_BestReturn : -26.350000381469727
TimeSinceStart : 2002.335711479187
Exploration Critic Loss : 58836.921875
Exploitation Critic Loss : 0.08226180821657181
Exploration Model Loss : 52.831390380859375
Actor Loss : 0.5134809017181396
Eval_AverageReturn : -41.91666793823242
Eval_StdReturn : 15.641602516174316
Eval_MaxReturn : -24.0
Eval_MinReturn : -85.0
Eval_AverageEpLen : 42.916666666666664
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -26.420000
best mean reward -26.350000
running time 2055.951000
At timestep:     41 / 1000At timestep:     82 / 1000At timestep:     135 / 1000At timestep:     163 / 1000At timestep:     199 / 1000At timestep:     236 / 1000At timestep:     266 / 1000At timestep:     290 / 1000At timestep:     330 / 1000At timestep:     357 / 1000At timestep:     386 / 1000At timestep:     421 / 1000At timestep:     472 / 1000At timestep:     511 / 1000At timestep:     564 / 1000At timestep:     593 / 1000At timestep:     636 / 1000At timestep:     662 / 1000At timestep:     705 / 1000At timestep:     761 / 1000At timestep:     786 / 1000At timestep:     806 / 1000At timestep:     846 / 1000At timestep:     881 / 1000At timestep:     942 / 1000At timestep:     976 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -26.420000076293945
Train_BestReturn : -26.350000381469727
TimeSinceStart : 2055.9510004520416
Exploration Critic Loss : 40259.59375
Exploitation Critic Loss : 0.0811055451631546
Exploration Model Loss : 26.488571166992188
Actor Loss : 0.48766884207725525
Eval_AverageReturn : -37.44444274902344
Eval_StdReturn : 11.28530216217041
Eval_MaxReturn : -19.0
Eval_MinReturn : -61.0
Eval_AverageEpLen : 38.44444444444444
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -27.200001
best mean reward -26.350000
running time 2108.392114
At timestep:     28 / 1000At timestep:     75 / 1000At timestep:     109 / 1000At timestep:     144 / 1000At timestep:     174 / 1000At timestep:     206 / 1000At timestep:     235 / 1000At timestep:     273 / 1000At timestep:     303 / 1000At timestep:     324 / 1000At timestep:     351 / 1000At timestep:     396 / 1000At timestep:     446 / 1000At timestep:     491 / 1000At timestep:     524 / 1000At timestep:     551 / 1000At timestep:     587 / 1000At timestep:     617 / 1000At timestep:     676 / 1000At timestep:     751 / 1000At timestep:     790 / 1000At timestep:     828 / 1000At timestep:     878 / 1000At timestep:     917 / 1000At timestep:     951 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -27.200000762939453
Train_BestReturn : -26.350000381469727
TimeSinceStart : 2108.392114162445
Exploration Critic Loss : 49806.5546875
Exploitation Critic Loss : 0.09933318197727203
Exploration Model Loss : 35.30198669433594
Actor Loss : 0.5361125469207764
Eval_AverageReturn : -37.53845977783203
Eval_StdReturn : 11.51998519897461
Eval_MaxReturn : -20.0
Eval_MinReturn : -74.0
Eval_AverageEpLen : 38.53846153846154
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -27.209999
best mean reward -26.350000
running time 2158.043134
At timestep:     42 / 1000At timestep:     73 / 1000At timestep:     119 / 1000At timestep:     143 / 1000At timestep:     196 / 1000At timestep:     250 / 1000At timestep:     280 / 1000At timestep:     329 / 1000At timestep:     352 / 1000At timestep:     386 / 1000At timestep:     434 / 1000At timestep:     472 / 1000At timestep:     500 / 1000At timestep:     528 / 1000At timestep:     556 / 1000At timestep:     587 / 1000At timestep:     640 / 1000At timestep:     687 / 1000At timestep:     714 / 1000At timestep:     773 / 1000At timestep:     816 / 1000At timestep:     843 / 1000At timestep:     872 / 1000At timestep:     892 / 1000At timestep:     919 / 1000At timestep:     945 / 1000At timestep:     984 / 1000At timestep:     1049 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -27.209999084472656
Train_BestReturn : -26.350000381469727
TimeSinceStart : 2158.0431339740753
Exploration Critic Loss : 64344.08203125
Exploitation Critic Loss : 0.13960790634155273
Exploration Model Loss : 52.83111572265625
Actor Loss : 0.4627658724784851
Eval_AverageReturn : -36.46428680419922
Eval_StdReturn : 12.081515312194824
Eval_MaxReturn : -19.0
Eval_MinReturn : -64.0
Eval_AverageEpLen : 37.464285714285715
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -27.490000
best mean reward -26.350000
running time 2213.575355
At timestep:     34 / 1000At timestep:     70 / 1000At timestep:     93 / 1000At timestep:     114 / 1000At timestep:     159 / 1000At timestep:     219 / 1000At timestep:     261 / 1000At timestep:     309 / 1000At timestep:     336 / 1000At timestep:     403 / 1000At timestep:     451 / 1000At timestep:     502 / 1000At timestep:     543 / 1000At timestep:     587 / 1000At timestep:     618 / 1000At timestep:     647 / 1000At timestep:     676 / 1000At timestep:     731 / 1000At timestep:     780 / 1000At timestep:     822 / 1000At timestep:     888 / 1000At timestep:     926 / 1000At timestep:     974 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -27.489999771118164
Train_BestReturn : -26.350000381469727
TimeSinceStart : 2213.575355052948
Exploration Critic Loss : 48348.5625
Exploitation Critic Loss : 0.190843403339386
Exploration Model Loss : 26.488861083984375
Actor Loss : 0.5037195086479187
Eval_AverageReturn : -41.16666793823242
Eval_StdReturn : 12.184918403625488
Eval_MaxReturn : -20.0
Eval_MinReturn : -66.0
Eval_AverageEpLen : 42.166666666666664
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -27.820000
best mean reward -26.350000
running time 2275.510259
At timestep:     29 / 1000At timestep:     56 / 1000At timestep:     88 / 1000At timestep:     113 / 1000At timestep:     152 / 1000At timestep:     176 / 1000At timestep:     203 / 1000At timestep:     276 / 1000At timestep:     321 / 1000At timestep:     372 / 1000At timestep:     401 / 1000At timestep:     436 / 1000At timestep:     484 / 1000At timestep:     524 / 1000At timestep:     564 / 1000At timestep:     617 / 1000At timestep:     651 / 1000At timestep:     720 / 1000At timestep:     767 / 1000At timestep:     813 / 1000At timestep:     843 / 1000At timestep:     863 / 1000At timestep:     903 / 1000At timestep:     937 / 1000At timestep:     983 / 1000At timestep:     1035 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_medium_supervised_lam10_PointmassMedium-v0_22-11-2022_23-08-43/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -27.81999969482422
Train_BestReturn : -26.350000381469727
TimeSinceStart : 2275.5102593898773
Exploration Critic Loss : 31016.4140625
Exploitation Critic Loss : 0.07274001836776733
Exploration Model Loss : 35.30226135253906
Actor Loss : 0.48402541875839233
Eval_AverageReturn : -38.80769348144531
Eval_StdReturn : 12.854269981384277
Eval_MaxReturn : -19.0
Eval_MinReturn : -72.0
Eval_AverageEpLen : 39.80769230769231
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam20_PointmassMedium-v0_22-11-2022_23-47-45 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam20_PointmassMedium-v0_22-11-2022_23-47-45
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.004316
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.004315614700317383
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -149.833328
best mean reward -inf
running time 12.461267
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     666 / 1000At timestep:     816 / 1000At timestep:     966 / 1000At timestep:     1116 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -149.8333282470703
TimeSinceStart : 12.46126675605774
Eval_AverageReturn : -139.375
Eval_StdReturn : 28.111106872558594
Eval_MaxReturn : -65.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 139.5
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -147.384613
best mean reward -inf
running time 24.087632
At timestep:     54 / 1000At timestep:     204 / 1000At timestep:     354 / 1000At timestep:     504 / 1000At timestep:     654 / 1000At timestep:     804 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -147.38461303710938
TimeSinceStart : 24.087631940841675
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -148.300003
best mean reward -inf
running time 75.859148
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     591 / 1000At timestep:     741 / 1000At timestep:     891 / 1000At timestep:     1041 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -148.3000030517578
TimeSinceStart : 75.85914754867554
Exploration Critic Loss : 1234137.0
Exploitation Critic Loss : 0.0003074251872021705
Exploration Model Loss : 81.21014404296875
Actor Loss : 1.1259126663208008
Eval_AverageReturn : -148.57142639160156
Eval_StdReturn : 3.4992711544036865
Eval_MaxReturn : -140.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 148.71428571428572
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -148.692307
best mean reward -inf
running time 126.442299
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -148.6923065185547
TimeSinceStart : 126.44229888916016
Exploration Critic Loss : 6489996.0
Exploitation Critic Loss : 0.05241250619292259
Exploration Model Loss : 37.84335708618164
Actor Loss : 0.8432060480117798
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -148.969696
best mean reward -inf
running time 178.093091
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -148.96969604492188
TimeSinceStart : 178.09309124946594
Exploration Critic Loss : 6665788.0
Exploitation Critic Loss : 0.07922202348709106
Exploration Model Loss : 52.54803466796875
Actor Loss : 0.7446767091751099
Eval_AverageReturn : -144.14285278320312
Eval_StdReturn : 14.34701156616211
Eval_MaxReturn : -109.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 144.28571428571428
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -148.350006
best mean reward -inf
running time 228.022225
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -148.35000610351562
TimeSinceStart : 228.0222246646881
Exploration Critic Loss : 4982113.5
Exploitation Critic Loss : 0.06642868369817734
Exploration Model Loss : 48.45733642578125
Actor Loss : 0.6157335638999939
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -142.346939
best mean reward -inf
running time 280.741745
At timestep:     150 / 1000At timestep:     271 / 1000At timestep:     401 / 1000At timestep:     551 / 1000At timestep:     701 / 1000At timestep:     851 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -142.34693908691406
TimeSinceStart : 280.7417449951172
Exploration Critic Loss : 5881555.0
Exploitation Critic Loss : 0.00041568547021597624
Exploration Model Loss : 38.15205383300781
Actor Loss : 0.55901700258255
Eval_AverageReturn : -142.7142791748047
Eval_StdReturn : 11.768168449401855
Eval_MaxReturn : -120.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 143.0
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -129.524597
best mean reward -inf
running time 332.343879
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     1050 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -129.52459716796875
TimeSinceStart : 332.34387850761414
Exploration Critic Loss : 8449496.0
Exploitation Critic Loss : 0.09067761152982712
Exploration Model Loss : 28.836227416992188
Actor Loss : 0.5159899592399597
Eval_AverageReturn : -150.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -150.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 150.0
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -120.337837
best mean reward -inf
running time 384.486750
At timestep:     150 / 1000At timestep:     300 / 1000At timestep:     450 / 1000At timestep:     600 / 1000At timestep:     750 / 1000At timestep:     900 / 1000At timestep:     954 / 1000At timestep:     1104 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -120.33783721923828
TimeSinceStart : 384.4867503643036
Exploration Critic Loss : 16234012.0
Exploitation Critic Loss : 0.01200028881430626
Exploration Model Loss : 26.934524536132812
Actor Loss : 0.47812461853027344
Eval_AverageReturn : -137.875
Eval_StdReturn : 32.079734802246094
Eval_MaxReturn : -53.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 138.0
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -119.891563
best mean reward -inf
running time 443.187756
At timestep:     60 / 1000At timestep:     115 / 1000At timestep:     222 / 1000At timestep:     372 / 1000At timestep:     466 / 1000At timestep:     546 / 1000At timestep:     591 / 1000At timestep:     686 / 1000At timestep:     836 / 1000At timestep:     933 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -119.89156341552734
TimeSinceStart : 443.1877555847168
Exploration Critic Loss : 15140210.0
Exploitation Critic Loss : 0.15663543343544006
Exploration Model Loss : 26.338226318359375
Actor Loss : 0.49606460332870483
Eval_AverageReturn : -90.90908813476562
Eval_StdReturn : 33.3397216796875
Eval_MaxReturn : -44.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 91.72727272727273
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -119.890106
best mean reward -inf
running time 496.096908
At timestep:     48 / 1000At timestep:     198 / 1000At timestep:     348 / 1000At timestep:     498 / 1000At timestep:     641 / 1000At timestep:     791 / 1000At timestep:     868 / 1000At timestep:     960 / 1000At timestep:     1110 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -119.89010620117188
TimeSinceStart : 496.0969080924988
Exploration Critic Loss : 12058866.0
Exploitation Critic Loss : 0.12892234325408936
Exploration Model Loss : 26.130477905273438
Actor Loss : 0.43284890055656433
Eval_AverageReturn : -122.88888549804688
Eval_StdReturn : 38.02760696411133
Eval_MaxReturn : -47.0
Eval_MinReturn : -150.0
Eval_AverageEpLen : 123.33333333333333
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -114.879997
best mean reward -114.879997
running time 551.025625
At timestep:     103 / 1000At timestep:     184 / 1000At timestep:     266 / 1000At timestep:     346 / 1000At timestep:     454 / 1000At timestep:     604 / 1000At timestep:     746 / 1000At timestep:     833 / 1000At timestep:     888 / 1000At timestep:     978 / 1000At timestep:     1095 / 1000