


LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam0.1_PointmassEasy-v0_22-11-2022_20-21-48 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam0.1_PointmassEasy-v0_22-11-2022_20-21-48
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.001384
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0013844966888427734
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 8.431892
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -50.0
TimeSinceStart : 8.431891918182373
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -49.900002
best mean reward -inf
running time 16.890401
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -49.900001525878906
TimeSinceStart : 16.890400648117065
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -49.650002
best mean reward -inf
running time 37.280423
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     838 / 1000At timestep:     888 / 1000At timestep:     938 / 1000At timestep:     988 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -49.650001525878906
TimeSinceStart : 37.280423402786255
Exploration Critic Loss : 514722.25
Exploitation Critic Loss : 0.04783647507429123
Exploration Model Loss : 29.57921600341797
Actor Loss : 1.1861402988433838
Eval_AverageReturn : -49.380950927734375
Eval_StdReturn : 2.768465280532837
Eval_MaxReturn : -37.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 49.42857142857143
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -45.459770
best mean reward -inf
running time 59.656454
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     246 / 1000At timestep:     296 / 1000At timestep:     346 / 1000At timestep:     390 / 1000At timestep:     429 / 1000At timestep:     479 / 1000At timestep:     529 / 1000At timestep:     562 / 1000At timestep:     612 / 1000At timestep:     662 / 1000At timestep:     712 / 1000At timestep:     742 / 1000At timestep:     792 / 1000At timestep:     842 / 1000At timestep:     892 / 1000At timestep:     942 / 1000At timestep:     992 / 1000At timestep:     1042 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -45.45977020263672
TimeSinceStart : 59.65645384788513
Exploration Critic Loss : 8609957.0
Exploitation Critic Loss : 0.08058296889066696
Exploration Model Loss : 46.464820861816406
Actor Loss : 0.9482282996177673
Eval_AverageReturn : -47.1363639831543
Eval_StdReturn : 6.039984226226807
Eval_MaxReturn : -29.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 47.36363636363637
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -39.400002
best mean reward -39.400002
running time 84.253026
At timestep:     50 / 1000At timestep:     68 / 1000At timestep:     118 / 1000At timestep:     168 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     388 / 1000At timestep:     438 / 1000At timestep:     467 / 1000At timestep:     517 / 1000At timestep:     567 / 1000At timestep:     607 / 1000At timestep:     645 / 1000At timestep:     677 / 1000At timestep:     727 / 1000At timestep:     777 / 1000At timestep:     805 / 1000At timestep:     855 / 1000At timestep:     875 / 1000At timestep:     902 / 1000At timestep:     952 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -39.400001525878906
Train_BestReturn : -39.400001525878906
TimeSinceStart : 84.25302600860596
Exploration Critic Loss : 9389708.0
Exploitation Critic Loss : 0.0907408595085144
Exploration Model Loss : 54.55191421508789
Actor Loss : 0.8023326396942139
Eval_AverageReturn : -41.33333206176758
Eval_StdReturn : 11.212592124938965
Eval_MaxReturn : -17.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 41.75
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -27.830000
best mean reward -27.830000
running time 110.754225
At timestep:     28 / 1000At timestep:     45 / 1000At timestep:     94 / 1000At timestep:     114 / 1000At timestep:     141 / 1000At timestep:     166 / 1000At timestep:     216 / 1000At timestep:     266 / 1000At timestep:     316 / 1000At timestep:     366 / 1000At timestep:     416 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     525 / 1000At timestep:     575 / 1000At timestep:     623 / 1000At timestep:     654 / 1000At timestep:     690 / 1000At timestep:     740 / 1000At timestep:     790 / 1000At timestep:     824 / 1000At timestep:     862 / 1000At timestep:     912 / 1000At timestep:     957 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -27.829999923706055
Train_BestReturn : -27.829999923706055
TimeSinceStart : 110.7542245388031
Exploration Critic Loss : 21974652.0
Exploitation Critic Loss : 0.13956081867218018
Exploration Model Loss : 120.81013488769531
Actor Loss : 0.7006763219833374
Eval_AverageReturn : -39.720001220703125
Eval_StdReturn : 11.581088066101074
Eval_MaxReturn : -16.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 40.28
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -19.889999
best mean reward -19.889999
running time 139.382212
At timestep:     50 / 1000At timestep:     96 / 1000At timestep:     133 / 1000At timestep:     183 / 1000At timestep:     219 / 1000At timestep:     256 / 1000At timestep:     280 / 1000At timestep:     316 / 1000At timestep:     366 / 1000At timestep:     416 / 1000At timestep:     440 / 1000At timestep:     473 / 1000At timestep:     522 / 1000At timestep:     541 / 1000At timestep:     591 / 1000At timestep:     620 / 1000At timestep:     647 / 1000At timestep:     672 / 1000At timestep:     701 / 1000At timestep:     723 / 1000At timestep:     772 / 1000At timestep:     805 / 1000At timestep:     839 / 1000At timestep:     867 / 1000At timestep:     917 / 1000At timestep:     967 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -19.889999389648438
Train_BestReturn : -19.889999389648438
TimeSinceStart : 139.38221216201782
Exploration Critic Loss : 10675348.0
Exploitation Critic Loss : 0.08102263510227203
Exploration Model Loss : 40.796478271484375
Actor Loss : 0.6160825490951538
Eval_AverageReturn : -36.592594146728516
Eval_StdReturn : 10.767782211303711
Eval_MaxReturn : -18.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 37.333333333333336
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -17.799999
best mean reward -17.799999
running time 168.148801
At timestep:     46 / 1000At timestep:     74 / 1000At timestep:     96 / 1000At timestep:     119 / 1000At timestep:     143 / 1000At timestep:     175 / 1000At timestep:     216 / 1000At timestep:     261 / 1000At timestep:     297 / 1000At timestep:     318 / 1000At timestep:     356 / 1000At timestep:     392 / 1000At timestep:     411 / 1000At timestep:     449 / 1000At timestep:     478 / 1000At timestep:     528 / 1000At timestep:     553 / 1000At timestep:     594 / 1000At timestep:     624 / 1000At timestep:     644 / 1000At timestep:     666 / 1000At timestep:     716 / 1000At timestep:     755 / 1000At timestep:     801 / 1000At timestep:     851 / 1000At timestep:     884 / 1000At timestep:     934 / 1000At timestep:     984 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -17.799999237060547
Train_BestReturn : -17.799999237060547
TimeSinceStart : 168.14880084991455
Exploration Critic Loss : 9219720.0
Exploitation Critic Loss : 0.12250234186649323
Exploration Model Loss : 36.81867980957031
Actor Loss : 0.6038799285888672
Eval_AverageReturn : -34.0
Eval_StdReturn : 10.612289428710938
Eval_MaxReturn : -18.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 34.86206896551724
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -17.230000
best mean reward -17.230000
running time 198.029640
At timestep:     42 / 1000At timestep:     76 / 1000At timestep:     110 / 1000At timestep:     130 / 1000At timestep:     156 / 1000At timestep:     186 / 1000At timestep:     226 / 1000At timestep:     254 / 1000At timestep:     273 / 1000At timestep:     307 / 1000At timestep:     333 / 1000At timestep:     359 / 1000At timestep:     383 / 1000At timestep:     422 / 1000At timestep:     439 / 1000At timestep:     460 / 1000At timestep:     484 / 1000At timestep:     514 / 1000At timestep:     537 / 1000At timestep:     587 / 1000At timestep:     605 / 1000At timestep:     628 / 1000At timestep:     644 / 1000At timestep:     676 / 1000At timestep:     689 / 1000At timestep:     717 / 1000At timestep:     737 / 1000At timestep:     776 / 1000At timestep:     798 / 1000At timestep:     830 / 1000At timestep:     849 / 1000At timestep:     889 / 1000At timestep:     916 / 1000At timestep:     932 / 1000At timestep:     976 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -17.229999542236328
Train_BestReturn : -17.229999542236328
TimeSinceStart : 198.02963972091675
Exploration Critic Loss : 9227400.0
Exploitation Critic Loss : 0.10211291909217834
Exploration Model Loss : 23.838043212890625
Actor Loss : 0.5902109146118164
Eval_AverageReturn : -26.88888931274414
Eval_StdReturn : 8.918699264526367
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 27.86111111111111
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -17.500000
best mean reward -17.230000
running time 228.622802
At timestep:     31 / 1000At timestep:     67 / 1000At timestep:     98 / 1000At timestep:     121 / 1000At timestep:     151 / 1000At timestep:     184 / 1000At timestep:     208 / 1000At timestep:     238 / 1000At timestep:     260 / 1000At timestep:     283 / 1000At timestep:     305 / 1000At timestep:     322 / 1000At timestep:     343 / 1000At timestep:     372 / 1000At timestep:     398 / 1000At timestep:     420 / 1000At timestep:     470 / 1000At timestep:     510 / 1000At timestep:     548 / 1000At timestep:     590 / 1000At timestep:     619 / 1000At timestep:     669 / 1000At timestep:     719 / 1000At timestep:     737 / 1000At timestep:     779 / 1000At timestep:     806 / 1000At timestep:     828 / 1000At timestep:     849 / 1000At timestep:     897 / 1000At timestep:     947 / 1000At timestep:     965 / 1000At timestep:     981 / 1000At timestep:     1030 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -17.5
Train_BestReturn : -17.229999542236328
TimeSinceStart : 228.6228015422821
Exploration Critic Loss : 11522185.0
Exploitation Critic Loss : 0.07382214069366455
Exploration Model Loss : 18.316268920898438
Actor Loss : 0.6094801425933838
Eval_AverageReturn : -30.303030014038086
Eval_StdReturn : 11.123234748840332
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 31.21212121212121
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -18.230000
best mean reward -17.230000
running time 262.367636
At timestep:     40 / 1000At timestep:     58 / 1000At timestep:     78 / 1000At timestep:     109 / 1000At timestep:     133 / 1000At timestep:     158 / 1000At timestep:     186 / 1000At timestep:     204 / 1000At timestep:     226 / 1000At timestep:     258 / 1000At timestep:     294 / 1000At timestep:     328 / 1000At timestep:     344 / 1000At timestep:     363 / 1000At timestep:     380 / 1000At timestep:     407 / 1000At timestep:     428 / 1000At timestep:     447 / 1000At timestep:     488 / 1000At timestep:     511 / 1000At timestep:     532 / 1000At timestep:     562 / 1000At timestep:     586 / 1000At timestep:     614 / 1000At timestep:     644 / 1000At timestep:     664 / 1000At timestep:     708 / 1000At timestep:     726 / 1000At timestep:     749 / 1000At timestep:     784 / 1000At timestep:     801 / 1000At timestep:     851 / 1000At timestep:     878 / 1000At timestep:     899 / 1000At timestep:     932 / 1000At timestep:     952 / 1000At timestep:     994 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -18.229999542236328
Train_BestReturn : -17.229999542236328
TimeSinceStart : 262.36763620376587
Exploration Critic Loss : 17344808.0
Exploitation Critic Loss : 0.13524162769317627
Exploration Model Loss : 16.988510131835938
Actor Loss : 0.6206904649734497
Eval_AverageReturn : -25.578947067260742
Eval_StdReturn : 8.749832153320312
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 26.55263157894737
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -17.969999
best mean reward -17.230000
running time 299.589986
At timestep:     22 / 1000At timestep:     52 / 1000At timestep:     86 / 1000At timestep:     102 / 1000At timestep:     123 / 1000At timestep:     162 / 1000At timestep:     195 / 1000At timestep:     221 / 1000At timestep:     264 / 1000At timestep:     291 / 1000At timestep:     317 / 1000At timestep:     337 / 1000At timestep:     357 / 1000At timestep:     383 / 1000At timestep:     396 / 1000At timestep:     413 / 1000At timestep:     436 / 1000At timestep:     459 / 1000At timestep:     476 / 1000At timestep:     517 / 1000At timestep:     554 / 1000At timestep:     574 / 1000At timestep:     596 / 1000At timestep:     613 / 1000At timestep:     650 / 1000At timestep:     666 / 1000At timestep:     687 / 1000At timestep:     713 / 1000At timestep:     761 / 1000At timestep:     777 / 1000At timestep:     804 / 1000At timestep:     823 / 1000At timestep:     854 / 1000At timestep:     873 / 1000At timestep:     899 / 1000At timestep:     930 / 1000At timestep:     947 / 1000At timestep:     988 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -17.969999313354492
Train_BestReturn : -17.229999542236328
TimeSinceStart : 299.58998560905457
Exploration Critic Loss : 16969956.0
Exploitation Critic Loss : 0.08040091395378113
Exploration Model Loss : 16.557479858398438
Actor Loss : 0.5999170541763306
Eval_AverageReturn : -24.846153259277344
Eval_StdReturn : 8.69582462310791
Eval_MaxReturn : -12.0
Eval_MinReturn : -47.0
Eval_AverageEpLen : 25.846153846153847
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -18.190001
best mean reward -17.230000
running time 337.581376
At timestep:     18 / 1000At timestep:     49 / 1000At timestep:     69 / 1000At timestep:     104 / 1000At timestep:     122 / 1000At timestep:     155 / 1000At timestep:     175 / 1000At timestep:     194 / 1000At timestep:     216 / 1000At timestep:     252 / 1000At timestep:     278 / 1000At timestep:     299 / 1000At timestep:     349 / 1000At timestep:     374 / 1000At timestep:     402 / 1000At timestep:     418 / 1000At timestep:     441 / 1000At timestep:     456 / 1000At timestep:     471 / 1000At timestep:     493 / 1000At timestep:     512 / 1000At timestep:     540 / 1000At timestep:     559 / 1000At timestep:     609 / 1000At timestep:     631 / 1000At timestep:     681 / 1000At timestep:     726 / 1000At timestep:     751 / 1000At timestep:     778 / 1000At timestep:     793 / 1000At timestep:     813 / 1000At timestep:     863 / 1000At timestep:     895 / 1000At timestep:     929 / 1000At timestep:     952 / 1000At timestep:     991 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -18.190000534057617
Train_BestReturn : -17.229999542236328
TimeSinceStart : 337.58137559890747
Exploration Critic Loss : 17652636.0
Exploitation Critic Loss : 0.11476673185825348
Exploration Model Loss : 16.405426025390625
Actor Loss : 0.5318301916122437
Eval_AverageReturn : -26.324323654174805
Eval_StdReturn : 10.865402221679688
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 27.216216216216218
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -17.360001
best mean reward -17.230000
running time 376.387804
At timestep:     21 / 1000At timestep:     36 / 1000At timestep:     50 / 1000At timestep:     74 / 1000At timestep:     102 / 1000At timestep:     113 / 1000At timestep:     132 / 1000At timestep:     155 / 1000At timestep:     205 / 1000At timestep:     224 / 1000At timestep:     251 / 1000At timestep:     288 / 1000At timestep:     315 / 1000At timestep:     331 / 1000At timestep:     356 / 1000At timestep:     395 / 1000At timestep:     421 / 1000At timestep:     452 / 1000At timestep:     471 / 1000At timestep:     492 / 1000At timestep:     506 / 1000At timestep:     532 / 1000At timestep:     550 / 1000At timestep:     578 / 1000At timestep:     599 / 1000At timestep:     634 / 1000At timestep:     653 / 1000At timestep:     672 / 1000At timestep:     715 / 1000At timestep:     729 / 1000At timestep:     764 / 1000At timestep:     791 / 1000At timestep:     817 / 1000At timestep:     853 / 1000At timestep:     875 / 1000At timestep:     908 / 1000At timestep:     927 / 1000At timestep:     947 / 1000At timestep:     968 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -17.360000610351562
Train_BestReturn : -17.229999542236328
TimeSinceStart : 376.38780426979065
Exploration Critic Loss : 19992038.0
Exploitation Critic Loss : 0.15095114707946777
Exploration Model Loss : 16.350631713867188
Actor Loss : 0.564909815788269
Eval_AverageReturn : -24.125
Eval_StdReturn : 8.692489624023438
Eval_MaxReturn : -10.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 25.1
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -17.450001
best mean reward -17.230000
running time 415.176167
At timestep:     35 / 1000At timestep:     50 / 1000At timestep:     84 / 1000At timestep:     100 / 1000At timestep:     121 / 1000At timestep:     150 / 1000At timestep:     168 / 1000At timestep:     183 / 1000At timestep:     200 / 1000At timestep:     232 / 1000At timestep:     249 / 1000At timestep:     270 / 1000At timestep:     299 / 1000At timestep:     311 / 1000At timestep:     323 / 1000At timestep:     347 / 1000At timestep:     367 / 1000At timestep:     383 / 1000At timestep:     408 / 1000At timestep:     435 / 1000At timestep:     476 / 1000At timestep:     495 / 1000At timestep:     525 / 1000At timestep:     556 / 1000At timestep:     585 / 1000At timestep:     604 / 1000At timestep:     629 / 1000At timestep:     643 / 1000At timestep:     679 / 1000At timestep:     706 / 1000At timestep:     740 / 1000At timestep:     777 / 1000At timestep:     807 / 1000At timestep:     826 / 1000At timestep:     850 / 1000At timestep:     874 / 1000At timestep:     905 / 1000At timestep:     936 / 1000At timestep:     957 / 1000At timestep:     975 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -17.450000762939453
Train_BestReturn : -17.229999542236328
TimeSinceStart : 415.1761665344238
Exploration Critic Loss : 20948488.0
Exploitation Critic Loss : 0.10048571974039078
Exploration Model Loss : 16.330490112304688
Actor Loss : 0.5690733194351196
Eval_AverageReturn : -23.7560977935791
Eval_StdReturn : 7.808002471923828
Eval_MaxReturn : -11.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 24.75609756097561
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -17.530001
best mean reward -17.230000
running time 455.000643
At timestep:     50 / 1000At timestep:     69 / 1000At timestep:     85 / 1000At timestep:     101 / 1000At timestep:     121 / 1000At timestep:     145 / 1000At timestep:     161 / 1000At timestep:     187 / 1000At timestep:     207 / 1000At timestep:     242 / 1000At timestep:     259 / 1000At timestep:     283 / 1000At timestep:     301 / 1000At timestep:     316 / 1000At timestep:     340 / 1000At timestep:     360 / 1000At timestep:     384 / 1000At timestep:     406 / 1000At timestep:     445 / 1000At timestep:     462 / 1000At timestep:     484 / 1000At timestep:     501 / 1000At timestep:     518 / 1000At timestep:     568 / 1000At timestep:     586 / 1000At timestep:     608 / 1000At timestep:     634 / 1000At timestep:     661 / 1000At timestep:     678 / 1000At timestep:     712 / 1000At timestep:     732 / 1000At timestep:     753 / 1000At timestep:     773 / 1000At timestep:     796 / 1000At timestep:     818 / 1000At timestep:     857 / 1000At timestep:     885 / 1000At timestep:     922 / 1000At timestep:     941 / 1000At timestep:     963 / 1000At timestep:     982 / 1000At timestep:     998 / 1000At timestep:     1037 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -17.530000686645508
Train_BestReturn : -17.229999542236328
TimeSinceStart : 455.00064277648926
Exploration Critic Loss : 12533500.0
Exploitation Critic Loss : 0.11452662199735641
Exploration Model Loss : 16.322998046875
Actor Loss : 0.6199332475662231
Eval_AverageReturn : -23.13953399658203
Eval_StdReturn : 8.81484317779541
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 24.11627906976744
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -17.900000
best mean reward -17.230000
running time 493.242424
At timestep:     21 / 1000At timestep:     35 / 1000At timestep:     56 / 1000At timestep:     93 / 1000At timestep:     114 / 1000At timestep:     135 / 1000At timestep:     154 / 1000At timestep:     182 / 1000At timestep:     201 / 1000At timestep:     214 / 1000At timestep:     236 / 1000At timestep:     259 / 1000At timestep:     282 / 1000At timestep:     301 / 1000At timestep:     329 / 1000At timestep:     348 / 1000At timestep:     371 / 1000At timestep:     402 / 1000At timestep:     417 / 1000At timestep:     440 / 1000At timestep:     458 / 1000At timestep:     486 / 1000At timestep:     500 / 1000At timestep:     523 / 1000At timestep:     543 / 1000At timestep:     573 / 1000At timestep:     594 / 1000At timestep:     610 / 1000At timestep:     638 / 1000At timestep:     679 / 1000At timestep:     697 / 1000At timestep:     715 / 1000At timestep:     731 / 1000At timestep:     755 / 1000At timestep:     781 / 1000At timestep:     798 / 1000At timestep:     819 / 1000At timestep:     836 / 1000At timestep:     857 / 1000At timestep:     877 / 1000At timestep:     901 / 1000At timestep:     938 / 1000At timestep:     954 / 1000At timestep:     979 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -17.899999618530273
Train_BestReturn : -17.229999542236328
TimeSinceStart : 493.2424244880676
Exploration Critic Loss : 11542616.0
Exploitation Critic Loss : 0.1023181825876236
Exploration Model Loss : 16.319503784179688
Actor Loss : 0.6012061834335327
Eval_AverageReturn : -21.311111450195312
Eval_StdReturn : 6.069311141967773
Eval_MaxReturn : -12.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 22.31111111111111
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -17.770000
best mean reward -17.230000
running time 533.735136
At timestep:     25 / 1000At timestep:     52 / 1000At timestep:     73 / 1000At timestep:     102 / 1000At timestep:     121 / 1000At timestep:     147 / 1000At timestep:     163 / 1000At timestep:     181 / 1000At timestep:     194 / 1000At timestep:     217 / 1000At timestep:     239 / 1000At timestep:     255 / 1000At timestep:     267 / 1000At timestep:     302 / 1000At timestep:     333 / 1000At timestep:     361 / 1000At timestep:     399 / 1000At timestep:     410 / 1000At timestep:     429 / 1000At timestep:     466 / 1000At timestep:     491 / 1000At timestep:     509 / 1000At timestep:     532 / 1000At timestep:     556 / 1000At timestep:     573 / 1000At timestep:     600 / 1000At timestep:     618 / 1000At timestep:     651 / 1000At timestep:     670 / 1000At timestep:     701 / 1000At timestep:     716 / 1000At timestep:     738 / 1000At timestep:     755 / 1000At timestep:     789 / 1000At timestep:     816 / 1000At timestep:     848 / 1000At timestep:     864 / 1000At timestep:     880 / 1000At timestep:     902 / 1000At timestep:     932 / 1000At timestep:     964 / 1000At timestep:     980 / 1000At timestep:     996 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -17.770000457763672
Train_BestReturn : -17.229999542236328
TimeSinceStart : 533.7351357936859
Exploration Critic Loss : 22032836.0
Exploitation Critic Loss : 0.10644234716892242
Exploration Model Loss : 16.318740844726562
Actor Loss : 0.5244622230529785
Eval_AverageReturn : -22.136363983154297
Eval_StdReturn : 7.034302234649658
Eval_MaxReturn : -10.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 23.136363636363637
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.469999
best mean reward -17.230000
running time 574.422122
At timestep:     27 / 1000At timestep:     41 / 1000At timestep:     53 / 1000At timestep:     71 / 1000At timestep:     98 / 1000At timestep:     113 / 1000At timestep:     125 / 1000At timestep:     152 / 1000At timestep:     180 / 1000At timestep:     202 / 1000At timestep:     217 / 1000At timestep:     232 / 1000At timestep:     254 / 1000At timestep:     266 / 1000At timestep:     284 / 1000At timestep:     320 / 1000At timestep:     341 / 1000At timestep:     362 / 1000At timestep:     412 / 1000At timestep:     431 / 1000At timestep:     450 / 1000At timestep:     481 / 1000At timestep:     497 / 1000At timestep:     514 / 1000At timestep:     533 / 1000At timestep:     553 / 1000At timestep:     574 / 1000At timestep:     604 / 1000At timestep:     623 / 1000At timestep:     645 / 1000At timestep:     660 / 1000At timestep:     684 / 1000At timestep:     706 / 1000At timestep:     723 / 1000At timestep:     739 / 1000At timestep:     759 / 1000At timestep:     778 / 1000At timestep:     798 / 1000At timestep:     822 / 1000At timestep:     840 / 1000At timestep:     857 / 1000At timestep:     874 / 1000At timestep:     893 / 1000At timestep:     907 / 1000At timestep:     927 / 1000At timestep:     961 / 1000At timestep:     990 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.469999313354492
Train_BestReturn : -17.229999542236328
TimeSinceStart : 574.422122001648
Exploration Critic Loss : 24484756.0
Exploitation Critic Loss : 0.08217131346464157
Exploration Model Loss : 16.318191528320312
Actor Loss : 0.5708907842636108
Eval_AverageReturn : -20.1875
Eval_StdReturn : 7.064158916473389
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 21.166666666666668
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.719999
best mean reward -17.230000
running time 615.395958
At timestep:     27 / 1000At timestep:     48 / 1000At timestep:     67 / 1000At timestep:     83 / 1000At timestep:     109 / 1000At timestep:     129 / 1000At timestep:     147 / 1000At timestep:     173 / 1000At timestep:     204 / 1000At timestep:     218 / 1000At timestep:     231 / 1000At timestep:     263 / 1000At timestep:     282 / 1000At timestep:     297 / 1000At timestep:     330 / 1000At timestep:     348 / 1000At timestep:     369 / 1000At timestep:     383 / 1000At timestep:     404 / 1000At timestep:     434 / 1000At timestep:     448 / 1000At timestep:     463 / 1000At timestep:     487 / 1000At timestep:     512 / 1000At timestep:     530 / 1000At timestep:     551 / 1000At timestep:     565 / 1000At timestep:     586 / 1000At timestep:     604 / 1000At timestep:     624 / 1000At timestep:     646 / 1000At timestep:     669 / 1000At timestep:     715 / 1000At timestep:     736 / 1000At timestep:     769 / 1000At timestep:     806 / 1000At timestep:     825 / 1000At timestep:     839 / 1000At timestep:     855 / 1000At timestep:     870 / 1000At timestep:     887 / 1000At timestep:     912 / 1000At timestep:     935 / 1000At timestep:     947 / 1000At timestep:     960 / 1000At timestep:     980 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.719999313354492
Train_BestReturn : -17.229999542236328
TimeSinceStart : 615.3959581851959
Exploration Critic Loss : 27103352.0
Exploitation Critic Loss : 0.11528168618679047
Exploration Model Loss : 16.317733764648438
Actor Loss : 0.5838497877120972
Eval_AverageReturn : -20.319149017333984
Eval_StdReturn : 6.968336582183838
Eval_MaxReturn : -11.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 21.319148936170212
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.360001
best mean reward -17.230000
running time 656.269380
At timestep:     34 / 1000At timestep:     55 / 1000At timestep:     75 / 1000At timestep:     108 / 1000At timestep:     128 / 1000At timestep:     149 / 1000At timestep:     178 / 1000At timestep:     203 / 1000At timestep:     239 / 1000At timestep:     264 / 1000At timestep:     285 / 1000At timestep:     297 / 1000At timestep:     308 / 1000At timestep:     341 / 1000At timestep:     366 / 1000At timestep:     388 / 1000At timestep:     409 / 1000At timestep:     434 / 1000At timestep:     454 / 1000At timestep:     486 / 1000At timestep:     508 / 1000At timestep:     528 / 1000At timestep:     546 / 1000At timestep:     568 / 1000At timestep:     591 / 1000At timestep:     610 / 1000At timestep:     635 / 1000At timestep:     662 / 1000At timestep:     680 / 1000At timestep:     713 / 1000At timestep:     737 / 1000At timestep:     759 / 1000At timestep:     782 / 1000At timestep:     798 / 1000At timestep:     817 / 1000At timestep:     833 / 1000At timestep:     848 / 1000At timestep:     867 / 1000At timestep:     895 / 1000At timestep:     921 / 1000At timestep:     952 / 1000At timestep:     977 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.360000610351562
Train_BestReturn : -17.229999542236328
TimeSinceStart : 656.2693798542023
Exploration Critic Loss : 31584248.0
Exploitation Critic Loss : 0.07622155547142029
Exploration Model Loss : 16.317214965820312
Actor Loss : 0.5666866302490234
Eval_AverageReturn : -22.418603897094727
Eval_StdReturn : 5.843830108642578
Eval_MaxReturn : -10.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 23.41860465116279
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.480000
best mean reward -17.230000
running time 697.064332
At timestep:     13 / 1000At timestep:     29 / 1000At timestep:     46 / 1000At timestep:     59 / 1000At timestep:     77 / 1000At timestep:     98 / 1000At timestep:     121 / 1000At timestep:     133 / 1000At timestep:     157 / 1000At timestep:     185 / 1000At timestep:     206 / 1000At timestep:     224 / 1000At timestep:     243 / 1000At timestep:     258 / 1000At timestep:     275 / 1000At timestep:     308 / 1000At timestep:     319 / 1000At timestep:     366 / 1000At timestep:     389 / 1000At timestep:     405 / 1000At timestep:     422 / 1000At timestep:     453 / 1000At timestep:     492 / 1000At timestep:     512 / 1000At timestep:     525 / 1000At timestep:     548 / 1000At timestep:     576 / 1000At timestep:     592 / 1000At timestep:     611 / 1000At timestep:     626 / 1000At timestep:     646 / 1000At timestep:     685 / 1000At timestep:     729 / 1000At timestep:     755 / 1000At timestep:     777 / 1000At timestep:     807 / 1000At timestep:     820 / 1000At timestep:     833 / 1000At timestep:     859 / 1000At timestep:     879 / 1000At timestep:     912 / 1000At timestep:     929 / 1000At timestep:     946 / 1000At timestep:     966 / 1000At timestep:     991 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.479999542236328
Train_BestReturn : -17.229999542236328
TimeSinceStart : 697.064332485199
Exploration Critic Loss : 34002632.0
Exploitation Critic Loss : 0.10514628887176514
Exploration Model Loss : 16.316726684570312
Actor Loss : 0.532407283782959
Eval_AverageReturn : -21.173913955688477
Eval_StdReturn : 8.426935195922852
Eval_MaxReturn : -10.0
Eval_MinReturn : -46.0
Eval_AverageEpLen : 22.17391304347826
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -17.530001
best mean reward -17.230000
running time 738.488521
At timestep:     14 / 1000At timestep:     30 / 1000At timestep:     48 / 1000At timestep:     74 / 1000At timestep:     104 / 1000At timestep:     119 / 1000At timestep:     139 / 1000At timestep:     169 / 1000At timestep:     188 / 1000At timestep:     223 / 1000At timestep:     242 / 1000At timestep:     269 / 1000At timestep:     284 / 1000At timestep:     303 / 1000At timestep:     328 / 1000At timestep:     348 / 1000At timestep:     374 / 1000At timestep:     389 / 1000At timestep:     404 / 1000At timestep:     428 / 1000At timestep:     459 / 1000At timestep:     483 / 1000At timestep:     498 / 1000At timestep:     526 / 1000At timestep:     554 / 1000At timestep:     583 / 1000At timestep:     621 / 1000At timestep:     653 / 1000At timestep:     677 / 1000At timestep:     693 / 1000At timestep:     711 / 1000At timestep:     734 / 1000At timestep:     756 / 1000At timestep:     777 / 1000At timestep:     802 / 1000At timestep:     830 / 1000At timestep:     846 / 1000At timestep:     870 / 1000At timestep:     888 / 1000At timestep:     906 / 1000At timestep:     916 / 1000At timestep:     938 / 1000At timestep:     962 / 1000At timestep:     977 / 1000At timestep:     995 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -17.530000686645508
Train_BestReturn : -17.229999542236328
TimeSinceStart : 738.4885213375092
Exploration Critic Loss : 21886938.0
Exploitation Critic Loss : 0.07794719934463501
Exploration Model Loss : 16.316268920898438
Actor Loss : 0.5858258008956909
Eval_AverageReturn : -21.065217971801758
Eval_StdReturn : 6.10915994644165
Eval_MaxReturn : -9.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 22.065217391304348
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -17.770000
best mean reward -17.230000
running time 779.592803
At timestep:     18 / 1000At timestep:     41 / 1000At timestep:     65 / 1000At timestep:     94 / 1000At timestep:     114 / 1000At timestep:     136 / 1000At timestep:     154 / 1000At timestep:     174 / 1000At timestep:     202 / 1000At timestep:     215 / 1000At timestep:     240 / 1000At timestep:     265 / 1000At timestep:     279 / 1000At timestep:     293 / 1000At timestep:     314 / 1000At timestep:     344 / 1000At timestep:     358 / 1000At timestep:     383 / 1000At timestep:     421 / 1000At timestep:     447 / 1000At timestep:     462 / 1000At timestep:     496 / 1000At timestep:     518 / 1000At timestep:     538 / 1000At timestep:     557 / 1000At timestep:     576 / 1000At timestep:     607 / 1000At timestep:     623 / 1000At timestep:     639 / 1000At timestep:     652 / 1000At timestep:     664 / 1000At timestep:     686 / 1000At timestep:     703 / 1000At timestep:     730 / 1000At timestep:     748 / 1000At timestep:     762 / 1000At timestep:     779 / 1000At timestep:     795 / 1000At timestep:     812 / 1000At timestep:     834 / 1000At timestep:     850 / 1000At timestep:     883 / 1000At timestep:     912 / 1000At timestep:     924 / 1000At timestep:     938 / 1000At timestep:     955 / 1000At timestep:     979 / 1000At timestep:     994 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -17.770000457763672
Train_BestReturn : -17.229999542236328
TimeSinceStart : 779.5928027629852
Exploration Critic Loss : 22874356.0
Exploitation Critic Loss : 0.08422911167144775
Exploration Model Loss : 16.315826416015625
Actor Loss : 0.49401330947875977
Eval_AverageReturn : -19.67346954345703
Eval_StdReturn : 6.2315449714660645
Eval_MaxReturn : -11.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 20.6734693877551
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -18.010000
best mean reward -17.230000
running time 821.370469
At timestep:     17 / 1000At timestep:     33 / 1000At timestep:     64 / 1000At timestep:     79 / 1000At timestep:     102 / 1000At timestep:     120 / 1000At timestep:     146 / 1000At timestep:     161 / 1000At timestep:     178 / 1000At timestep:     200 / 1000At timestep:     220 / 1000At timestep:     237 / 1000At timestep:     253 / 1000At timestep:     276 / 1000At timestep:     293 / 1000At timestep:     306 / 1000At timestep:     328 / 1000At timestep:     350 / 1000At timestep:     369 / 1000At timestep:     383 / 1000At timestep:     412 / 1000At timestep:     440 / 1000At timestep:     454 / 1000At timestep:     478 / 1000At timestep:     501 / 1000At timestep:     524 / 1000At timestep:     547 / 1000At timestep:     562 / 1000At timestep:     581 / 1000At timestep:     601 / 1000At timestep:     624 / 1000At timestep:     646 / 1000At timestep:     668 / 1000At timestep:     693 / 1000At timestep:     706 / 1000At timestep:     729 / 1000At timestep:     746 / 1000At timestep:     773 / 1000At timestep:     790 / 1000At timestep:     818 / 1000At timestep:     839 / 1000At timestep:     856 / 1000At timestep:     874 / 1000At timestep:     889 / 1000At timestep:     904 / 1000At timestep:     926 / 1000At timestep:     941 / 1000At timestep:     963 / 1000At timestep:     987 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -18.010000228881836
Train_BestReturn : -17.229999542236328
TimeSinceStart : 821.3704688549042
Exploration Critic Loss : 26789216.0
Exploitation Critic Loss : 0.08626780658960342
Exploration Model Loss : 16.315383911132812
Actor Loss : 0.45524758100509644
Eval_AverageReturn : -19.15999984741211
Eval_StdReturn : 4.446841239929199
Eval_MaxReturn : -12.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 20.16
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -18.160000
best mean reward -17.230000
running time 863.295802
At timestep:     21 / 1000At timestep:     38 / 1000At timestep:     53 / 1000At timestep:     70 / 1000At timestep:     100 / 1000At timestep:     113 / 1000At timestep:     135 / 1000At timestep:     160 / 1000At timestep:     179 / 1000At timestep:     204 / 1000At timestep:     229 / 1000At timestep:     242 / 1000At timestep:     261 / 1000At timestep:     280 / 1000At timestep:     299 / 1000At timestep:     323 / 1000At timestep:     341 / 1000At timestep:     359 / 1000At timestep:     376 / 1000At timestep:     396 / 1000At timestep:     410 / 1000At timestep:     423 / 1000At timestep:     444 / 1000At timestep:     459 / 1000At timestep:     481 / 1000At timestep:     502 / 1000At timestep:     527 / 1000At timestep:     547 / 1000At timestep:     562 / 1000At timestep:     577 / 1000At timestep:     603 / 1000At timestep:     627 / 1000At timestep:     648 / 1000At timestep:     678 / 1000At timestep:     694 / 1000At timestep:     706 / 1000At timestep:     725 / 1000At timestep:     739 / 1000At timestep:     760 / 1000At timestep:     775 / 1000At timestep:     799 / 1000At timestep:     822 / 1000At timestep:     841 / 1000At timestep:     856 / 1000At timestep:     883 / 1000At timestep:     921 / 1000At timestep:     935 / 1000At timestep:     951 / 1000At timestep:     970 / 1000At timestep:     990 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -18.15999984741211
Train_BestReturn : -17.229999542236328
TimeSinceStart : 863.2958023548126
Exploration Critic Loss : 34172120.0
Exploitation Critic Loss : 0.14428342878818512
Exploration Model Loss : 16.314865112304688
Actor Loss : 0.5042985081672668
Eval_AverageReturn : -18.901960372924805
Eval_StdReturn : 5.138302326202393
Eval_MaxReturn : -11.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 19.901960784313726
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -17.580000
best mean reward -17.230000
running time 905.820156
At timestep:     27 / 1000At timestep:     58 / 1000At timestep:     77 / 1000At timestep:     98 / 1000At timestep:     117 / 1000At timestep:     134 / 1000At timestep:     151 / 1000At timestep:     165 / 1000At timestep:     189 / 1000At timestep:     207 / 1000At timestep:     229 / 1000At timestep:     249 / 1000At timestep:     262 / 1000At timestep:     293 / 1000At timestep:     314 / 1000At timestep:     360 / 1000At timestep:     382 / 1000At timestep:     414 / 1000At timestep:     434 / 1000At timestep:     450 / 1000At timestep:     475 / 1000At timestep:     488 / 1000At timestep:     508 / 1000At timestep:     538 / 1000At timestep:     555 / 1000At timestep:     572 / 1000At timestep:     586 / 1000At timestep:     602 / 1000At timestep:     625 / 1000At timestep:     640 / 1000At timestep:     661 / 1000At timestep:     679 / 1000At timestep:     699 / 1000At timestep:     726 / 1000At timestep:     745 / 1000At timestep:     774 / 1000At timestep:     803 / 1000At timestep:     829 / 1000At timestep:     852 / 1000At timestep:     869 / 1000At timestep:     891 / 1000At timestep:     911 / 1000At timestep:     923 / 1000At timestep:     947 / 1000At timestep:     967 / 1000At timestep:     982 / 1000At timestep:     997 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -17.579999923706055
Train_BestReturn : -17.229999542236328
TimeSinceStart : 905.8201563358307
Exploration Critic Loss : 31177008.0
Exploitation Critic Loss : 0.14321936666965485
Exploration Model Loss : 16.314407348632812
Actor Loss : 0.5705536007881165
Eval_AverageReturn : -20.22916603088379
Eval_StdReturn : 6.25246524810791
Eval_MaxReturn : -11.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 21.229166666666668
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -17.040001
best mean reward -17.040001
running time 948.093445
At timestep:     27 / 1000At timestep:     45 / 1000At timestep:     69 / 1000At timestep:     93 / 1000At timestep:     113 / 1000At timestep:     126 / 1000At timestep:     146 / 1000At timestep:     164 / 1000At timestep:     184 / 1000At timestep:     201 / 1000At timestep:     222 / 1000At timestep:     247 / 1000At timestep:     263 / 1000At timestep:     282 / 1000At timestep:     310 / 1000At timestep:     325 / 1000At timestep:     353 / 1000At timestep:     382 / 1000At timestep:     398 / 1000At timestep:     430 / 1000At timestep:     448 / 1000At timestep:     467 / 1000At timestep:     485 / 1000At timestep:     510 / 1000At timestep:     522 / 1000At timestep:     540 / 1000At timestep:     555 / 1000At timestep:     578 / 1000At timestep:     601 / 1000At timestep:     614 / 1000At timestep:     633 / 1000At timestep:     665 / 1000At timestep:     690 / 1000At timestep:     720 / 1000At timestep:     741 / 1000At timestep:     754 / 1000At timestep:     766 / 1000At timestep:     781 / 1000At timestep:     797 / 1000At timestep:     818 / 1000At timestep:     834 / 1000At timestep:     862 / 1000At timestep:     888 / 1000At timestep:     907 / 1000At timestep:     925 / 1000At timestep:     947 / 1000At timestep:     960 / 1000At timestep:     985 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -17.040000915527344
Train_BestReturn : -17.040000915527344
TimeSinceStart : 948.0934450626373
Exploration Critic Loss : 32877980.0
Exploitation Critic Loss : 0.09868712723255157
Exploration Model Loss : 16.313934326171875
Actor Loss : 0.5380549430847168
Eval_AverageReturn : -19.489795684814453
Eval_StdReturn : 5.330399513244629
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 20.489795918367346
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -16.900000
best mean reward -16.900000
running time 990.843111
At timestep:     22 / 1000At timestep:     45 / 1000At timestep:     75 / 1000At timestep:     102 / 1000At timestep:     116 / 1000At timestep:     130 / 1000At timestep:     148 / 1000At timestep:     165 / 1000At timestep:     187 / 1000At timestep:     205 / 1000At timestep:     220 / 1000At timestep:     243 / 1000At timestep:     263 / 1000At timestep:     284 / 1000At timestep:     302 / 1000At timestep:     319 / 1000At timestep:     356 / 1000At timestep:     372 / 1000At timestep:     406 / 1000At timestep:     423 / 1000At timestep:     437 / 1000At timestep:     470 / 1000At timestep:     490 / 1000At timestep:     511 / 1000At timestep:     533 / 1000At timestep:     559 / 1000At timestep:     573 / 1000At timestep:     593 / 1000At timestep:     620 / 1000At timestep:     644 / 1000At timestep:     666 / 1000At timestep:     686 / 1000At timestep:     703 / 1000At timestep:     725 / 1000At timestep:     746 / 1000At timestep:     769 / 1000At timestep:     788 / 1000At timestep:     809 / 1000At timestep:     825 / 1000At timestep:     845 / 1000At timestep:     862 / 1000At timestep:     880 / 1000At timestep:     897 / 1000At timestep:     925 / 1000At timestep:     939 / 1000At timestep:     966 / 1000At timestep:     981 / 1000At timestep:     999 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -16.899999618530273
Train_BestReturn : -16.899999618530273
TimeSinceStart : 990.8431112766266
Exploration Critic Loss : 31563120.0
Exploitation Critic Loss : 0.08763444423675537
Exploration Model Loss : 16.313400268554688
Actor Loss : 0.49771690368652344
Eval_AverageReturn : -19.73469352722168
Eval_StdReturn : 5.321406364440918
Eval_MaxReturn : -13.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 20.73469387755102
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -17.510000
best mean reward -16.900000
running time 1033.671266
At timestep:     19 / 1000At timestep:     37 / 1000At timestep:     55 / 1000At timestep:     66 / 1000At timestep:     85 / 1000At timestep:     109 / 1000At timestep:     127 / 1000At timestep:     147 / 1000At timestep:     172 / 1000At timestep:     194 / 1000At timestep:     205 / 1000At timestep:     222 / 1000At timestep:     235 / 1000At timestep:     259 / 1000At timestep:     280 / 1000At timestep:     303 / 1000At timestep:     320 / 1000At timestep:     335 / 1000At timestep:     354 / 1000At timestep:     371 / 1000At timestep:     390 / 1000At timestep:     408 / 1000At timestep:     435 / 1000At timestep:     453 / 1000At timestep:     475 / 1000At timestep:     488 / 1000At timestep:     505 / 1000At timestep:     516 / 1000At timestep:     536 / 1000At timestep:     557 / 1000At timestep:     583 / 1000At timestep:     607 / 1000At timestep:     623 / 1000At timestep:     642 / 1000At timestep:     662 / 1000At timestep:     684 / 1000At timestep:     704 / 1000At timestep:     724 / 1000At timestep:     739 / 1000At timestep:     758 / 1000At timestep:     778 / 1000At timestep:     798 / 1000At timestep:     821 / 1000At timestep:     842 / 1000At timestep:     868 / 1000At timestep:     888 / 1000At timestep:     910 / 1000At timestep:     924 / 1000At timestep:     939 / 1000At timestep:     964 / 1000At timestep:     977 / 1000At timestep:     997 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -17.510000228881836
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1033.6712663173676
Exploration Critic Loss : 27274432.0
Exploitation Critic Loss : 0.10306486487388611
Exploration Model Loss : 16.31292724609375
Actor Loss : 0.48019343614578247
Eval_AverageReturn : -18.09433937072754
Eval_StdReturn : 3.92507266998291
Eval_MaxReturn : -10.0
Eval_MinReturn : -26.0
Eval_AverageEpLen : 19.09433962264151
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -17.049999
best mean reward -16.900000
running time 1081.910671
At timestep:     19 / 1000At timestep:     38 / 1000At timestep:     52 / 1000At timestep:     76 / 1000At timestep:     103 / 1000At timestep:     131 / 1000At timestep:     152 / 1000At timestep:     168 / 1000At timestep:     192 / 1000At timestep:     209 / 1000At timestep:     230 / 1000At timestep:     249 / 1000At timestep:     266 / 1000At timestep:     289 / 1000At timestep:     316 / 1000At timestep:     339 / 1000At timestep:     364 / 1000At timestep:     382 / 1000At timestep:     420 / 1000At timestep:     443 / 1000At timestep:     460 / 1000At timestep:     477 / 1000At timestep:     500 / 1000At timestep:     528 / 1000At timestep:     550 / 1000At timestep:     579 / 1000At timestep:     596 / 1000At timestep:     622 / 1000At timestep:     639 / 1000At timestep:     656 / 1000At timestep:     688 / 1000At timestep:     709 / 1000At timestep:     724 / 1000At timestep:     743 / 1000At timestep:     755 / 1000At timestep:     773 / 1000At timestep:     794 / 1000At timestep:     815 / 1000At timestep:     831 / 1000At timestep:     853 / 1000At timestep:     868 / 1000At timestep:     889 / 1000At timestep:     907 / 1000At timestep:     926 / 1000At timestep:     958 / 1000At timestep:     981 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -17.049999237060547
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1081.9106707572937
Exploration Critic Loss : 24851564.0
Exploitation Critic Loss : 0.0706631988286972
Exploration Model Loss : 16.312484741210938
Actor Loss : 0.4798150658607483
Eval_AverageReturn : -20.276596069335938
Eval_StdReturn : 5.172357559204102
Eval_MaxReturn : -11.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 21.27659574468085
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -16.910000
best mean reward -16.900000
running time 1126.627726
At timestep:     21 / 1000At timestep:     55 / 1000At timestep:     73 / 1000At timestep:     97 / 1000At timestep:     109 / 1000At timestep:     124 / 1000At timestep:     150 / 1000At timestep:     171 / 1000At timestep:     186 / 1000At timestep:     196 / 1000At timestep:     216 / 1000At timestep:     236 / 1000At timestep:     251 / 1000At timestep:     263 / 1000At timestep:     288 / 1000At timestep:     308 / 1000At timestep:     326 / 1000At timestep:     350 / 1000At timestep:     370 / 1000At timestep:     388 / 1000At timestep:     405 / 1000At timestep:     423 / 1000At timestep:     441 / 1000At timestep:     462 / 1000At timestep:     483 / 1000At timestep:     500 / 1000At timestep:     522 / 1000At timestep:     540 / 1000At timestep:     556 / 1000At timestep:     576 / 1000At timestep:     593 / 1000At timestep:     621 / 1000At timestep:     648 / 1000At timestep:     671 / 1000At timestep:     691 / 1000At timestep:     708 / 1000At timestep:     724 / 1000At timestep:     738 / 1000At timestep:     763 / 1000At timestep:     780 / 1000At timestep:     817 / 1000At timestep:     844 / 1000At timestep:     865 / 1000At timestep:     898 / 1000At timestep:     916 / 1000At timestep:     936 / 1000At timestep:     964 / 1000At timestep:     983 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -16.90999984741211
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1126.6277258396149
Exploration Critic Loss : 32463644.0
Exploitation Critic Loss : 0.09795643389225006
Exploration Model Loss : 16.312057495117188
Actor Loss : 0.49971911311149597
Eval_AverageReturn : -19.693878173828125
Eval_StdReturn : 5.639525413513184
Eval_MaxReturn : -9.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 20.693877551020407
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -17.360001
best mean reward -16.900000
running time 1170.441517
At timestep:     28 / 1000At timestep:     57 / 1000At timestep:     81 / 1000At timestep:     101 / 1000At timestep:     125 / 1000At timestep:     141 / 1000At timestep:     162 / 1000At timestep:     180 / 1000At timestep:     198 / 1000At timestep:     215 / 1000At timestep:     235 / 1000At timestep:     249 / 1000At timestep:     269 / 1000At timestep:     303 / 1000At timestep:     323 / 1000At timestep:     344 / 1000At timestep:     361 / 1000At timestep:     381 / 1000At timestep:     402 / 1000At timestep:     422 / 1000At timestep:     440 / 1000At timestep:     461 / 1000At timestep:     480 / 1000At timestep:     502 / 1000At timestep:     520 / 1000At timestep:     542 / 1000At timestep:     558 / 1000At timestep:     589 / 1000At timestep:     609 / 1000At timestep:     631 / 1000At timestep:     648 / 1000At timestep:     667 / 1000At timestep:     686 / 1000At timestep:     703 / 1000At timestep:     729 / 1000At timestep:     749 / 1000At timestep:     768 / 1000At timestep:     779 / 1000At timestep:     798 / 1000At timestep:     814 / 1000At timestep:     831 / 1000At timestep:     855 / 1000At timestep:     870 / 1000At timestep:     887 / 1000At timestep:     903 / 1000At timestep:     927 / 1000At timestep:     950 / 1000At timestep:     969 / 1000At timestep:     991 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -17.360000610351562
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1170.4415168762207
Exploration Critic Loss : 30204768.0
Exploitation Critic Loss : 0.16788415610790253
Exploration Model Loss : 16.311538696289062
Actor Loss : 0.4547176659107208
Eval_AverageReturn : -19.18000030517578
Eval_StdReturn : 4.184208393096924
Eval_MaxReturn : -10.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 20.18
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -17.230000
best mean reward -16.900000
running time 1213.952516
At timestep:     22 / 1000At timestep:     40 / 1000At timestep:     55 / 1000At timestep:     79 / 1000At timestep:     100 / 1000At timestep:     119 / 1000At timestep:     138 / 1000At timestep:     153 / 1000At timestep:     166 / 1000At timestep:     179 / 1000At timestep:     196 / 1000At timestep:     214 / 1000At timestep:     234 / 1000At timestep:     257 / 1000At timestep:     277 / 1000At timestep:     295 / 1000At timestep:     313 / 1000At timestep:     333 / 1000At timestep:     354 / 1000At timestep:     380 / 1000At timestep:     398 / 1000At timestep:     418 / 1000At timestep:     437 / 1000At timestep:     464 / 1000At timestep:     480 / 1000At timestep:     502 / 1000At timestep:     523 / 1000At timestep:     539 / 1000At timestep:     559 / 1000At timestep:     584 / 1000At timestep:     601 / 1000At timestep:     621 / 1000At timestep:     641 / 1000At timestep:     662 / 1000At timestep:     680 / 1000At timestep:     709 / 1000At timestep:     730 / 1000At timestep:     749 / 1000At timestep:     766 / 1000At timestep:     788 / 1000At timestep:     814 / 1000At timestep:     830 / 1000At timestep:     844 / 1000At timestep:     865 / 1000At timestep:     897 / 1000At timestep:     921 / 1000At timestep:     963 / 1000At timestep:     977 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -17.229999542236328
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1213.9525163173676
Exploration Critic Loss : 27313630.0
Exploitation Critic Loss : 0.08474180847406387
Exploration Model Loss : 16.311080932617188
Actor Loss : 0.5274562835693359
Eval_AverageReturn : -19.612245559692383
Eval_StdReturn : 5.321563243865967
Eval_MaxReturn : -12.0
Eval_MinReturn : -41.0
Eval_AverageEpLen : 20.612244897959183
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -17.549999
best mean reward -16.900000
running time 1255.860761
At timestep:     22 / 1000At timestep:     46 / 1000At timestep:     71 / 1000At timestep:     85 / 1000At timestep:     106 / 1000At timestep:     124 / 1000At timestep:     147 / 1000At timestep:     169 / 1000At timestep:     187 / 1000At timestep:     204 / 1000At timestep:     223 / 1000At timestep:     244 / 1000At timestep:     259 / 1000At timestep:     278 / 1000At timestep:     304 / 1000At timestep:     321 / 1000At timestep:     334 / 1000At timestep:     355 / 1000At timestep:     372 / 1000At timestep:     385 / 1000At timestep:     400 / 1000At timestep:     421 / 1000At timestep:     440 / 1000At timestep:     461 / 1000At timestep:     491 / 1000At timestep:     513 / 1000At timestep:     534 / 1000At timestep:     551 / 1000At timestep:     569 / 1000At timestep:     585 / 1000At timestep:     602 / 1000At timestep:     628 / 1000At timestep:     641 / 1000At timestep:     657 / 1000At timestep:     678 / 1000At timestep:     709 / 1000At timestep:     731 / 1000At timestep:     751 / 1000At timestep:     770 / 1000At timestep:     789 / 1000At timestep:     805 / 1000At timestep:     825 / 1000At timestep:     844 / 1000At timestep:     856 / 1000At timestep:     886 / 1000At timestep:     912 / 1000At timestep:     941 / 1000At timestep:     958 / 1000At timestep:     981 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -17.549999237060547
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1255.8607609272003
Exploration Critic Loss : 29924948.0
Exploitation Critic Loss : 0.07352226972579956
Exploration Model Loss : 16.310653686523438
Actor Loss : 0.4886194169521332
Eval_AverageReturn : -19.020000457763672
Eval_StdReturn : 4.492171287536621
Eval_MaxReturn : -11.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 20.02
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -17.000000
best mean reward -16.900000
running time 1298.808778
At timestep:     17 / 1000At timestep:     42 / 1000At timestep:     61 / 1000At timestep:     76 / 1000At timestep:     99 / 1000At timestep:     119 / 1000At timestep:     142 / 1000At timestep:     159 / 1000At timestep:     173 / 1000At timestep:     190 / 1000At timestep:     226 / 1000At timestep:     241 / 1000At timestep:     257 / 1000At timestep:     278 / 1000At timestep:     297 / 1000At timestep:     312 / 1000At timestep:     334 / 1000At timestep:     359 / 1000At timestep:     382 / 1000At timestep:     404 / 1000At timestep:     429 / 1000At timestep:     462 / 1000At timestep:     485 / 1000At timestep:     512 / 1000At timestep:     530 / 1000At timestep:     552 / 1000At timestep:     570 / 1000At timestep:     588 / 1000At timestep:     603 / 1000At timestep:     620 / 1000At timestep:     639 / 1000At timestep:     664 / 1000At timestep:     687 / 1000At timestep:     708 / 1000At timestep:     723 / 1000At timestep:     737 / 1000At timestep:     765 / 1000At timestep:     785 / 1000At timestep:     805 / 1000At timestep:     828 / 1000At timestep:     846 / 1000At timestep:     882 / 1000At timestep:     904 / 1000At timestep:     922 / 1000At timestep:     942 / 1000At timestep:     955 / 1000At timestep:     970 / 1000At timestep:     995 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -17.0
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1298.808777809143
Exploration Critic Loss : 34603912.0
Exploitation Critic Loss : 0.1091766431927681
Exploration Model Loss : 16.310165405273438
Actor Loss : 0.5293039679527283
Eval_AverageReturn : -19.67346954345703
Eval_StdReturn : 5.187849998474121
Eval_MaxReturn : -12.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 20.6734693877551
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -17.250000
best mean reward -16.900000
running time 1341.443572
At timestep:     15 / 1000At timestep:     40 / 1000At timestep:     58 / 1000At timestep:     76 / 1000At timestep:     105 / 1000At timestep:     124 / 1000At timestep:     137 / 1000At timestep:     158 / 1000At timestep:     173 / 1000At timestep:     189 / 1000At timestep:     208 / 1000At timestep:     230 / 1000At timestep:     245 / 1000At timestep:     260 / 1000At timestep:     274 / 1000At timestep:     292 / 1000At timestep:     309 / 1000At timestep:     326 / 1000At timestep:     353 / 1000At timestep:     374 / 1000At timestep:     386 / 1000At timestep:     404 / 1000At timestep:     422 / 1000At timestep:     450 / 1000At timestep:     473 / 1000At timestep:     487 / 1000At timestep:     510 / 1000At timestep:     529 / 1000At timestep:     552 / 1000At timestep:     563 / 1000At timestep:     589 / 1000At timestep:     610 / 1000At timestep:     632 / 1000At timestep:     651 / 1000At timestep:     664 / 1000At timestep:     691 / 1000At timestep:     712 / 1000At timestep:     731 / 1000At timestep:     748 / 1000At timestep:     764 / 1000At timestep:     776 / 1000At timestep:     795 / 1000At timestep:     817 / 1000At timestep:     838 / 1000At timestep:     853 / 1000At timestep:     869 / 1000At timestep:     891 / 1000At timestep:     908 / 1000At timestep:     933 / 1000At timestep:     959 / 1000At timestep:     985 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -17.25
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1341.4435720443726
Exploration Critic Loss : 29206864.0
Exploitation Critic Loss : 0.11135641485452652
Exploration Model Loss : 16.309707641601562
Actor Loss : 0.3663925528526306
Eval_AverageReturn : -18.30769157409668
Eval_StdReturn : 4.470150947570801
Eval_MaxReturn : -10.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 19.307692307692307
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -17.590000
best mean reward -16.900000
running time 1384.427835
At timestep:     17 / 1000At timestep:     32 / 1000At timestep:     48 / 1000At timestep:     65 / 1000At timestep:     76 / 1000At timestep:     97 / 1000At timestep:     113 / 1000At timestep:     130 / 1000At timestep:     142 / 1000At timestep:     156 / 1000At timestep:     173 / 1000At timestep:     189 / 1000At timestep:     207 / 1000At timestep:     225 / 1000At timestep:     244 / 1000At timestep:     264 / 1000At timestep:     283 / 1000At timestep:     302 / 1000At timestep:     321 / 1000At timestep:     350 / 1000At timestep:     371 / 1000At timestep:     389 / 1000At timestep:     405 / 1000At timestep:     426 / 1000At timestep:     447 / 1000At timestep:     460 / 1000At timestep:     481 / 1000At timestep:     505 / 1000At timestep:     519 / 1000At timestep:     536 / 1000At timestep:     554 / 1000At timestep:     574 / 1000At timestep:     594 / 1000At timestep:     608 / 1000At timestep:     629 / 1000At timestep:     655 / 1000At timestep:     675 / 1000At timestep:     687 / 1000At timestep:     696 / 1000At timestep:     713 / 1000At timestep:     731 / 1000At timestep:     761 / 1000At timestep:     777 / 1000At timestep:     807 / 1000At timestep:     834 / 1000At timestep:     853 / 1000At timestep:     870 / 1000At timestep:     891 / 1000At timestep:     906 / 1000At timestep:     945 / 1000At timestep:     957 / 1000At timestep:     976 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -17.59000015258789
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1384.4278354644775
Exploration Critic Loss : 31295166.0
Exploitation Critic Loss : 0.07515084743499756
Exploration Model Loss : 16.309249877929688
Actor Loss : 0.48750942945480347
Eval_AverageReturn : -17.90566062927246
Eval_StdReturn : 5.306685447692871
Eval_MaxReturn : -8.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 18.90566037735849
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -17.139999
best mean reward -16.900000
running time 1428.293169
At timestep:     20 / 1000At timestep:     45 / 1000At timestep:     59 / 1000At timestep:     74 / 1000At timestep:     99 / 1000At timestep:     118 / 1000At timestep:     134 / 1000At timestep:     156 / 1000At timestep:     176 / 1000At timestep:     192 / 1000At timestep:     213 / 1000At timestep:     235 / 1000At timestep:     252 / 1000At timestep:     276 / 1000At timestep:     294 / 1000At timestep:     309 / 1000At timestep:     340 / 1000At timestep:     361 / 1000At timestep:     377 / 1000At timestep:     397 / 1000At timestep:     415 / 1000At timestep:     430 / 1000At timestep:     451 / 1000At timestep:     466 / 1000At timestep:     484 / 1000At timestep:     510 / 1000At timestep:     538 / 1000At timestep:     559 / 1000At timestep:     584 / 1000At timestep:     600 / 1000At timestep:     618 / 1000At timestep:     632 / 1000At timestep:     663 / 1000At timestep:     684 / 1000At timestep:     706 / 1000At timestep:     727 / 1000At timestep:     743 / 1000At timestep:     763 / 1000At timestep:     777 / 1000At timestep:     798 / 1000At timestep:     819 / 1000At timestep:     831 / 1000At timestep:     855 / 1000At timestep:     873 / 1000At timestep:     887 / 1000At timestep:     905 / 1000At timestep:     924 / 1000At timestep:     941 / 1000At timestep:     965 / 1000At timestep:     979 / 1000At timestep:     993 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -17.139999389648438
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1428.2931685447693
Exploration Critic Loss : 46567160.0
Exploitation Critic Loss : 0.07782173156738281
Exploration Model Loss : 16.30877685546875
Actor Loss : 0.48818546533584595
Eval_AverageReturn : -18.384614944458008
Eval_StdReturn : 4.372791290283203
Eval_MaxReturn : -11.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 19.384615384615383
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -16.910000
best mean reward -16.900000
running time 1471.905461
At timestep:     15 / 1000At timestep:     32 / 1000At timestep:     46 / 1000At timestep:     66 / 1000At timestep:     83 / 1000At timestep:     104 / 1000At timestep:     127 / 1000At timestep:     138 / 1000At timestep:     160 / 1000At timestep:     176 / 1000At timestep:     193 / 1000At timestep:     217 / 1000At timestep:     230 / 1000At timestep:     246 / 1000At timestep:     276 / 1000At timestep:     295 / 1000At timestep:     313 / 1000At timestep:     332 / 1000At timestep:     349 / 1000At timestep:     364 / 1000At timestep:     385 / 1000At timestep:     401 / 1000At timestep:     416 / 1000At timestep:     437 / 1000At timestep:     457 / 1000At timestep:     479 / 1000At timestep:     499 / 1000At timestep:     515 / 1000At timestep:     532 / 1000At timestep:     543 / 1000At timestep:     557 / 1000At timestep:     573 / 1000At timestep:     591 / 1000At timestep:     615 / 1000At timestep:     629 / 1000At timestep:     644 / 1000At timestep:     662 / 1000At timestep:     678 / 1000At timestep:     695 / 1000At timestep:     708 / 1000At timestep:     739 / 1000At timestep:     761 / 1000At timestep:     785 / 1000At timestep:     808 / 1000At timestep:     825 / 1000At timestep:     852 / 1000At timestep:     871 / 1000At timestep:     893 / 1000At timestep:     911 / 1000At timestep:     940 / 1000At timestep:     958 / 1000At timestep:     979 / 1000At timestep:     997 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -16.90999984741211
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1471.9054610729218
Exploration Critic Loss : 26300156.0
Exploitation Critic Loss : 0.07045502960681915
Exploration Model Loss : 16.308334350585938
Actor Loss : 0.4886794090270996
Eval_AverageReturn : -17.94444465637207
Eval_StdReturn : 4.444791793823242
Eval_MaxReturn : -10.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 18.944444444444443
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -17.209999
best mean reward -16.900000
running time 1515.041532
At timestep:     20 / 1000At timestep:     37 / 1000At timestep:     53 / 1000At timestep:     69 / 1000At timestep:     93 / 1000At timestep:     128 / 1000At timestep:     142 / 1000At timestep:     162 / 1000At timestep:     181 / 1000At timestep:     197 / 1000At timestep:     217 / 1000At timestep:     232 / 1000At timestep:     258 / 1000At timestep:     272 / 1000At timestep:     287 / 1000At timestep:     300 / 1000At timestep:     316 / 1000At timestep:     339 / 1000At timestep:     361 / 1000At timestep:     382 / 1000At timestep:     404 / 1000At timestep:     426 / 1000At timestep:     447 / 1000At timestep:     470 / 1000At timestep:     484 / 1000At timestep:     504 / 1000At timestep:     521 / 1000At timestep:     533 / 1000At timestep:     554 / 1000At timestep:     572 / 1000At timestep:     597 / 1000At timestep:     617 / 1000At timestep:     639 / 1000At timestep:     656 / 1000At timestep:     672 / 1000At timestep:     690 / 1000At timestep:     704 / 1000At timestep:     729 / 1000At timestep:     751 / 1000At timestep:     766 / 1000At timestep:     782 / 1000At timestep:     799 / 1000At timestep:     823 / 1000At timestep:     839 / 1000At timestep:     857 / 1000At timestep:     880 / 1000At timestep:     902 / 1000At timestep:     915 / 1000At timestep:     935 / 1000At timestep:     958 / 1000At timestep:     977 / 1000At timestep:     996 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -17.209999084472656
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1515.0415315628052
Exploration Critic Loss : 30127972.0
Exploitation Critic Loss : 0.08553459495306015
Exploration Model Loss : 16.307907104492188
Actor Loss : 0.4524240493774414
Eval_AverageReturn : -18.188678741455078
Eval_StdReturn : 4.157541751861572
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.18867924528302
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -17.170000
best mean reward -16.900000
running time 1559.405989
At timestep:     24 / 1000At timestep:     43 / 1000At timestep:     64 / 1000At timestep:     80 / 1000At timestep:     97 / 1000At timestep:     113 / 1000At timestep:     123 / 1000At timestep:     145 / 1000At timestep:     162 / 1000At timestep:     179 / 1000At timestep:     206 / 1000At timestep:     230 / 1000At timestep:     244 / 1000At timestep:     257 / 1000At timestep:     273 / 1000At timestep:     286 / 1000At timestep:     313 / 1000At timestep:     334 / 1000At timestep:     346 / 1000At timestep:     364 / 1000At timestep:     386 / 1000At timestep:     408 / 1000At timestep:     432 / 1000At timestep:     448 / 1000At timestep:     466 / 1000At timestep:     485 / 1000At timestep:     506 / 1000At timestep:     527 / 1000At timestep:     549 / 1000At timestep:     567 / 1000At timestep:     584 / 1000At timestep:     605 / 1000At timestep:     621 / 1000At timestep:     639 / 1000At timestep:     666 / 1000At timestep:     682 / 1000At timestep:     708 / 1000At timestep:     727 / 1000At timestep:     749 / 1000At timestep:     773 / 1000At timestep:     789 / 1000At timestep:     810 / 1000At timestep:     834 / 1000At timestep:     859 / 1000At timestep:     886 / 1000At timestep:     916 / 1000At timestep:     937 / 1000At timestep:     961 / 1000At timestep:     977 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -17.170000076293945
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1559.4059889316559
Exploration Critic Loss : 35976016.0
Exploitation Critic Loss : 0.13704945147037506
Exploration Model Loss : 16.30743408203125
Actor Loss : 0.5684689283370972
Eval_AverageReturn : -19.0
Eval_StdReturn : 4.404542922973633
Eval_MaxReturn : -9.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 20.0
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -17.209999
best mean reward -16.900000
running time 1602.350650
At timestep:     44 / 1000At timestep:     61 / 1000At timestep:     84 / 1000At timestep:     99 / 1000At timestep:     119 / 1000At timestep:     136 / 1000At timestep:     155 / 1000At timestep:     178 / 1000At timestep:     192 / 1000At timestep:     213 / 1000At timestep:     230 / 1000At timestep:     242 / 1000At timestep:     259 / 1000At timestep:     280 / 1000At timestep:     293 / 1000At timestep:     306 / 1000At timestep:     329 / 1000At timestep:     352 / 1000At timestep:     368 / 1000At timestep:     385 / 1000At timestep:     396 / 1000At timestep:     414 / 1000At timestep:     436 / 1000At timestep:     462 / 1000At timestep:     486 / 1000At timestep:     507 / 1000At timestep:     524 / 1000At timestep:     550 / 1000At timestep:     565 / 1000At timestep:     590 / 1000At timestep:     616 / 1000At timestep:     641 / 1000At timestep:     657 / 1000At timestep:     676 / 1000At timestep:     691 / 1000At timestep:     709 / 1000At timestep:     731 / 1000At timestep:     751 / 1000At timestep:     766 / 1000At timestep:     787 / 1000At timestep:     813 / 1000At timestep:     833 / 1000At timestep:     848 / 1000At timestep:     872 / 1000At timestep:     886 / 1000At timestep:     905 / 1000At timestep:     929 / 1000At timestep:     949 / 1000At timestep:     971 / 1000At timestep:     988 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -17.209999084472656
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1602.3506500720978
Exploration Critic Loss : 34985632.0
Exploitation Critic Loss : 0.11762251704931259
Exploration Model Loss : 16.306991577148438
Actor Loss : 0.5129988193511963
Eval_AverageReturn : -19.078432083129883
Eval_StdReturn : 5.725221633911133
Eval_MaxReturn : -10.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 20.07843137254902
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -17.190001
best mean reward -16.900000
running time 1644.937142
At timestep:     20 / 1000At timestep:     42 / 1000At timestep:     62 / 1000At timestep:     77 / 1000At timestep:     110 / 1000At timestep:     140 / 1000At timestep:     156 / 1000At timestep:     174 / 1000At timestep:     193 / 1000At timestep:     213 / 1000At timestep:     234 / 1000At timestep:     243 / 1000At timestep:     265 / 1000At timestep:     286 / 1000At timestep:     306 / 1000At timestep:     324 / 1000At timestep:     349 / 1000At timestep:     367 / 1000At timestep:     393 / 1000At timestep:     411 / 1000At timestep:     435 / 1000At timestep:     451 / 1000At timestep:     468 / 1000At timestep:     490 / 1000At timestep:     505 / 1000At timestep:     537 / 1000At timestep:     555 / 1000At timestep:     578 / 1000At timestep:     605 / 1000At timestep:     634 / 1000At timestep:     663 / 1000At timestep:     682 / 1000At timestep:     706 / 1000At timestep:     727 / 1000At timestep:     743 / 1000At timestep:     757 / 1000At timestep:     778 / 1000At timestep:     805 / 1000At timestep:     821 / 1000At timestep:     847 / 1000At timestep:     863 / 1000At timestep:     880 / 1000At timestep:     910 / 1000At timestep:     923 / 1000At timestep:     938 / 1000At timestep:     957 / 1000At timestep:     980 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -17.190000534057617
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1644.9371421337128
Exploration Critic Loss : 30600016.0
Exploitation Critic Loss : 0.08248968422412872
Exploration Model Loss : 16.306503295898438
Actor Loss : 0.5515356063842773
Eval_AverageReturn : -19.85416603088379
Eval_StdReturn : 5.212123870849609
Eval_MaxReturn : -8.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 20.854166666666668
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -17.500000
best mean reward -16.900000
running time 1687.816837
At timestep:     19 / 1000At timestep:     37 / 1000At timestep:     56 / 1000At timestep:     71 / 1000At timestep:     90 / 1000At timestep:     112 / 1000At timestep:     129 / 1000At timestep:     149 / 1000At timestep:     163 / 1000At timestep:     183 / 1000At timestep:     201 / 1000At timestep:     219 / 1000At timestep:     237 / 1000At timestep:     255 / 1000At timestep:     276 / 1000At timestep:     292 / 1000At timestep:     314 / 1000At timestep:     330 / 1000At timestep:     354 / 1000At timestep:     370 / 1000At timestep:     391 / 1000At timestep:     414 / 1000At timestep:     430 / 1000At timestep:     448 / 1000At timestep:     470 / 1000At timestep:     488 / 1000At timestep:     507 / 1000At timestep:     521 / 1000At timestep:     537 / 1000At timestep:     554 / 1000At timestep:     576 / 1000At timestep:     591 / 1000At timestep:     607 / 1000At timestep:     629 / 1000At timestep:     654 / 1000At timestep:     679 / 1000At timestep:     708 / 1000At timestep:     735 / 1000At timestep:     752 / 1000At timestep:     766 / 1000At timestep:     782 / 1000At timestep:     816 / 1000At timestep:     850 / 1000At timestep:     873 / 1000At timestep:     892 / 1000At timestep:     907 / 1000At timestep:     926 / 1000At timestep:     944 / 1000At timestep:     964 / 1000At timestep:     984 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -17.5
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1687.816837310791
Exploration Critic Loss : 42283776.0
Exploitation Critic Loss : 0.07714678347110748
Exploration Model Loss : 16.306076049804688
Actor Loss : 0.422799289226532
Eval_AverageReturn : -18.6862735748291
Eval_StdReturn : 4.390231132507324
Eval_MaxReturn : -13.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 19.686274509803923
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -17.570000
best mean reward -16.900000
running time 1730.993328
At timestep:     21 / 1000At timestep:     37 / 1000At timestep:     54 / 1000At timestep:     74 / 1000At timestep:     94 / 1000At timestep:     106 / 1000At timestep:     124 / 1000At timestep:     149 / 1000At timestep:     167 / 1000At timestep:     185 / 1000At timestep:     201 / 1000At timestep:     220 / 1000At timestep:     235 / 1000At timestep:     264 / 1000At timestep:     287 / 1000At timestep:     304 / 1000At timestep:     325 / 1000At timestep:     345 / 1000At timestep:     365 / 1000At timestep:     386 / 1000At timestep:     401 / 1000At timestep:     412 / 1000At timestep:     433 / 1000At timestep:     459 / 1000At timestep:     482 / 1000At timestep:     500 / 1000At timestep:     520 / 1000At timestep:     540 / 1000At timestep:     556 / 1000At timestep:     572 / 1000At timestep:     586 / 1000At timestep:     610 / 1000At timestep:     625 / 1000At timestep:     641 / 1000At timestep:     668 / 1000At timestep:     689 / 1000At timestep:     704 / 1000At timestep:     716 / 1000At timestep:     734 / 1000At timestep:     752 / 1000At timestep:     773 / 1000At timestep:     792 / 1000At timestep:     819 / 1000At timestep:     840 / 1000At timestep:     860 / 1000At timestep:     884 / 1000At timestep:     913 / 1000At timestep:     934 / 1000At timestep:     958 / 1000At timestep:     984 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -17.56999969482422
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1730.9933276176453
Exploration Critic Loss : 38968656.0
Exploitation Critic Loss : 0.11130893975496292
Exploration Model Loss : 16.30560302734375
Actor Loss : 0.4922540783882141
Eval_AverageReturn : -18.627450942993164
Eval_StdReturn : 4.205322265625
Eval_MaxReturn : -10.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 19.627450980392158
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -17.209999
best mean reward -16.900000
running time 1775.166350
At timestep:     15 / 1000At timestep:     31 / 1000At timestep:     47 / 1000At timestep:     75 / 1000At timestep:     90 / 1000At timestep:     109 / 1000At timestep:     122 / 1000At timestep:     151 / 1000At timestep:     166 / 1000At timestep:     184 / 1000At timestep:     205 / 1000At timestep:     223 / 1000At timestep:     241 / 1000At timestep:     257 / 1000At timestep:     275 / 1000At timestep:     297 / 1000At timestep:     314 / 1000At timestep:     329 / 1000At timestep:     353 / 1000At timestep:     368 / 1000At timestep:     385 / 1000At timestep:     403 / 1000At timestep:     426 / 1000At timestep:     452 / 1000At timestep:     483 / 1000At timestep:     503 / 1000At timestep:     523 / 1000At timestep:     539 / 1000At timestep:     558 / 1000At timestep:     582 / 1000At timestep:     605 / 1000At timestep:     628 / 1000At timestep:     648 / 1000At timestep:     670 / 1000At timestep:     686 / 1000At timestep:     703 / 1000At timestep:     717 / 1000At timestep:     741 / 1000At timestep:     763 / 1000At timestep:     782 / 1000At timestep:     812 / 1000At timestep:     826 / 1000At timestep:     853 / 1000At timestep:     872 / 1000At timestep:     892 / 1000At timestep:     914 / 1000At timestep:     931 / 1000At timestep:     949 / 1000At timestep:     966 / 1000At timestep:     990 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -17.209999084472656
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1775.1663501262665
Exploration Critic Loss : 40548616.0
Exploitation Critic Loss : 0.0792454406619072
Exploration Model Loss : 16.305130004882812
Actor Loss : 0.4664488434791565
Eval_AverageReturn : -18.784313201904297
Eval_StdReturn : 4.330981731414795
Eval_MaxReturn : -12.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 19.784313725490197
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -17.030001
best mean reward -16.900000
running time 1819.266421
At timestep:     22 / 1000At timestep:     42 / 1000At timestep:     64 / 1000At timestep:     83 / 1000At timestep:     100 / 1000At timestep:     124 / 1000At timestep:     140 / 1000At timestep:     167 / 1000At timestep:     183 / 1000At timestep:     198 / 1000At timestep:     219 / 1000At timestep:     247 / 1000At timestep:     261 / 1000At timestep:     277 / 1000At timestep:     290 / 1000At timestep:     314 / 1000At timestep:     335 / 1000At timestep:     358 / 1000At timestep:     374 / 1000At timestep:     393 / 1000At timestep:     409 / 1000At timestep:     425 / 1000At timestep:     445 / 1000At timestep:     458 / 1000At timestep:     476 / 1000At timestep:     498 / 1000At timestep:     518 / 1000At timestep:     541 / 1000At timestep:     562 / 1000At timestep:     592 / 1000At timestep:     608 / 1000At timestep:     627 / 1000At timestep:     642 / 1000At timestep:     667 / 1000At timestep:     698 / 1000At timestep:     721 / 1000At timestep:     738 / 1000At timestep:     752 / 1000At timestep:     772 / 1000At timestep:     786 / 1000At timestep:     808 / 1000At timestep:     827 / 1000At timestep:     839 / 1000At timestep:     856 / 1000At timestep:     876 / 1000At timestep:     898 / 1000At timestep:     914 / 1000At timestep:     933 / 1000At timestep:     955 / 1000At timestep:     978 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -17.030000686645508
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1819.2664213180542
Exploration Critic Loss : 44020256.0
Exploitation Critic Loss : 0.07999129593372345
Exploration Model Loss : 16.304672241210938
Actor Loss : 0.46999281644821167
Eval_AverageReturn : -18.843137741088867
Eval_StdReturn : 4.7128214836120605
Eval_MaxReturn : -11.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 19.84313725490196
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -17.820000
best mean reward -16.900000
running time 1862.662908
At timestep:     17 / 1000At timestep:     31 / 1000At timestep:     46 / 1000At timestep:     61 / 1000At timestep:     79 / 1000At timestep:     99 / 1000At timestep:     119 / 1000At timestep:     142 / 1000At timestep:     157 / 1000At timestep:     178 / 1000At timestep:     193 / 1000At timestep:     214 / 1000At timestep:     233 / 1000At timestep:     250 / 1000At timestep:     275 / 1000At timestep:     290 / 1000At timestep:     313 / 1000At timestep:     334 / 1000At timestep:     354 / 1000At timestep:     375 / 1000At timestep:     390 / 1000At timestep:     417 / 1000At timestep:     437 / 1000At timestep:     462 / 1000At timestep:     475 / 1000At timestep:     503 / 1000At timestep:     522 / 1000At timestep:     542 / 1000At timestep:     555 / 1000At timestep:     576 / 1000At timestep:     593 / 1000At timestep:     610 / 1000At timestep:     631 / 1000At timestep:     652 / 1000At timestep:     671 / 1000At timestep:     687 / 1000At timestep:     705 / 1000At timestep:     734 / 1000At timestep:     754 / 1000At timestep:     775 / 1000At timestep:     797 / 1000At timestep:     822 / 1000At timestep:     835 / 1000At timestep:     859 / 1000At timestep:     876 / 1000At timestep:     894 / 1000At timestep:     912 / 1000At timestep:     925 / 1000At timestep:     946 / 1000At timestep:     975 / 1000At timestep:     990 / 1000At timestep:     1015 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_easy_unsupervised_lam0.1_PointmassEasy-v0_22-11-2022_20-21-48/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -17.81999969482422
Train_BestReturn : -16.899999618530273
TimeSinceStart : 1862.6629076004028
Exploration Critic Loss : 34386616.0
Exploitation Critic Loss : 0.07029054313898087
Exploration Model Loss : 16.304275512695312
Actor Loss : 0.4343610107898712
Eval_AverageReturn : -18.519229888916016
Eval_StdReturn : 4.1532673835754395
Eval_MaxReturn : -12.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 19.51923076923077
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam1_PointmassEasy-v0_22-11-2022_20-53-37 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam1_PointmassEasy-v0_22-11-2022_20-53-37
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002497
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.002497434616088867
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 12.237350
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -50.0
TimeSinceStart : 12.23734974861145
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 24.626589
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -50.0
TimeSinceStart : 24.62658929824829
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 55.301188
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -50.0
TimeSinceStart : 55.30118799209595
Exploration Critic Loss : 347698.4375
Exploitation Critic Loss : 0.05177082121372223
Exploration Model Loss : 20.40058135986328
Actor Loss : 1.1819759607315063
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -47.783131
best mean reward -inf
running time 86.600096
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     475 / 1000At timestep:     525 / 1000At timestep:     575 / 1000At timestep:     603 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -47.78313064575195
TimeSinceStart : 86.60009598731995
Exploration Critic Loss : 7365087.0
Exploitation Critic Loss : 0.11951542645692825
Exploration Model Loss : 35.12671661376953
Actor Loss : 0.9265903234481812
Eval_AverageReturn : -47.47618865966797
Eval_StdReturn : 7.1953630447387695
Eval_MaxReturn : -24.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 47.61904761904762
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -39.009998
best mean reward -39.009998
running time 120.540486
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     141 / 1000At timestep:     174 / 1000At timestep:     224 / 1000At timestep:     274 / 1000At timestep:     324 / 1000At timestep:     374 / 1000At timestep:     424 / 1000At timestep:     474 / 1000At timestep:     524 / 1000At timestep:     574 / 1000At timestep:     624 / 1000At timestep:     674 / 1000At timestep:     724 / 1000At timestep:     762 / 1000At timestep:     799 / 1000At timestep:     849 / 1000At timestep:     885 / 1000At timestep:     935 / 1000At timestep:     967 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -39.0099983215332
Train_BestReturn : -39.0099983215332
TimeSinceStart : 120.5404863357544
Exploration Critic Loss : 21909328.0
Exploitation Critic Loss : 0.10243327915668488
Exploration Model Loss : 97.82394409179688
Actor Loss : 0.8092109560966492
Eval_AverageReturn : -45.95454406738281
Eval_StdReturn : 6.7922234535217285
Eval_MaxReturn : -31.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 46.22727272727273
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -29.420000
best mean reward -29.420000
running time 155.321298
At timestep:     50 / 1000At timestep:     89 / 1000At timestep:     121 / 1000At timestep:     171 / 1000At timestep:     211 / 1000At timestep:     228 / 1000At timestep:     274 / 1000At timestep:     324 / 1000At timestep:     374 / 1000At timestep:     420 / 1000At timestep:     470 / 1000At timestep:     499 / 1000At timestep:     529 / 1000At timestep:     574 / 1000At timestep:     603 / 1000At timestep:     653 / 1000At timestep:     703 / 1000At timestep:     753 / 1000At timestep:     781 / 1000At timestep:     831 / 1000At timestep:     857 / 1000At timestep:     892 / 1000At timestep:     942 / 1000At timestep:     992 / 1000At timestep:     1037 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -29.420000076293945
Train_BestReturn : -29.420000076293945
TimeSinceStart : 155.321298122406
Exploration Critic Loss : 18588020.0
Exploitation Critic Loss : 0.1692618876695633
Exploration Model Loss : 67.74283599853516
Actor Loss : 0.6371978521347046
Eval_AverageReturn : -40.91999816894531
Eval_StdReturn : 10.318604469299316
Eval_MaxReturn : -16.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 41.48
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -22.309999
best mean reward -22.309999
running time 191.835628
At timestep:     30 / 1000At timestep:     80 / 1000At timestep:     119 / 1000At timestep:     151 / 1000At timestep:     175 / 1000At timestep:     219 / 1000At timestep:     255 / 1000At timestep:     285 / 1000At timestep:     328 / 1000At timestep:     356 / 1000At timestep:     391 / 1000At timestep:     441 / 1000At timestep:     474 / 1000At timestep:     508 / 1000At timestep:     554 / 1000At timestep:     580 / 1000At timestep:     622 / 1000At timestep:     654 / 1000At timestep:     692 / 1000At timestep:     707 / 1000At timestep:     739 / 1000At timestep:     767 / 1000At timestep:     808 / 1000At timestep:     834 / 1000At timestep:     880 / 1000At timestep:     905 / 1000At timestep:     955 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -22.309999465942383
Train_BestReturn : -22.309999465942383
TimeSinceStart : 191.83562779426575
Exploration Critic Loss : 9973940.0
Exploitation Critic Loss : 0.19401271641254425
Exploration Model Loss : 48.65118408203125
Actor Loss : 0.6105352640151978
Eval_AverageReturn : -35.0
Eval_StdReturn : 9.308215141296387
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 35.892857142857146
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -19.889999
best mean reward -19.889999
running time 229.720455
At timestep:     19 / 1000At timestep:     69 / 1000At timestep:     119 / 1000At timestep:     157 / 1000At timestep:     192 / 1000At timestep:     219 / 1000At timestep:     261 / 1000At timestep:     304 / 1000At timestep:     354 / 1000At timestep:     404 / 1000At timestep:     442 / 1000At timestep:     487 / 1000At timestep:     514 / 1000At timestep:     564 / 1000At timestep:     592 / 1000At timestep:     623 / 1000At timestep:     663 / 1000At timestep:     688 / 1000At timestep:     716 / 1000At timestep:     762 / 1000At timestep:     792 / 1000At timestep:     834 / 1000At timestep:     870 / 1000At timestep:     920 / 1000At timestep:     946 / 1000At timestep:     996 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -19.889999389648438
Train_BestReturn : -19.889999389648438
TimeSinceStart : 229.7204554080963
Exploration Critic Loss : 7718932.5
Exploitation Critic Loss : 0.1542331725358963
Exploration Model Loss : 38.73414611816406
Actor Loss : 0.6092149019241333
Eval_AverageReturn : -37.185184478759766
Eval_StdReturn : 9.914587020874023
Eval_MaxReturn : -18.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 37.96296296296296
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -18.309999
best mean reward -18.309999
running time 268.868070
At timestep:     31 / 1000At timestep:     69 / 1000At timestep:     113 / 1000At timestep:     150 / 1000At timestep:     163 / 1000At timestep:     181 / 1000At timestep:     223 / 1000At timestep:     270 / 1000At timestep:     297 / 1000At timestep:     323 / 1000At timestep:     345 / 1000At timestep:     367 / 1000At timestep:     406 / 1000At timestep:     451 / 1000At timestep:     485 / 1000At timestep:     509 / 1000At timestep:     527 / 1000At timestep:     568 / 1000At timestep:     593 / 1000At timestep:     621 / 1000At timestep:     646 / 1000At timestep:     667 / 1000At timestep:     711 / 1000At timestep:     750 / 1000At timestep:     769 / 1000At timestep:     791 / 1000At timestep:     824 / 1000At timestep:     846 / 1000At timestep:     873 / 1000At timestep:     919 / 1000At timestep:     962 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -18.309999465942383
Train_BestReturn : -18.309999465942383
TimeSinceStart : 268.8680703639984
Exploration Critic Loss : 12106468.0
Exploitation Critic Loss : 0.13508304953575134
Exploration Model Loss : 43.168052673339844
Actor Loss : 0.5958755612373352
Eval_AverageReturn : -30.53125
Eval_StdReturn : 10.108735084533691
Eval_MaxReturn : -12.0
Eval_MinReturn : -46.0
Eval_AverageEpLen : 31.53125
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -18.299999
best mean reward -18.299999
running time 307.959489
At timestep:     17 / 1000At timestep:     35 / 1000At timestep:     60 / 1000At timestep:     91 / 1000At timestep:     118 / 1000At timestep:     137 / 1000At timestep:     154 / 1000At timestep:     179 / 1000At timestep:     215 / 1000At timestep:     241 / 1000At timestep:     260 / 1000At timestep:     280 / 1000At timestep:     302 / 1000At timestep:     321 / 1000At timestep:     349 / 1000At timestep:     366 / 1000At timestep:     389 / 1000At timestep:     405 / 1000At timestep:     423 / 1000At timestep:     446 / 1000At timestep:     485 / 1000At timestep:     526 / 1000At timestep:     551 / 1000At timestep:     590 / 1000At timestep:     621 / 1000At timestep:     644 / 1000At timestep:     683 / 1000At timestep:     721 / 1000At timestep:     755 / 1000At timestep:     780 / 1000At timestep:     804 / 1000At timestep:     833 / 1000At timestep:     856 / 1000At timestep:     883 / 1000At timestep:     911 / 1000At timestep:     961 / 1000At timestep:     977 / 1000At timestep:     997 / 1000At timestep:     1037 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -18.299999237060547
Train_BestReturn : -18.299999237060547
TimeSinceStart : 307.9594888687134
Exploration Critic Loss : 9562941.0
Exploitation Critic Loss : 0.1444314569234848
Exploration Model Loss : 35.62554931640625
Actor Loss : 0.6259066462516785
Eval_AverageReturn : -25.615385055541992
Eval_StdReturn : 8.42041301727295
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 26.58974358974359
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -17.830000
best mean reward -17.830000
running time 349.237290
At timestep:     25 / 1000At timestep:     48 / 1000At timestep:     70 / 1000At timestep:     91 / 1000At timestep:     110 / 1000At timestep:     131 / 1000At timestep:     146 / 1000At timestep:     174 / 1000At timestep:     199 / 1000At timestep:     226 / 1000At timestep:     245 / 1000At timestep:     270 / 1000At timestep:     319 / 1000At timestep:     343 / 1000At timestep:     360 / 1000At timestep:     380 / 1000At timestep:     393 / 1000At timestep:     412 / 1000At timestep:     434 / 1000At timestep:     448 / 1000At timestep:     477 / 1000At timestep:     499 / 1000At timestep:     525 / 1000At timestep:     546 / 1000At timestep:     592 / 1000At timestep:     627 / 1000At timestep:     659 / 1000At timestep:     691 / 1000At timestep:     723 / 1000At timestep:     746 / 1000At timestep:     765 / 1000At timestep:     806 / 1000At timestep:     834 / 1000At timestep:     867 / 1000At timestep:     895 / 1000At timestep:     914 / 1000At timestep:     942 / 1000At timestep:     965 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -17.829999923706055
Train_BestReturn : -17.829999923706055
TimeSinceStart : 349.2372896671295
Exploration Critic Loss : 8555094.0
Exploitation Critic Loss : 0.08431088924407959
Exploration Model Loss : 53.003990173339844
Actor Loss : 0.5989220142364502
Eval_AverageReturn : -24.923076629638672
Eval_StdReturn : 8.428841590881348
Eval_MaxReturn : -12.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 25.923076923076923
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -17.320000
best mean reward -17.320000
running time 389.673759
At timestep:     34 / 1000At timestep:     63 / 1000At timestep:     87 / 1000At timestep:     132 / 1000At timestep:     172 / 1000At timestep:     199 / 1000At timestep:     231 / 1000At timestep:     250 / 1000At timestep:     273 / 1000At timestep:     304 / 1000At timestep:     333 / 1000At timestep:     361 / 1000At timestep:     373 / 1000At timestep:     405 / 1000At timestep:     425 / 1000At timestep:     451 / 1000At timestep:     475 / 1000At timestep:     496 / 1000At timestep:     521 / 1000At timestep:     547 / 1000At timestep:     563 / 1000At timestep:     591 / 1000At timestep:     607 / 1000At timestep:     621 / 1000At timestep:     641 / 1000At timestep:     665 / 1000At timestep:     677 / 1000At timestep:     707 / 1000At timestep:     728 / 1000At timestep:     756 / 1000At timestep:     781 / 1000At timestep:     808 / 1000At timestep:     825 / 1000At timestep:     873 / 1000At timestep:     895 / 1000At timestep:     926 / 1000At timestep:     954 / 1000At timestep:     974 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -17.31999969482422
Train_BestReturn : -17.31999969482422
TimeSinceStart : 389.67375922203064
Exploration Critic Loss : 12206734.0
Exploitation Critic Loss : 0.07774669677019119
Exploration Model Loss : 53.26133346557617
Actor Loss : 0.6248781681060791
Eval_AverageReturn : -24.897436141967773
Eval_StdReturn : 7.912322998046875
Eval_MaxReturn : -11.0
Eval_MinReturn : -47.0
Eval_AverageEpLen : 25.897435897435898
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -17.360001
best mean reward -17.320000
running time 432.781689
At timestep:     16 / 1000At timestep:     37 / 1000At timestep:     65 / 1000At timestep:     107 / 1000At timestep:     139 / 1000At timestep:     168 / 1000At timestep:     193 / 1000At timestep:     207 / 1000At timestep:     231 / 1000At timestep:     256 / 1000At timestep:     273 / 1000At timestep:     290 / 1000At timestep:     311 / 1000At timestep:     358 / 1000At timestep:     391 / 1000At timestep:     420 / 1000At timestep:     439 / 1000At timestep:     459 / 1000At timestep:     478 / 1000At timestep:     506 / 1000At timestep:     526 / 1000At timestep:     549 / 1000At timestep:     575 / 1000At timestep:     599 / 1000At timestep:     638 / 1000At timestep:     666 / 1000At timestep:     694 / 1000At timestep:     712 / 1000At timestep:     759 / 1000At timestep:     775 / 1000At timestep:     794 / 1000At timestep:     825 / 1000At timestep:     851 / 1000At timestep:     891 / 1000At timestep:     920 / 1000At timestep:     937 / 1000At timestep:     960 / 1000At timestep:     985 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -17.360000610351562
Train_BestReturn : -17.31999969482422
TimeSinceStart : 432.78168869018555
Exploration Critic Loss : 9663226.0
Exploitation Critic Loss : 0.07082623988389969
Exploration Model Loss : 23.182571411132812
Actor Loss : 0.6255751252174377
Eval_AverageReturn : -24.769229888916016
Eval_StdReturn : 8.225855827331543
Eval_MaxReturn : -13.0
Eval_MinReturn : -46.0
Eval_AverageEpLen : 25.76923076923077
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -17.340000
best mean reward -17.320000
running time 474.419630
At timestep:     26 / 1000At timestep:     47 / 1000At timestep:     63 / 1000At timestep:     86 / 1000At timestep:     103 / 1000At timestep:     122 / 1000At timestep:     136 / 1000At timestep:     152 / 1000At timestep:     174 / 1000At timestep:     193 / 1000At timestep:     215 / 1000At timestep:     236 / 1000At timestep:     268 / 1000At timestep:     305 / 1000At timestep:     337 / 1000At timestep:     358 / 1000At timestep:     374 / 1000At timestep:     405 / 1000At timestep:     427 / 1000At timestep:     457 / 1000At timestep:     475 / 1000At timestep:     493 / 1000At timestep:     512 / 1000At timestep:     555 / 1000At timestep:     568 / 1000At timestep:     586 / 1000At timestep:     604 / 1000At timestep:     628 / 1000At timestep:     653 / 1000At timestep:     670 / 1000At timestep:     686 / 1000At timestep:     729 / 1000At timestep:     741 / 1000At timestep:     786 / 1000At timestep:     805 / 1000At timestep:     819 / 1000At timestep:     848 / 1000At timestep:     879 / 1000At timestep:     905 / 1000At timestep:     923 / 1000At timestep:     937 / 1000At timestep:     955 / 1000At timestep:     978 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -17.34000015258789
Train_BestReturn : -17.31999969482422
TimeSinceStart : 474.41963028907776
Exploration Critic Loss : 5848278.5
Exploitation Critic Loss : 0.16238734126091003
Exploration Model Loss : 32.85044860839844
Actor Loss : 0.6616576910018921
Eval_AverageReturn : -22.136363983154297
Eval_StdReturn : 8.430439949035645
Eval_MaxReturn : -11.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 23.136363636363637
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -17.450001
best mean reward -17.320000
running time 517.122381
At timestep:     41 / 1000At timestep:     62 / 1000At timestep:     107 / 1000At timestep:     129 / 1000At timestep:     152 / 1000At timestep:     168 / 1000At timestep:     191 / 1000At timestep:     228 / 1000At timestep:     254 / 1000At timestep:     293 / 1000At timestep:     308 / 1000At timestep:     320 / 1000At timestep:     357 / 1000At timestep:     403 / 1000At timestep:     430 / 1000At timestep:     477 / 1000At timestep:     494 / 1000At timestep:     528 / 1000At timestep:     578 / 1000At timestep:     595 / 1000At timestep:     624 / 1000At timestep:     648 / 1000At timestep:     669 / 1000At timestep:     699 / 1000At timestep:     734 / 1000At timestep:     751 / 1000At timestep:     772 / 1000At timestep:     803 / 1000At timestep:     823 / 1000At timestep:     846 / 1000At timestep:     885 / 1000At timestep:     903 / 1000At timestep:     938 / 1000At timestep:     957 / 1000At timestep:     973 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -17.450000762939453
Train_BestReturn : -17.31999969482422
TimeSinceStart : 517.12238073349
Exploration Critic Loss : 8047716.0
Exploitation Critic Loss : 0.136127308011055
Exploration Model Loss : 37.79325866699219
Actor Loss : 0.6619351506233215
Eval_AverageReturn : -27.47222137451172
Eval_StdReturn : 10.986066818237305
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 28.416666666666668
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -17.080000
best mean reward -17.080000
running time 559.217522
At timestep:     43 / 1000At timestep:     64 / 1000At timestep:     87 / 1000At timestep:     107 / 1000At timestep:     137 / 1000At timestep:     155 / 1000At timestep:     179 / 1000At timestep:     207 / 1000At timestep:     236 / 1000At timestep:     254 / 1000At timestep:     270 / 1000At timestep:     288 / 1000At timestep:     302 / 1000At timestep:     327 / 1000At timestep:     341 / 1000At timestep:     364 / 1000At timestep:     381 / 1000At timestep:     397 / 1000At timestep:     441 / 1000At timestep:     473 / 1000At timestep:     508 / 1000At timestep:     540 / 1000At timestep:     568 / 1000At timestep:     581 / 1000At timestep:     600 / 1000At timestep:     621 / 1000At timestep:     634 / 1000At timestep:     664 / 1000At timestep:     684 / 1000At timestep:     710 / 1000At timestep:     734 / 1000At timestep:     784 / 1000At timestep:     804 / 1000At timestep:     830 / 1000At timestep:     856 / 1000At timestep:     879 / 1000At timestep:     917 / 1000At timestep:     933 / 1000At timestep:     964 / 1000At timestep:     983 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -17.079999923706055
Train_BestReturn : -17.079999923706055
TimeSinceStart : 559.2175221443176
Exploration Critic Loss : 11845786.0
Exploitation Critic Loss : 0.10027901083230972
Exploration Model Loss : 54.1170654296875
Actor Loss : 0.5950772166252136
Eval_AverageReturn : -23.85365867614746
Eval_StdReturn : 8.733342170715332
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 24.829268292682926
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -17.559999
best mean reward -17.080000
running time 600.547116
At timestep:     21 / 1000At timestep:     33 / 1000At timestep:     56 / 1000At timestep:     101 / 1000At timestep:     124 / 1000At timestep:     145 / 1000At timestep:     169 / 1000At timestep:     199 / 1000At timestep:     219 / 1000At timestep:     248 / 1000At timestep:     275 / 1000At timestep:     289 / 1000At timestep:     310 / 1000At timestep:     344 / 1000At timestep:     364 / 1000At timestep:     407 / 1000At timestep:     421 / 1000At timestep:     456 / 1000At timestep:     470 / 1000At timestep:     490 / 1000At timestep:     510 / 1000At timestep:     530 / 1000At timestep:     549 / 1000At timestep:     567 / 1000At timestep:     592 / 1000At timestep:     614 / 1000At timestep:     647 / 1000At timestep:     683 / 1000At timestep:     714 / 1000At timestep:     735 / 1000At timestep:     750 / 1000At timestep:     777 / 1000At timestep:     794 / 1000At timestep:     811 / 1000At timestep:     834 / 1000At timestep:     851 / 1000At timestep:     880 / 1000At timestep:     908 / 1000At timestep:     947 / 1000At timestep:     962 / 1000At timestep:     983 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -17.559999465942383
Train_BestReturn : -17.079999923706055
TimeSinceStart : 600.5471158027649
Exploration Critic Loss : 6627920.0
Exploitation Critic Loss : 0.0795702189207077
Exploration Model Loss : 26.701080322265625
Actor Loss : 0.552054762840271
Eval_AverageReturn : -23.0238094329834
Eval_StdReturn : 7.857323169708252
Eval_MaxReturn : -11.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 24.023809523809526
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -17.860001
best mean reward -17.080000
running time 643.024748
At timestep:     18 / 1000At timestep:     34 / 1000At timestep:     72 / 1000At timestep:     106 / 1000At timestep:     125 / 1000At timestep:     147 / 1000At timestep:     166 / 1000At timestep:     187 / 1000At timestep:     205 / 1000At timestep:     234 / 1000At timestep:     261 / 1000At timestep:     283 / 1000At timestep:     308 / 1000At timestep:     332 / 1000At timestep:     348 / 1000At timestep:     383 / 1000At timestep:     402 / 1000At timestep:     418 / 1000At timestep:     438 / 1000At timestep:     461 / 1000At timestep:     486 / 1000At timestep:     502 / 1000At timestep:     537 / 1000At timestep:     566 / 1000At timestep:     582 / 1000At timestep:     602 / 1000At timestep:     634 / 1000At timestep:     652 / 1000At timestep:     666 / 1000At timestep:     687 / 1000At timestep:     711 / 1000At timestep:     731 / 1000At timestep:     747 / 1000At timestep:     778 / 1000At timestep:     813 / 1000At timestep:     833 / 1000At timestep:     856 / 1000At timestep:     875 / 1000At timestep:     890 / 1000At timestep:     912 / 1000At timestep:     930 / 1000At timestep:     973 / 1000At timestep:     998 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -17.860000610351562
Train_BestReturn : -17.079999923706055
TimeSinceStart : 643.0247483253479
Exploration Critic Loss : 8408903.0
Exploitation Critic Loss : 0.13016733527183533
Exploration Model Loss : 35.39164733886719
Actor Loss : 0.589501678943634
Eval_AverageReturn : -22.18181800842285
Eval_StdReturn : 6.866495132446289
Eval_MaxReturn : -13.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 23.181818181818183
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.840000
best mean reward -17.080000
running time 684.656527
At timestep:     21 / 1000At timestep:     48 / 1000At timestep:     60 / 1000At timestep:     82 / 1000At timestep:     113 / 1000At timestep:     129 / 1000At timestep:     159 / 1000At timestep:     203 / 1000At timestep:     237 / 1000At timestep:     261 / 1000At timestep:     281 / 1000At timestep:     317 / 1000At timestep:     341 / 1000At timestep:     367 / 1000At timestep:     399 / 1000At timestep:     421 / 1000At timestep:     438 / 1000At timestep:     457 / 1000At timestep:     485 / 1000At timestep:     507 / 1000At timestep:     528 / 1000At timestep:     547 / 1000At timestep:     567 / 1000At timestep:     617 / 1000At timestep:     637 / 1000At timestep:     658 / 1000At timestep:     691 / 1000At timestep:     715 / 1000At timestep:     739 / 1000At timestep:     771 / 1000At timestep:     792 / 1000At timestep:     824 / 1000At timestep:     847 / 1000At timestep:     865 / 1000At timestep:     891 / 1000At timestep:     915 / 1000At timestep:     934 / 1000At timestep:     952 / 1000At timestep:     989 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.84000015258789
Train_BestReturn : -17.079999923706055
TimeSinceStart : 684.6565265655518
Exploration Critic Loss : 11760653.0
Exploitation Critic Loss : 0.09994974732398987
Exploration Model Loss : 52.915130615234375
Actor Loss : 0.5888814926147461
Eval_AverageReturn : -24.424999237060547
Eval_StdReturn : 7.684033393859863
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 25.4
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.799999
best mean reward -17.080000
running time 727.378196
At timestep:     23 / 1000At timestep:     39 / 1000At timestep:     60 / 1000At timestep:     73 / 1000At timestep:     89 / 1000At timestep:     107 / 1000At timestep:     126 / 1000At timestep:     145 / 1000At timestep:     160 / 1000At timestep:     188 / 1000At timestep:     216 / 1000At timestep:     230 / 1000At timestep:     272 / 1000At timestep:     290 / 1000At timestep:     309 / 1000At timestep:     326 / 1000At timestep:     340 / 1000At timestep:     357 / 1000At timestep:     375 / 1000At timestep:     392 / 1000At timestep:     406 / 1000At timestep:     438 / 1000At timestep:     455 / 1000At timestep:     481 / 1000At timestep:     505 / 1000At timestep:     521 / 1000At timestep:     538 / 1000At timestep:     557 / 1000At timestep:     575 / 1000At timestep:     607 / 1000At timestep:     628 / 1000At timestep:     649 / 1000At timestep:     686 / 1000At timestep:     713 / 1000At timestep:     736 / 1000At timestep:     755 / 1000At timestep:     775 / 1000At timestep:     807 / 1000At timestep:     832 / 1000At timestep:     846 / 1000At timestep:     861 / 1000At timestep:     877 / 1000At timestep:     894 / 1000At timestep:     922 / 1000At timestep:     946 / 1000At timestep:     964 / 1000At timestep:     988 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.799999237060547
Train_BestReturn : -17.079999923706055
TimeSinceStart : 727.37819647789
Exploration Critic Loss : 5950440.0
Exploitation Critic Loss : 0.07833319902420044
Exploration Model Loss : 26.4759521484375
Actor Loss : 0.5631946921348572
Eval_AverageReturn : -20.0
Eval_StdReturn : 6.3146653175354
Eval_MaxReturn : -12.0
Eval_MinReturn : -41.0
Eval_AverageEpLen : 21.0
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.469999
best mean reward -17.080000
running time 773.037213
At timestep:     26 / 1000At timestep:     45 / 1000At timestep:     68 / 1000At timestep:     108 / 1000At timestep:     130 / 1000At timestep:     149 / 1000At timestep:     182 / 1000At timestep:     197 / 1000At timestep:     218 / 1000At timestep:     255 / 1000At timestep:     276 / 1000At timestep:     316 / 1000At timestep:     355 / 1000At timestep:     368 / 1000At timestep:     385 / 1000At timestep:     404 / 1000At timestep:     429 / 1000At timestep:     454 / 1000At timestep:     472 / 1000At timestep:     492 / 1000At timestep:     509 / 1000At timestep:     523 / 1000At timestep:     542 / 1000At timestep:     588 / 1000At timestep:     602 / 1000At timestep:     628 / 1000At timestep:     666 / 1000At timestep:     716 / 1000At timestep:     730 / 1000At timestep:     743 / 1000At timestep:     758 / 1000At timestep:     779 / 1000At timestep:     796 / 1000At timestep:     817 / 1000At timestep:     836 / 1000At timestep:     853 / 1000At timestep:     879 / 1000At timestep:     896 / 1000At timestep:     916 / 1000At timestep:     944 / 1000At timestep:     966 / 1000At timestep:     982 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.469999313354492
Train_BestReturn : -17.079999923706055
TimeSinceStart : 773.0372133255005
Exploration Critic Loss : 7378286.0
Exploitation Critic Loss : 0.08023764193058014
Exploration Model Loss : 35.28550720214844
Actor Loss : 0.5068936347961426
Eval_AverageReturn : -22.34883689880371
Eval_StdReturn : 9.23563289642334
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 23.325581395348838
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.440001
best mean reward -17.080000
running time 814.658391
At timestep:     13 / 1000At timestep:     43 / 1000At timestep:     56 / 1000At timestep:     79 / 1000At timestep:     106 / 1000At timestep:     146 / 1000At timestep:     174 / 1000At timestep:     192 / 1000At timestep:     217 / 1000At timestep:     237 / 1000At timestep:     253 / 1000At timestep:     273 / 1000At timestep:     297 / 1000At timestep:     320 / 1000At timestep:     349 / 1000At timestep:     385 / 1000At timestep:     401 / 1000At timestep:     430 / 1000At timestep:     446 / 1000At timestep:     483 / 1000At timestep:     499 / 1000At timestep:     520 / 1000At timestep:     533 / 1000At timestep:     570 / 1000At timestep:     591 / 1000At timestep:     617 / 1000At timestep:     637 / 1000At timestep:     657 / 1000At timestep:     678 / 1000At timestep:     701 / 1000At timestep:     717 / 1000At timestep:     736 / 1000At timestep:     766 / 1000At timestep:     781 / 1000At timestep:     816 / 1000At timestep:     830 / 1000At timestep:     867 / 1000At timestep:     889 / 1000At timestep:     912 / 1000At timestep:     931 / 1000At timestep:     948 / 1000At timestep:     965 / 1000At timestep:     994 / 1000At timestep:     1028 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.440000534057617
Train_BestReturn : -17.079999923706055
TimeSinceStart : 814.6583914756775
Exploration Critic Loss : 14440240.0
Exploitation Critic Loss : 0.07525696605443954
Exploration Model Loss : 52.85479736328125
Actor Loss : 0.5813270807266235
Eval_AverageReturn : -22.363636016845703
Eval_StdReturn : 7.437897205352783
Eval_MaxReturn : -12.0
Eval_MinReturn : -39.0
Eval_AverageEpLen : 23.363636363636363
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -17.240000
best mean reward -17.080000
running time 856.597670
At timestep:     17 / 1000At timestep:     37 / 1000At timestep:     49 / 1000At timestep:     75 / 1000At timestep:     103 / 1000At timestep:     122 / 1000At timestep:     165 / 1000At timestep:     187 / 1000At timestep:     216 / 1000At timestep:     227 / 1000At timestep:     245 / 1000At timestep:     275 / 1000At timestep:     295 / 1000At timestep:     317 / 1000At timestep:     335 / 1000At timestep:     349 / 1000At timestep:     380 / 1000At timestep:     400 / 1000At timestep:     425 / 1000At timestep:     451 / 1000At timestep:     478 / 1000At timestep:     494 / 1000At timestep:     517 / 1000At timestep:     567 / 1000At timestep:     583 / 1000At timestep:     612 / 1000At timestep:     636 / 1000At timestep:     655 / 1000At timestep:     675 / 1000At timestep:     693 / 1000At timestep:     728 / 1000At timestep:     758 / 1000At timestep:     776 / 1000At timestep:     799 / 1000At timestep:     821 / 1000At timestep:     837 / 1000At timestep:     848 / 1000At timestep:     871 / 1000At timestep:     895 / 1000At timestep:     910 / 1000At timestep:     934 / 1000At timestep:     956 / 1000At timestep:     974 / 1000At timestep:     997 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -17.239999771118164
Train_BestReturn : -17.079999923706055
TimeSinceStart : 856.5976703166962
Exploration Critic Loss : 8209140.5
Exploitation Critic Loss : 0.10996517539024353
Exploration Model Loss : 26.4678955078125
Actor Loss : 0.531787097454071
Eval_AverageReturn : -21.622222900390625
Eval_StdReturn : 7.54921293258667
Eval_MaxReturn : -10.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 22.6
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -17.490000
best mean reward -17.080000
running time 898.810998
At timestep:     30 / 1000At timestep:     46 / 1000At timestep:     61 / 1000At timestep:     81 / 1000At timestep:     106 / 1000At timestep:     141 / 1000At timestep:     160 / 1000At timestep:     196 / 1000At timestep:     217 / 1000At timestep:     232 / 1000At timestep:     253 / 1000At timestep:     271 / 1000At timestep:     288 / 1000At timestep:     301 / 1000At timestep:     336 / 1000At timestep:     362 / 1000At timestep:     381 / 1000At timestep:     396 / 1000At timestep:     419 / 1000At timestep:     441 / 1000At timestep:     456 / 1000At timestep:     481 / 1000At timestep:     500 / 1000At timestep:     518 / 1000At timestep:     539 / 1000At timestep:     551 / 1000At timestep:     566 / 1000At timestep:     592 / 1000At timestep:     615 / 1000At timestep:     628 / 1000At timestep:     641 / 1000At timestep:     657 / 1000At timestep:     673 / 1000At timestep:     687 / 1000At timestep:     705 / 1000At timestep:     721 / 1000At timestep:     742 / 1000At timestep:     756 / 1000At timestep:     774 / 1000At timestep:     791 / 1000At timestep:     811 / 1000At timestep:     835 / 1000At timestep:     850 / 1000At timestep:     878 / 1000At timestep:     904 / 1000At timestep:     920 / 1000At timestep:     936 / 1000At timestep:     967 / 1000At timestep:     985 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -17.489999771118164
Train_BestReturn : -17.079999923706055
TimeSinceStart : 898.8109984397888
Exploration Critic Loss : 10238759.0
Exploitation Critic Loss : 0.1462285965681076
Exploration Model Loss : 35.28117370605469
Actor Loss : 0.47907188534736633
Eval_AverageReturn : -19.1200008392334
Eval_StdReturn : 5.935115814208984
Eval_MaxReturn : -11.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 20.12
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -18.129999
best mean reward -17.080000
running time 941.909438
At timestep:     16 / 1000At timestep:     36 / 1000At timestep:     64 / 1000At timestep:     81 / 1000At timestep:     105 / 1000At timestep:     120 / 1000At timestep:     138 / 1000At timestep:     163 / 1000At timestep:     180 / 1000At timestep:     205 / 1000At timestep:     224 / 1000At timestep:     237 / 1000At timestep:     255 / 1000At timestep:     275 / 1000At timestep:     295 / 1000At timestep:     310 / 1000At timestep:     331 / 1000At timestep:     353 / 1000At timestep:     371 / 1000At timestep:     386 / 1000At timestep:     409 / 1000At timestep:     435 / 1000At timestep:     447 / 1000At timestep:     481 / 1000At timestep:     495 / 1000At timestep:     521 / 1000At timestep:     536 / 1000At timestep:     554 / 1000At timestep:     575 / 1000At timestep:     596 / 1000At timestep:     621 / 1000At timestep:     640 / 1000At timestep:     665 / 1000At timestep:     686 / 1000At timestep:     705 / 1000At timestep:     732 / 1000At timestep:     755 / 1000At timestep:     773 / 1000At timestep:     789 / 1000At timestep:     815 / 1000At timestep:     834 / 1000At timestep:     846 / 1000At timestep:     870 / 1000At timestep:     887 / 1000At timestep:     904 / 1000At timestep:     926 / 1000At timestep:     950 / 1000At timestep:     971 / 1000At timestep:     991 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -18.1299991607666
Train_BestReturn : -17.079999923706055
TimeSinceStart : 941.9094376564026
Exploration Critic Loss : 19602300.0
Exploitation Critic Loss : 0.10596118122339249
Exploration Model Loss : 52.85186767578125
Actor Loss : 0.5333296060562134
Eval_AverageReturn : -19.15999984741211
Eval_StdReturn : 4.4915924072265625
Eval_MaxReturn : -11.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 20.16
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -18.260000
best mean reward -17.080000
running time 984.539726
At timestep:     19 / 1000At timestep:     33 / 1000At timestep:     55 / 1000At timestep:     82 / 1000At timestep:     96 / 1000At timestep:     112 / 1000At timestep:     130 / 1000At timestep:     149 / 1000At timestep:     179 / 1000At timestep:     204 / 1000At timestep:     223 / 1000At timestep:     241 / 1000At timestep:     265 / 1000At timestep:     285 / 1000At timestep:     300 / 1000At timestep:     319 / 1000At timestep:     337 / 1000At timestep:     353 / 1000At timestep:     367 / 1000At timestep:     389 / 1000At timestep:     405 / 1000At timestep:     420 / 1000At timestep:     445 / 1000At timestep:     460 / 1000At timestep:     486 / 1000At timestep:     521 / 1000At timestep:     544 / 1000At timestep:     562 / 1000At timestep:     582 / 1000At timestep:     596 / 1000At timestep:     616 / 1000At timestep:     634 / 1000At timestep:     652 / 1000At timestep:     676 / 1000At timestep:     695 / 1000At timestep:     711 / 1000At timestep:     725 / 1000At timestep:     743 / 1000At timestep:     760 / 1000At timestep:     773 / 1000At timestep:     808 / 1000At timestep:     821 / 1000At timestep:     843 / 1000At timestep:     862 / 1000At timestep:     884 / 1000At timestep:     905 / 1000At timestep:     932 / 1000At timestep:     952 / 1000At timestep:     973 / 1000At timestep:     991 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -18.260000228881836
Train_BestReturn : -17.079999923706055
TimeSinceStart : 984.5397255420685
Exploration Critic Loss : 7402365.5
Exploitation Critic Loss : 0.10787779092788696
Exploration Model Loss : 26.468170166015625
Actor Loss : 0.601856529712677
Eval_AverageReturn : -18.6862735748291
Eval_StdReturn : 5.050684928894043
Eval_MaxReturn : -12.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.686274509803923
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -17.770000
best mean reward -17.080000
running time 1027.656701
At timestep:     21 / 1000At timestep:     43 / 1000At timestep:     63 / 1000At timestep:     82 / 1000At timestep:     103 / 1000At timestep:     119 / 1000At timestep:     138 / 1000At timestep:     157 / 1000At timestep:     180 / 1000At timestep:     210 / 1000At timestep:     230 / 1000At timestep:     250 / 1000At timestep:     261 / 1000At timestep:     281 / 1000At timestep:     299 / 1000At timestep:     323 / 1000At timestep:     339 / 1000At timestep:     362 / 1000At timestep:     393 / 1000At timestep:     415 / 1000At timestep:     439 / 1000At timestep:     459 / 1000At timestep:     481 / 1000At timestep:     496 / 1000At timestep:     518 / 1000At timestep:     544 / 1000At timestep:     562 / 1000At timestep:     586 / 1000At timestep:     603 / 1000At timestep:     620 / 1000At timestep:     642 / 1000At timestep:     665 / 1000At timestep:     679 / 1000At timestep:     695 / 1000At timestep:     713 / 1000At timestep:     737 / 1000At timestep:     757 / 1000At timestep:     778 / 1000At timestep:     806 / 1000At timestep:     827 / 1000At timestep:     852 / 1000At timestep:     877 / 1000At timestep:     901 / 1000At timestep:     923 / 1000At timestep:     956 / 1000At timestep:     982 / 1000At timestep:     999 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -17.770000457763672
Train_BestReturn : -17.079999923706055
TimeSinceStart : 1027.656700849533
Exploration Critic Loss : 13087018.0
Exploitation Critic Loss : 0.07363986223936081
Exploration Model Loss : 35.28166198730469
Actor Loss : 0.5709609985351562
Eval_AverageReturn : -20.16666603088379
Eval_StdReturn : 4.297932147979736
Eval_MaxReturn : -10.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 21.166666666666668
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -16.910000
best mean reward -16.910000
running time 1071.453344
At timestep:     22 / 1000At timestep:     39 / 1000At timestep:     64 / 1000At timestep:     92 / 1000At timestep:     123 / 1000At timestep:     140 / 1000At timestep:     155 / 1000At timestep:     174 / 1000At timestep:     200 / 1000At timestep:     213 / 1000At timestep:     232 / 1000At timestep:     250 / 1000At timestep:     273 / 1000At timestep:     298 / 1000At timestep:     317 / 1000At timestep:     338 / 1000At timestep:     369 / 1000At timestep:     395 / 1000At timestep:     411 / 1000At timestep:     438 / 1000At timestep:     456 / 1000At timestep:     480 / 1000At timestep:     497 / 1000At timestep:     523 / 1000At timestep:     540 / 1000At timestep:     558 / 1000At timestep:     575 / 1000At timestep:     596 / 1000At timestep:     621 / 1000At timestep:     650 / 1000At timestep:     671 / 1000At timestep:     695 / 1000At timestep:     716 / 1000At timestep:     739 / 1000At timestep:     764 / 1000At timestep:     777 / 1000At timestep:     795 / 1000At timestep:     811 / 1000At timestep:     825 / 1000At timestep:     843 / 1000At timestep:     873 / 1000At timestep:     891 / 1000At timestep:     918 / 1000At timestep:     935 / 1000At timestep:     955 / 1000At timestep:     971 / 1000At timestep:     994 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -16.90999984741211
Train_BestReturn : -16.90999984741211
TimeSinceStart : 1071.4533443450928
Exploration Critic Loss : 13638908.0
Exploitation Critic Loss : 0.08186094462871552
Exploration Model Loss : 52.851409912109375
Actor Loss : 0.5018563866615295
Eval_AverageReturn : -20.04166603088379
Eval_StdReturn : 4.812995910644531
Eval_MaxReturn : -12.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 21.041666666666668
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -17.219999
best mean reward -16.910000
running time 1114.098095
At timestep:     20 / 1000At timestep:     38 / 1000At timestep:     54 / 1000At timestep:     86 / 1000At timestep:     105 / 1000At timestep:     120 / 1000At timestep:     135 / 1000At timestep:     147 / 1000At timestep:     165 / 1000At timestep:     184 / 1000At timestep:     212 / 1000At timestep:     230 / 1000At timestep:     245 / 1000At timestep:     264 / 1000At timestep:     289 / 1000At timestep:     301 / 1000At timestep:     322 / 1000At timestep:     351 / 1000At timestep:     376 / 1000At timestep:     399 / 1000At timestep:     433 / 1000At timestep:     478 / 1000At timestep:     489 / 1000At timestep:     503 / 1000At timestep:     535 / 1000At timestep:     554 / 1000At timestep:     575 / 1000At timestep:     596 / 1000At timestep:     617 / 1000At timestep:     647 / 1000At timestep:     664 / 1000At timestep:     684 / 1000At timestep:     697 / 1000At timestep:     713 / 1000At timestep:     731 / 1000At timestep:     750 / 1000At timestep:     773 / 1000At timestep:     794 / 1000At timestep:     814 / 1000At timestep:     836 / 1000At timestep:     872 / 1000At timestep:     897 / 1000At timestep:     913 / 1000At timestep:     929 / 1000At timestep:     945 / 1000At timestep:     969 / 1000At timestep:     986 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -17.219999313354492
Train_BestReturn : -16.90999984741211
TimeSinceStart : 1114.0980949401855
Exploration Critic Loss : 8631097.0
Exploitation Critic Loss : 0.09311680495738983
Exploration Model Loss : 26.4686279296875
Actor Loss : 0.5197919607162476
Eval_AverageReturn : -20.0
Eval_StdReturn : 6.72681188583374
Eval_MaxReturn : -10.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 21.0
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -17.639999
best mean reward -16.910000
running time 1157.098929
At timestep:     15 / 1000At timestep:     31 / 1000At timestep:     49 / 1000At timestep:     61 / 1000At timestep:     77 / 1000At timestep:     91 / 1000At timestep:     105 / 1000At timestep:     131 / 1000At timestep:     143 / 1000At timestep:     174 / 1000At timestep:     195 / 1000At timestep:     209 / 1000At timestep:     220 / 1000At timestep:     233 / 1000At timestep:     257 / 1000At timestep:     280 / 1000At timestep:     298 / 1000At timestep:     321 / 1000At timestep:     338 / 1000At timestep:     357 / 1000At timestep:     375 / 1000At timestep:     389 / 1000At timestep:     407 / 1000At timestep:     420 / 1000At timestep:     446 / 1000At timestep:     466 / 1000At timestep:     484 / 1000At timestep:     498 / 1000At timestep:     516 / 1000At timestep:     538 / 1000At timestep:     553 / 1000At timestep:     572 / 1000At timestep:     590 / 1000At timestep:     609 / 1000At timestep:     625 / 1000At timestep:     642 / 1000At timestep:     661 / 1000At timestep:     679 / 1000At timestep:     695 / 1000At timestep:     718 / 1000At timestep:     734 / 1000At timestep:     748 / 1000At timestep:     770 / 1000At timestep:     787 / 1000At timestep:     810 / 1000At timestep:     829 / 1000At timestep:     867 / 1000At timestep:     883 / 1000At timestep:     904 / 1000At timestep:     925 / 1000At timestep:     939 / 1000At timestep:     952 / 1000At timestep:     978 / 1000At timestep:     997 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -17.639999389648438
Train_BestReturn : -16.90999984741211
TimeSinceStart : 1157.0989289283752
Exploration Critic Loss : 11649972.0
Exploitation Critic Loss : 0.10762467235326767
Exploration Model Loss : 35.282135009765625
Actor Loss : 0.5226919651031494
Eval_AverageReturn : -17.418182373046875
Eval_StdReturn : 4.886682987213135
Eval_MaxReturn : -10.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 18.418181818181818
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -16.770000
best mean reward -16.770000
running time 1201.712539
At timestep:     24 / 1000At timestep:     40 / 1000At timestep:     60 / 1000At timestep:     81 / 1000At timestep:     101 / 1000At timestep:     128 / 1000At timestep:     146 / 1000At timestep:     166 / 1000At timestep:     188 / 1000At timestep:     210 / 1000At timestep:     224 / 1000At timestep:     246 / 1000At timestep:     270 / 1000At timestep:     287 / 1000At timestep:     307 / 1000At timestep:     340 / 1000At timestep:     364 / 1000At timestep:     381 / 1000At timestep:     399 / 1000At timestep:     421 / 1000At timestep:     438 / 1000At timestep:     457 / 1000At timestep:     477 / 1000At timestep:     490 / 1000At timestep:     510 / 1000At timestep:     533 / 1000At timestep:     554 / 1000At timestep:     572 / 1000At timestep:     595 / 1000At timestep:     608 / 1000At timestep:     640 / 1000At timestep:     657 / 1000At timestep:     675 / 1000At timestep:     691 / 1000At timestep:     720 / 1000At timestep:     735 / 1000At timestep:     755 / 1000At timestep:     775 / 1000At timestep:     792 / 1000At timestep:     813 / 1000At timestep:     838 / 1000At timestep:     857 / 1000At timestep:     874 / 1000At timestep:     895 / 1000At timestep:     911 / 1000At timestep:     938 / 1000At timestep:     964 / 1000At timestep:     987 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -16.770000457763672
Train_BestReturn : -16.770000457763672
TimeSinceStart : 1201.7125391960144
Exploration Critic Loss : 13261194.0
Exploitation Critic Loss : 0.11545255035161972
Exploration Model Loss : 52.850921630859375
Actor Loss : 0.5509505271911621
Eval_AverageReturn : -19.510204315185547
Eval_StdReturn : 4.328936576843262
Eval_MaxReturn : -12.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 20.510204081632654
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -16.629999
best mean reward -16.629999
running time 1244.518953
At timestep:     22 / 1000At timestep:     43 / 1000At timestep:     60 / 1000At timestep:     84 / 1000At timestep:     105 / 1000At timestep:     117 / 1000At timestep:     134 / 1000At timestep:     164 / 1000At timestep:     181 / 1000At timestep:     199 / 1000At timestep:     219 / 1000At timestep:     236 / 1000At timestep:     252 / 1000At timestep:     273 / 1000At timestep:     293 / 1000At timestep:     314 / 1000At timestep:     332 / 1000At timestep:     360 / 1000At timestep:     380 / 1000At timestep:     401 / 1000At timestep:     417 / 1000At timestep:     444 / 1000At timestep:     458 / 1000At timestep:     477 / 1000At timestep:     498 / 1000At timestep:     517 / 1000At timestep:     544 / 1000At timestep:     564 / 1000At timestep:     586 / 1000At timestep:     597 / 1000At timestep:     611 / 1000At timestep:     650 / 1000At timestep:     671 / 1000At timestep:     693 / 1000At timestep:     708 / 1000At timestep:     724 / 1000At timestep:     744 / 1000At timestep:     764 / 1000At timestep:     780 / 1000At timestep:     819 / 1000At timestep:     843 / 1000At timestep:     859 / 1000At timestep:     886 / 1000At timestep:     906 / 1000At timestep:     930 / 1000At timestep:     945 / 1000At timestep:     970 / 1000At timestep:     984 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -16.6299991607666
Train_BestReturn : -16.6299991607666
TimeSinceStart : 1244.5189526081085
Exploration Critic Loss : 9153487.0
Exploitation Critic Loss : 0.10587431490421295
Exploration Model Loss : 26.468582153320312
Actor Loss : 0.5233871936798096
Eval_AverageReturn : -19.775510787963867
Eval_StdReturn : 5.92561149597168
Eval_MaxReturn : -10.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 20.775510204081634
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -16.799999
best mean reward -16.629999
running time 1287.209546
At timestep:     22 / 1000At timestep:     43 / 1000At timestep:     58 / 1000At timestep:     84 / 1000At timestep:     107 / 1000At timestep:     126 / 1000At timestep:     141 / 1000At timestep:     170 / 1000At timestep:     209 / 1000At timestep:     226 / 1000At timestep:     240 / 1000At timestep:     260 / 1000At timestep:     286 / 1000At timestep:     307 / 1000At timestep:     334 / 1000At timestep:     353 / 1000At timestep:     374 / 1000At timestep:     406 / 1000At timestep:     425 / 1000At timestep:     440 / 1000At timestep:     463 / 1000At timestep:     480 / 1000At timestep:     504 / 1000At timestep:     522 / 1000At timestep:     547 / 1000At timestep:     572 / 1000At timestep:     594 / 1000At timestep:     611 / 1000At timestep:     634 / 1000At timestep:     651 / 1000At timestep:     672 / 1000At timestep:     690 / 1000At timestep:     702 / 1000At timestep:     720 / 1000At timestep:     742 / 1000At timestep:     754 / 1000At timestep:     778 / 1000At timestep:     795 / 1000At timestep:     811 / 1000At timestep:     831 / 1000At timestep:     860 / 1000At timestep:     878 / 1000At timestep:     904 / 1000At timestep:     925 / 1000At timestep:     939 / 1000At timestep:     954 / 1000At timestep:     976 / 1000At timestep:     995 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -16.799999237060547
Train_BestReturn : -16.6299991607666
TimeSinceStart : 1287.2095460891724
Exploration Critic Loss : 11523693.0
Exploitation Critic Loss : 0.08075080811977386
Exploration Model Loss : 35.28208923339844
Actor Loss : 0.594782829284668
Eval_AverageReturn : -19.67346954345703
Eval_StdReturn : 5.1839141845703125
Eval_MaxReturn : -11.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 20.6734693877551
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -16.590000
best mean reward -16.590000
running time 1330.666475
At timestep:     22 / 1000At timestep:     40 / 1000At timestep:     57 / 1000At timestep:     76 / 1000At timestep:     103 / 1000At timestep:     117 / 1000At timestep:     135 / 1000At timestep:     153 / 1000At timestep:     168 / 1000At timestep:     184 / 1000At timestep:     209 / 1000At timestep:     228 / 1000At timestep:     252 / 1000At timestep:     267 / 1000At timestep:     285 / 1000At timestep:     299 / 1000At timestep:     316 / 1000At timestep:     339 / 1000At timestep:     361 / 1000At timestep:     377 / 1000At timestep:     397 / 1000At timestep:     417 / 1000At timestep:     437 / 1000At timestep:     455 / 1000At timestep:     474 / 1000At timestep:     493 / 1000At timestep:     528 / 1000At timestep:     546 / 1000At timestep:     562 / 1000At timestep:     590 / 1000At timestep:     605 / 1000At timestep:     621 / 1000At timestep:     658 / 1000At timestep:     679 / 1000At timestep:     696 / 1000At timestep:     711 / 1000At timestep:     735 / 1000At timestep:     752 / 1000At timestep:     772 / 1000At timestep:     792 / 1000At timestep:     815 / 1000At timestep:     831 / 1000At timestep:     848 / 1000At timestep:     868 / 1000At timestep:     890 / 1000At timestep:     910 / 1000At timestep:     928 / 1000At timestep:     962 / 1000At timestep:     985 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -16.59000015258789
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1330.6664745807648
Exploration Critic Loss : 11333358.0
Exploitation Critic Loss : 0.054701946675777435
Exploration Model Loss : 52.8509521484375
Actor Loss : 0.6320441961288452
Eval_AverageReturn : -19.200000762939453
Eval_StdReturn : 5.0438079833984375
Eval_MaxReturn : -13.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 20.2
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -17.440001
best mean reward -16.590000
running time 1373.482997
At timestep:     12 / 1000At timestep:     25 / 1000At timestep:     47 / 1000At timestep:     70 / 1000At timestep:     94 / 1000At timestep:     117 / 1000At timestep:     130 / 1000At timestep:     148 / 1000At timestep:     183 / 1000At timestep:     201 / 1000At timestep:     219 / 1000At timestep:     243 / 1000At timestep:     260 / 1000At timestep:     281 / 1000At timestep:     308 / 1000At timestep:     323 / 1000At timestep:     348 / 1000At timestep:     379 / 1000At timestep:     397 / 1000At timestep:     425 / 1000At timestep:     443 / 1000At timestep:     460 / 1000At timestep:     486 / 1000At timestep:     511 / 1000At timestep:     528 / 1000At timestep:     550 / 1000At timestep:     568 / 1000At timestep:     585 / 1000At timestep:     601 / 1000At timestep:     630 / 1000At timestep:     641 / 1000At timestep:     656 / 1000At timestep:     681 / 1000At timestep:     714 / 1000At timestep:     735 / 1000At timestep:     751 / 1000At timestep:     771 / 1000At timestep:     789 / 1000At timestep:     803 / 1000At timestep:     825 / 1000At timestep:     842 / 1000At timestep:     863 / 1000At timestep:     879 / 1000At timestep:     907 / 1000At timestep:     939 / 1000At timestep:     956 / 1000At timestep:     979 / 1000At timestep:     998 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -17.440000534057617
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1373.4829967021942
Exploration Critic Loss : 9417708.0
Exploitation Critic Loss : 0.09560800343751907
Exploration Model Loss : 26.469070434570312
Actor Loss : 0.5383658409118652
Eval_AverageReturn : -19.775510787963867
Eval_StdReturn : 5.592507362365723
Eval_MaxReturn : -10.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 20.775510204081634
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -17.799999
best mean reward -16.590000
running time 1415.717738
At timestep:     17 / 1000At timestep:     35 / 1000At timestep:     51 / 1000At timestep:     69 / 1000At timestep:     88 / 1000At timestep:     119 / 1000At timestep:     136 / 1000At timestep:     150 / 1000At timestep:     171 / 1000At timestep:     194 / 1000At timestep:     211 / 1000At timestep:     232 / 1000At timestep:     259 / 1000At timestep:     288 / 1000At timestep:     305 / 1000At timestep:     320 / 1000At timestep:     338 / 1000At timestep:     364 / 1000At timestep:     380 / 1000At timestep:     394 / 1000At timestep:     422 / 1000At timestep:     448 / 1000At timestep:     467 / 1000At timestep:     496 / 1000At timestep:     521 / 1000At timestep:     539 / 1000At timestep:     564 / 1000At timestep:     586 / 1000At timestep:     603 / 1000At timestep:     637 / 1000At timestep:     658 / 1000At timestep:     677 / 1000At timestep:     694 / 1000At timestep:     720 / 1000At timestep:     743 / 1000At timestep:     755 / 1000At timestep:     771 / 1000At timestep:     787 / 1000At timestep:     806 / 1000At timestep:     826 / 1000At timestep:     858 / 1000At timestep:     880 / 1000At timestep:     899 / 1000At timestep:     917 / 1000At timestep:     933 / 1000At timestep:     949 / 1000At timestep:     971 / 1000At timestep:     991 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -17.799999237060547
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1415.7177376747131
Exploration Critic Loss : 14525922.0
Exploitation Critic Loss : 0.11529460549354553
Exploration Model Loss : 35.282562255859375
Actor Loss : 0.5112221240997314
Eval_AverageReturn : -19.46938705444336
Eval_StdReturn : 5.182789325714111
Eval_MaxReturn : -11.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 20.46938775510204
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -17.850000
best mean reward -16.590000
running time 1458.161276
At timestep:     17 / 1000At timestep:     46 / 1000At timestep:     61 / 1000At timestep:     81 / 1000At timestep:     101 / 1000At timestep:     125 / 1000At timestep:     145 / 1000At timestep:     163 / 1000At timestep:     182 / 1000At timestep:     208 / 1000At timestep:     226 / 1000At timestep:     241 / 1000At timestep:     258 / 1000At timestep:     275 / 1000At timestep:     289 / 1000At timestep:     307 / 1000At timestep:     331 / 1000At timestep:     352 / 1000At timestep:     369 / 1000At timestep:     384 / 1000At timestep:     401 / 1000At timestep:     423 / 1000At timestep:     443 / 1000At timestep:     466 / 1000At timestep:     489 / 1000At timestep:     503 / 1000At timestep:     520 / 1000At timestep:     542 / 1000At timestep:     555 / 1000At timestep:     574 / 1000At timestep:     589 / 1000At timestep:     605 / 1000At timestep:     627 / 1000At timestep:     644 / 1000At timestep:     669 / 1000At timestep:     694 / 1000At timestep:     712 / 1000At timestep:     734 / 1000At timestep:     748 / 1000At timestep:     764 / 1000At timestep:     786 / 1000At timestep:     806 / 1000At timestep:     829 / 1000At timestep:     847 / 1000At timestep:     862 / 1000At timestep:     885 / 1000At timestep:     901 / 1000At timestep:     932 / 1000At timestep:     969 / 1000At timestep:     989 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -17.850000381469727
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1458.1612763404846
Exploration Critic Loss : 15423372.0
Exploitation Critic Loss : 0.11171838641166687
Exploration Model Loss : 52.8504638671875
Actor Loss : 0.48640963435173035
Eval_AverageReturn : -18.6862735748291
Eval_StdReturn : 4.646272659301758
Eval_MaxReturn : -12.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 19.686274509803923
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -17.650000
best mean reward -16.590000
running time 1501.108832
At timestep:     16 / 1000At timestep:     33 / 1000At timestep:     46 / 1000At timestep:     60 / 1000At timestep:     75 / 1000At timestep:     91 / 1000At timestep:     105 / 1000At timestep:     123 / 1000At timestep:     135 / 1000At timestep:     153 / 1000At timestep:     171 / 1000At timestep:     187 / 1000At timestep:     213 / 1000At timestep:     230 / 1000At timestep:     254 / 1000At timestep:     275 / 1000At timestep:     291 / 1000At timestep:     313 / 1000At timestep:     337 / 1000At timestep:     353 / 1000At timestep:     371 / 1000At timestep:     388 / 1000At timestep:     404 / 1000At timestep:     424 / 1000At timestep:     439 / 1000At timestep:     453 / 1000At timestep:     476 / 1000At timestep:     494 / 1000At timestep:     510 / 1000At timestep:     527 / 1000At timestep:     557 / 1000At timestep:     577 / 1000At timestep:     595 / 1000At timestep:     617 / 1000At timestep:     631 / 1000At timestep:     647 / 1000At timestep:     665 / 1000At timestep:     679 / 1000At timestep:     691 / 1000At timestep:     707 / 1000At timestep:     727 / 1000At timestep:     750 / 1000At timestep:     766 / 1000At timestep:     798 / 1000At timestep:     818 / 1000At timestep:     843 / 1000At timestep:     859 / 1000At timestep:     881 / 1000At timestep:     895 / 1000At timestep:     938 / 1000At timestep:     953 / 1000At timestep:     969 / 1000At timestep:     999 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -17.649999618530273
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1501.1088318824768
Exploration Critic Loss : 10264142.0
Exploitation Critic Loss : 0.15083783864974976
Exploration Model Loss : 26.469528198242188
Actor Loss : 0.5297920107841492
Eval_AverageReturn : -17.851852416992188
Eval_StdReturn : 5.5290727615356445
Eval_MaxReturn : -11.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 18.85185185185185
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -17.520000
best mean reward -16.590000
running time 1544.559828
At timestep:     27 / 1000At timestep:     46 / 1000At timestep:     71 / 1000At timestep:     91 / 1000At timestep:     108 / 1000At timestep:     123 / 1000At timestep:     139 / 1000At timestep:     165 / 1000At timestep:     193 / 1000At timestep:     210 / 1000At timestep:     227 / 1000At timestep:     245 / 1000At timestep:     267 / 1000At timestep:     282 / 1000At timestep:     312 / 1000At timestep:     328 / 1000At timestep:     345 / 1000At timestep:     361 / 1000At timestep:     375 / 1000At timestep:     404 / 1000At timestep:     425 / 1000At timestep:     438 / 1000At timestep:     457 / 1000At timestep:     489 / 1000At timestep:     516 / 1000At timestep:     547 / 1000At timestep:     565 / 1000At timestep:     583 / 1000At timestep:     602 / 1000At timestep:     624 / 1000At timestep:     646 / 1000At timestep:     661 / 1000At timestep:     682 / 1000At timestep:     698 / 1000At timestep:     712 / 1000At timestep:     737 / 1000At timestep:     751 / 1000At timestep:     768 / 1000At timestep:     794 / 1000At timestep:     806 / 1000At timestep:     829 / 1000At timestep:     847 / 1000At timestep:     861 / 1000At timestep:     880 / 1000At timestep:     906 / 1000At timestep:     920 / 1000At timestep:     935 / 1000At timestep:     951 / 1000At timestep:     965 / 1000At timestep:     983 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -17.520000457763672
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1544.5598278045654
Exploration Critic Loss : 19119128.0
Exploitation Critic Loss : 0.07926398515701294
Exploration Model Loss : 35.28306579589844
Actor Loss : 0.5831835269927979
Eval_AverageReturn : -18.60784339904785
Eval_StdReturn : 5.179440975189209
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.607843137254903
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -17.080000
best mean reward -16.590000
running time 1587.953438
At timestep:     15 / 1000At timestep:     39 / 1000At timestep:     56 / 1000At timestep:     73 / 1000At timestep:     95 / 1000At timestep:     113 / 1000At timestep:     127 / 1000At timestep:     148 / 1000At timestep:     167 / 1000At timestep:     186 / 1000At timestep:     199 / 1000At timestep:     212 / 1000At timestep:     231 / 1000At timestep:     252 / 1000At timestep:     266 / 1000At timestep:     288 / 1000At timestep:     305 / 1000At timestep:     333 / 1000At timestep:     348 / 1000At timestep:     370 / 1000At timestep:     387 / 1000At timestep:     402 / 1000At timestep:     425 / 1000At timestep:     445 / 1000At timestep:     462 / 1000At timestep:     486 / 1000At timestep:     503 / 1000At timestep:     517 / 1000At timestep:     529 / 1000At timestep:     546 / 1000At timestep:     563 / 1000At timestep:     580 / 1000At timestep:     597 / 1000At timestep:     615 / 1000At timestep:     627 / 1000At timestep:     642 / 1000At timestep:     660 / 1000At timestep:     680 / 1000At timestep:     693 / 1000At timestep:     719 / 1000At timestep:     741 / 1000At timestep:     772 / 1000At timestep:     797 / 1000At timestep:     814 / 1000At timestep:     831 / 1000At timestep:     851 / 1000At timestep:     876 / 1000At timestep:     894 / 1000At timestep:     935 / 1000At timestep:     959 / 1000At timestep:     972 / 1000At timestep:     991 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -17.079999923706055
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1587.9534378051758
Exploration Critic Loss : 18155474.0
Exploitation Critic Loss : 0.08856552839279175
Exploration Model Loss : 52.850006103515625
Actor Loss : 0.4557691216468811
Eval_AverageReturn : -18.075471878051758
Eval_StdReturn : 5.1426777839660645
Eval_MaxReturn : -11.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 19.07547169811321
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -17.270000
best mean reward -16.590000
running time 1632.056213
At timestep:     30 / 1000At timestep:     48 / 1000At timestep:     71 / 1000At timestep:     92 / 1000At timestep:     121 / 1000At timestep:     139 / 1000At timestep:     161 / 1000At timestep:     179 / 1000At timestep:     197 / 1000At timestep:     218 / 1000At timestep:     248 / 1000At timestep:     263 / 1000At timestep:     282 / 1000At timestep:     296 / 1000At timestep:     309 / 1000At timestep:     331 / 1000At timestep:     354 / 1000At timestep:     376 / 1000At timestep:     400 / 1000At timestep:     425 / 1000At timestep:     451 / 1000At timestep:     477 / 1000At timestep:     499 / 1000At timestep:     517 / 1000At timestep:     531 / 1000At timestep:     550 / 1000At timestep:     568 / 1000At timestep:     593 / 1000At timestep:     614 / 1000At timestep:     631 / 1000At timestep:     647 / 1000At timestep:     661 / 1000At timestep:     681 / 1000At timestep:     698 / 1000At timestep:     723 / 1000At timestep:     743 / 1000At timestep:     757 / 1000At timestep:     775 / 1000At timestep:     790 / 1000At timestep:     808 / 1000At timestep:     826 / 1000At timestep:     844 / 1000At timestep:     868 / 1000At timestep:     893 / 1000At timestep:     906 / 1000At timestep:     923 / 1000At timestep:     943 / 1000At timestep:     957 / 1000At timestep:     971 / 1000At timestep:     984 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -17.270000457763672
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1632.0562126636505
Exploration Critic Loss : 10015722.0
Exploitation Critic Loss : 0.07659390568733215
Exploration Model Loss : 26.47003173828125
Actor Loss : 0.46861281991004944
Eval_AverageReturn : -18.823530197143555
Eval_StdReturn : 4.574892520904541
Eval_MaxReturn : -12.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 19.823529411764707
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -17.030001
best mean reward -16.590000
running time 1676.988798
At timestep:     20 / 1000At timestep:     36 / 1000At timestep:     57 / 1000At timestep:     73 / 1000At timestep:     89 / 1000At timestep:     117 / 1000At timestep:     133 / 1000At timestep:     148 / 1000At timestep:     165 / 1000At timestep:     180 / 1000At timestep:     204 / 1000At timestep:     227 / 1000At timestep:     245 / 1000At timestep:     260 / 1000At timestep:     276 / 1000At timestep:     288 / 1000At timestep:     313 / 1000At timestep:     331 / 1000At timestep:     346 / 1000At timestep:     359 / 1000At timestep:     385 / 1000At timestep:     410 / 1000At timestep:     439 / 1000At timestep:     468 / 1000At timestep:     482 / 1000At timestep:     502 / 1000At timestep:     526 / 1000At timestep:     545 / 1000At timestep:     567 / 1000At timestep:     582 / 1000At timestep:     605 / 1000At timestep:     620 / 1000At timestep:     640 / 1000At timestep:     670 / 1000At timestep:     690 / 1000At timestep:     711 / 1000At timestep:     723 / 1000At timestep:     740 / 1000At timestep:     767 / 1000At timestep:     790 / 1000At timestep:     811 / 1000At timestep:     837 / 1000At timestep:     860 / 1000At timestep:     886 / 1000At timestep:     918 / 1000At timestep:     934 / 1000At timestep:     965 / 1000At timestep:     979 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -17.030000686645508
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1676.988797903061
Exploration Critic Loss : 15551679.0
Exploitation Critic Loss : 0.11640766263008118
Exploration Model Loss : 35.28352355957031
Actor Loss : 0.4333626627922058
Eval_AverageReturn : -19.46938705444336
Eval_StdReturn : 5.345614433288574
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 20.46938775510204
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -16.990000
best mean reward -16.590000
running time 1727.846519
At timestep:     42 / 1000At timestep:     61 / 1000At timestep:     87 / 1000At timestep:     102 / 1000At timestep:     123 / 1000At timestep:     138 / 1000At timestep:     161 / 1000At timestep:     179 / 1000At timestep:     199 / 1000At timestep:     218 / 1000At timestep:     231 / 1000At timestep:     243 / 1000At timestep:     260 / 1000At timestep:     272 / 1000At timestep:     289 / 1000At timestep:     304 / 1000At timestep:     320 / 1000At timestep:     341 / 1000At timestep:     360 / 1000At timestep:     384 / 1000At timestep:     396 / 1000At timestep:     411 / 1000At timestep:     432 / 1000At timestep:     452 / 1000At timestep:     474 / 1000At timestep:     497 / 1000At timestep:     525 / 1000At timestep:     550 / 1000At timestep:     564 / 1000At timestep:     590 / 1000At timestep:     617 / 1000At timestep:     644 / 1000At timestep:     665 / 1000At timestep:     687 / 1000At timestep:     705 / 1000At timestep:     728 / 1000At timestep:     752 / 1000At timestep:     768 / 1000At timestep:     787 / 1000At timestep:     812 / 1000At timestep:     833 / 1000At timestep:     862 / 1000At timestep:     877 / 1000At timestep:     899 / 1000At timestep:     922 / 1000At timestep:     936 / 1000At timestep:     953 / 1000At timestep:     974 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -16.989999771118164
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1727.8465192317963
Exploration Critic Loss : 23506046.0
Exploitation Critic Loss : 0.09685328602790833
Exploration Model Loss : 52.849517822265625
Actor Loss : 0.5304343700408936
Eval_AverageReturn : -19.46938705444336
Eval_StdReturn : 5.558967590332031
Eval_MaxReturn : -11.0
Eval_MinReturn : -41.0
Eval_AverageEpLen : 20.46938775510204
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -17.209999
best mean reward -16.590000
running time 1780.380461
At timestep:     15 / 1000At timestep:     44 / 1000At timestep:     65 / 1000At timestep:     88 / 1000At timestep:     112 / 1000At timestep:     131 / 1000At timestep:     162 / 1000At timestep:     178 / 1000At timestep:     200 / 1000At timestep:     221 / 1000At timestep:     240 / 1000At timestep:     260 / 1000At timestep:     287 / 1000At timestep:     313 / 1000At timestep:     331 / 1000At timestep:     348 / 1000At timestep:     372 / 1000At timestep:     401 / 1000At timestep:     422 / 1000At timestep:     444 / 1000At timestep:     465 / 1000At timestep:     482 / 1000At timestep:     499 / 1000At timestep:     522 / 1000At timestep:     550 / 1000At timestep:     572 / 1000At timestep:     591 / 1000At timestep:     617 / 1000At timestep:     635 / 1000At timestep:     657 / 1000At timestep:     685 / 1000At timestep:     702 / 1000At timestep:     726 / 1000At timestep:     750 / 1000At timestep:     765 / 1000At timestep:     779 / 1000At timestep:     801 / 1000At timestep:     816 / 1000At timestep:     839 / 1000At timestep:     881 / 1000At timestep:     898 / 1000At timestep:     919 / 1000At timestep:     932 / 1000At timestep:     949 / 1000At timestep:     965 / 1000At timestep:     987 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -17.209999084472656
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1780.3804605007172
Exploration Critic Loss : 19637590.0
Exploitation Critic Loss : 0.07786357402801514
Exploration Model Loss : 26.470489501953125
Actor Loss : 0.46793869137763977
Eval_AverageReturn : -20.510639190673828
Eval_StdReturn : 5.262764930725098
Eval_MaxReturn : -12.0
Eval_MinReturn : -41.0
Eval_AverageEpLen : 21.51063829787234
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -17.889999
best mean reward -16.590000
running time 1832.593512
At timestep:     25 / 1000At timestep:     47 / 1000At timestep:     63 / 1000At timestep:     79 / 1000At timestep:     102 / 1000At timestep:     122 / 1000At timestep:     144 / 1000At timestep:     167 / 1000At timestep:     190 / 1000At timestep:     208 / 1000At timestep:     227 / 1000At timestep:     243 / 1000At timestep:     264 / 1000At timestep:     285 / 1000At timestep:     302 / 1000At timestep:     323 / 1000At timestep:     340 / 1000At timestep:     376 / 1000At timestep:     398 / 1000At timestep:     419 / 1000At timestep:     432 / 1000At timestep:     445 / 1000At timestep:     467 / 1000At timestep:     481 / 1000At timestep:     499 / 1000At timestep:     515 / 1000At timestep:     531 / 1000At timestep:     547 / 1000At timestep:     575 / 1000At timestep:     597 / 1000At timestep:     612 / 1000At timestep:     641 / 1000At timestep:     663 / 1000At timestep:     695 / 1000At timestep:     724 / 1000At timestep:     752 / 1000At timestep:     770 / 1000At timestep:     785 / 1000At timestep:     802 / 1000At timestep:     838 / 1000At timestep:     864 / 1000At timestep:     885 / 1000At timestep:     908 / 1000At timestep:     918 / 1000At timestep:     934 / 1000At timestep:     960 / 1000At timestep:     979 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -17.889999389648438
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1832.593512058258
Exploration Critic Loss : 18425822.0
Exploitation Critic Loss : 0.10715676844120026
Exploration Model Loss : 35.28398132324219
Actor Loss : 0.4875759482383728
Eval_AverageReturn : -19.91666603088379
Eval_StdReturn : 5.645179748535156
Eval_MaxReturn : -9.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 20.916666666666668
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -17.750000
best mean reward -16.590000
running time 1893.055160
At timestep:     15 / 1000At timestep:     31 / 1000At timestep:     46 / 1000At timestep:     60 / 1000At timestep:     79 / 1000At timestep:     91 / 1000At timestep:     105 / 1000At timestep:     118 / 1000At timestep:     134 / 1000At timestep:     157 / 1000At timestep:     174 / 1000At timestep:     191 / 1000At timestep:     207 / 1000At timestep:     227 / 1000At timestep:     242 / 1000At timestep:     265 / 1000At timestep:     286 / 1000At timestep:     299 / 1000At timestep:     311 / 1000At timestep:     333 / 1000At timestep:     352 / 1000At timestep:     374 / 1000At timestep:     394 / 1000At timestep:     407 / 1000At timestep:     418 / 1000At timestep:     445 / 1000At timestep:     467 / 1000At timestep:     487 / 1000At timestep:     500 / 1000At timestep:     521 / 1000At timestep:     541 / 1000At timestep:     559 / 1000At timestep:     580 / 1000At timestep:     593 / 1000At timestep:     616 / 1000At timestep:     630 / 1000At timestep:     648 / 1000At timestep:     672 / 1000At timestep:     695 / 1000At timestep:     710 / 1000At timestep:     722 / 1000At timestep:     740 / 1000At timestep:     754 / 1000At timestep:     775 / 1000At timestep:     805 / 1000At timestep:     835 / 1000At timestep:     848 / 1000At timestep:     859 / 1000At timestep:     876 / 1000At timestep:     900 / 1000At timestep:     928 / 1000At timestep:     942 / 1000At timestep:     968 / 1000At timestep:     991 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -17.75
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1893.0551595687866
Exploration Critic Loss : 31016924.0
Exploitation Critic Loss : 0.087426096200943
Exploration Model Loss : 52.84906005859375
Actor Loss : 0.4427848756313324
Eval_AverageReturn : -17.309091567993164
Eval_StdReturn : 4.89479398727417
Eval_MaxReturn : -10.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 18.30909090909091
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -17.580000
best mean reward -16.590000
running time 1959.212731
At timestep:     15 / 1000At timestep:     28 / 1000At timestep:     51 / 1000At timestep:     70 / 1000At timestep:     86 / 1000At timestep:     109 / 1000At timestep:     124 / 1000At timestep:     141 / 1000At timestep:     160 / 1000At timestep:     173 / 1000At timestep:     193 / 1000At timestep:     224 / 1000At timestep:     243 / 1000At timestep:     257 / 1000At timestep:     276 / 1000At timestep:     295 / 1000At timestep:     307 / 1000At timestep:     332 / 1000At timestep:     354 / 1000At timestep:     368 / 1000At timestep:     385 / 1000At timestep:     405 / 1000At timestep:     428 / 1000At timestep:     452 / 1000At timestep:     489 / 1000At timestep:     504 / 1000At timestep:     525 / 1000At timestep:     540 / 1000At timestep:     560 / 1000At timestep:     580 / 1000At timestep:     597 / 1000At timestep:     616 / 1000At timestep:     631 / 1000At timestep:     649 / 1000At timestep:     670 / 1000At timestep:     686 / 1000At timestep:     702 / 1000At timestep:     715 / 1000At timestep:     743 / 1000At timestep:     764 / 1000At timestep:     782 / 1000At timestep:     812 / 1000At timestep:     825 / 1000At timestep:     849 / 1000At timestep:     869 / 1000At timestep:     894 / 1000At timestep:     917 / 1000At timestep:     934 / 1000At timestep:     960 / 1000At timestep:     980 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -17.579999923706055
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1959.212730884552
Exploration Critic Loss : 10255645.0
Exploitation Critic Loss : 0.08634831756353378
Exploration Model Loss : 26.470962524414062
Actor Loss : 0.5528136491775513
Eval_AverageReturn : -18.647058486938477
Eval_StdReturn : 5.0247368812561035
Eval_MaxReturn : -11.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 19.647058823529413
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -17.670000
best mean reward -16.590000
running time 2023.422587
At timestep:     30 / 1000At timestep:     51 / 1000At timestep:     71 / 1000At timestep:     89 / 1000At timestep:     106 / 1000At timestep:     129 / 1000At timestep:     143 / 1000At timestep:     168 / 1000At timestep:     180 / 1000At timestep:     194 / 1000At timestep:     212 / 1000At timestep:     232 / 1000At timestep:     258 / 1000At timestep:     277 / 1000At timestep:     294 / 1000At timestep:     313 / 1000At timestep:     330 / 1000At timestep:     346 / 1000At timestep:     373 / 1000At timestep:     393 / 1000At timestep:     411 / 1000At timestep:     430 / 1000At timestep:     452 / 1000At timestep:     467 / 1000At timestep:     485 / 1000At timestep:     512 / 1000At timestep:     529 / 1000At timestep:     549 / 1000At timestep:     568 / 1000At timestep:     595 / 1000At timestep:     612 / 1000At timestep:     629 / 1000At timestep:     648 / 1000At timestep:     664 / 1000At timestep:     699 / 1000At timestep:     719 / 1000At timestep:     743 / 1000At timestep:     758 / 1000At timestep:     776 / 1000At timestep:     790 / 1000At timestep:     813 / 1000At timestep:     833 / 1000At timestep:     845 / 1000At timestep:     862 / 1000At timestep:     881 / 1000At timestep:     892 / 1000At timestep:     915 / 1000At timestep:     937 / 1000At timestep:     957 / 1000At timestep:     972 / 1000At timestep:     987 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -17.670000076293945
Train_BestReturn : -16.59000015258789
TimeSinceStart : 2023.4225869178772
Exploration Critic Loss : 20395300.0
Exploitation Critic Loss : 0.09837926179170609
Exploration Model Loss : 35.28446960449219
Actor Loss : 0.4752049744129181
Eval_AverageReturn : -18.69230842590332
Eval_StdReturn : 5.201842784881592
Eval_MaxReturn : -10.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 19.692307692307693
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -18.090000
best mean reward -16.590000
running time 2086.888752
At timestep:     23 / 1000At timestep:     38 / 1000At timestep:     55 / 1000At timestep:     68 / 1000At timestep:     86 / 1000At timestep:     103 / 1000At timestep:     118 / 1000At timestep:     137 / 1000At timestep:     153 / 1000At timestep:     172 / 1000At timestep:     188 / 1000At timestep:     209 / 1000At timestep:     231 / 1000At timestep:     259 / 1000At timestep:     275 / 1000At timestep:     292 / 1000At timestep:     310 / 1000At timestep:     329 / 1000At timestep:     350 / 1000At timestep:     378 / 1000At timestep:     395 / 1000At timestep:     417 / 1000At timestep:     435 / 1000At timestep:     461 / 1000At timestep:     486 / 1000At timestep:     517 / 1000At timestep:     538 / 1000At timestep:     552 / 1000At timestep:     574 / 1000At timestep:     592 / 1000At timestep:     610 / 1000At timestep:     634 / 1000At timestep:     652 / 1000At timestep:     674 / 1000At timestep:     692 / 1000At timestep:     723 / 1000At timestep:     743 / 1000At timestep:     759 / 1000At timestep:     780 / 1000At timestep:     810 / 1000At timestep:     825 / 1000At timestep:     846 / 1000At timestep:     861 / 1000At timestep:     876 / 1000At timestep:     892 / 1000At timestep:     907 / 1000At timestep:     920 / 1000At timestep:     937 / 1000At timestep:     972 / 1000At timestep:     992 / 1000At timestep:     1017 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_easy_unsupervised_lam1_PointmassEasy-v0_22-11-2022_20-53-37/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -18.09000015258789
Train_BestReturn : -16.59000015258789
TimeSinceStart : 2086.8887519836426
Exploration Critic Loss : 29933884.0
Exploitation Critic Loss : 0.09164588898420334
Exploration Model Loss : 52.84857177734375
Actor Loss : 0.483426570892334
Eval_AverageReturn : -18.941177368164062
Eval_StdReturn : 4.983942031860352
Eval_MaxReturn : -12.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.941176470588236
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam2_PointmassEasy-v0_22-11-2022_21-29-33 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam2_PointmassEasy-v0_22-11-2022_21-29-33
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002363
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0023632049560546875
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 17.529902
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -50.0
TimeSinceStart : 17.52990174293518
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 34.226819
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -50.0
TimeSinceStart : 34.22681927680969
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 80.874447
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     399 / 1000At timestep:     449 / 1000At timestep:     499 / 1000At timestep:     549 / 1000At timestep:     599 / 1000At timestep:     649 / 1000At timestep:     699 / 1000At timestep:     749 / 1000At timestep:     799 / 1000At timestep:     849 / 1000At timestep:     899 / 1000At timestep:     949 / 1000At timestep:     999 / 1000At timestep:     1049 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -50.0
TimeSinceStart : 80.87444686889648
Exploration Critic Loss : 716336.125
Exploitation Critic Loss : 0.031697046011686325
Exploration Model Loss : 42.3253059387207
Actor Loss : 1.15312922000885
Eval_AverageReturn : -49.904762268066406
Eval_StdReturn : 0.42591768503189087
Eval_MaxReturn : -48.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 49.95238095238095
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -49.974998
best mean reward -inf
running time 128.852945
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -49.974998474121094
TimeSinceStart : 128.8529450893402
Exploration Critic Loss : 10874706.0
Exploitation Critic Loss : 0.03371480107307434
Exploration Model Loss : 45.23441696166992
Actor Loss : 0.9960743188858032
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -48.650002
best mean reward -48.650002
running time 176.735485
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -48.650001525878906
Train_BestReturn : -48.650001525878906
TimeSinceStart : 176.7354850769043
Exploration Critic Loss : 10334954.0
Exploitation Critic Loss : 0.1478845775127411
Exploration Model Loss : 46.21416091918945
Actor Loss : 0.8058523535728455
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -39.970001
best mean reward -39.970001
running time 228.890285
At timestep:     50 / 1000At timestep:     89 / 1000At timestep:     139 / 1000At timestep:     176 / 1000At timestep:     196 / 1000At timestep:     246 / 1000At timestep:     296 / 1000At timestep:     346 / 1000At timestep:     393 / 1000At timestep:     443 / 1000At timestep:     493 / 1000At timestep:     543 / 1000At timestep:     580 / 1000At timestep:     630 / 1000At timestep:     680 / 1000At timestep:     730 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     845 / 1000At timestep:     893 / 1000At timestep:     943 / 1000At timestep:     993 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -39.970001220703125
Train_BestReturn : -39.970001220703125
TimeSinceStart : 228.89028549194336
Exploration Critic Loss : 10801600.0
Exploitation Critic Loss : 0.22369056940078735
Exploration Model Loss : 74.81109619140625
Actor Loss : 0.6377393007278442
Eval_AverageReturn : -44.434783935546875
Eval_StdReturn : 9.268113136291504
Eval_MaxReturn : -19.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 44.82608695652174
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -32.910000
best mean reward -32.910000
running time 280.072191
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     333 / 1000At timestep:     376 / 1000At timestep:     424 / 1000At timestep:     449 / 1000At timestep:     481 / 1000At timestep:     522 / 1000At timestep:     553 / 1000At timestep:     595 / 1000At timestep:     633 / 1000At timestep:     666 / 1000At timestep:     704 / 1000At timestep:     748 / 1000At timestep:     793 / 1000At timestep:     828 / 1000At timestep:     850 / 1000At timestep:     899 / 1000At timestep:     949 / 1000At timestep:     999 / 1000At timestep:     1049 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -32.90999984741211
Train_BestReturn : -32.90999984741211
TimeSinceStart : 280.0721912384033
Exploration Critic Loss : 7229096.0
Exploitation Critic Loss : 0.21334289014339447
Exploration Model Loss : 24.234115600585938
Actor Loss : 0.5792033672332764
Eval_AverageReturn : -41.279998779296875
Eval_StdReturn : 8.861241340637207
Eval_MaxReturn : -21.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 41.96
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -26.559999
best mean reward -26.559999
running time 333.780339
At timestep:     39 / 1000At timestep:     71 / 1000At timestep:     121 / 1000At timestep:     148 / 1000At timestep:     198 / 1000At timestep:     235 / 1000At timestep:     285 / 1000At timestep:     335 / 1000At timestep:     381 / 1000At timestep:     418 / 1000At timestep:     450 / 1000At timestep:     494 / 1000At timestep:     521 / 1000At timestep:     550 / 1000At timestep:     585 / 1000At timestep:     616 / 1000At timestep:     666 / 1000At timestep:     704 / 1000At timestep:     730 / 1000At timestep:     778 / 1000At timestep:     802 / 1000At timestep:     844 / 1000At timestep:     863 / 1000At timestep:     913 / 1000At timestep:     963 / 1000At timestep:     987 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -26.559999465942383
Train_BestReturn : -26.559999465942383
TimeSinceStart : 333.78033900260925
Exploration Critic Loss : 11032970.0
Exploitation Critic Loss : 0.17169296741485596
Exploration Model Loss : 19.387344360351562
Actor Loss : 0.5433231592178345
Eval_AverageReturn : -37.074073791503906
Eval_StdReturn : 10.066173553466797
Eval_MaxReturn : -18.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 37.851851851851855
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -22.010000
best mean reward -22.010000
running time 392.571651
At timestep:     50 / 1000At timestep:     89 / 1000At timestep:     129 / 1000At timestep:     161 / 1000At timestep:     211 / 1000At timestep:     261 / 1000At timestep:     296 / 1000At timestep:     338 / 1000At timestep:     384 / 1000At timestep:     423 / 1000At timestep:     462 / 1000At timestep:     497 / 1000At timestep:     547 / 1000At timestep:     578 / 1000At timestep:     598 / 1000At timestep:     648 / 1000At timestep:     698 / 1000At timestep:     748 / 1000At timestep:     766 / 1000At timestep:     804 / 1000At timestep:     854 / 1000At timestep:     904 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -22.010000228881836
Train_BestReturn : -22.010000228881836
TimeSinceStart : 392.5716505050659
Exploration Critic Loss : 10929119.0
Exploitation Critic Loss : 0.23504190146923065
Exploration Model Loss : 18.178268432617188
Actor Loss : 0.5913934707641602
Eval_AverageReturn : -41.08333206176758
Eval_StdReturn : 9.720754623413086
Eval_MaxReturn : -17.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 41.666666666666664
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -18.540001
best mean reward -18.540001
running time 448.064238
At timestep:     43 / 1000At timestep:     72 / 1000At timestep:     96 / 1000At timestep:     146 / 1000At timestep:     183 / 1000At timestep:     205 / 1000At timestep:     223 / 1000At timestep:     258 / 1000At timestep:     276 / 1000At timestep:     306 / 1000At timestep:     333 / 1000At timestep:     366 / 1000At timestep:     406 / 1000At timestep:     429 / 1000At timestep:     470 / 1000At timestep:     506 / 1000At timestep:     541 / 1000At timestep:     563 / 1000At timestep:     613 / 1000At timestep:     652 / 1000At timestep:     702 / 1000At timestep:     724 / 1000At timestep:     746 / 1000At timestep:     796 / 1000At timestep:     818 / 1000At timestep:     855 / 1000At timestep:     868 / 1000At timestep:     918 / 1000At timestep:     945 / 1000At timestep:     995 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -18.540000915527344
Train_BestReturn : -18.540000915527344
TimeSinceStart : 448.06423807144165
Exploration Critic Loss : 11610566.0
Exploitation Critic Loss : 0.20626211166381836
Exploration Model Loss : 17.782699584960938
Actor Loss : 0.5636118650436401
Eval_AverageReturn : -32.225807189941406
Eval_StdReturn : 11.284330368041992
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 33.064516129032256
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -18.780001
best mean reward -18.540001
running time 505.037289
At timestep:     30 / 1000At timestep:     53 / 1000At timestep:     85 / 1000At timestep:     122 / 1000At timestep:     145 / 1000At timestep:     173 / 1000At timestep:     207 / 1000At timestep:     236 / 1000At timestep:     273 / 1000At timestep:     294 / 1000At timestep:     333 / 1000At timestep:     349 / 1000At timestep:     383 / 1000At timestep:     433 / 1000At timestep:     480 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     579 / 1000At timestep:     612 / 1000At timestep:     631 / 1000At timestep:     659 / 1000At timestep:     679 / 1000At timestep:     710 / 1000At timestep:     760 / 1000At timestep:     788 / 1000At timestep:     822 / 1000At timestep:     851 / 1000At timestep:     901 / 1000At timestep:     931 / 1000At timestep:     961 / 1000At timestep:     984 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -18.780000686645508
Train_BestReturn : -18.540000915527344
TimeSinceStart : 505.03728890419006
Exploration Critic Loss : 14292191.0
Exploitation Critic Loss : 0.14790035784244537
Exploration Model Loss : 17.642593383789062
Actor Loss : 0.6651192903518677
Eval_AverageReturn : -30.40625
Eval_StdReturn : 9.73029899597168
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 31.34375
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -18.400000
best mean reward -18.400000
running time 564.760432
At timestep:     24 / 1000At timestep:     66 / 1000At timestep:     91 / 1000At timestep:     134 / 1000At timestep:     184 / 1000At timestep:     214 / 1000At timestep:     242 / 1000At timestep:     271 / 1000At timestep:     292 / 1000At timestep:     325 / 1000At timestep:     349 / 1000At timestep:     373 / 1000At timestep:     409 / 1000At timestep:     439 / 1000At timestep:     458 / 1000At timestep:     501 / 1000At timestep:     525 / 1000At timestep:     553 / 1000At timestep:     568 / 1000At timestep:     591 / 1000At timestep:     641 / 1000At timestep:     661 / 1000At timestep:     680 / 1000At timestep:     720 / 1000At timestep:     759 / 1000At timestep:     775 / 1000At timestep:     798 / 1000At timestep:     826 / 1000At timestep:     847 / 1000At timestep:     873 / 1000At timestep:     919 / 1000At timestep:     941 / 1000At timestep:     961 / 1000At timestep:     983 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -18.399999618530273
Train_BestReturn : -18.399999618530273
TimeSinceStart : 564.76043176651
Exploration Critic Loss : 13525493.0
Exploitation Critic Loss : 0.13292348384857178
Exploration Model Loss : 17.591293334960938
Actor Loss : 0.6576168537139893
Eval_AverageReturn : -27.91428565979004
Eval_StdReturn : 9.65511417388916
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 28.857142857142858
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -17.860001
best mean reward -17.860001
running time 625.689055
At timestep:     21 / 1000At timestep:     71 / 1000At timestep:     97 / 1000At timestep:     130 / 1000At timestep:     166 / 1000At timestep:     202 / 1000At timestep:     226 / 1000At timestep:     252 / 1000At timestep:     302 / 1000At timestep:     351 / 1000At timestep:     379 / 1000At timestep:     394 / 1000At timestep:     421 / 1000At timestep:     448 / 1000At timestep:     462 / 1000At timestep:     492 / 1000At timestep:     520 / 1000At timestep:     552 / 1000At timestep:     579 / 1000At timestep:     629 / 1000At timestep:     668 / 1000At timestep:     702 / 1000At timestep:     724 / 1000At timestep:     756 / 1000At timestep:     791 / 1000At timestep:     830 / 1000At timestep:     880 / 1000At timestep:     914 / 1000At timestep:     964 / 1000At timestep:     996 / 1000At timestep:     1030 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -17.860000610351562
Train_BestReturn : -17.860000610351562
TimeSinceStart : 625.6890552043915
Exploration Critic Loss : 9288064.0
Exploitation Critic Loss : 0.1605176329612732
Exploration Model Loss : 17.57183837890625
Actor Loss : 0.721673309803009
Eval_AverageReturn : -32.3870964050293
Eval_StdReturn : 10.266379356384277
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 33.225806451612904
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -17.200001
best mean reward -17.200001
running time 685.971849
At timestep:     46 / 1000At timestep:     60 / 1000At timestep:     74 / 1000At timestep:     111 / 1000At timestep:     147 / 1000At timestep:     188 / 1000At timestep:     209 / 1000At timestep:     233 / 1000At timestep:     262 / 1000At timestep:     283 / 1000At timestep:     310 / 1000At timestep:     349 / 1000At timestep:     378 / 1000At timestep:     399 / 1000At timestep:     414 / 1000At timestep:     436 / 1000At timestep:     467 / 1000At timestep:     489 / 1000At timestep:     525 / 1000At timestep:     548 / 1000At timestep:     581 / 1000At timestep:     598 / 1000At timestep:     617 / 1000At timestep:     661 / 1000At timestep:     676 / 1000At timestep:     711 / 1000At timestep:     733 / 1000At timestep:     772 / 1000At timestep:     798 / 1000At timestep:     838 / 1000At timestep:     861 / 1000At timestep:     877 / 1000At timestep:     896 / 1000At timestep:     940 / 1000At timestep:     967 / 1000At timestep:     995 / 1000At timestep:     1025 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -17.200000762939453
Train_BestReturn : -17.200000762939453
TimeSinceStart : 685.9718489646912
Exploration Critic Loss : 12633788.0
Exploitation Critic Loss : 0.09151648730039597
Exploration Model Loss : 17.565048217773438
Actor Loss : 0.6989774107933044
Eval_AverageReturn : -26.70270347595215
Eval_StdReturn : 9.255721092224121
Eval_MaxReturn : -13.0
Eval_MinReturn : -45.0
Eval_AverageEpLen : 27.7027027027027
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -17.709999
best mean reward -17.200001
running time 744.994838
At timestep:     22 / 1000At timestep:     41 / 1000At timestep:     85 / 1000At timestep:     112 / 1000At timestep:     149 / 1000At timestep:     177 / 1000At timestep:     227 / 1000At timestep:     239 / 1000At timestep:     277 / 1000At timestep:     320 / 1000At timestep:     357 / 1000At timestep:     399 / 1000At timestep:     422 / 1000At timestep:     449 / 1000At timestep:     496 / 1000At timestep:     522 / 1000At timestep:     551 / 1000At timestep:     579 / 1000At timestep:     615 / 1000At timestep:     662 / 1000At timestep:     690 / 1000At timestep:     724 / 1000At timestep:     766 / 1000At timestep:     801 / 1000At timestep:     844 / 1000At timestep:     862 / 1000At timestep:     888 / 1000At timestep:     935 / 1000At timestep:     965 / 1000At timestep:     988 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -17.709999084472656
Train_BestReturn : -17.200000762939453
TimeSinceStart : 744.9948377609253
Exploration Critic Loss : 9021532.0
Exploitation Critic Loss : 0.0814063549041748
Exploration Model Loss : 17.562667846679688
Actor Loss : 0.6746431589126587
Eval_AverageReturn : -32.032257080078125
Eval_StdReturn : 9.660307884216309
Eval_MaxReturn : -11.0
Eval_MinReturn : -49.0
Eval_AverageEpLen : 33.03225806451613
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -17.650000
best mean reward -17.200001
running time 803.303121
At timestep:     19 / 1000At timestep:     41 / 1000At timestep:     71 / 1000At timestep:     101 / 1000At timestep:     124 / 1000At timestep:     155 / 1000At timestep:     170 / 1000At timestep:     202 / 1000At timestep:     228 / 1000At timestep:     256 / 1000At timestep:     277 / 1000At timestep:     308 / 1000At timestep:     356 / 1000At timestep:     377 / 1000At timestep:     401 / 1000At timestep:     428 / 1000At timestep:     469 / 1000At timestep:     492 / 1000At timestep:     515 / 1000At timestep:     535 / 1000At timestep:     551 / 1000At timestep:     591 / 1000At timestep:     627 / 1000At timestep:     653 / 1000At timestep:     675 / 1000At timestep:     698 / 1000At timestep:     720 / 1000At timestep:     741 / 1000At timestep:     768 / 1000At timestep:     796 / 1000At timestep:     829 / 1000At timestep:     849 / 1000At timestep:     877 / 1000At timestep:     896 / 1000At timestep:     917 / 1000At timestep:     947 / 1000At timestep:     969 / 1000At timestep:     994 / 1000At timestep:     1044 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -17.649999618530273
Train_BestReturn : -17.200000762939453
TimeSinceStart : 803.3031213283539
Exploration Critic Loss : 9704558.0
Exploitation Critic Loss : 0.16156494617462158
Exploration Model Loss : 17.561614990234375
Actor Loss : 0.6729153394699097
Eval_AverageReturn : -25.794872283935547
Eval_StdReturn : 7.841962814331055
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 26.76923076923077
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -17.590000
best mean reward -17.200001
running time 863.526068
At timestep:     25 / 1000At timestep:     45 / 1000At timestep:     66 / 1000At timestep:     102 / 1000At timestep:     133 / 1000At timestep:     163 / 1000At timestep:     191 / 1000At timestep:     222 / 1000At timestep:     254 / 1000At timestep:     279 / 1000At timestep:     310 / 1000At timestep:     336 / 1000At timestep:     365 / 1000At timestep:     388 / 1000At timestep:     407 / 1000At timestep:     443 / 1000At timestep:     460 / 1000At timestep:     482 / 1000At timestep:     508 / 1000At timestep:     538 / 1000At timestep:     558 / 1000At timestep:     583 / 1000At timestep:     604 / 1000At timestep:     624 / 1000At timestep:     641 / 1000At timestep:     664 / 1000At timestep:     688 / 1000At timestep:     708 / 1000At timestep:     736 / 1000At timestep:     752 / 1000At timestep:     790 / 1000At timestep:     821 / 1000At timestep:     845 / 1000At timestep:     865 / 1000At timestep:     888 / 1000At timestep:     911 / 1000At timestep:     934 / 1000At timestep:     970 / 1000At timestep:     989 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -17.59000015258789
Train_BestReturn : -17.200000762939453
TimeSinceStart : 863.5260679721832
Exploration Critic Loss : 10141134.0
Exploitation Critic Loss : 0.08092945069074631
Exploration Model Loss : 17.560791015625
Actor Loss : 0.6198261976242065
Eval_AverageReturn : -24.575000762939453
Eval_StdReturn : 5.804685592651367
Eval_MaxReturn : -15.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 25.575
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -17.690001
best mean reward -17.200001
running time 924.915481
At timestep:     18 / 1000At timestep:     46 / 1000At timestep:     78 / 1000At timestep:     98 / 1000At timestep:     119 / 1000At timestep:     146 / 1000At timestep:     166 / 1000At timestep:     191 / 1000At timestep:     227 / 1000At timestep:     249 / 1000At timestep:     284 / 1000At timestep:     304 / 1000At timestep:     320 / 1000At timestep:     340 / 1000At timestep:     357 / 1000At timestep:     391 / 1000At timestep:     413 / 1000At timestep:     451 / 1000At timestep:     472 / 1000At timestep:     497 / 1000At timestep:     514 / 1000At timestep:     537 / 1000At timestep:     555 / 1000At timestep:     594 / 1000At timestep:     625 / 1000At timestep:     654 / 1000At timestep:     684 / 1000At timestep:     714 / 1000At timestep:     736 / 1000At timestep:     753 / 1000At timestep:     772 / 1000At timestep:     801 / 1000At timestep:     824 / 1000At timestep:     858 / 1000At timestep:     875 / 1000At timestep:     895 / 1000At timestep:     917 / 1000At timestep:     941 / 1000At timestep:     982 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -17.690000534057617
Train_BestReturn : -17.200000762939453
TimeSinceStart : 924.9154813289642
Exploration Critic Loss : 10497302.0
Exploitation Critic Loss : 0.1388738453388214
Exploration Model Loss : 17.560501098632812
Actor Loss : 0.6038380861282349
Eval_AverageReturn : -24.25
Eval_StdReturn : 6.825503826141357
Eval_MaxReturn : -15.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 25.25
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.559999
best mean reward -17.200001
running time 985.787509
At timestep:     42 / 1000At timestep:     77 / 1000At timestep:     108 / 1000At timestep:     127 / 1000At timestep:     147 / 1000At timestep:     160 / 1000At timestep:     210 / 1000At timestep:     228 / 1000At timestep:     257 / 1000At timestep:     267 / 1000At timestep:     297 / 1000At timestep:     312 / 1000At timestep:     334 / 1000At timestep:     348 / 1000At timestep:     370 / 1000At timestep:     385 / 1000At timestep:     405 / 1000At timestep:     430 / 1000At timestep:     452 / 1000At timestep:     475 / 1000At timestep:     505 / 1000At timestep:     530 / 1000At timestep:     550 / 1000At timestep:     587 / 1000At timestep:     621 / 1000At timestep:     642 / 1000At timestep:     674 / 1000At timestep:     702 / 1000At timestep:     728 / 1000At timestep:     748 / 1000At timestep:     763 / 1000At timestep:     788 / 1000At timestep:     802 / 1000At timestep:     824 / 1000At timestep:     865 / 1000At timestep:     897 / 1000At timestep:     919 / 1000At timestep:     936 / 1000At timestep:     953 / 1000At timestep:     986 / 1000At timestep:     998 / 1000At timestep:     1038 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.559999465942383
Train_BestReturn : -17.200000762939453
TimeSinceStart : 985.7875089645386
Exploration Critic Loss : 10038559.0
Exploitation Critic Loss : 0.13316072523593903
Exploration Model Loss : 17.5601806640625
Actor Loss : 0.5623215436935425
Eval_AverageReturn : -23.738094329833984
Eval_StdReturn : 9.128864288330078
Eval_MaxReturn : -9.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 24.714285714285715
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.379999
best mean reward -17.200001
running time 1048.148159
At timestep:     22 / 1000At timestep:     38 / 1000At timestep:     66 / 1000At timestep:     88 / 1000At timestep:     105 / 1000At timestep:     123 / 1000At timestep:     159 / 1000At timestep:     175 / 1000At timestep:     207 / 1000At timestep:     226 / 1000At timestep:     249 / 1000At timestep:     262 / 1000At timestep:     282 / 1000At timestep:     325 / 1000At timestep:     340 / 1000At timestep:     362 / 1000At timestep:     387 / 1000At timestep:     410 / 1000At timestep:     439 / 1000At timestep:     482 / 1000At timestep:     504 / 1000At timestep:     525 / 1000At timestep:     557 / 1000At timestep:     586 / 1000At timestep:     610 / 1000At timestep:     634 / 1000At timestep:     653 / 1000At timestep:     682 / 1000At timestep:     702 / 1000At timestep:     720 / 1000At timestep:     736 / 1000At timestep:     755 / 1000At timestep:     788 / 1000At timestep:     818 / 1000At timestep:     835 / 1000At timestep:     857 / 1000At timestep:     894 / 1000At timestep:     928 / 1000At timestep:     944 / 1000At timestep:     966 / 1000At timestep:     995 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.3799991607666
Train_BestReturn : -17.200000762939453
TimeSinceStart : 1048.1481585502625
Exploration Critic Loss : 8912158.0
Exploitation Critic Loss : 0.11550094187259674
Exploration Model Loss : 17.55987548828125
Actor Loss : 0.5974204540252686
Eval_AverageReturn : -23.190475463867188
Eval_StdReturn : 7.358953475952148
Eval_MaxReturn : -12.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 24.19047619047619
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.270000
best mean reward -17.200001
running time 1110.342780
At timestep:     18 / 1000At timestep:     58 / 1000At timestep:     80 / 1000At timestep:     111 / 1000At timestep:     137 / 1000At timestep:     154 / 1000At timestep:     168 / 1000At timestep:     211 / 1000At timestep:     231 / 1000At timestep:     250 / 1000At timestep:     271 / 1000At timestep:     298 / 1000At timestep:     320 / 1000At timestep:     342 / 1000At timestep:     363 / 1000At timestep:     377 / 1000At timestep:     390 / 1000At timestep:     407 / 1000At timestep:     429 / 1000At timestep:     460 / 1000At timestep:     478 / 1000At timestep:     499 / 1000At timestep:     529 / 1000At timestep:     545 / 1000At timestep:     575 / 1000At timestep:     594 / 1000At timestep:     613 / 1000At timestep:     636 / 1000At timestep:     654 / 1000At timestep:     678 / 1000At timestep:     696 / 1000At timestep:     713 / 1000At timestep:     735 / 1000At timestep:     755 / 1000At timestep:     774 / 1000At timestep:     799 / 1000At timestep:     816 / 1000At timestep:     840 / 1000At timestep:     870 / 1000At timestep:     881 / 1000At timestep:     899 / 1000At timestep:     923 / 1000At timestep:     949 / 1000At timestep:     971 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.270000457763672
Train_BestReturn : -17.200000762939453
TimeSinceStart : 1110.3427798748016
Exploration Critic Loss : 10454947.0
Exploitation Critic Loss : 0.0716174840927124
Exploration Model Loss : 17.5595703125
Actor Loss : 0.5749650001525879
Eval_AverageReturn : -21.711111068725586
Eval_StdReturn : 7.614671230316162
Eval_MaxReturn : -10.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 22.68888888888889
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.559999
best mean reward -17.200001
running time 1173.316037
At timestep:     29 / 1000At timestep:     54 / 1000At timestep:     73 / 1000At timestep:     90 / 1000At timestep:     108 / 1000At timestep:     131 / 1000At timestep:     169 / 1000At timestep:     212 / 1000At timestep:     237 / 1000At timestep:     261 / 1000At timestep:     284 / 1000At timestep:     332 / 1000At timestep:     351 / 1000At timestep:     369 / 1000At timestep:     387 / 1000At timestep:     419 / 1000At timestep:     448 / 1000At timestep:     467 / 1000At timestep:     501 / 1000At timestep:     521 / 1000At timestep:     542 / 1000At timestep:     555 / 1000At timestep:     574 / 1000At timestep:     607 / 1000At timestep:     631 / 1000At timestep:     654 / 1000At timestep:     668 / 1000At timestep:     683 / 1000At timestep:     733 / 1000At timestep:     764 / 1000At timestep:     784 / 1000At timestep:     813 / 1000At timestep:     836 / 1000At timestep:     865 / 1000At timestep:     882 / 1000At timestep:     894 / 1000At timestep:     924 / 1000At timestep:     950 / 1000At timestep:     969 / 1000At timestep:     998 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.559999465942383
Train_BestReturn : -17.200000762939453
TimeSinceStart : 1173.3160371780396
Exploration Critic Loss : 10698446.0
Exploitation Critic Loss : 0.10984162986278534
Exploration Model Loss : 17.559280395507812
Actor Loss : 0.5643044710159302
Eval_AverageReturn : -23.85365867614746
Eval_StdReturn : 8.73892593383789
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 24.829268292682926
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -17.200001
best mean reward -17.200001
running time 1235.449709
At timestep:     23 / 1000At timestep:     54 / 1000At timestep:     71 / 1000At timestep:     86 / 1000At timestep:     106 / 1000At timestep:     136 / 1000At timestep:     152 / 1000At timestep:     175 / 1000At timestep:     194 / 1000At timestep:     220 / 1000At timestep:     234 / 1000At timestep:     255 / 1000At timestep:     276 / 1000At timestep:     303 / 1000At timestep:     328 / 1000At timestep:     372 / 1000At timestep:     402 / 1000At timestep:     427 / 1000At timestep:     464 / 1000At timestep:     483 / 1000At timestep:     521 / 1000At timestep:     542 / 1000At timestep:     555 / 1000At timestep:     584 / 1000At timestep:     605 / 1000At timestep:     623 / 1000At timestep:     647 / 1000At timestep:     669 / 1000At timestep:     685 / 1000At timestep:     698 / 1000At timestep:     714 / 1000At timestep:     747 / 1000At timestep:     762 / 1000At timestep:     796 / 1000At timestep:     809 / 1000At timestep:     827 / 1000At timestep:     844 / 1000At timestep:     873 / 1000At timestep:     890 / 1000At timestep:     910 / 1000At timestep:     935 / 1000At timestep:     960 / 1000At timestep:     998 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -17.200000762939453
Train_BestReturn : -17.200000762939453
TimeSinceStart : 1235.4497091770172
Exploration Critic Loss : 12119995.0
Exploitation Critic Loss : 0.12219621986150742
Exploration Model Loss : 17.5589599609375
Actor Loss : 0.5883200168609619
Eval_AverageReturn : -22.136363983154297
Eval_StdReturn : 7.497244834899902
Eval_MaxReturn : -12.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 23.136363636363637
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -17.510000
best mean reward -17.200001
running time 1298.176354
At timestep:     25 / 1000At timestep:     44 / 1000At timestep:     60 / 1000At timestep:     84 / 1000At timestep:     103 / 1000At timestep:     121 / 1000At timestep:     144 / 1000At timestep:     168 / 1000At timestep:     212 / 1000At timestep:     231 / 1000At timestep:     265 / 1000At timestep:     281 / 1000At timestep:     304 / 1000At timestep:     329 / 1000At timestep:     353 / 1000At timestep:     379 / 1000At timestep:     399 / 1000At timestep:     411 / 1000At timestep:     425 / 1000At timestep:     454 / 1000At timestep:     477 / 1000At timestep:     489 / 1000At timestep:     505 / 1000At timestep:     541 / 1000At timestep:     560 / 1000At timestep:     577 / 1000At timestep:     611 / 1000At timestep:     629 / 1000At timestep:     649 / 1000At timestep:     664 / 1000At timestep:     680 / 1000At timestep:     702 / 1000At timestep:     730 / 1000At timestep:     748 / 1000At timestep:     758 / 1000At timestep:     775 / 1000At timestep:     794 / 1000At timestep:     811 / 1000At timestep:     830 / 1000At timestep:     852 / 1000At timestep:     872 / 1000At timestep:     893 / 1000At timestep:     922 / 1000At timestep:     938 / 1000At timestep:     964 / 1000At timestep:     986 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -17.510000228881836
Train_BestReturn : -17.200000762939453
TimeSinceStart : 1298.176353931427
Exploration Critic Loss : 12747399.0
Exploitation Critic Loss : 0.07099505513906479
Exploration Model Loss : 17.55865478515625
Actor Loss : 0.590359091758728
Eval_AverageReturn : -20.46808433532715
Eval_StdReturn : 6.493371486663818
Eval_MaxReturn : -9.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 21.46808510638298
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -17.990000
best mean reward -17.200001
running time 1361.879678
At timestep:     29 / 1000At timestep:     49 / 1000At timestep:     65 / 1000At timestep:     83 / 1000At timestep:     106 / 1000At timestep:     128 / 1000At timestep:     150 / 1000At timestep:     168 / 1000At timestep:     184 / 1000At timestep:     199 / 1000At timestep:     213 / 1000At timestep:     260 / 1000At timestep:     280 / 1000At timestep:     304 / 1000At timestep:     316 / 1000At timestep:     353 / 1000At timestep:     383 / 1000At timestep:     406 / 1000At timestep:     428 / 1000At timestep:     450 / 1000At timestep:     480 / 1000At timestep:     499 / 1000At timestep:     531 / 1000At timestep:     549 / 1000At timestep:     571 / 1000At timestep:     603 / 1000At timestep:     622 / 1000At timestep:     640 / 1000At timestep:     655 / 1000At timestep:     689 / 1000At timestep:     702 / 1000At timestep:     721 / 1000At timestep:     741 / 1000At timestep:     771 / 1000At timestep:     795 / 1000At timestep:     815 / 1000At timestep:     828 / 1000At timestep:     847 / 1000At timestep:     885 / 1000At timestep:     902 / 1000At timestep:     924 / 1000At timestep:     955 / 1000At timestep:     972 / 1000At timestep:     998 / 1000At timestep:     1029 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -17.989999771118164
Train_BestReturn : -17.200000762939453
TimeSinceStart : 1361.8796780109406
Exploration Critic Loss : 10160367.0
Exploitation Critic Loss : 0.08500213921070099
Exploration Model Loss : 17.558624267578125
Actor Loss : 0.5608551502227783
Eval_AverageReturn : -21.866666793823242
Eval_StdReturn : 7.464285850524902
Eval_MaxReturn : -11.0
Eval_MinReturn : -46.0
Eval_AverageEpLen : 22.866666666666667
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -17.790001
best mean reward -17.200001
running time 1423.323599
At timestep:     18 / 1000At timestep:     35 / 1000At timestep:     54 / 1000At timestep:     74 / 1000At timestep:     88 / 1000At timestep:     107 / 1000At timestep:     144 / 1000At timestep:     164 / 1000At timestep:     189 / 1000At timestep:     206 / 1000At timestep:     224 / 1000At timestep:     243 / 1000At timestep:     260 / 1000At timestep:     275 / 1000At timestep:     301 / 1000At timestep:     331 / 1000At timestep:     354 / 1000At timestep:     370 / 1000At timestep:     401 / 1000At timestep:     419 / 1000At timestep:     440 / 1000At timestep:     465 / 1000At timestep:     490 / 1000At timestep:     509 / 1000At timestep:     525 / 1000At timestep:     545 / 1000At timestep:     562 / 1000At timestep:     585 / 1000At timestep:     616 / 1000At timestep:     632 / 1000At timestep:     657 / 1000At timestep:     672 / 1000At timestep:     693 / 1000At timestep:     725 / 1000At timestep:     741 / 1000At timestep:     768 / 1000At timestep:     780 / 1000At timestep:     810 / 1000At timestep:     835 / 1000At timestep:     856 / 1000At timestep:     879 / 1000At timestep:     897 / 1000At timestep:     908 / 1000At timestep:     928 / 1000At timestep:     947 / 1000At timestep:     968 / 1000At timestep:     985 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -17.790000915527344
Train_BestReturn : -17.200000762939453
TimeSinceStart : 1423.3235986232758
Exploration Critic Loss : 11596938.0
Exploitation Critic Loss : 0.11376813054084778
Exploration Model Loss : 17.558609008789062
Actor Loss : 0.6038711667060852
Eval_AverageReturn : -20.0
Eval_StdReturn : 5.4734206199646
Eval_MaxReturn : -10.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 21.0
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -17.620001
best mean reward -17.200001
running time 1487.014131
At timestep:     26 / 1000At timestep:     42 / 1000At timestep:     63 / 1000At timestep:     77 / 1000At timestep:     102 / 1000At timestep:     137 / 1000At timestep:     160 / 1000At timestep:     182 / 1000At timestep:     198 / 1000At timestep:     216 / 1000At timestep:     232 / 1000At timestep:     265 / 1000At timestep:     283 / 1000At timestep:     299 / 1000At timestep:     329 / 1000At timestep:     361 / 1000At timestep:     381 / 1000At timestep:     395 / 1000At timestep:     418 / 1000At timestep:     436 / 1000At timestep:     456 / 1000At timestep:     471 / 1000At timestep:     486 / 1000At timestep:     502 / 1000At timestep:     514 / 1000At timestep:     539 / 1000At timestep:     576 / 1000At timestep:     596 / 1000At timestep:     630 / 1000At timestep:     656 / 1000At timestep:     677 / 1000At timestep:     701 / 1000At timestep:     723 / 1000At timestep:     740 / 1000At timestep:     761 / 1000At timestep:     790 / 1000At timestep:     806 / 1000At timestep:     824 / 1000At timestep:     839 / 1000At timestep:     861 / 1000At timestep:     884 / 1000At timestep:     903 / 1000At timestep:     918 / 1000At timestep:     945 / 1000At timestep:     970 / 1000At timestep:     999 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -17.6200008392334
Train_BestReturn : -17.200000762939453
TimeSinceStart : 1487.0141308307648
Exploration Critic Loss : 11730022.0
Exploitation Critic Loss : 0.06823559105396271
Exploration Model Loss : 17.558624267578125
Actor Loss : 0.6382455825805664
Eval_AverageReturn : -20.744680404663086
Eval_StdReturn : 6.124482154846191
Eval_MaxReturn : -11.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 21.74468085106383
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -17.100000
best mean reward -17.100000
running time 1550.732111
At timestep:     30 / 1000At timestep:     43 / 1000At timestep:     71 / 1000At timestep:     91 / 1000At timestep:     114 / 1000At timestep:     132 / 1000At timestep:     158 / 1000At timestep:     173 / 1000At timestep:     193 / 1000At timestep:     213 / 1000At timestep:     228 / 1000At timestep:     257 / 1000At timestep:     280 / 1000At timestep:     297 / 1000At timestep:     323 / 1000At timestep:     339 / 1000At timestep:     359 / 1000At timestep:     391 / 1000At timestep:     421 / 1000At timestep:     439 / 1000At timestep:     481 / 1000At timestep:     504 / 1000At timestep:     548 / 1000At timestep:     566 / 1000At timestep:     589 / 1000At timestep:     605 / 1000At timestep:     625 / 1000At timestep:     643 / 1000At timestep:     665 / 1000At timestep:     680 / 1000At timestep:     714 / 1000At timestep:     735 / 1000At timestep:     752 / 1000At timestep:     772 / 1000At timestep:     797 / 1000At timestep:     819 / 1000At timestep:     833 / 1000At timestep:     853 / 1000At timestep:     867 / 1000At timestep:     885 / 1000At timestep:     912 / 1000At timestep:     936 / 1000At timestep:     954 / 1000At timestep:     974 / 1000At timestep:     992 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -17.100000381469727
Train_BestReturn : -17.100000381469727
TimeSinceStart : 1550.7321112155914
Exploration Critic Loss : 11524000.0
Exploitation Critic Loss : 0.1231079176068306
Exploration Model Loss : 17.558624267578125
Actor Loss : 0.5769032835960388
Eval_AverageReturn : -20.978260040283203
Eval_StdReturn : 6.6707963943481445
Eval_MaxReturn : -12.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 21.97826086956522
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -17.430000
best mean reward -17.100000
running time 1613.853011
At timestep:     14 / 1000At timestep:     40 / 1000At timestep:     65 / 1000At timestep:     90 / 1000At timestep:     108 / 1000At timestep:     125 / 1000At timestep:     155 / 1000At timestep:     182 / 1000At timestep:     198 / 1000At timestep:     229 / 1000At timestep:     255 / 1000At timestep:     274 / 1000At timestep:     288 / 1000At timestep:     304 / 1000At timestep:     335 / 1000At timestep:     355 / 1000At timestep:     372 / 1000At timestep:     387 / 1000At timestep:     413 / 1000At timestep:     429 / 1000At timestep:     451 / 1000At timestep:     470 / 1000At timestep:     491 / 1000At timestep:     511 / 1000At timestep:     531 / 1000At timestep:     556 / 1000At timestep:     579 / 1000At timestep:     601 / 1000At timestep:     626 / 1000At timestep:     648 / 1000At timestep:     672 / 1000At timestep:     685 / 1000At timestep:     700 / 1000At timestep:     723 / 1000At timestep:     742 / 1000At timestep:     754 / 1000At timestep:     770 / 1000At timestep:     795 / 1000At timestep:     809 / 1000At timestep:     826 / 1000At timestep:     848 / 1000At timestep:     863 / 1000At timestep:     887 / 1000At timestep:     904 / 1000At timestep:     920 / 1000At timestep:     934 / 1000At timestep:     971 / 1000At timestep:     987 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -17.43000030517578
Train_BestReturn : -17.100000381469727
TimeSinceStart : 1613.8530106544495
Exploration Critic Loss : 10262444.0
Exploitation Critic Loss : 0.0892815813422203
Exploration Model Loss : 17.558609008789062
Actor Loss : 0.5425599813461304
Eval_AverageReturn : -19.693878173828125
Eval_StdReturn : 5.4891510009765625
Eval_MaxReturn : -11.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 20.693877551020407
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -17.639999
best mean reward -17.100000
running time 1678.002549
At timestep:     22 / 1000At timestep:     51 / 1000At timestep:     79 / 1000At timestep:     104 / 1000At timestep:     118 / 1000At timestep:     135 / 1000At timestep:     149 / 1000At timestep:     163 / 1000At timestep:     181 / 1000At timestep:     198 / 1000At timestep:     216 / 1000At timestep:     257 / 1000At timestep:     279 / 1000At timestep:     300 / 1000At timestep:     321 / 1000At timestep:     345 / 1000At timestep:     364 / 1000At timestep:     387 / 1000At timestep:     404 / 1000At timestep:     427 / 1000At timestep:     449 / 1000At timestep:     468 / 1000At timestep:     486 / 1000At timestep:     524 / 1000At timestep:     546 / 1000At timestep:     569 / 1000At timestep:     588 / 1000At timestep:     606 / 1000At timestep:     630 / 1000At timestep:     659 / 1000At timestep:     682 / 1000At timestep:     703 / 1000At timestep:     723 / 1000At timestep:     746 / 1000At timestep:     772 / 1000At timestep:     793 / 1000At timestep:     807 / 1000At timestep:     823 / 1000At timestep:     843 / 1000At timestep:     857 / 1000At timestep:     874 / 1000At timestep:     902 / 1000At timestep:     928 / 1000At timestep:     948 / 1000At timestep:     976 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -17.639999389648438
Train_BestReturn : -17.100000381469727
TimeSinceStart : 1678.0025489330292
Exploration Critic Loss : 11522720.0
Exploitation Critic Loss : 0.11697553098201752
Exploration Model Loss : 17.558624267578125
Actor Loss : 0.5911550521850586
Eval_AverageReturn : -20.7608699798584
Eval_StdReturn : 5.607389450073242
Eval_MaxReturn : -13.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 21.76086956521739
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -17.040001
best mean reward -17.040001
running time 1740.966456
At timestep:     21 / 1000At timestep:     48 / 1000At timestep:     70 / 1000At timestep:     93 / 1000At timestep:     112 / 1000At timestep:     137 / 1000At timestep:     167 / 1000At timestep:     185 / 1000At timestep:     209 / 1000At timestep:     228 / 1000At timestep:     253 / 1000At timestep:     268 / 1000At timestep:     286 / 1000At timestep:     300 / 1000At timestep:     318 / 1000At timestep:     353 / 1000At timestep:     374 / 1000At timestep:     403 / 1000At timestep:     418 / 1000At timestep:     436 / 1000At timestep:     449 / 1000At timestep:     477 / 1000At timestep:     490 / 1000At timestep:     511 / 1000At timestep:     529 / 1000At timestep:     561 / 1000At timestep:     576 / 1000At timestep:     597 / 1000At timestep:     620 / 1000At timestep:     642 / 1000At timestep:     658 / 1000At timestep:     687 / 1000At timestep:     711 / 1000At timestep:     726 / 1000At timestep:     747 / 1000At timestep:     766 / 1000At timestep:     792 / 1000At timestep:     814 / 1000At timestep:     833 / 1000At timestep:     852 / 1000At timestep:     877 / 1000At timestep:     894 / 1000At timestep:     921 / 1000At timestep:     942 / 1000At timestep:     960 / 1000At timestep:     993 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -17.040000915527344
Train_BestReturn : -17.040000915527344
TimeSinceStart : 1740.966456413269
Exploration Critic Loss : 8493874.0
Exploitation Critic Loss : 0.08395697921514511
Exploration Model Loss : 17.558609008789062
Actor Loss : 0.5177348852157593
Eval_AverageReturn : -20.723403930664062
Eval_StdReturn : 5.366175174713135
Eval_MaxReturn : -12.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 21.72340425531915
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -16.980000
best mean reward -16.980000
running time 1804.588927
At timestep:     17 / 1000At timestep:     31 / 1000At timestep:     53 / 1000At timestep:     75 / 1000At timestep:     101 / 1000At timestep:     115 / 1000At timestep:     151 / 1000At timestep:     176 / 1000At timestep:     189 / 1000At timestep:     213 / 1000At timestep:     237 / 1000At timestep:     259 / 1000At timestep:     285 / 1000At timestep:     303 / 1000At timestep:     318 / 1000At timestep:     340 / 1000At timestep:     371 / 1000At timestep:     386 / 1000At timestep:     415 / 1000At timestep:     450 / 1000At timestep:     482 / 1000At timestep:     498 / 1000At timestep:     519 / 1000At timestep:     533 / 1000At timestep:     557 / 1000At timestep:     596 / 1000At timestep:     634 / 1000At timestep:     662 / 1000At timestep:     684 / 1000At timestep:     704 / 1000At timestep:     723 / 1000At timestep:     753 / 1000At timestep:     770 / 1000At timestep:     796 / 1000At timestep:     831 / 1000At timestep:     871 / 1000At timestep:     893 / 1000At timestep:     913 / 1000At timestep:     935 / 1000At timestep:     958 / 1000At timestep:     974 / 1000At timestep:     998 / 1000At timestep:     1022 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -16.979999542236328
Train_BestReturn : -16.979999542236328
TimeSinceStart : 1804.588927268982
Exploration Critic Loss : 12652777.0
Exploitation Critic Loss : 0.09077499806880951
Exploration Model Loss : 17.558624267578125
Actor Loss : 0.5213559865951538
Eval_AverageReturn : -22.76744270324707
Eval_StdReturn : 7.158788204193115
Eval_MaxReturn : -12.0
Eval_MinReturn : -39.0
Eval_AverageEpLen : 23.767441860465116
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -17.049999
best mean reward -16.980000
running time 1867.707617
At timestep:     22 / 1000At timestep:     44 / 1000At timestep:     59 / 1000At timestep:     109 / 1000At timestep:     139 / 1000At timestep:     156 / 1000At timestep:     179 / 1000At timestep:     196 / 1000At timestep:     212 / 1000At timestep:     235 / 1000At timestep:     250 / 1000At timestep:     272 / 1000At timestep:     291 / 1000At timestep:     323 / 1000At timestep:     337 / 1000At timestep:     366 / 1000At timestep:     387 / 1000At timestep:     426 / 1000At timestep:     441 / 1000At timestep:     459 / 1000At timestep:     484 / 1000At timestep:     523 / 1000At timestep:     537 / 1000At timestep:     559 / 1000At timestep:     590 / 1000At timestep:     603 / 1000At timestep:     629 / 1000At timestep:     657 / 1000At timestep:     674 / 1000At timestep:     689 / 1000At timestep:     723 / 1000At timestep:     744 / 1000At timestep:     767 / 1000At timestep:     786 / 1000At timestep:     803 / 1000At timestep:     820 / 1000At timestep:     833 / 1000At timestep:     859 / 1000At timestep:     877 / 1000At timestep:     891 / 1000At timestep:     904 / 1000At timestep:     921 / 1000At timestep:     950 / 1000At timestep:     963 / 1000At timestep:     984 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -17.049999237060547
Train_BestReturn : -16.979999542236328
TimeSinceStart : 1867.7076168060303
Exploration Critic Loss : 13043162.0
Exploitation Critic Loss : 0.13297632336616516
Exploration Model Loss : 17.558624267578125
Actor Loss : 0.5671231746673584
Eval_AverageReturn : -20.913043975830078
Eval_StdReturn : 7.991370677947998
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 21.891304347826086
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -16.590000
best mean reward -16.590000
running time 1933.136784
At timestep:     23 / 1000At timestep:     39 / 1000At timestep:     64 / 1000At timestep:     80 / 1000At timestep:     94 / 1000At timestep:     111 / 1000At timestep:     127 / 1000At timestep:     142 / 1000At timestep:     169 / 1000At timestep:     194 / 1000At timestep:     233 / 1000At timestep:     267 / 1000At timestep:     280 / 1000At timestep:     298 / 1000At timestep:     318 / 1000At timestep:     339 / 1000At timestep:     359 / 1000At timestep:     381 / 1000At timestep:     404 / 1000At timestep:     419 / 1000At timestep:     439 / 1000At timestep:     457 / 1000At timestep:     477 / 1000At timestep:     503 / 1000At timestep:     527 / 1000At timestep:     548 / 1000At timestep:     567 / 1000At timestep:     584 / 1000At timestep:     610 / 1000At timestep:     621 / 1000At timestep:     643 / 1000At timestep:     666 / 1000At timestep:     693 / 1000At timestep:     716 / 1000At timestep:     733 / 1000At timestep:     758 / 1000At timestep:     779 / 1000At timestep:     801 / 1000At timestep:     820 / 1000At timestep:     839 / 1000At timestep:     857 / 1000At timestep:     873 / 1000At timestep:     892 / 1000At timestep:     926 / 1000At timestep:     945 / 1000At timestep:     961 / 1000At timestep:     979 / 1000At timestep:     999 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -16.59000015258789
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1933.136783838272
Exploration Critic Loss : 11344581.0
Exploitation Critic Loss : 0.08005581796169281
Exploration Model Loss : 17.558624267578125
Actor Loss : 0.5486829876899719
Eval_AverageReturn : -19.775510787963867
Eval_StdReturn : 5.30792760848999
Eval_MaxReturn : -10.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 20.775510204081634
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -17.650000
best mean reward -16.590000
running time 1995.983485
At timestep:     27 / 1000At timestep:     47 / 1000At timestep:     61 / 1000At timestep:     89 / 1000At timestep:     106 / 1000At timestep:     133 / 1000At timestep:     152 / 1000At timestep:     171 / 1000At timestep:     186 / 1000At timestep:     202 / 1000At timestep:     224 / 1000At timestep:     241 / 1000At timestep:     267 / 1000At timestep:     300 / 1000At timestep:     314 / 1000At timestep:     346 / 1000At timestep:     369 / 1000At timestep:     383 / 1000At timestep:     410 / 1000At timestep:     427 / 1000At timestep:     446 / 1000At timestep:     461 / 1000At timestep:     489 / 1000At timestep:     521 / 1000At timestep:     533 / 1000At timestep:     554 / 1000At timestep:     570 / 1000At timestep:     585 / 1000At timestep:     611 / 1000At timestep:     627 / 1000At timestep:     642 / 1000At timestep:     673 / 1000At timestep:     689 / 1000At timestep:     712 / 1000At timestep:     732 / 1000At timestep:     758 / 1000At timestep:     774 / 1000At timestep:     797 / 1000At timestep:     823 / 1000At timestep:     844 / 1000At timestep:     865 / 1000At timestep:     878 / 1000At timestep:     905 / 1000At timestep:     923 / 1000At timestep:     965 / 1000At timestep:     981 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -17.649999618530273
Train_BestReturn : -16.59000015258789
TimeSinceStart : 1995.9834849834442
Exploration Critic Loss : 11178986.0
Exploitation Critic Loss : 0.09841631352901459
Exploration Model Loss : 17.558609008789062
Actor Loss : 0.609337329864502
Eval_AverageReturn : -20.404254913330078
Eval_StdReturn : 6.436794281005859
Eval_MaxReturn : -11.0
Eval_MinReturn : -41.0
Eval_AverageEpLen : 21.404255319148938
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -17.930000
best mean reward -16.590000
running time 2058.994644
At timestep:     20 / 1000At timestep:     34 / 1000At timestep:     50 / 1000At timestep:     63 / 1000At timestep:     90 / 1000At timestep:     109 / 1000At timestep:     132 / 1000At timestep:     155 / 1000At timestep:     170 / 1000At timestep:     187 / 1000At timestep:     209 / 1000At timestep:     241 / 1000At timestep:     262 / 1000At timestep:     288 / 1000At timestep:     311 / 1000At timestep:     325 / 1000At timestep:     344 / 1000At timestep:     363 / 1000At timestep:     382 / 1000At timestep:     400 / 1000At timestep:     420 / 1000At timestep:     442 / 1000At timestep:     459 / 1000At timestep:     479 / 1000At timestep:     493 / 1000At timestep:     507 / 1000At timestep:     524 / 1000At timestep:     545 / 1000At timestep:     567 / 1000At timestep:     586 / 1000At timestep:     604 / 1000At timestep:     625 / 1000At timestep:     655 / 1000At timestep:     672 / 1000At timestep:     694 / 1000At timestep:     706 / 1000At timestep:     729 / 1000At timestep:     744 / 1000At timestep:     764 / 1000At timestep:     779 / 1000At timestep:     795 / 1000At timestep:     810 / 1000At timestep:     826 / 1000At timestep:     868 / 1000At timestep:     882 / 1000At timestep:     932 / 1000At timestep:     950 / 1000At timestep:     967 / 1000At timestep:     985 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -17.93000030517578
Train_BestReturn : -16.59000015258789
TimeSinceStart : 2058.994644165039
Exploration Critic Loss : 14363623.0
Exploitation Critic Loss : 0.09231825172901154
Exploration Model Loss : 17.558624267578125
Actor Loss : 0.5607202053070068
Eval_AverageReturn : -19.219999313354492
Eval_StdReturn : 6.868158340454102
Eval_MaxReturn : -11.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 20.2
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -17.580000
best mean reward -16.590000
running time 2122.503063
At timestep:     16 / 1000At timestep:     44 / 1000At timestep:     59 / 1000At timestep:     74 / 1000At timestep:     89 / 1000At timestep:     108 / 1000At timestep:     127 / 1000At timestep:     149 / 1000At timestep:     166 / 1000At timestep:     186 / 1000At timestep:     204 / 1000At timestep:     225 / 1000At timestep:     250 / 1000At timestep:     267 / 1000At timestep:     292 / 1000At timestep:     312 / 1000At timestep:     329 / 1000At timestep:     347 / 1000At timestep:     369 / 1000At timestep:     387 / 1000At timestep:     406 / 1000At timestep:     433 / 1000At timestep:     450 / 1000At timestep:     470 / 1000At timestep:     502 / 1000At timestep:     519 / 1000At timestep:     535 / 1000At timestep:     559 / 1000At timestep:     575 / 1000At timestep:     593 / 1000At timestep:     619 / 1000At timestep:     634 / 1000At timestep:     652 / 1000At timestep:     672 / 1000At timestep:     690 / 1000At timestep:     708 / 1000At timestep:     725 / 1000At timestep:     749 / 1000At timestep:     775 / 1000At timestep:     795 / 1000At timestep:     814 / 1000At timestep:     826 / 1000At timestep:     850 / 1000At timestep:     870 / 1000At timestep:     882 / 1000At timestep:     907 / 1000At timestep:     928 / 1000At timestep:     941 / 1000At timestep:     971 / 1000At timestep:     991 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -17.579999923706055
Train_BestReturn : -16.59000015258789
TimeSinceStart : 2122.5030629634857
Exploration Critic Loss : 15994044.0
Exploitation Critic Loss : 0.08954472839832306
Exploration Model Loss : 17.558563232421875
Actor Loss : 0.5080626010894775
Eval_AverageReturn : -18.941177368164062
Eval_StdReturn : 4.447567462921143
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.941176470588236
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -17.320000
best mean reward -16.590000
running time 2187.481796
At timestep:     23 / 1000At timestep:     51 / 1000At timestep:     69 / 1000At timestep:     86 / 1000At timestep:     113 / 1000At timestep:     138 / 1000At timestep:     151 / 1000At timestep:     171 / 1000At timestep:     190 / 1000At timestep:     203 / 1000At timestep:     218 / 1000At timestep:     240 / 1000At timestep:     265 / 1000At timestep:     283 / 1000At timestep:     302 / 1000At timestep:     315 / 1000At timestep:     340 / 1000At timestep:     360 / 1000At timestep:     383 / 1000At timestep:     400 / 1000At timestep:     424 / 1000At timestep:     450 / 1000At timestep:     465 / 1000At timestep:     479 / 1000At timestep:     497 / 1000At timestep:     525 / 1000At timestep:     541 / 1000At timestep:     573 / 1000At timestep:     598 / 1000At timestep:     628 / 1000At timestep:     644 / 1000At timestep:     668 / 1000At timestep:     686 / 1000At timestep:     700 / 1000At timestep:     718 / 1000At timestep:     733 / 1000At timestep:     749 / 1000At timestep:     767 / 1000At timestep:     794 / 1000At timestep:     817 / 1000At timestep:     845 / 1000At timestep:     858 / 1000At timestep:     874 / 1000At timestep:     893 / 1000At timestep:     918 / 1000At timestep:     938 / 1000At timestep:     960 / 1000At timestep:     980 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -17.31999969482422
Train_BestReturn : -16.59000015258789
TimeSinceStart : 2187.4817957878113
Exploration Critic Loss : 13501532.0
Exploitation Critic Loss : 0.09190914779901505
Exploration Model Loss : 17.558258056640625
Actor Loss : 0.5028360486030579
Eval_AverageReturn : -19.428571701049805
Eval_StdReturn : 4.96107292175293
Eval_MaxReturn : -12.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 20.428571428571427
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -17.080000
best mean reward -16.590000
running time 2250.093705
At timestep:     23 / 1000At timestep:     36 / 1000At timestep:     60 / 1000At timestep:     80 / 1000At timestep:     103 / 1000At timestep:     119 / 1000At timestep:     137 / 1000At timestep:     158 / 1000At timestep:     173 / 1000At timestep:     187 / 1000At timestep:     208 / 1000At timestep:     222 / 1000At timestep:     247 / 1000At timestep:     261 / 1000At timestep:     279 / 1000At timestep:     306 / 1000At timestep:     332 / 1000At timestep:     349 / 1000At timestep:     365 / 1000At timestep:     382 / 1000At timestep:     396 / 1000At timestep:     426 / 1000At timestep:     450 / 1000At timestep:     466 / 1000At timestep:     483 / 1000At timestep:     503 / 1000At timestep:     521 / 1000At timestep:     532 / 1000At timestep:     559 / 1000At timestep:     578 / 1000At timestep:     593 / 1000At timestep:     613 / 1000At timestep:     628 / 1000At timestep:     656 / 1000At timestep:     677 / 1000At timestep:     701 / 1000At timestep:     720 / 1000At timestep:     734 / 1000At timestep:     748 / 1000At timestep:     768 / 1000At timestep:     789 / 1000At timestep:     814 / 1000At timestep:     832 / 1000At timestep:     852 / 1000At timestep:     866 / 1000At timestep:     884 / 1000At timestep:     905 / 1000At timestep:     918 / 1000At timestep:     932 / 1000At timestep:     948 / 1000At timestep:     969 / 1000At timestep:     988 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -17.079999923706055
Train_BestReturn : -16.59000015258789
TimeSinceStart : 2250.0937049388885
Exploration Critic Loss : 14229706.0
Exploitation Critic Loss : 0.09439264237880707
Exploration Model Loss : 17.557952880859375
Actor Loss : 0.46164876222610474
Eval_AverageReturn : -18.20754623413086
Eval_StdReturn : 4.643349647521973
Eval_MaxReturn : -10.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 19.20754716981132
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -16.490000
best mean reward -16.490000
running time 2314.391442
At timestep:     26 / 1000At timestep:     44 / 1000At timestep:     62 / 1000At timestep:     82 / 1000At timestep:     108 / 1000At timestep:     122 / 1000At timestep:     141 / 1000At timestep:     157 / 1000At timestep:     180 / 1000At timestep:     199 / 1000At timestep:     215 / 1000At timestep:     233 / 1000At timestep:     257 / 1000At timestep:     282 / 1000At timestep:     297 / 1000At timestep:     315 / 1000At timestep:     329 / 1000At timestep:     346 / 1000At timestep:     370 / 1000At timestep:     390 / 1000At timestep:     402 / 1000At timestep:     429 / 1000At timestep:     443 / 1000At timestep:     462 / 1000At timestep:     487 / 1000At timestep:     535 / 1000At timestep:     565 / 1000At timestep:     584 / 1000At timestep:     607 / 1000At timestep:     633 / 1000At timestep:     648 / 1000At timestep:     670 / 1000At timestep:     687 / 1000At timestep:     719 / 1000At timestep:     736 / 1000At timestep:     771 / 1000At timestep:     789 / 1000At timestep:     809 / 1000At timestep:     840 / 1000At timestep:     859 / 1000At timestep:     879 / 1000At timestep:     896 / 1000At timestep:     925 / 1000At timestep:     943 / 1000At timestep:     959 / 1000At timestep:     974 / 1000At timestep:     993 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -16.489999771118164
Train_BestReturn : -16.489999771118164
TimeSinceStart : 2314.3914415836334
Exploration Critic Loss : 12492724.0
Exploitation Critic Loss : 0.07749376446008682
Exploration Model Loss : 17.5576171875
Actor Loss : 0.5322182178497314
Eval_AverageReturn : -20.3125
Eval_StdReturn : 6.605415344238281
Eval_MaxReturn : -11.0
Eval_MinReturn : -47.0
Eval_AverageEpLen : 21.3125
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -16.889999
best mean reward -16.490000
running time 2377.530642
At timestep:     19 / 1000At timestep:     40 / 1000At timestep:     62 / 1000At timestep:     81 / 1000At timestep:     99 / 1000At timestep:     118 / 1000At timestep:     141 / 1000At timestep:     163 / 1000At timestep:     187 / 1000At timestep:     207 / 1000At timestep:     238 / 1000At timestep:     257 / 1000At timestep:     275 / 1000At timestep:     290 / 1000At timestep:     306 / 1000At timestep:     325 / 1000At timestep:     359 / 1000At timestep:     384 / 1000At timestep:     400 / 1000At timestep:     414 / 1000At timestep:     436 / 1000At timestep:     452 / 1000At timestep:     471 / 1000At timestep:     488 / 1000At timestep:     502 / 1000At timestep:     520 / 1000At timestep:     536 / 1000At timestep:     555 / 1000At timestep:     579 / 1000At timestep:     606 / 1000At timestep:     622 / 1000At timestep:     636 / 1000At timestep:     665 / 1000At timestep:     680 / 1000At timestep:     695 / 1000At timestep:     718 / 1000At timestep:     733 / 1000At timestep:     749 / 1000At timestep:     767 / 1000At timestep:     792 / 1000At timestep:     808 / 1000At timestep:     823 / 1000At timestep:     843 / 1000At timestep:     866 / 1000At timestep:     886 / 1000At timestep:     905 / 1000At timestep:     928 / 1000At timestep:     949 / 1000At timestep:     963 / 1000At timestep:     981 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -16.889999389648438
Train_BestReturn : -16.489999771118164
TimeSinceStart : 2377.5306420326233
Exploration Critic Loss : 14257145.0
Exploitation Critic Loss : 0.13584862649440765
Exploration Model Loss : 17.557281494140625
Actor Loss : 0.5904625654220581
Eval_AverageReturn : -18.6862735748291
Eval_StdReturn : 4.430244445800781
Eval_MaxReturn : -13.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 19.686274509803923
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -17.480000
best mean reward -16.490000
running time 2441.400872
At timestep:     23 / 1000At timestep:     37 / 1000At timestep:     56 / 1000At timestep:     69 / 1000At timestep:     89 / 1000At timestep:     106 / 1000At timestep:     124 / 1000At timestep:     139 / 1000At timestep:     163 / 1000At timestep:     192 / 1000At timestep:     215 / 1000At timestep:     227 / 1000At timestep:     244 / 1000At timestep:     259 / 1000At timestep:     273 / 1000At timestep:     291 / 1000At timestep:     309 / 1000At timestep:     335 / 1000At timestep:     355 / 1000At timestep:     375 / 1000At timestep:     403 / 1000At timestep:     418 / 1000At timestep:     442 / 1000At timestep:     461 / 1000At timestep:     490 / 1000At timestep:     508 / 1000At timestep:     532 / 1000At timestep:     560 / 1000At timestep:     577 / 1000At timestep:     608 / 1000At timestep:     637 / 1000At timestep:     672 / 1000At timestep:     705 / 1000At timestep:     735 / 1000At timestep:     754 / 1000At timestep:     764 / 1000At timestep:     780 / 1000At timestep:     800 / 1000At timestep:     836 / 1000At timestep:     852 / 1000At timestep:     871 / 1000At timestep:     891 / 1000At timestep:     904 / 1000At timestep:     926 / 1000At timestep:     950 / 1000At timestep:     966 / 1000At timestep:     984 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -17.479999542236328
Train_BestReturn : -16.489999771118164
TimeSinceStart : 2441.4008717536926
Exploration Critic Loss : 13163772.0
Exploitation Critic Loss : 0.08673670887947083
Exploration Model Loss : 17.556976318359375
Actor Loss : 0.5797654390335083
Eval_AverageReturn : -19.875
Eval_StdReturn : 6.19012975692749
Eval_MaxReturn : -9.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 20.875
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -17.660000
best mean reward -16.490000
running time 2504.820776
At timestep:     17 / 1000At timestep:     33 / 1000At timestep:     48 / 1000At timestep:     67 / 1000At timestep:     78 / 1000At timestep:     97 / 1000At timestep:     116 / 1000At timestep:     141 / 1000At timestep:     171 / 1000At timestep:     187 / 1000At timestep:     205 / 1000At timestep:     220 / 1000At timestep:     239 / 1000At timestep:     261 / 1000At timestep:     284 / 1000At timestep:     305 / 1000At timestep:     340 / 1000At timestep:     363 / 1000At timestep:     390 / 1000At timestep:     404 / 1000At timestep:     427 / 1000At timestep:     447 / 1000At timestep:     471 / 1000At timestep:     489 / 1000At timestep:     507 / 1000At timestep:     527 / 1000At timestep:     546 / 1000At timestep:     557 / 1000At timestep:     574 / 1000At timestep:     594 / 1000At timestep:     612 / 1000At timestep:     625 / 1000At timestep:     640 / 1000At timestep:     670 / 1000At timestep:     697 / 1000At timestep:     712 / 1000At timestep:     732 / 1000At timestep:     754 / 1000At timestep:     786 / 1000At timestep:     809 / 1000At timestep:     822 / 1000At timestep:     839 / 1000At timestep:     858 / 1000At timestep:     888 / 1000At timestep:     904 / 1000At timestep:     922 / 1000At timestep:     939 / 1000At timestep:     962 / 1000At timestep:     979 / 1000At timestep:     994 / 1000At timestep:     1024 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -17.65999984741211
Train_BestReturn : -16.489999771118164
TimeSinceStart : 2504.8207762241364
Exploration Critic Loss : 23007304.0
Exploitation Critic Loss : 0.10787229239940643
Exploration Model Loss : 17.556610107421875
Actor Loss : 0.5489867925643921
Eval_AverageReturn : -19.078432083129883
Eval_StdReturn : 5.426308631896973
Eval_MaxReturn : -10.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 20.07843137254902
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -17.690001
best mean reward -16.490000
running time 2568.681601
At timestep:     18 / 1000At timestep:     42 / 1000At timestep:     66 / 1000At timestep:     81 / 1000At timestep:     107 / 1000At timestep:     129 / 1000At timestep:     148 / 1000At timestep:     176 / 1000At timestep:     200 / 1000At timestep:     217 / 1000At timestep:     238 / 1000At timestep:     257 / 1000At timestep:     277 / 1000At timestep:     291 / 1000At timestep:     325 / 1000At timestep:     348 / 1000At timestep:     384 / 1000At timestep:     419 / 1000At timestep:     446 / 1000At timestep:     462 / 1000At timestep:     487 / 1000At timestep:     512 / 1000At timestep:     532 / 1000At timestep:     546 / 1000At timestep:     562 / 1000At timestep:     586 / 1000At timestep:     602 / 1000At timestep:     620 / 1000At timestep:     644 / 1000At timestep:     660 / 1000At timestep:     674 / 1000At timestep:     689 / 1000At timestep:     703 / 1000At timestep:     717 / 1000At timestep:     733 / 1000At timestep:     749 / 1000At timestep:     777 / 1000At timestep:     793 / 1000At timestep:     812 / 1000At timestep:     829 / 1000At timestep:     849 / 1000At timestep:     869 / 1000At timestep:     890 / 1000At timestep:     908 / 1000At timestep:     928 / 1000At timestep:     950 / 1000At timestep:     966 / 1000At timestep:     981 / 1000At timestep:     996 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -17.690000534057617
Train_BestReturn : -16.489999771118164
TimeSinceStart : 2568.6816012859344
Exploration Critic Loss : 14703526.0
Exploitation Critic Loss : 0.07542888820171356
Exploration Model Loss : 17.5562744140625
Actor Loss : 0.49472635984420776
Eval_AverageReturn : -19.280000686645508
Eval_StdReturn : 5.440735340118408
Eval_MaxReturn : -13.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 20.28
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -17.629999
best mean reward -16.490000
running time 2632.022351
At timestep:     17 / 1000At timestep:     30 / 1000At timestep:     46 / 1000At timestep:     69 / 1000At timestep:     88 / 1000At timestep:     108 / 1000At timestep:     124 / 1000At timestep:     142 / 1000At timestep:     156 / 1000At timestep:     178 / 1000At timestep:     201 / 1000At timestep:     220 / 1000At timestep:     242 / 1000At timestep:     263 / 1000At timestep:     275 / 1000At timestep:     293 / 1000At timestep:     318 / 1000At timestep:     332 / 1000At timestep:     353 / 1000At timestep:     372 / 1000At timestep:     387 / 1000At timestep:     403 / 1000At timestep:     427 / 1000At timestep:     444 / 1000At timestep:     470 / 1000At timestep:     488 / 1000At timestep:     509 / 1000At timestep:     526 / 1000At timestep:     542 / 1000At timestep:     553 / 1000At timestep:     572 / 1000At timestep:     592 / 1000At timestep:     608 / 1000At timestep:     628 / 1000At timestep:     661 / 1000At timestep:     676 / 1000At timestep:     691 / 1000At timestep:     712 / 1000At timestep:     734 / 1000At timestep:     752 / 1000At timestep:     770 / 1000At timestep:     782 / 1000At timestep:     797 / 1000At timestep:     817 / 1000At timestep:     866 / 1000At timestep:     879 / 1000At timestep:     898 / 1000At timestep:     913 / 1000At timestep:     927 / 1000At timestep:     948 / 1000At timestep:     968 / 1000At timestep:     988 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -17.6299991607666
Train_BestReturn : -16.489999771118164
TimeSinceStart : 2632.022350549698
Exploration Critic Loss : 24834480.0
Exploitation Critic Loss : 0.10186415910720825
Exploration Model Loss : 17.55596923828125
Actor Loss : 0.5035414695739746
Eval_AverageReturn : -18.01886749267578
Eval_StdReturn : 5.7346696853637695
Eval_MaxReturn : -10.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 19.0188679245283
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -17.870001
best mean reward -16.490000
running time 2695.722547
At timestep:     40 / 1000At timestep:     57 / 1000At timestep:     78 / 1000At timestep:     100 / 1000At timestep:     116 / 1000At timestep:     136 / 1000At timestep:     158 / 1000At timestep:     179 / 1000At timestep:     194 / 1000At timestep:     213 / 1000At timestep:     235 / 1000At timestep:     263 / 1000At timestep:     280 / 1000At timestep:     299 / 1000At timestep:     314 / 1000At timestep:     329 / 1000At timestep:     348 / 1000At timestep:     366 / 1000At timestep:     380 / 1000At timestep:     398 / 1000At timestep:     428 / 1000At timestep:     444 / 1000At timestep:     464 / 1000At timestep:     480 / 1000At timestep:     504 / 1000At timestep:     527 / 1000At timestep:     550 / 1000At timestep:     590 / 1000At timestep:     605 / 1000At timestep:     619 / 1000At timestep:     645 / 1000At timestep:     669 / 1000At timestep:     685 / 1000At timestep:     719 / 1000At timestep:     741 / 1000At timestep:     755 / 1000At timestep:     775 / 1000At timestep:     791 / 1000At timestep:     810 / 1000At timestep:     832 / 1000At timestep:     856 / 1000At timestep:     879 / 1000At timestep:     895 / 1000At timestep:     912 / 1000At timestep:     934 / 1000At timestep:     949 / 1000At timestep:     968 / 1000At timestep:     988 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -17.8700008392334
Train_BestReturn : -16.489999771118164
TimeSinceStart : 2695.722546815872
Exploration Critic Loss : 21436980.0
Exploitation Critic Loss : 0.08530299365520477
Exploration Model Loss : 17.5556640625
Actor Loss : 0.48446226119995117
Eval_AverageReturn : -19.53061294555664
Eval_StdReturn : 5.813784122467041
Eval_MaxReturn : -13.0
Eval_MinReturn : -39.0
Eval_AverageEpLen : 20.53061224489796
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -16.940001
best mean reward -16.490000
running time 2760.143435
At timestep:     18 / 1000At timestep:     35 / 1000At timestep:     55 / 1000At timestep:     70 / 1000At timestep:     88 / 1000At timestep:     110 / 1000At timestep:     122 / 1000At timestep:     138 / 1000At timestep:     155 / 1000At timestep:     183 / 1000At timestep:     204 / 1000At timestep:     226 / 1000At timestep:     242 / 1000At timestep:     261 / 1000At timestep:     278 / 1000At timestep:     301 / 1000At timestep:     319 / 1000At timestep:     340 / 1000At timestep:     365 / 1000At timestep:     388 / 1000At timestep:     405 / 1000At timestep:     440 / 1000At timestep:     456 / 1000At timestep:     469 / 1000At timestep:     483 / 1000At timestep:     501 / 1000At timestep:     525 / 1000At timestep:     546 / 1000At timestep:     570 / 1000At timestep:     584 / 1000At timestep:     606 / 1000At timestep:     627 / 1000At timestep:     645 / 1000At timestep:     660 / 1000At timestep:     683 / 1000At timestep:     700 / 1000At timestep:     724 / 1000At timestep:     750 / 1000At timestep:     765 / 1000At timestep:     789 / 1000At timestep:     820 / 1000At timestep:     837 / 1000At timestep:     855 / 1000At timestep:     871 / 1000At timestep:     890 / 1000At timestep:     905 / 1000At timestep:     923 / 1000At timestep:     948 / 1000At timestep:     965 / 1000At timestep:     982 / 1000At timestep:     996 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -16.940000534057617
Train_BestReturn : -16.489999771118164
TimeSinceStart : 2760.1434354782104
Exploration Critic Loss : 14941279.0
Exploitation Critic Loss : 0.0736979991197586
Exploration Model Loss : 17.555313110351562
Actor Loss : 0.5076403617858887
Eval_AverageReturn : -18.403846740722656
Eval_StdReturn : 4.642030239105225
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.403846153846153
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -17.250000
best mean reward -16.490000
running time 2823.657179
At timestep:     24 / 1000At timestep:     44 / 1000At timestep:     62 / 1000At timestep:     78 / 1000At timestep:     110 / 1000At timestep:     129 / 1000At timestep:     162 / 1000At timestep:     181 / 1000At timestep:     201 / 1000At timestep:     219 / 1000At timestep:     238 / 1000At timestep:     259 / 1000At timestep:     283 / 1000At timestep:     312 / 1000At timestep:     333 / 1000At timestep:     369 / 1000At timestep:     389 / 1000At timestep:     413 / 1000At timestep:     441 / 1000At timestep:     478 / 1000At timestep:     495 / 1000At timestep:     516 / 1000At timestep:     534 / 1000At timestep:     551 / 1000At timestep:     577 / 1000At timestep:     591 / 1000At timestep:     608 / 1000At timestep:     622 / 1000At timestep:     647 / 1000At timestep:     665 / 1000At timestep:     687 / 1000At timestep:     708 / 1000At timestep:     725 / 1000At timestep:     758 / 1000At timestep:     778 / 1000At timestep:     797 / 1000At timestep:     819 / 1000At timestep:     845 / 1000At timestep:     863 / 1000At timestep:     882 / 1000At timestep:     898 / 1000At timestep:     919 / 1000At timestep:     932 / 1000At timestep:     961 / 1000At timestep:     978 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -17.25
Train_BestReturn : -16.489999771118164
TimeSinceStart : 2823.6571786403656
Exploration Critic Loss : 23969086.0
Exploitation Critic Loss : 0.1113576889038086
Exploration Model Loss : 17.55511474609375
Actor Loss : 0.6015238761901855
Eval_AverageReturn : -20.89130401611328
Eval_StdReturn : 5.805650234222412
Eval_MaxReturn : -12.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 21.891304347826086
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -17.690001
best mean reward -16.490000
running time 2885.884967
At timestep:     17 / 1000At timestep:     40 / 1000At timestep:     65 / 1000At timestep:     81 / 1000At timestep:     100 / 1000At timestep:     121 / 1000At timestep:     144 / 1000At timestep:     159 / 1000At timestep:     185 / 1000At timestep:     199 / 1000At timestep:     217 / 1000At timestep:     234 / 1000At timestep:     255 / 1000At timestep:     278 / 1000At timestep:     299 / 1000At timestep:     315 / 1000At timestep:     331 / 1000At timestep:     345 / 1000At timestep:     368 / 1000At timestep:     383 / 1000At timestep:     416 / 1000At timestep:     439 / 1000At timestep:     455 / 1000At timestep:     493 / 1000At timestep:     520 / 1000At timestep:     537 / 1000At timestep:     562 / 1000At timestep:     590 / 1000At timestep:     607 / 1000At timestep:     626 / 1000At timestep:     648 / 1000At timestep:     666 / 1000At timestep:     683 / 1000At timestep:     700 / 1000At timestep:     718 / 1000At timestep:     742 / 1000At timestep:     765 / 1000At timestep:     788 / 1000At timestep:     815 / 1000At timestep:     835 / 1000At timestep:     848 / 1000At timestep:     867 / 1000At timestep:     884 / 1000At timestep:     906 / 1000At timestep:     928 / 1000At timestep:     941 / 1000At timestep:     958 / 1000At timestep:     985 / 1000At timestep:     1006 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_easy_unsupervised_lam2_PointmassEasy-v0_22-11-2022_21-29-33/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -17.690000534057617
Train_BestReturn : -16.489999771118164
TimeSinceStart : 2885.884966611862
Exploration Critic Loss : 26941316.0
Exploitation Critic Loss : 0.08577918261289597
Exploration Model Loss : 17.554916381835938
Actor Loss : 0.38752591609954834
Eval_AverageReturn : -19.53061294555664
Eval_StdReturn : 5.034987926483154
Eval_MaxReturn : -12.0
Eval_MinReturn : -37.0
Eval_AverageEpLen : 20.53061224489796
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam10_PointmassEasy-v0_22-11-2022_22-18-47 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam10_PointmassEasy-v0_22-11-2022_22-18-47
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002193
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.002192974090576172
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -50.000000
best mean reward -inf
running time 16.976932
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -50.0
TimeSinceStart : 16.976932287216187
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -49.174999
best mean reward -inf
running time 34.007384
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -49.17499923706055
TimeSinceStart : 34.007384300231934
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -40.902779
best mean reward -inf
running time 83.231826
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     238 / 1000At timestep:     288 / 1000At timestep:     332 / 1000At timestep:     382 / 1000At timestep:     432 / 1000At timestep:     482 / 1000At timestep:     532 / 1000At timestep:     582 / 1000At timestep:     632 / 1000At timestep:     682 / 1000At timestep:     732 / 1000At timestep:     782 / 1000At timestep:     817 / 1000At timestep:     852 / 1000At timestep:     897 / 1000At timestep:     947 / 1000At timestep:     997 / 1000At timestep:     1047 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -40.90277862548828
TimeSinceStart : 83.23182559013367
Exploration Critic Loss : 745702.875
Exploitation Critic Loss : 0.023287680000066757
Exploration Model Loss : 43.501380920410156
Actor Loss : 1.1694090366363525
Eval_AverageReturn : -47.318180084228516
Eval_StdReturn : 5.260369300842285
Eval_MaxReturn : -34.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 47.59090909090909
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -33.669998
best mean reward -33.669998
running time 135.520021
At timestep:     39 / 1000At timestep:     79 / 1000At timestep:     129 / 1000At timestep:     174 / 1000At timestep:     210 / 1000At timestep:     239 / 1000At timestep:     289 / 1000At timestep:     339 / 1000At timestep:     371 / 1000At timestep:     421 / 1000At timestep:     471 / 1000At timestep:     497 / 1000At timestep:     546 / 1000At timestep:     583 / 1000At timestep:     604 / 1000At timestep:     650 / 1000At timestep:     696 / 1000At timestep:     738 / 1000At timestep:     767 / 1000At timestep:     817 / 1000At timestep:     854 / 1000At timestep:     904 / 1000At timestep:     945 / 1000At timestep:     973 / 1000At timestep:     1023 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -33.66999816894531
Train_BestReturn : -33.66999816894531
TimeSinceStart : 135.5200207233429
Exploration Critic Loss : 7361553.0
Exploitation Critic Loss : 0.030855031684041023
Exploration Model Loss : 32.04978942871094
Actor Loss : 0.9405755996704102
Eval_AverageReturn : -40.20000076293945
Eval_StdReturn : 9.243375778198242
Eval_MaxReturn : -20.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 40.92
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -22.370001
best mean reward -22.370001
running time 190.722191
At timestep:     50 / 1000At timestep:     71 / 1000At timestep:     97 / 1000At timestep:     147 / 1000At timestep:     193 / 1000At timestep:     243 / 1000At timestep:     264 / 1000At timestep:     314 / 1000At timestep:     364 / 1000At timestep:     393 / 1000At timestep:     426 / 1000At timestep:     455 / 1000At timestep:     480 / 1000At timestep:     530 / 1000At timestep:     567 / 1000At timestep:     601 / 1000At timestep:     645 / 1000At timestep:     664 / 1000At timestep:     692 / 1000At timestep:     711 / 1000At timestep:     751 / 1000At timestep:     801 / 1000At timestep:     843 / 1000At timestep:     885 / 1000At timestep:     910 / 1000At timestep:     960 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -22.3700008392334
Train_BestReturn : -22.3700008392334
TimeSinceStart : 190.7221908569336
Exploration Critic Loss : 8688679.0
Exploitation Critic Loss : 0.07501611858606339
Exploration Model Loss : 50.31386184692383
Actor Loss : 0.7463657855987549
Eval_AverageReturn : -36.74074172973633
Eval_StdReturn : 11.771252632141113
Eval_MaxReturn : -18.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 37.407407407407405
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -18.620001
best mean reward -18.620001
running time 248.687820
At timestep:     40 / 1000At timestep:     90 / 1000At timestep:     113 / 1000At timestep:     136 / 1000At timestep:     159 / 1000At timestep:     207 / 1000At timestep:     257 / 1000At timestep:     301 / 1000At timestep:     323 / 1000At timestep:     361 / 1000At timestep:     411 / 1000At timestep:     450 / 1000At timestep:     491 / 1000At timestep:     507 / 1000At timestep:     534 / 1000At timestep:     566 / 1000At timestep:     598 / 1000At timestep:     629 / 1000At timestep:     674 / 1000At timestep:     701 / 1000At timestep:     751 / 1000At timestep:     784 / 1000At timestep:     804 / 1000At timestep:     826 / 1000At timestep:     864 / 1000At timestep:     909 / 1000At timestep:     946 / 1000At timestep:     966 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -18.6200008392334
Train_BestReturn : -18.6200008392334
TimeSinceStart : 248.6878204345703
Exploration Critic Loss : 7390645.0
Exploitation Critic Loss : 0.0774727463722229
Exploration Model Loss : 39.532447814941406
Actor Loss : 0.6805763840675354
Eval_AverageReturn : -34.2068977355957
Eval_StdReturn : 11.102591514587402
Eval_MaxReturn : -15.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 35.03448275862069
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -17.180000
best mean reward -17.180000
running time 308.269884
At timestep:     42 / 1000At timestep:     92 / 1000At timestep:     115 / 1000At timestep:     134 / 1000At timestep:     175 / 1000At timestep:     204 / 1000At timestep:     249 / 1000At timestep:     279 / 1000At timestep:     307 / 1000At timestep:     341 / 1000At timestep:     391 / 1000At timestep:     421 / 1000At timestep:     449 / 1000At timestep:     468 / 1000At timestep:     496 / 1000At timestep:     512 / 1000At timestep:     531 / 1000At timestep:     545 / 1000At timestep:     595 / 1000At timestep:     615 / 1000At timestep:     631 / 1000At timestep:     644 / 1000At timestep:     665 / 1000At timestep:     693 / 1000At timestep:     715 / 1000At timestep:     745 / 1000At timestep:     776 / 1000At timestep:     796 / 1000At timestep:     815 / 1000At timestep:     833 / 1000At timestep:     862 / 1000At timestep:     891 / 1000At timestep:     915 / 1000At timestep:     965 / 1000At timestep:     998 / 1000At timestep:     1031 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -17.18000030517578
Train_BestReturn : -17.18000030517578
TimeSinceStart : 308.2698838710785
Exploration Critic Loss : 15080757.0
Exploitation Critic Loss : 0.1329834759235382
Exploration Model Loss : 96.7356948852539
Actor Loss : 0.6376841068267822
Eval_AverageReturn : -27.75
Eval_StdReturn : 10.86118221282959
Eval_MaxReturn : -12.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 28.63888888888889
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -16.719999
best mean reward -16.719999
running time 368.482904
At timestep:     18 / 1000At timestep:     43 / 1000At timestep:     71 / 1000At timestep:     105 / 1000At timestep:     141 / 1000At timestep:     172 / 1000At timestep:     203 / 1000At timestep:     230 / 1000At timestep:     248 / 1000At timestep:     290 / 1000At timestep:     333 / 1000At timestep:     362 / 1000At timestep:     380 / 1000At timestep:     411 / 1000At timestep:     437 / 1000At timestep:     479 / 1000At timestep:     494 / 1000At timestep:     521 / 1000At timestep:     546 / 1000At timestep:     573 / 1000At timestep:     595 / 1000At timestep:     622 / 1000At timestep:     637 / 1000At timestep:     680 / 1000At timestep:     704 / 1000At timestep:     732 / 1000At timestep:     759 / 1000At timestep:     803 / 1000At timestep:     839 / 1000At timestep:     856 / 1000At timestep:     880 / 1000At timestep:     915 / 1000At timestep:     956 / 1000At timestep:     978 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -16.719999313354492
Train_BestReturn : -16.719999313354492
TimeSinceStart : 368.48290395736694
Exploration Critic Loss : 10080836.0
Exploitation Critic Loss : 0.09096063673496246
Exploration Model Loss : 62.35162353515625
Actor Loss : 0.6796582937240601
Eval_AverageReturn : -27.91428565979004
Eval_StdReturn : 8.288965225219727
Eval_MaxReturn : -14.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 28.914285714285715
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -17.030001
best mean reward -16.719999
running time 428.618412
At timestep:     14 / 1000At timestep:     45 / 1000At timestep:     79 / 1000At timestep:     101 / 1000At timestep:     121 / 1000At timestep:     140 / 1000At timestep:     172 / 1000At timestep:     217 / 1000At timestep:     239 / 1000At timestep:     264 / 1000At timestep:     283 / 1000At timestep:     301 / 1000At timestep:     326 / 1000At timestep:     353 / 1000At timestep:     401 / 1000At timestep:     420 / 1000At timestep:     436 / 1000At timestep:     479 / 1000At timestep:     503 / 1000At timestep:     529 / 1000At timestep:     548 / 1000At timestep:     573 / 1000At timestep:     600 / 1000At timestep:     616 / 1000At timestep:     646 / 1000At timestep:     660 / 1000At timestep:     692 / 1000At timestep:     710 / 1000At timestep:     741 / 1000At timestep:     772 / 1000At timestep:     798 / 1000At timestep:     816 / 1000At timestep:     853 / 1000At timestep:     884 / 1000At timestep:     901 / 1000At timestep:     914 / 1000At timestep:     954 / 1000At timestep:     974 / 1000At timestep:     990 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -17.030000686645508
Train_BestReturn : -16.719999313354492
TimeSinceStart : 428.61841225624084
Exploration Critic Loss : 5217555.5
Exploitation Critic Loss : 0.11524063348770142
Exploration Model Loss : 28.593963623046875
Actor Loss : 0.6136693954467773
Eval_AverageReturn : -24.375
Eval_StdReturn : 8.699676513671875
Eval_MaxReturn : -12.0
Eval_MinReturn : -47.0
Eval_AverageEpLen : 25.375
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -17.629999
best mean reward -16.719999
running time 488.882533
At timestep:     27 / 1000At timestep:     51 / 1000At timestep:     72 / 1000At timestep:     87 / 1000At timestep:     108 / 1000At timestep:     140 / 1000At timestep:     159 / 1000At timestep:     178 / 1000At timestep:     200 / 1000At timestep:     218 / 1000At timestep:     239 / 1000At timestep:     264 / 1000At timestep:     282 / 1000At timestep:     302 / 1000At timestep:     330 / 1000At timestep:     350 / 1000At timestep:     376 / 1000At timestep:     407 / 1000At timestep:     449 / 1000At timestep:     470 / 1000At timestep:     503 / 1000At timestep:     522 / 1000At timestep:     559 / 1000At timestep:     582 / 1000At timestep:     605 / 1000At timestep:     632 / 1000At timestep:     666 / 1000At timestep:     686 / 1000At timestep:     703 / 1000At timestep:     735 / 1000At timestep:     749 / 1000At timestep:     771 / 1000At timestep:     793 / 1000At timestep:     820 / 1000At timestep:     842 / 1000At timestep:     859 / 1000At timestep:     909 / 1000At timestep:     929 / 1000At timestep:     947 / 1000At timestep:     981 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -17.6299991607666
Train_BestReturn : -16.719999313354492
TimeSinceStart : 488.8825328350067
Exploration Critic Loss : 8517825.0
Exploitation Critic Loss : 0.07679380476474762
Exploration Model Loss : 36.5421142578125
Actor Loss : 0.6067414879798889
Eval_AverageReturn : -23.80487823486328
Eval_StdReturn : 7.654807090759277
Eval_MaxReturn : -13.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 24.78048780487805
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -17.520000
best mean reward -16.719999
running time 547.575042
At timestep:     18 / 1000At timestep:     36 / 1000At timestep:     52 / 1000At timestep:     80 / 1000At timestep:     97 / 1000At timestep:     115 / 1000At timestep:     131 / 1000At timestep:     147 / 1000At timestep:     169 / 1000At timestep:     186 / 1000At timestep:     207 / 1000At timestep:     231 / 1000At timestep:     265 / 1000At timestep:     286 / 1000At timestep:     310 / 1000At timestep:     335 / 1000At timestep:     353 / 1000At timestep:     371 / 1000At timestep:     394 / 1000At timestep:     413 / 1000At timestep:     441 / 1000At timestep:     474 / 1000At timestep:     490 / 1000At timestep:     505 / 1000At timestep:     537 / 1000At timestep:     574 / 1000At timestep:     591 / 1000At timestep:     616 / 1000At timestep:     651 / 1000At timestep:     679 / 1000At timestep:     696 / 1000At timestep:     719 / 1000At timestep:     740 / 1000At timestep:     756 / 1000At timestep:     775 / 1000At timestep:     805 / 1000At timestep:     829 / 1000At timestep:     855 / 1000At timestep:     873 / 1000At timestep:     898 / 1000At timestep:     917 / 1000At timestep:     940 / 1000At timestep:     952 / 1000At timestep:     975 / 1000At timestep:     990 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -17.520000457763672
Train_BestReturn : -16.719999313354492
TimeSinceStart : 547.5750424861908
Exploration Critic Loss : 11150352.0
Exploitation Critic Loss : 0.07314696907997131
Exploration Model Loss : 52.71098327636719
Actor Loss : 0.5979054570198059
Eval_AverageReturn : -20.84782600402832
Eval_StdReturn : 5.9744648933410645
Eval_MaxReturn : -11.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 21.847826086956523
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -17.709999
best mean reward -16.719999
running time 609.180390
At timestep:     21 / 1000At timestep:     52 / 1000At timestep:     67 / 1000At timestep:     86 / 1000At timestep:     117 / 1000At timestep:     143 / 1000At timestep:     178 / 1000At timestep:     207 / 1000At timestep:     225 / 1000At timestep:     257 / 1000At timestep:     275 / 1000At timestep:     298 / 1000At timestep:     314 / 1000At timestep:     333 / 1000At timestep:     352 / 1000At timestep:     365 / 1000At timestep:     387 / 1000At timestep:     410 / 1000At timestep:     434 / 1000At timestep:     448 / 1000At timestep:     485 / 1000At timestep:     511 / 1000At timestep:     535 / 1000At timestep:     552 / 1000At timestep:     572 / 1000At timestep:     593 / 1000At timestep:     612 / 1000At timestep:     642 / 1000At timestep:     665 / 1000At timestep:     683 / 1000At timestep:     712 / 1000At timestep:     740 / 1000At timestep:     768 / 1000At timestep:     789 / 1000At timestep:     818 / 1000At timestep:     835 / 1000At timestep:     855 / 1000At timestep:     870 / 1000At timestep:     906 / 1000At timestep:     924 / 1000At timestep:     943 / 1000At timestep:     962 / 1000At timestep:     976 / 1000At timestep:     993 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -17.709999084472656
Train_BestReturn : -16.719999313354492
TimeSinceStart : 609.1803903579712
Exploration Critic Loss : 6453039.5
Exploitation Critic Loss : 0.09783171862363815
Exploration Model Loss : 27.069290161132812
Actor Loss : 0.5601949691772461
Eval_AverageReturn : -21.35555648803711
Eval_StdReturn : 6.325101852416992
Eval_MaxReturn : -12.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 22.355555555555554
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -17.889999
best mean reward -16.719999
running time 671.496338
At timestep:     17 / 1000At timestep:     45 / 1000At timestep:     74 / 1000At timestep:     105 / 1000At timestep:     127 / 1000At timestep:     158 / 1000At timestep:     176 / 1000At timestep:     192 / 1000At timestep:     227 / 1000At timestep:     247 / 1000At timestep:     266 / 1000At timestep:     311 / 1000At timestep:     349 / 1000At timestep:     365 / 1000At timestep:     394 / 1000At timestep:     418 / 1000At timestep:     435 / 1000At timestep:     466 / 1000At timestep:     493 / 1000At timestep:     514 / 1000At timestep:     532 / 1000At timestep:     571 / 1000At timestep:     603 / 1000At timestep:     635 / 1000At timestep:     667 / 1000At timestep:     697 / 1000At timestep:     725 / 1000At timestep:     758 / 1000At timestep:     773 / 1000At timestep:     793 / 1000At timestep:     813 / 1000At timestep:     852 / 1000At timestep:     868 / 1000At timestep:     893 / 1000At timestep:     922 / 1000At timestep:     965 / 1000At timestep:     983 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -17.889999389648438
Train_BestReturn : -16.719999313354492
TimeSinceStart : 671.4963383674622
Exploration Critic Loss : 8168863.5
Exploitation Critic Loss : 0.09831798821687698
Exploration Model Loss : 35.84422302246094
Actor Loss : 0.5081074833869934
Eval_AverageReturn : -25.36842155456543
Eval_StdReturn : 8.141579627990723
Eval_MaxReturn : -14.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 26.36842105263158
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -17.299999
best mean reward -16.719999
running time 733.141233
At timestep:     14 / 1000At timestep:     33 / 1000At timestep:     58 / 1000At timestep:     76 / 1000At timestep:     95 / 1000At timestep:     111 / 1000At timestep:     133 / 1000At timestep:     160 / 1000At timestep:     184 / 1000At timestep:     200 / 1000At timestep:     227 / 1000At timestep:     242 / 1000At timestep:     267 / 1000At timestep:     291 / 1000At timestep:     308 / 1000At timestep:     322 / 1000At timestep:     338 / 1000At timestep:     349 / 1000At timestep:     382 / 1000At timestep:     421 / 1000At timestep:     435 / 1000At timestep:     453 / 1000At timestep:     472 / 1000At timestep:     490 / 1000At timestep:     507 / 1000At timestep:     525 / 1000At timestep:     547 / 1000At timestep:     563 / 1000At timestep:     581 / 1000At timestep:     606 / 1000At timestep:     622 / 1000At timestep:     643 / 1000At timestep:     657 / 1000At timestep:     678 / 1000At timestep:     692 / 1000At timestep:     705 / 1000At timestep:     730 / 1000At timestep:     750 / 1000At timestep:     770 / 1000At timestep:     789 / 1000At timestep:     814 / 1000At timestep:     831 / 1000At timestep:     854 / 1000At timestep:     877 / 1000At timestep:     897 / 1000At timestep:     913 / 1000At timestep:     929 / 1000At timestep:     956 / 1000At timestep:     986 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -17.299999237060547
Train_BestReturn : -16.719999313354492
TimeSinceStart : 733.1412332057953
Exploration Critic Loss : 11624083.0
Exploitation Critic Loss : 0.13378313183784485
Exploration Model Loss : 52.342254638671875
Actor Loss : 0.557317316532135
Eval_AverageReturn : -19.059999465942383
Eval_StdReturn : 5.420000076293945
Eval_MaxReturn : -10.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 20.06
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -17.799999
best mean reward -16.719999
running time 797.398495
At timestep:     29 / 1000At timestep:     43 / 1000At timestep:     74 / 1000At timestep:     86 / 1000At timestep:     122 / 1000At timestep:     143 / 1000At timestep:     159 / 1000At timestep:     180 / 1000At timestep:     206 / 1000At timestep:     224 / 1000At timestep:     239 / 1000At timestep:     259 / 1000At timestep:     286 / 1000At timestep:     297 / 1000At timestep:     322 / 1000At timestep:     339 / 1000At timestep:     362 / 1000At timestep:     404 / 1000At timestep:     448 / 1000At timestep:     461 / 1000At timestep:     474 / 1000At timestep:     498 / 1000At timestep:     521 / 1000At timestep:     553 / 1000At timestep:     574 / 1000At timestep:     590 / 1000At timestep:     612 / 1000At timestep:     636 / 1000At timestep:     658 / 1000At timestep:     680 / 1000At timestep:     700 / 1000At timestep:     719 / 1000At timestep:     752 / 1000At timestep:     774 / 1000At timestep:     793 / 1000At timestep:     810 / 1000At timestep:     826 / 1000At timestep:     857 / 1000At timestep:     887 / 1000At timestep:     902 / 1000At timestep:     921 / 1000At timestep:     940 / 1000At timestep:     952 / 1000At timestep:     978 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -17.799999237060547
Train_BestReturn : -16.719999313354492
TimeSinceStart : 797.3984949588776
Exploration Critic Loss : 6205857.0
Exploitation Critic Loss : 0.18337984383106232
Exploration Model Loss : 26.998550415039062
Actor Loss : 0.5484696626663208
Eval_AverageReturn : -21.244443893432617
Eval_StdReturn : 7.477835178375244
Eval_MaxReturn : -10.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 22.244444444444444
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -17.240000
best mean reward -16.719999
running time 861.692019
At timestep:     19 / 1000At timestep:     50 / 1000At timestep:     68 / 1000At timestep:     83 / 1000At timestep:     99 / 1000At timestep:     120 / 1000At timestep:     150 / 1000At timestep:     170 / 1000At timestep:     184 / 1000At timestep:     203 / 1000At timestep:     239 / 1000At timestep:     251 / 1000At timestep:     273 / 1000At timestep:     291 / 1000At timestep:     303 / 1000At timestep:     321 / 1000At timestep:     335 / 1000At timestep:     351 / 1000At timestep:     378 / 1000At timestep:     395 / 1000At timestep:     424 / 1000At timestep:     443 / 1000At timestep:     475 / 1000At timestep:     495 / 1000At timestep:     513 / 1000At timestep:     544 / 1000At timestep:     571 / 1000At timestep:     597 / 1000At timestep:     622 / 1000At timestep:     640 / 1000At timestep:     660 / 1000At timestep:     704 / 1000At timestep:     719 / 1000At timestep:     735 / 1000At timestep:     755 / 1000At timestep:     777 / 1000At timestep:     799 / 1000At timestep:     817 / 1000At timestep:     834 / 1000At timestep:     854 / 1000At timestep:     877 / 1000At timestep:     909 / 1000At timestep:     928 / 1000At timestep:     951 / 1000At timestep:     967 / 1000At timestep:     990 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -17.239999771118164
Train_BestReturn : -16.719999313354492
TimeSinceStart : 861.6920187473297
Exploration Critic Loss : 6413044.0
Exploitation Critic Loss : 0.0799529105424881
Exploration Model Loss : 35.81001281738281
Actor Loss : 0.5236246585845947
Eval_AverageReturn : -20.489360809326172
Eval_StdReturn : 6.519601345062256
Eval_MaxReturn : -11.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 21.48936170212766
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -17.330000
best mean reward -16.719999
running time 923.913978
At timestep:     19 / 1000At timestep:     37 / 1000At timestep:     52 / 1000At timestep:     66 / 1000At timestep:     82 / 1000At timestep:     108 / 1000At timestep:     133 / 1000At timestep:     156 / 1000At timestep:     174 / 1000At timestep:     201 / 1000At timestep:     216 / 1000At timestep:     233 / 1000At timestep:     251 / 1000At timestep:     275 / 1000At timestep:     311 / 1000At timestep:     334 / 1000At timestep:     352 / 1000At timestep:     374 / 1000At timestep:     387 / 1000At timestep:     410 / 1000At timestep:     425 / 1000At timestep:     442 / 1000At timestep:     474 / 1000At timestep:     489 / 1000At timestep:     508 / 1000At timestep:     524 / 1000At timestep:     545 / 1000At timestep:     566 / 1000At timestep:     593 / 1000At timestep:     619 / 1000At timestep:     650 / 1000At timestep:     664 / 1000At timestep:     700 / 1000At timestep:     722 / 1000At timestep:     740 / 1000At timestep:     769 / 1000At timestep:     786 / 1000At timestep:     819 / 1000At timestep:     839 / 1000At timestep:     852 / 1000At timestep:     865 / 1000At timestep:     887 / 1000At timestep:     901 / 1000At timestep:     920 / 1000At timestep:     943 / 1000At timestep:     965 / 1000At timestep:     990 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -17.329999923706055
Train_BestReturn : -16.719999313354492
TimeSinceStart : 923.9139783382416
Exploration Critic Loss : 9429551.0
Exploitation Critic Loss : 0.07923199236392975
Exploration Model Loss : 52.32469177246094
Actor Loss : 0.6531196236610413
Eval_AverageReturn : -20.39583396911621
Eval_StdReturn : 6.372922897338867
Eval_MaxReturn : -12.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 21.395833333333332
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -17.840000
best mean reward -16.719999
running time 986.772078
At timestep:     25 / 1000At timestep:     49 / 1000At timestep:     72 / 1000At timestep:     90 / 1000At timestep:     109 / 1000At timestep:     127 / 1000At timestep:     152 / 1000At timestep:     172 / 1000At timestep:     184 / 1000At timestep:     205 / 1000At timestep:     224 / 1000At timestep:     242 / 1000At timestep:     261 / 1000At timestep:     280 / 1000At timestep:     303 / 1000At timestep:     338 / 1000At timestep:     365 / 1000At timestep:     385 / 1000At timestep:     399 / 1000At timestep:     419 / 1000At timestep:     441 / 1000At timestep:     455 / 1000At timestep:     483 / 1000At timestep:     502 / 1000At timestep:     525 / 1000At timestep:     550 / 1000At timestep:     564 / 1000At timestep:     584 / 1000At timestep:     608 / 1000At timestep:     629 / 1000At timestep:     643 / 1000At timestep:     659 / 1000At timestep:     681 / 1000At timestep:     699 / 1000At timestep:     726 / 1000At timestep:     745 / 1000At timestep:     771 / 1000At timestep:     787 / 1000At timestep:     806 / 1000At timestep:     823 / 1000At timestep:     848 / 1000At timestep:     863 / 1000At timestep:     876 / 1000At timestep:     909 / 1000At timestep:     930 / 1000At timestep:     949 / 1000At timestep:     974 / 1000At timestep:     986 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -17.84000015258789
Train_BestReturn : -16.719999313354492
TimeSinceStart : 986.7720782756805
Exploration Critic Loss : 5012047.0
Exploitation Critic Loss : 0.07660633325576782
Exploration Model Loss : 26.995254516601562
Actor Loss : 0.5577298402786255
Eval_AverageReturn : -19.489795684814453
Eval_StdReturn : 4.928561210632324
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 20.489795918367346
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.790001
best mean reward -16.719999
running time 1050.883167
At timestep:     18 / 1000At timestep:     38 / 1000At timestep:     56 / 1000At timestep:     72 / 1000At timestep:     98 / 1000At timestep:     117 / 1000At timestep:     130 / 1000At timestep:     151 / 1000At timestep:     176 / 1000At timestep:     189 / 1000At timestep:     221 / 1000At timestep:     257 / 1000At timestep:     271 / 1000At timestep:     287 / 1000At timestep:     319 / 1000At timestep:     340 / 1000At timestep:     361 / 1000At timestep:     377 / 1000At timestep:     407 / 1000At timestep:     428 / 1000At timestep:     447 / 1000At timestep:     461 / 1000At timestep:     481 / 1000At timestep:     495 / 1000At timestep:     513 / 1000At timestep:     531 / 1000At timestep:     549 / 1000At timestep:     569 / 1000At timestep:     588 / 1000At timestep:     613 / 1000At timestep:     635 / 1000At timestep:     651 / 1000At timestep:     680 / 1000At timestep:     704 / 1000At timestep:     724 / 1000At timestep:     737 / 1000At timestep:     761 / 1000At timestep:     791 / 1000At timestep:     812 / 1000At timestep:     846 / 1000At timestep:     867 / 1000At timestep:     886 / 1000At timestep:     907 / 1000At timestep:     921 / 1000At timestep:     941 / 1000At timestep:     955 / 1000At timestep:     973 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.790000915527344
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1050.8831667900085
Exploration Critic Loss : 9116574.0
Exploitation Critic Loss : 0.15221187472343445
Exploration Model Loss : 35.80854797363281
Actor Loss : 0.5227952003479004
Eval_AverageReturn : -20.04166603088379
Eval_StdReturn : 6.133780479431152
Eval_MaxReturn : -12.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 21.041666666666668
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.629999
best mean reward -16.719999
running time 1115.013070
At timestep:     31 / 1000At timestep:     47 / 1000At timestep:     61 / 1000At timestep:     77 / 1000At timestep:     92 / 1000At timestep:     114 / 1000At timestep:     134 / 1000At timestep:     154 / 1000At timestep:     170 / 1000At timestep:     190 / 1000At timestep:     213 / 1000At timestep:     225 / 1000At timestep:     241 / 1000At timestep:     260 / 1000At timestep:     285 / 1000At timestep:     301 / 1000At timestep:     318 / 1000At timestep:     343 / 1000At timestep:     358 / 1000At timestep:     380 / 1000At timestep:     397 / 1000At timestep:     414 / 1000At timestep:     436 / 1000At timestep:     454 / 1000At timestep:     470 / 1000At timestep:     492 / 1000At timestep:     521 / 1000At timestep:     536 / 1000At timestep:     550 / 1000At timestep:     569 / 1000At timestep:     583 / 1000At timestep:     606 / 1000At timestep:     627 / 1000At timestep:     652 / 1000At timestep:     669 / 1000At timestep:     698 / 1000At timestep:     720 / 1000At timestep:     737 / 1000At timestep:     756 / 1000At timestep:     780 / 1000At timestep:     813 / 1000At timestep:     829 / 1000At timestep:     856 / 1000At timestep:     870 / 1000At timestep:     884 / 1000At timestep:     904 / 1000At timestep:     927 / 1000At timestep:     948 / 1000At timestep:     960 / 1000At timestep:     973 / 1000At timestep:     990 / 1000At timestep:     1020 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.6299991607666
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1115.0130698680878
Exploration Critic Loss : 12282818.0
Exploitation Critic Loss : 0.07220524549484253
Exploration Model Loss : 52.32466125488281
Actor Loss : 0.5749300718307495
Eval_AverageReturn : -18.615385055541992
Eval_StdReturn : 5.073133945465088
Eval_MaxReturn : -11.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 19.615384615384617
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.290001
best mean reward -16.719999
running time 1179.231854
At timestep:     16 / 1000At timestep:     36 / 1000At timestep:     54 / 1000At timestep:     83 / 1000At timestep:     113 / 1000At timestep:     122 / 1000At timestep:     139 / 1000At timestep:     151 / 1000At timestep:     193 / 1000At timestep:     236 / 1000At timestep:     252 / 1000At timestep:     276 / 1000At timestep:     289 / 1000At timestep:     304 / 1000At timestep:     329 / 1000At timestep:     347 / 1000At timestep:     373 / 1000At timestep:     390 / 1000At timestep:     406 / 1000At timestep:     430 / 1000At timestep:     448 / 1000At timestep:     474 / 1000At timestep:     496 / 1000At timestep:     514 / 1000At timestep:     529 / 1000At timestep:     545 / 1000At timestep:     562 / 1000At timestep:     585 / 1000At timestep:     610 / 1000At timestep:     630 / 1000At timestep:     649 / 1000At timestep:     668 / 1000At timestep:     690 / 1000At timestep:     708 / 1000At timestep:     726 / 1000At timestep:     739 / 1000At timestep:     754 / 1000At timestep:     771 / 1000At timestep:     788 / 1000At timestep:     805 / 1000At timestep:     821 / 1000At timestep:     840 / 1000At timestep:     856 / 1000At timestep:     872 / 1000At timestep:     893 / 1000At timestep:     924 / 1000At timestep:     947 / 1000At timestep:     962 / 1000At timestep:     983 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.290000915527344
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1179.2318539619446
Exploration Critic Loss : 6363576.0
Exploitation Critic Loss : 0.09106490015983582
Exploration Model Loss : 26.995193481445312
Actor Loss : 0.5786845684051514
Eval_AverageReturn : -19.0
Eval_StdReturn : 6.452906131744385
Eval_MaxReturn : -8.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 20.0
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.480000
best mean reward -16.719999
running time 1243.474837
At timestep:     18 / 1000At timestep:     38 / 1000At timestep:     55 / 1000At timestep:     70 / 1000At timestep:     83 / 1000At timestep:     99 / 1000At timestep:     124 / 1000At timestep:     140 / 1000At timestep:     160 / 1000At timestep:     188 / 1000At timestep:     204 / 1000At timestep:     228 / 1000At timestep:     245 / 1000At timestep:     261 / 1000At timestep:     283 / 1000At timestep:     321 / 1000At timestep:     363 / 1000At timestep:     385 / 1000At timestep:     402 / 1000At timestep:     418 / 1000At timestep:     443 / 1000At timestep:     462 / 1000At timestep:     488 / 1000At timestep:     507 / 1000At timestep:     525 / 1000At timestep:     539 / 1000At timestep:     559 / 1000At timestep:     584 / 1000At timestep:     628 / 1000At timestep:     649 / 1000At timestep:     664 / 1000At timestep:     681 / 1000At timestep:     708 / 1000At timestep:     725 / 1000At timestep:     748 / 1000At timestep:     769 / 1000At timestep:     801 / 1000At timestep:     816 / 1000At timestep:     830 / 1000At timestep:     847 / 1000At timestep:     874 / 1000At timestep:     897 / 1000At timestep:     919 / 1000At timestep:     941 / 1000At timestep:     957 / 1000At timestep:     978 / 1000At timestep:     999 / 1000At timestep:     1034 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.479999542236328
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1243.4748368263245
Exploration Critic Loss : 7446760.0
Exploitation Critic Loss : 0.11984758824110031
Exploration Model Loss : 35.80848693847656
Actor Loss : 0.46195441484451294
Eval_AverageReturn : -20.54166603088379
Eval_StdReturn : 6.9550886154174805
Eval_MaxReturn : -12.0
Eval_MinReturn : -43.0
Eval_AverageEpLen : 21.541666666666668
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -17.430000
best mean reward -16.719999
running time 1306.805827
At timestep:     18 / 1000At timestep:     34 / 1000At timestep:     57 / 1000At timestep:     81 / 1000At timestep:     103 / 1000At timestep:     125 / 1000At timestep:     154 / 1000At timestep:     178 / 1000At timestep:     203 / 1000At timestep:     217 / 1000At timestep:     235 / 1000At timestep:     250 / 1000At timestep:     275 / 1000At timestep:     292 / 1000At timestep:     307 / 1000At timestep:     332 / 1000At timestep:     353 / 1000At timestep:     371 / 1000At timestep:     386 / 1000At timestep:     408 / 1000At timestep:     435 / 1000At timestep:     455 / 1000At timestep:     476 / 1000At timestep:     490 / 1000At timestep:     523 / 1000At timestep:     548 / 1000At timestep:     578 / 1000At timestep:     602 / 1000At timestep:     623 / 1000At timestep:     643 / 1000At timestep:     659 / 1000At timestep:     677 / 1000At timestep:     693 / 1000At timestep:     718 / 1000At timestep:     747 / 1000At timestep:     767 / 1000At timestep:     785 / 1000At timestep:     802 / 1000At timestep:     819 / 1000At timestep:     833 / 1000At timestep:     855 / 1000At timestep:     877 / 1000At timestep:     893 / 1000At timestep:     907 / 1000At timestep:     924 / 1000At timestep:     945 / 1000At timestep:     962 / 1000At timestep:     981 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -17.43000030517578
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1306.8058273792267
Exploration Critic Loss : 12021718.0
Exploitation Critic Loss : 0.08763357251882553
Exploration Model Loss : 52.32472229003906
Actor Loss : 0.5505945682525635
Eval_AverageReturn : -19.4489803314209
Eval_StdReturn : 4.5761637687683105
Eval_MaxReturn : -13.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 20.448979591836736
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -18.040001
best mean reward -16.719999
running time 1369.438558
At timestep:     26 / 1000At timestep:     47 / 1000At timestep:     64 / 1000At timestep:     82 / 1000At timestep:     111 / 1000At timestep:     135 / 1000At timestep:     148 / 1000At timestep:     172 / 1000At timestep:     197 / 1000At timestep:     215 / 1000At timestep:     227 / 1000At timestep:     250 / 1000At timestep:     273 / 1000At timestep:     289 / 1000At timestep:     307 / 1000At timestep:     328 / 1000At timestep:     352 / 1000At timestep:     373 / 1000At timestep:     393 / 1000At timestep:     421 / 1000At timestep:     444 / 1000At timestep:     459 / 1000At timestep:     474 / 1000At timestep:     510 / 1000At timestep:     525 / 1000At timestep:     546 / 1000At timestep:     567 / 1000At timestep:     583 / 1000At timestep:     608 / 1000At timestep:     625 / 1000At timestep:     644 / 1000At timestep:     660 / 1000At timestep:     679 / 1000At timestep:     694 / 1000At timestep:     721 / 1000At timestep:     740 / 1000At timestep:     756 / 1000At timestep:     772 / 1000At timestep:     794 / 1000At timestep:     823 / 1000At timestep:     839 / 1000At timestep:     855 / 1000At timestep:     873 / 1000At timestep:     897 / 1000At timestep:     919 / 1000At timestep:     931 / 1000At timestep:     944 / 1000At timestep:     959 / 1000At timestep:     986 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -18.040000915527344
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1369.4385578632355
Exploration Critic Loss : 6671772.0
Exploitation Critic Loss : 0.1156785637140274
Exploration Model Loss : 26.995132446289062
Actor Loss : 0.5560178756713867
Eval_AverageReturn : -19.219999313354492
Eval_StdReturn : 5.084446907043457
Eval_MaxReturn : -11.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 20.22
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -18.190001
best mean reward -16.719999
running time 1432.970642
At timestep:     13 / 1000At timestep:     30 / 1000At timestep:     53 / 1000At timestep:     72 / 1000At timestep:     97 / 1000At timestep:     112 / 1000At timestep:     133 / 1000At timestep:     155 / 1000At timestep:     181 / 1000At timestep:     197 / 1000At timestep:     215 / 1000At timestep:     231 / 1000At timestep:     247 / 1000At timestep:     271 / 1000At timestep:     289 / 1000At timestep:     312 / 1000At timestep:     329 / 1000At timestep:     345 / 1000At timestep:     365 / 1000At timestep:     380 / 1000At timestep:     404 / 1000At timestep:     420 / 1000At timestep:     438 / 1000At timestep:     455 / 1000At timestep:     469 / 1000At timestep:     481 / 1000At timestep:     509 / 1000At timestep:     531 / 1000At timestep:     552 / 1000At timestep:     567 / 1000At timestep:     588 / 1000At timestep:     619 / 1000At timestep:     638 / 1000At timestep:     664 / 1000At timestep:     697 / 1000At timestep:     722 / 1000At timestep:     738 / 1000At timestep:     760 / 1000At timestep:     777 / 1000At timestep:     792 / 1000At timestep:     811 / 1000At timestep:     838 / 1000At timestep:     862 / 1000At timestep:     878 / 1000At timestep:     895 / 1000At timestep:     907 / 1000At timestep:     926 / 1000At timestep:     945 / 1000At timestep:     965 / 1000At timestep:     987 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -18.190000534057617
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1432.9706423282623
Exploration Critic Loss : 7510870.0
Exploitation Critic Loss : 0.11330659687519073
Exploration Model Loss : 35.80842590332031
Actor Loss : 0.5099118947982788
Eval_AverageReturn : -18.784313201904297
Eval_StdReturn : 4.641305446624756
Eval_MaxReturn : -11.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 19.784313725490197
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -18.090000
best mean reward -16.719999
running time 1496.699869
At timestep:     17 / 1000At timestep:     34 / 1000At timestep:     50 / 1000At timestep:     65 / 1000At timestep:     89 / 1000At timestep:     106 / 1000At timestep:     123 / 1000At timestep:     142 / 1000At timestep:     166 / 1000At timestep:     183 / 1000At timestep:     200 / 1000At timestep:     218 / 1000At timestep:     237 / 1000At timestep:     258 / 1000At timestep:     271 / 1000At timestep:     293 / 1000At timestep:     316 / 1000At timestep:     329 / 1000At timestep:     352 / 1000At timestep:     366 / 1000At timestep:     388 / 1000At timestep:     404 / 1000At timestep:     416 / 1000At timestep:     435 / 1000At timestep:     453 / 1000At timestep:     469 / 1000At timestep:     486 / 1000At timestep:     504 / 1000At timestep:     527 / 1000At timestep:     556 / 1000At timestep:     585 / 1000At timestep:     597 / 1000At timestep:     618 / 1000At timestep:     645 / 1000At timestep:     670 / 1000At timestep:     689 / 1000At timestep:     701 / 1000At timestep:     716 / 1000At timestep:     731 / 1000At timestep:     743 / 1000At timestep:     762 / 1000At timestep:     777 / 1000At timestep:     799 / 1000At timestep:     819 / 1000At timestep:     841 / 1000At timestep:     852 / 1000At timestep:     872 / 1000At timestep:     888 / 1000At timestep:     908 / 1000At timestep:     926 / 1000At timestep:     946 / 1000At timestep:     965 / 1000At timestep:     982 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -18.09000015258789
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1496.6998689174652
Exploration Critic Loss : 11935220.0
Exploitation Critic Loss : 0.11148785054683685
Exploration Model Loss : 52.32478332519531
Actor Loss : 0.46379661560058594
Eval_AverageReturn : -17.518518447875977
Eval_StdReturn : 4.144343376159668
Eval_MaxReturn : -10.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 18.51851851851852
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -17.580000
best mean reward -16.719999
running time 1561.513253
At timestep:     20 / 1000At timestep:     52 / 1000At timestep:     67 / 1000At timestep:     90 / 1000At timestep:     112 / 1000At timestep:     129 / 1000At timestep:     147 / 1000At timestep:     160 / 1000At timestep:     180 / 1000At timestep:     198 / 1000At timestep:     218 / 1000At timestep:     238 / 1000At timestep:     257 / 1000At timestep:     276 / 1000At timestep:     299 / 1000At timestep:     320 / 1000At timestep:     339 / 1000At timestep:     365 / 1000At timestep:     382 / 1000At timestep:     405 / 1000At timestep:     424 / 1000At timestep:     453 / 1000At timestep:     466 / 1000At timestep:     480 / 1000At timestep:     514 / 1000At timestep:     533 / 1000At timestep:     558 / 1000At timestep:     573 / 1000At timestep:     585 / 1000At timestep:     599 / 1000At timestep:     612 / 1000At timestep:     627 / 1000At timestep:     649 / 1000At timestep:     665 / 1000At timestep:     691 / 1000At timestep:     708 / 1000At timestep:     736 / 1000At timestep:     752 / 1000At timestep:     774 / 1000At timestep:     803 / 1000At timestep:     827 / 1000At timestep:     852 / 1000At timestep:     873 / 1000At timestep:     894 / 1000At timestep:     918 / 1000At timestep:     936 / 1000At timestep:     950 / 1000At timestep:     972 / 1000At timestep:     986 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -17.579999923706055
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1561.513253211975
Exploration Critic Loss : 6843702.0
Exploitation Critic Loss : 0.10681913048028946
Exploration Model Loss : 26.995071411132812
Actor Loss : 0.46103188395500183
Eval_AverageReturn : -19.059999465942383
Eval_StdReturn : 5.061264991760254
Eval_MaxReturn : -11.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 20.06
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -16.740000
best mean reward -16.719999
running time 1625.487027
At timestep:     29 / 1000At timestep:     41 / 1000At timestep:     70 / 1000At timestep:     89 / 1000At timestep:     115 / 1000At timestep:     134 / 1000At timestep:     152 / 1000At timestep:     169 / 1000At timestep:     188 / 1000At timestep:     205 / 1000At timestep:     222 / 1000At timestep:     239 / 1000At timestep:     251 / 1000At timestep:     272 / 1000At timestep:     290 / 1000At timestep:     310 / 1000At timestep:     328 / 1000At timestep:     344 / 1000At timestep:     370 / 1000At timestep:     393 / 1000At timestep:     408 / 1000At timestep:     424 / 1000At timestep:     439 / 1000At timestep:     462 / 1000At timestep:     487 / 1000At timestep:     501 / 1000At timestep:     525 / 1000At timestep:     541 / 1000At timestep:     576 / 1000At timestep:     595 / 1000At timestep:     612 / 1000At timestep:     632 / 1000At timestep:     655 / 1000At timestep:     678 / 1000At timestep:     695 / 1000At timestep:     710 / 1000At timestep:     731 / 1000At timestep:     753 / 1000At timestep:     778 / 1000At timestep:     799 / 1000At timestep:     815 / 1000At timestep:     837 / 1000At timestep:     853 / 1000At timestep:     871 / 1000At timestep:     890 / 1000At timestep:     916 / 1000At timestep:     937 / 1000At timestep:     952 / 1000At timestep:     971 / 1000At timestep:     987 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -16.739999771118164
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1625.4870269298553
Exploration Critic Loss : 6886626.0
Exploitation Critic Loss : 0.07028291374444962
Exploration Model Loss : 35.80836486816406
Actor Loss : 0.48623549938201904
Eval_AverageReturn : -18.784313201904297
Eval_StdReturn : 4.530133247375488
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.784313725490197
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -16.770000
best mean reward -16.719999
running time 1689.258467
At timestep:     30 / 1000At timestep:     46 / 1000At timestep:     63 / 1000At timestep:     81 / 1000At timestep:     107 / 1000At timestep:     128 / 1000At timestep:     143 / 1000At timestep:     158 / 1000At timestep:     176 / 1000At timestep:     195 / 1000At timestep:     224 / 1000At timestep:     239 / 1000At timestep:     272 / 1000At timestep:     287 / 1000At timestep:     309 / 1000At timestep:     325 / 1000At timestep:     340 / 1000At timestep:     379 / 1000At timestep:     395 / 1000At timestep:     418 / 1000At timestep:     442 / 1000At timestep:     457 / 1000At timestep:     468 / 1000At timestep:     493 / 1000At timestep:     506 / 1000At timestep:     535 / 1000At timestep:     554 / 1000At timestep:     573 / 1000At timestep:     591 / 1000At timestep:     609 / 1000At timestep:     633 / 1000At timestep:     650 / 1000At timestep:     677 / 1000At timestep:     692 / 1000At timestep:     711 / 1000At timestep:     726 / 1000At timestep:     745 / 1000At timestep:     767 / 1000At timestep:     784 / 1000At timestep:     803 / 1000At timestep:     832 / 1000At timestep:     847 / 1000At timestep:     868 / 1000At timestep:     892 / 1000At timestep:     910 / 1000At timestep:     934 / 1000At timestep:     949 / 1000At timestep:     962 / 1000At timestep:     977 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -16.770000457763672
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1689.2584674358368
Exploration Critic Loss : 11468368.0
Exploitation Critic Loss : 0.07351498305797577
Exploration Model Loss : 52.32484436035156
Actor Loss : 0.5100809931755066
Eval_AverageReturn : -19.079999923706055
Eval_StdReturn : 5.778719425201416
Eval_MaxReturn : -10.0
Eval_MinReturn : -38.0
Eval_AverageEpLen : 20.08
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -17.139999
best mean reward -16.719999
running time 1752.826553
At timestep:     17 / 1000At timestep:     33 / 1000At timestep:     50 / 1000At timestep:     66 / 1000At timestep:     81 / 1000At timestep:     96 / 1000At timestep:     109 / 1000At timestep:     123 / 1000At timestep:     145 / 1000At timestep:     158 / 1000At timestep:     179 / 1000At timestep:     201 / 1000At timestep:     222 / 1000At timestep:     236 / 1000At timestep:     249 / 1000At timestep:     267 / 1000At timestep:     283 / 1000At timestep:     307 / 1000At timestep:     324 / 1000At timestep:     341 / 1000At timestep:     359 / 1000At timestep:     375 / 1000At timestep:     396 / 1000At timestep:     415 / 1000At timestep:     431 / 1000At timestep:     446 / 1000At timestep:     474 / 1000At timestep:     489 / 1000At timestep:     517 / 1000At timestep:     533 / 1000At timestep:     553 / 1000At timestep:     575 / 1000At timestep:     600 / 1000At timestep:     624 / 1000At timestep:     647 / 1000At timestep:     662 / 1000At timestep:     681 / 1000At timestep:     702 / 1000At timestep:     718 / 1000At timestep:     739 / 1000At timestep:     761 / 1000At timestep:     777 / 1000At timestep:     798 / 1000At timestep:     816 / 1000At timestep:     837 / 1000At timestep:     853 / 1000At timestep:     876 / 1000At timestep:     902 / 1000At timestep:     926 / 1000At timestep:     949 / 1000At timestep:     962 / 1000At timestep:     978 / 1000At timestep:     996 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -17.139999389648438
Train_BestReturn : -16.719999313354492
TimeSinceStart : 1752.8265528678894
Exploration Critic Loss : 6188700.0
Exploitation Critic Loss : 0.07080402225255966
Exploration Model Loss : 26.995010375976562
Actor Loss : 0.5419223308563232
Eval_AverageReturn : -17.796297073364258
Eval_StdReturn : 3.8891093730926514
Eval_MaxReturn : -12.0
Eval_MinReturn : -27.0
Eval_AverageEpLen : 18.796296296296298
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -16.580000
best mean reward -16.580000
running time 1819.517518
At timestep:     10 / 1000At timestep:     27 / 1000At timestep:     46 / 1000At timestep:     69 / 1000At timestep:     85 / 1000At timestep:     106 / 1000At timestep:     122 / 1000At timestep:     142 / 1000At timestep:     156 / 1000At timestep:     172 / 1000At timestep:     188 / 1000At timestep:     213 / 1000At timestep:     232 / 1000At timestep:     254 / 1000At timestep:     271 / 1000At timestep:     284 / 1000At timestep:     300 / 1000At timestep:     328 / 1000At timestep:     344 / 1000At timestep:     373 / 1000At timestep:     389 / 1000At timestep:     416 / 1000At timestep:     445 / 1000At timestep:     465 / 1000At timestep:     480 / 1000At timestep:     495 / 1000At timestep:     516 / 1000At timestep:     533 / 1000At timestep:     552 / 1000At timestep:     575 / 1000At timestep:     594 / 1000At timestep:     613 / 1000At timestep:     629 / 1000At timestep:     647 / 1000At timestep:     676 / 1000At timestep:     694 / 1000At timestep:     707 / 1000At timestep:     738 / 1000At timestep:     757 / 1000At timestep:     775 / 1000At timestep:     788 / 1000At timestep:     808 / 1000At timestep:     822 / 1000At timestep:     840 / 1000At timestep:     863 / 1000At timestep:     887 / 1000At timestep:     908 / 1000At timestep:     923 / 1000At timestep:     939 / 1000At timestep:     959 / 1000At timestep:     980 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -16.579999923706055
Train_BestReturn : -16.579999923706055
TimeSinceStart : 1819.517517566681
Exploration Critic Loss : 8170989.5
Exploitation Critic Loss : 0.06679358333349228
Exploration Model Loss : 35.80830383300781
Actor Loss : 0.5238518118858337
Eval_AverageReturn : -18.25
Eval_StdReturn : 4.623830318450928
Eval_MaxReturn : -9.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 19.25
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -16.790001
best mean reward -16.580000
running time 1883.790753
At timestep:     17 / 1000At timestep:     34 / 1000At timestep:     56 / 1000At timestep:     78 / 1000At timestep:     97 / 1000At timestep:     113 / 1000At timestep:     131 / 1000At timestep:     146 / 1000At timestep:     163 / 1000At timestep:     187 / 1000At timestep:     202 / 1000At timestep:     224 / 1000At timestep:     248 / 1000At timestep:     269 / 1000At timestep:     283 / 1000At timestep:     296 / 1000At timestep:     318 / 1000At timestep:     337 / 1000At timestep:     356 / 1000At timestep:     372 / 1000At timestep:     398 / 1000At timestep:     414 / 1000At timestep:     440 / 1000At timestep:     458 / 1000At timestep:     480 / 1000At timestep:     494 / 1000At timestep:     514 / 1000At timestep:     532 / 1000At timestep:     547 / 1000At timestep:     567 / 1000At timestep:     589 / 1000At timestep:     611 / 1000At timestep:     631 / 1000At timestep:     653 / 1000At timestep:     683 / 1000At timestep:     701 / 1000At timestep:     724 / 1000At timestep:     741 / 1000At timestep:     753 / 1000At timestep:     771 / 1000At timestep:     789 / 1000At timestep:     803 / 1000At timestep:     829 / 1000At timestep:     858 / 1000At timestep:     878 / 1000At timestep:     898 / 1000At timestep:     926 / 1000At timestep:     944 / 1000At timestep:     969 / 1000At timestep:     983 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -16.790000915527344
Train_BestReturn : -16.579999923706055
TimeSinceStart : 1883.790753364563
Exploration Critic Loss : 10195794.0
Exploitation Critic Loss : 0.07547801733016968
Exploration Model Loss : 52.32490539550781
Actor Loss : 0.4517368972301483
Eval_AverageReturn : -18.66666603088379
Eval_StdReturn : 4.185252666473389
Eval_MaxReturn : -11.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 19.666666666666668
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -17.150000
best mean reward -16.580000
running time 1948.617422
At timestep:     22 / 1000At timestep:     44 / 1000At timestep:     73 / 1000At timestep:     88 / 1000At timestep:     116 / 1000At timestep:     134 / 1000At timestep:     154 / 1000At timestep:     172 / 1000At timestep:     190 / 1000At timestep:     210 / 1000At timestep:     233 / 1000At timestep:     252 / 1000At timestep:     269 / 1000At timestep:     284 / 1000At timestep:     299 / 1000At timestep:     316 / 1000At timestep:     343 / 1000At timestep:     363 / 1000At timestep:     389 / 1000At timestep:     405 / 1000At timestep:     439 / 1000At timestep:     450 / 1000At timestep:     465 / 1000At timestep:     492 / 1000At timestep:     504 / 1000At timestep:     519 / 1000At timestep:     533 / 1000At timestep:     551 / 1000At timestep:     577 / 1000At timestep:     594 / 1000At timestep:     610 / 1000At timestep:     627 / 1000At timestep:     649 / 1000At timestep:     671 / 1000At timestep:     689 / 1000At timestep:     709 / 1000At timestep:     727 / 1000At timestep:     740 / 1000At timestep:     757 / 1000At timestep:     780 / 1000At timestep:     795 / 1000At timestep:     810 / 1000At timestep:     824 / 1000At timestep:     842 / 1000At timestep:     857 / 1000At timestep:     871 / 1000At timestep:     892 / 1000At timestep:     924 / 1000At timestep:     941 / 1000At timestep:     962 / 1000At timestep:     972 / 1000At timestep:     994 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -17.149999618530273
Train_BestReturn : -16.579999923706055
TimeSinceStart : 1948.6174218654633
Exploration Critic Loss : 10023663.0
Exploitation Critic Loss : 0.07227083295583725
Exploration Model Loss : 26.994949340820312
Actor Loss : 0.4852433502674103
Eval_AverageReturn : -18.09433937072754
Eval_StdReturn : 5.08888578414917
Eval_MaxReturn : -9.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 19.09433962264151
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -16.889999
best mean reward -16.580000
running time 2013.511211
At timestep:     18 / 1000At timestep:     35 / 1000At timestep:     50 / 1000At timestep:     67 / 1000At timestep:     83 / 1000At timestep:     98 / 1000At timestep:     116 / 1000At timestep:     133 / 1000At timestep:     145 / 1000At timestep:     162 / 1000At timestep:     178 / 1000At timestep:     194 / 1000At timestep:     206 / 1000At timestep:     219 / 1000At timestep:     240 / 1000At timestep:     257 / 1000At timestep:     274 / 1000At timestep:     293 / 1000At timestep:     308 / 1000At timestep:     330 / 1000At timestep:     343 / 1000At timestep:     357 / 1000At timestep:     373 / 1000At timestep:     398 / 1000At timestep:     434 / 1000At timestep:     452 / 1000At timestep:     478 / 1000At timestep:     495 / 1000At timestep:     514 / 1000At timestep:     529 / 1000At timestep:     549 / 1000At timestep:     569 / 1000At timestep:     583 / 1000At timestep:     603 / 1000At timestep:     627 / 1000At timestep:     648 / 1000At timestep:     663 / 1000At timestep:     681 / 1000At timestep:     704 / 1000At timestep:     723 / 1000At timestep:     743 / 1000At timestep:     768 / 1000At timestep:     787 / 1000At timestep:     807 / 1000At timestep:     822 / 1000At timestep:     848 / 1000At timestep:     860 / 1000At timestep:     875 / 1000At timestep:     888 / 1000At timestep:     907 / 1000At timestep:     927 / 1000At timestep:     950 / 1000At timestep:     966 / 1000At timestep:     994 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -16.889999389648438
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2013.5112113952637
Exploration Critic Loss : 10347864.0
Exploitation Critic Loss : 0.07879061996936798
Exploration Model Loss : 35.80824279785156
Actor Loss : 0.433760404586792
Eval_AverageReturn : -17.472726821899414
Eval_StdReturn : 4.48777961730957
Eval_MaxReturn : -11.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 18.472727272727273
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -17.959999
best mean reward -16.580000
running time 2078.408035
At timestep:     18 / 1000At timestep:     38 / 1000At timestep:     59 / 1000At timestep:     79 / 1000At timestep:     104 / 1000At timestep:     118 / 1000At timestep:     143 / 1000At timestep:     170 / 1000At timestep:     184 / 1000At timestep:     201 / 1000At timestep:     221 / 1000At timestep:     238 / 1000At timestep:     252 / 1000At timestep:     276 / 1000At timestep:     293 / 1000At timestep:     311 / 1000At timestep:     339 / 1000At timestep:     355 / 1000At timestep:     376 / 1000At timestep:     399 / 1000At timestep:     423 / 1000At timestep:     438 / 1000At timestep:     456 / 1000At timestep:     477 / 1000At timestep:     497 / 1000At timestep:     539 / 1000At timestep:     559 / 1000At timestep:     581 / 1000At timestep:     603 / 1000At timestep:     620 / 1000At timestep:     633 / 1000At timestep:     665 / 1000At timestep:     683 / 1000At timestep:     698 / 1000At timestep:     715 / 1000At timestep:     757 / 1000At timestep:     772 / 1000At timestep:     786 / 1000At timestep:     805 / 1000At timestep:     821 / 1000At timestep:     838 / 1000At timestep:     859 / 1000At timestep:     875 / 1000At timestep:     890 / 1000At timestep:     913 / 1000At timestep:     933 / 1000At timestep:     955 / 1000At timestep:     975 / 1000At timestep:     993 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -17.959999084472656
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2078.408034801483
Exploration Critic Loss : 17621828.0
Exploitation Critic Loss : 0.10180729627609253
Exploration Model Loss : 52.32496643066406
Actor Loss : 0.5187288522720337
Eval_AverageReturn : -19.280000686645508
Eval_StdReturn : 5.922972202301025
Eval_MaxReturn : -12.0
Eval_MinReturn : -41.0
Eval_AverageEpLen : 20.28
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -17.820000
best mean reward -16.580000
running time 2141.643557
At timestep:     17 / 1000At timestep:     34 / 1000At timestep:     55 / 1000At timestep:     72 / 1000At timestep:     92 / 1000At timestep:     109 / 1000At timestep:     124 / 1000At timestep:     142 / 1000At timestep:     165 / 1000At timestep:     178 / 1000At timestep:     202 / 1000At timestep:     225 / 1000At timestep:     242 / 1000At timestep:     261 / 1000At timestep:     277 / 1000At timestep:     291 / 1000At timestep:     306 / 1000At timestep:     330 / 1000At timestep:     345 / 1000At timestep:     365 / 1000At timestep:     392 / 1000At timestep:     405 / 1000At timestep:     422 / 1000At timestep:     444 / 1000At timestep:     463 / 1000At timestep:     482 / 1000At timestep:     499 / 1000At timestep:     521 / 1000At timestep:     544 / 1000At timestep:     559 / 1000At timestep:     581 / 1000At timestep:     596 / 1000At timestep:     618 / 1000At timestep:     637 / 1000At timestep:     657 / 1000At timestep:     682 / 1000At timestep:     703 / 1000At timestep:     723 / 1000At timestep:     738 / 1000At timestep:     753 / 1000At timestep:     778 / 1000At timestep:     794 / 1000At timestep:     808 / 1000At timestep:     824 / 1000At timestep:     841 / 1000At timestep:     864 / 1000At timestep:     880 / 1000At timestep:     901 / 1000At timestep:     919 / 1000At timestep:     937 / 1000At timestep:     956 / 1000At timestep:     977 / 1000At timestep:     991 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -17.81999969482422
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2141.643557071686
Exploration Critic Loss : 9457067.0
Exploitation Critic Loss : 0.10889787971973419
Exploration Model Loss : 26.994888305664062
Actor Loss : 0.480538934469223
Eval_AverageReturn : -17.685184478759766
Eval_StdReturn : 3.4201152324676514
Eval_MaxReturn : -12.0
Eval_MinReturn : -26.0
Eval_AverageEpLen : 18.685185185185187
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -17.500000
best mean reward -16.580000
running time 2206.514553
At timestep:     19 / 1000At timestep:     37 / 1000At timestep:     54 / 1000At timestep:     81 / 1000At timestep:     103 / 1000At timestep:     128 / 1000At timestep:     144 / 1000At timestep:     162 / 1000At timestep:     186 / 1000At timestep:     207 / 1000At timestep:     227 / 1000At timestep:     252 / 1000At timestep:     281 / 1000At timestep:     292 / 1000At timestep:     306 / 1000At timestep:     323 / 1000At timestep:     347 / 1000At timestep:     364 / 1000At timestep:     388 / 1000At timestep:     402 / 1000At timestep:     418 / 1000At timestep:     439 / 1000At timestep:     458 / 1000At timestep:     481 / 1000At timestep:     495 / 1000At timestep:     523 / 1000At timestep:     538 / 1000At timestep:     561 / 1000At timestep:     584 / 1000At timestep:     602 / 1000At timestep:     623 / 1000At timestep:     642 / 1000At timestep:     670 / 1000At timestep:     686 / 1000At timestep:     703 / 1000At timestep:     730 / 1000At timestep:     750 / 1000At timestep:     766 / 1000At timestep:     788 / 1000At timestep:     799 / 1000At timestep:     818 / 1000At timestep:     844 / 1000At timestep:     862 / 1000At timestep:     878 / 1000At timestep:     895 / 1000At timestep:     915 / 1000At timestep:     930 / 1000At timestep:     949 / 1000At timestep:     963 / 1000At timestep:     989 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -17.5
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2206.514553308487
Exploration Critic Loss : 11938389.0
Exploitation Critic Loss : 0.07959733158349991
Exploration Model Loss : 35.808197021484375
Actor Loss : 0.4385531544685364
Eval_AverageReturn : -18.960784912109375
Eval_StdReturn : 4.639814376831055
Eval_MaxReturn : -10.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 19.96078431372549
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -17.809999
best mean reward -16.580000
running time 2270.231067
At timestep:     23 / 1000At timestep:     37 / 1000At timestep:     58 / 1000At timestep:     73 / 1000At timestep:     85 / 1000At timestep:     101 / 1000At timestep:     120 / 1000At timestep:     135 / 1000At timestep:     151 / 1000At timestep:     165 / 1000At timestep:     188 / 1000At timestep:     206 / 1000At timestep:     231 / 1000At timestep:     246 / 1000At timestep:     269 / 1000At timestep:     281 / 1000At timestep:     304 / 1000At timestep:     315 / 1000At timestep:     334 / 1000At timestep:     353 / 1000At timestep:     372 / 1000At timestep:     385 / 1000At timestep:     405 / 1000At timestep:     423 / 1000At timestep:     438 / 1000At timestep:     452 / 1000At timestep:     469 / 1000At timestep:     487 / 1000At timestep:     511 / 1000At timestep:     526 / 1000At timestep:     544 / 1000At timestep:     558 / 1000At timestep:     592 / 1000At timestep:     609 / 1000At timestep:     627 / 1000At timestep:     643 / 1000At timestep:     657 / 1000At timestep:     678 / 1000At timestep:     695 / 1000At timestep:     710 / 1000At timestep:     725 / 1000At timestep:     743 / 1000At timestep:     765 / 1000At timestep:     779 / 1000At timestep:     797 / 1000At timestep:     819 / 1000At timestep:     839 / 1000At timestep:     868 / 1000At timestep:     882 / 1000At timestep:     901 / 1000At timestep:     916 / 1000At timestep:     939 / 1000At timestep:     965 / 1000At timestep:     978 / 1000At timestep:     995 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -17.809999465942383
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2270.231066942215
Exploration Critic Loss : 19040442.0
Exploitation Critic Loss : 0.08048859983682632
Exploration Model Loss : 52.32502746582031
Actor Loss : 0.4378267228603363
Eval_AverageReturn : -17.160715103149414
Eval_StdReturn : 4.431127071380615
Eval_MaxReturn : -10.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 18.160714285714285
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -17.459999
best mean reward -16.580000
running time 2335.196852
At timestep:     19 / 1000At timestep:     39 / 1000At timestep:     53 / 1000At timestep:     66 / 1000At timestep:     89 / 1000At timestep:     104 / 1000At timestep:     123 / 1000At timestep:     135 / 1000At timestep:     152 / 1000At timestep:     168 / 1000At timestep:     186 / 1000At timestep:     202 / 1000At timestep:     222 / 1000At timestep:     240 / 1000At timestep:     255 / 1000At timestep:     275 / 1000At timestep:     295 / 1000At timestep:     309 / 1000At timestep:     334 / 1000At timestep:     347 / 1000At timestep:     362 / 1000At timestep:     378 / 1000At timestep:     394 / 1000At timestep:     414 / 1000At timestep:     431 / 1000At timestep:     450 / 1000At timestep:     469 / 1000At timestep:     488 / 1000At timestep:     508 / 1000At timestep:     533 / 1000At timestep:     548 / 1000At timestep:     571 / 1000At timestep:     587 / 1000At timestep:     606 / 1000At timestep:     619 / 1000At timestep:     633 / 1000At timestep:     659 / 1000At timestep:     680 / 1000At timestep:     697 / 1000At timestep:     718 / 1000At timestep:     737 / 1000At timestep:     753 / 1000At timestep:     771 / 1000At timestep:     788 / 1000At timestep:     817 / 1000At timestep:     830 / 1000At timestep:     847 / 1000At timestep:     868 / 1000At timestep:     882 / 1000At timestep:     899 / 1000At timestep:     922 / 1000At timestep:     937 / 1000At timestep:     951 / 1000At timestep:     968 / 1000At timestep:     983 / 1000At timestep:     999 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -17.459999084472656
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2335.1968519687653
Exploration Critic Loss : 12534680.0
Exploitation Critic Loss : 0.08186322450637817
Exploration Model Loss : 26.994827270507812
Actor Loss : 0.571696400642395
Eval_AverageReturn : -16.789474487304688
Eval_StdReturn : 3.5724916458129883
Eval_MaxReturn : -11.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 17.789473684210527
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -17.209999
best mean reward -16.580000
running time 2401.587748
At timestep:     18 / 1000At timestep:     34 / 1000At timestep:     47 / 1000At timestep:     68 / 1000At timestep:     84 / 1000At timestep:     100 / 1000At timestep:     117 / 1000At timestep:     130 / 1000At timestep:     148 / 1000At timestep:     163 / 1000At timestep:     183 / 1000At timestep:     198 / 1000At timestep:     213 / 1000At timestep:     228 / 1000At timestep:     244 / 1000At timestep:     266 / 1000At timestep:     279 / 1000At timestep:     302 / 1000At timestep:     319 / 1000At timestep:     342 / 1000At timestep:     356 / 1000At timestep:     375 / 1000At timestep:     391 / 1000At timestep:     412 / 1000At timestep:     429 / 1000At timestep:     443 / 1000At timestep:     461 / 1000At timestep:     485 / 1000At timestep:     508 / 1000At timestep:     525 / 1000At timestep:     537 / 1000At timestep:     551 / 1000At timestep:     568 / 1000At timestep:     588 / 1000At timestep:     604 / 1000At timestep:     620 / 1000At timestep:     633 / 1000At timestep:     650 / 1000At timestep:     664 / 1000At timestep:     679 / 1000At timestep:     701 / 1000At timestep:     721 / 1000At timestep:     741 / 1000At timestep:     759 / 1000At timestep:     782 / 1000At timestep:     802 / 1000At timestep:     822 / 1000At timestep:     839 / 1000At timestep:     856 / 1000At timestep:     874 / 1000At timestep:     900 / 1000At timestep:     913 / 1000At timestep:     945 / 1000At timestep:     962 / 1000At timestep:     977 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 40001
Train_AverageReturn : -17.209999084472656
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2401.5877482891083
Exploration Critic Loss : 14154125.0
Exploitation Critic Loss : 0.10461065173149109
Exploration Model Loss : 35.80812072753906
Actor Loss : 0.4746418595314026
Eval_AverageReturn : -16.946428298950195
Eval_StdReturn : 3.979499578475952
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 17.946428571428573
Buffer size : 40001
Done logging...




********** Iteration 41000 ************

Training agent...

Beginning logging procedure...
Timestep 41001
mean reward (100 episodes) -17.480000
best mean reward -16.580000
running time 2466.284338
At timestep:     22 / 1000At timestep:     38 / 1000At timestep:     59 / 1000At timestep:     76 / 1000At timestep:     96 / 1000At timestep:     120 / 1000At timestep:     145 / 1000At timestep:     166 / 1000At timestep:     186 / 1000At timestep:     202 / 1000At timestep:     222 / 1000At timestep:     237 / 1000At timestep:     264 / 1000At timestep:     277 / 1000At timestep:     292 / 1000At timestep:     308 / 1000At timestep:     328 / 1000At timestep:     349 / 1000At timestep:     367 / 1000At timestep:     387 / 1000At timestep:     408 / 1000At timestep:     427 / 1000At timestep:     446 / 1000At timestep:     466 / 1000At timestep:     490 / 1000At timestep:     507 / 1000At timestep:     530 / 1000At timestep:     541 / 1000At timestep:     557 / 1000At timestep:     576 / 1000At timestep:     598 / 1000At timestep:     623 / 1000At timestep:     640 / 1000At timestep:     653 / 1000At timestep:     668 / 1000At timestep:     688 / 1000At timestep:     708 / 1000At timestep:     726 / 1000At timestep:     742 / 1000At timestep:     763 / 1000At timestep:     780 / 1000At timestep:     794 / 1000At timestep:     812 / 1000At timestep:     830 / 1000At timestep:     847 / 1000At timestep:     865 / 1000At timestep:     882 / 1000At timestep:     903 / 1000At timestep:     925 / 1000At timestep:     943 / 1000At timestep:     960 / 1000At timestep:     979 / 1000At timestep:     994 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 41001
Train_AverageReturn : -17.479999542236328
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2466.284337759018
Exploration Critic Loss : 27383636.0
Exploitation Critic Loss : 0.09832534193992615
Exploration Model Loss : 52.3250732421875
Actor Loss : 0.5055679678916931
Eval_AverageReturn : -17.685184478759766
Eval_StdReturn : 3.264988422393799
Eval_MaxReturn : -10.0
Eval_MinReturn : -26.0
Eval_AverageEpLen : 18.685185185185187
Buffer size : 41001
Done logging...




********** Iteration 42000 ************

Training agent...

Beginning logging procedure...
Timestep 42001
mean reward (100 episodes) -17.860001
best mean reward -16.580000
running time 2531.275820
At timestep:     40 / 1000At timestep:     62 / 1000At timestep:     81 / 1000At timestep:     97 / 1000At timestep:     122 / 1000At timestep:     137 / 1000At timestep:     156 / 1000At timestep:     176 / 1000At timestep:     189 / 1000At timestep:     213 / 1000At timestep:     235 / 1000At timestep:     256 / 1000At timestep:     271 / 1000At timestep:     287 / 1000At timestep:     302 / 1000At timestep:     321 / 1000At timestep:     336 / 1000At timestep:     353 / 1000At timestep:     369 / 1000At timestep:     392 / 1000At timestep:     414 / 1000At timestep:     437 / 1000At timestep:     458 / 1000At timestep:     476 / 1000At timestep:     494 / 1000At timestep:     513 / 1000At timestep:     537 / 1000At timestep:     559 / 1000At timestep:     580 / 1000At timestep:     604 / 1000At timestep:     618 / 1000At timestep:     634 / 1000At timestep:     649 / 1000At timestep:     680 / 1000At timestep:     700 / 1000At timestep:     724 / 1000At timestep:     740 / 1000At timestep:     762 / 1000At timestep:     793 / 1000At timestep:     808 / 1000At timestep:     828 / 1000At timestep:     847 / 1000At timestep:     870 / 1000At timestep:     897 / 1000At timestep:     928 / 1000At timestep:     954 / 1000At timestep:     975 / 1000At timestep:     993 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 42001
Train_AverageReturn : -17.860000610351562
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2531.275819540024
Exploration Critic Loss : 12595676.0
Exploitation Critic Loss : 0.07312102615833282
Exploration Model Loss : 26.994766235351562
Actor Loss : 0.5445458889007568
Eval_AverageReturn : -19.67346954345703
Eval_StdReturn : 5.1641926765441895
Eval_MaxReturn : -12.0
Eval_MinReturn : -39.0
Eval_AverageEpLen : 20.6734693877551
Buffer size : 42001
Done logging...




********** Iteration 43000 ************

Training agent...

Beginning logging procedure...
Timestep 43001
mean reward (100 episodes) -17.110001
best mean reward -16.580000
running time 2595.084706
At timestep:     30 / 1000At timestep:     53 / 1000At timestep:     69 / 1000At timestep:     91 / 1000At timestep:     108 / 1000At timestep:     123 / 1000At timestep:     139 / 1000At timestep:     160 / 1000At timestep:     179 / 1000At timestep:     201 / 1000At timestep:     219 / 1000At timestep:     233 / 1000At timestep:     248 / 1000At timestep:     263 / 1000At timestep:     287 / 1000At timestep:     301 / 1000At timestep:     322 / 1000At timestep:     344 / 1000At timestep:     366 / 1000At timestep:     384 / 1000At timestep:     397 / 1000At timestep:     412 / 1000At timestep:     426 / 1000At timestep:     457 / 1000At timestep:     481 / 1000At timestep:     503 / 1000At timestep:     525 / 1000At timestep:     550 / 1000At timestep:     566 / 1000At timestep:     590 / 1000At timestep:     618 / 1000At timestep:     635 / 1000At timestep:     651 / 1000At timestep:     669 / 1000At timestep:     684 / 1000At timestep:     705 / 1000At timestep:     722 / 1000At timestep:     740 / 1000At timestep:     752 / 1000At timestep:     769 / 1000At timestep:     785 / 1000At timestep:     799 / 1000At timestep:     817 / 1000At timestep:     834 / 1000At timestep:     849 / 1000At timestep:     873 / 1000At timestep:     891 / 1000At timestep:     912 / 1000At timestep:     934 / 1000At timestep:     951 / 1000At timestep:     971 / 1000At timestep:     997 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 43001
Train_AverageReturn : -17.110000610351562
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2595.084705591202
Exploration Critic Loss : 12519269.0
Exploitation Critic Loss : 0.0703294426202774
Exploration Model Loss : 35.80805969238281
Actor Loss : 0.4407563805580139
Eval_AverageReturn : -18.264150619506836
Eval_StdReturn : 4.353055477142334
Eval_MaxReturn : -11.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 19.264150943396228
Buffer size : 43001
Done logging...




********** Iteration 44000 ************

Training agent...

Beginning logging procedure...
Timestep 44001
mean reward (100 episodes) -17.000000
best mean reward -16.580000
running time 2659.940349
At timestep:     23 / 1000At timestep:     44 / 1000At timestep:     60 / 1000At timestep:     78 / 1000At timestep:     98 / 1000At timestep:     121 / 1000At timestep:     143 / 1000At timestep:     161 / 1000At timestep:     184 / 1000At timestep:     198 / 1000At timestep:     220 / 1000At timestep:     243 / 1000At timestep:     261 / 1000At timestep:     290 / 1000At timestep:     306 / 1000At timestep:     332 / 1000At timestep:     355 / 1000At timestep:     375 / 1000At timestep:     397 / 1000At timestep:     416 / 1000At timestep:     436 / 1000At timestep:     448 / 1000At timestep:     466 / 1000At timestep:     489 / 1000At timestep:     507 / 1000At timestep:     522 / 1000At timestep:     554 / 1000At timestep:     579 / 1000At timestep:     600 / 1000At timestep:     617 / 1000At timestep:     641 / 1000At timestep:     660 / 1000At timestep:     680 / 1000At timestep:     702 / 1000At timestep:     721 / 1000At timestep:     743 / 1000At timestep:     757 / 1000At timestep:     770 / 1000At timestep:     786 / 1000At timestep:     805 / 1000At timestep:     823 / 1000At timestep:     843 / 1000At timestep:     865 / 1000At timestep:     882 / 1000At timestep:     898 / 1000At timestep:     915 / 1000At timestep:     930 / 1000At timestep:     947 / 1000At timestep:     967 / 1000At timestep:     998 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 44001
Train_AverageReturn : -17.0
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2659.9403488636017
Exploration Critic Loss : 32684168.0
Exploitation Critic Loss : 0.11395950615406036
Exploration Model Loss : 52.32513427734375
Actor Loss : 0.44557690620422363
Eval_AverageReturn : -18.86274528503418
Eval_StdReturn : 4.177529335021973
Eval_MaxReturn : -11.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.862745098039216
Buffer size : 44001
Done logging...




********** Iteration 45000 ************

Training agent...

Beginning logging procedure...
Timestep 45001
mean reward (100 episodes) -17.530001
best mean reward -16.580000
running time 2722.435911
At timestep:     16 / 1000At timestep:     35 / 1000At timestep:     55 / 1000At timestep:     78 / 1000At timestep:     100 / 1000At timestep:     117 / 1000At timestep:     133 / 1000At timestep:     149 / 1000At timestep:     162 / 1000At timestep:     179 / 1000At timestep:     207 / 1000At timestep:     224 / 1000At timestep:     243 / 1000At timestep:     261 / 1000At timestep:     281 / 1000At timestep:     303 / 1000At timestep:     319 / 1000At timestep:     341 / 1000At timestep:     357 / 1000At timestep:     378 / 1000At timestep:     400 / 1000At timestep:     419 / 1000At timestep:     437 / 1000At timestep:     454 / 1000At timestep:     475 / 1000At timestep:     494 / 1000At timestep:     508 / 1000At timestep:     526 / 1000At timestep:     549 / 1000At timestep:     571 / 1000At timestep:     585 / 1000At timestep:     606 / 1000At timestep:     623 / 1000At timestep:     636 / 1000At timestep:     661 / 1000At timestep:     697 / 1000At timestep:     725 / 1000At timestep:     744 / 1000At timestep:     770 / 1000At timestep:     785 / 1000At timestep:     811 / 1000At timestep:     845 / 1000At timestep:     877 / 1000At timestep:     897 / 1000At timestep:     910 / 1000At timestep:     928 / 1000At timestep:     944 / 1000At timestep:     963 / 1000At timestep:     988 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 45001
Train_AverageReturn : -17.530000686645508
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2722.435911178589
Exploration Critic Loss : 17477368.0
Exploitation Critic Loss : 0.08485829830169678
Exploration Model Loss : 26.99468994140625
Actor Loss : 0.44758886098861694
Eval_AverageReturn : -19.1200008392334
Eval_StdReturn : 5.093682289123535
Eval_MaxReturn : -12.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 20.12
Buffer size : 45001
Done logging...




********** Iteration 46000 ************

Training agent...

Beginning logging procedure...
Timestep 46001
mean reward (100 episodes) -17.850000
best mean reward -16.580000
running time 2788.436136
At timestep:     19 / 1000At timestep:     36 / 1000At timestep:     55 / 1000At timestep:     67 / 1000At timestep:     85 / 1000At timestep:     101 / 1000At timestep:     119 / 1000At timestep:     138 / 1000At timestep:     156 / 1000At timestep:     175 / 1000At timestep:     190 / 1000At timestep:     208 / 1000At timestep:     229 / 1000At timestep:     251 / 1000At timestep:     267 / 1000At timestep:     284 / 1000At timestep:     301 / 1000At timestep:     317 / 1000At timestep:     342 / 1000At timestep:     365 / 1000At timestep:     388 / 1000At timestep:     404 / 1000At timestep:     420 / 1000At timestep:     433 / 1000At timestep:     451 / 1000At timestep:     480 / 1000At timestep:     501 / 1000At timestep:     517 / 1000At timestep:     539 / 1000At timestep:     557 / 1000At timestep:     571 / 1000At timestep:     596 / 1000At timestep:     610 / 1000At timestep:     626 / 1000At timestep:     648 / 1000At timestep:     673 / 1000At timestep:     691 / 1000At timestep:     708 / 1000At timestep:     726 / 1000At timestep:     744 / 1000At timestep:     774 / 1000At timestep:     797 / 1000At timestep:     819 / 1000At timestep:     836 / 1000At timestep:     849 / 1000At timestep:     871 / 1000At timestep:     889 / 1000At timestep:     914 / 1000At timestep:     933 / 1000At timestep:     949 / 1000At timestep:     968 / 1000At timestep:     981 / 1000At timestep:     998 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 46001
Train_AverageReturn : -17.850000381469727
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2788.4361362457275
Exploration Critic Loss : 19884942.0
Exploitation Critic Loss : 0.09364908188581467
Exploration Model Loss : 35.8079833984375
Actor Loss : 0.5299160480499268
Eval_AverageReturn : -17.83333396911621
Eval_StdReturn : 3.8333332538604736
Eval_MaxReturn : -11.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 18.833333333333332
Buffer size : 46001
Done logging...




********** Iteration 47000 ************

Training agent...

Beginning logging procedure...
Timestep 47001
mean reward (100 episodes) -17.299999
best mean reward -16.580000
running time 2855.570920
At timestep:     18 / 1000At timestep:     40 / 1000At timestep:     53 / 1000At timestep:     75 / 1000At timestep:     89 / 1000At timestep:     106 / 1000At timestep:     124 / 1000At timestep:     142 / 1000At timestep:     161 / 1000At timestep:     184 / 1000At timestep:     205 / 1000At timestep:     218 / 1000At timestep:     232 / 1000At timestep:     250 / 1000At timestep:     268 / 1000At timestep:     283 / 1000At timestep:     311 / 1000At timestep:     327 / 1000At timestep:     338 / 1000At timestep:     357 / 1000At timestep:     377 / 1000At timestep:     412 / 1000At timestep:     434 / 1000At timestep:     450 / 1000At timestep:     469 / 1000At timestep:     483 / 1000At timestep:     515 / 1000At timestep:     531 / 1000At timestep:     547 / 1000At timestep:     576 / 1000At timestep:     601 / 1000At timestep:     615 / 1000At timestep:     629 / 1000At timestep:     656 / 1000At timestep:     672 / 1000At timestep:     689 / 1000At timestep:     710 / 1000At timestep:     728 / 1000At timestep:     750 / 1000At timestep:     773 / 1000At timestep:     789 / 1000At timestep:     819 / 1000At timestep:     833 / 1000At timestep:     859 / 1000At timestep:     880 / 1000At timestep:     896 / 1000At timestep:     912 / 1000At timestep:     935 / 1000At timestep:     949 / 1000At timestep:     970 / 1000At timestep:     996 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 47001
Train_AverageReturn : -17.299999237060547
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2855.570919752121
Exploration Critic Loss : 19955032.0
Exploitation Critic Loss : 0.0893254429101944
Exploration Model Loss : 52.32521057128906
Actor Loss : 0.5044412016868591
Eval_AverageReturn : -18.55769157409668
Eval_StdReturn : 5.238222599029541
Eval_MaxReturn : -10.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.557692307692307
Buffer size : 47001
Done logging...




********** Iteration 48000 ************

Training agent...

Beginning logging procedure...
Timestep 48001
mean reward (100 episodes) -17.440001
best mean reward -16.580000
running time 2919.310637
At timestep:     18 / 1000At timestep:     39 / 1000At timestep:     56 / 1000At timestep:     80 / 1000At timestep:     94 / 1000At timestep:     111 / 1000At timestep:     127 / 1000At timestep:     145 / 1000At timestep:     163 / 1000At timestep:     190 / 1000At timestep:     207 / 1000At timestep:     228 / 1000At timestep:     241 / 1000At timestep:     265 / 1000At timestep:     281 / 1000At timestep:     300 / 1000At timestep:     320 / 1000At timestep:     336 / 1000At timestep:     350 / 1000At timestep:     376 / 1000At timestep:     398 / 1000At timestep:     414 / 1000At timestep:     435 / 1000At timestep:     459 / 1000At timestep:     476 / 1000At timestep:     499 / 1000At timestep:     517 / 1000At timestep:     541 / 1000At timestep:     570 / 1000At timestep:     586 / 1000At timestep:     600 / 1000At timestep:     614 / 1000At timestep:     634 / 1000At timestep:     669 / 1000At timestep:     684 / 1000At timestep:     705 / 1000At timestep:     721 / 1000At timestep:     738 / 1000At timestep:     753 / 1000At timestep:     783 / 1000At timestep:     798 / 1000At timestep:     813 / 1000At timestep:     828 / 1000At timestep:     844 / 1000At timestep:     858 / 1000At timestep:     873 / 1000At timestep:     894 / 1000At timestep:     909 / 1000At timestep:     928 / 1000At timestep:     940 / 1000At timestep:     961 / 1000At timestep:     975 / 1000At timestep:     988 / 1000At timestep:     1007 / 1000Train_EnvstepsSoFar : 48001
Train_AverageReturn : -17.440000534057617
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2919.3106367588043
Exploration Critic Loss : 15771838.0
Exploitation Critic Loss : 0.07104329764842987
Exploration Model Loss : 26.994644165039062
Actor Loss : 0.4658910036087036
Eval_AverageReturn : -17.648147583007812
Eval_StdReturn : 4.722512722015381
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 18.64814814814815
Buffer size : 48001
Done logging...




********** Iteration 49000 ************

Training agent...

Beginning logging procedure...
Timestep 49001
mean reward (100 episodes) -17.969999
best mean reward -16.580000
running time 2988.212401
At timestep:     15 / 1000At timestep:     46 / 1000At timestep:     64 / 1000At timestep:     82 / 1000At timestep:     100 / 1000At timestep:     116 / 1000At timestep:     134 / 1000At timestep:     152 / 1000At timestep:     166 / 1000At timestep:     178 / 1000At timestep:     203 / 1000At timestep:     217 / 1000At timestep:     240 / 1000At timestep:     271 / 1000At timestep:     290 / 1000At timestep:     314 / 1000At timestep:     331 / 1000At timestep:     353 / 1000At timestep:     369 / 1000At timestep:     382 / 1000At timestep:     405 / 1000At timestep:     419 / 1000At timestep:     434 / 1000At timestep:     455 / 1000At timestep:     485 / 1000At timestep:     503 / 1000At timestep:     518 / 1000At timestep:     535 / 1000At timestep:     548 / 1000At timestep:     576 / 1000At timestep:     593 / 1000At timestep:     626 / 1000At timestep:     648 / 1000At timestep:     664 / 1000At timestep:     689 / 1000At timestep:     709 / 1000At timestep:     730 / 1000At timestep:     743 / 1000At timestep:     763 / 1000At timestep:     793 / 1000At timestep:     809 / 1000At timestep:     828 / 1000At timestep:     849 / 1000At timestep:     863 / 1000At timestep:     880 / 1000At timestep:     897 / 1000At timestep:     912 / 1000At timestep:     930 / 1000At timestep:     956 / 1000At timestep:     979 / 1000At timestep:     1000 / 1000/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/envs/registration.py:441: UserWarning: [33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.[0m
  "The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:472: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dist = np.full((height, width, height, width), np.float('inf'))
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/spaces/box.py:128: UserWarning: [33mWARN: Box bound precision lowered by casting to float64[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  "Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/wrappers/record_video.py:79: UserWarning: [33mWARN: Overwriting existing videos at /home/harvey/Documents/cs285/homework_fall2022/hw5/data/hw5_expl_q4_awac_easy_unsupervised_lam10_PointmassEasy-v0_22-11-2022_22-18-47/gym folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  f"Overwriting existing videos at {self.video_folder} folder "
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:191: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  "Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting."
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:196: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  "Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information."
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/dqn_utils.py:542: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.done     = np.empty([self.size],                     dtype=np.bool)
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py:394: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (i, j) = np.floor(resolution * state).astype(np.int)
/home/harvey/anaconda3/envs/cs285/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. [0m
  "Core environment is written in old step API which returns one bool instead of two. "
/home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  self.fig = plt.figure()
Train_EnvstepsSoFar : 49001
Train_AverageReturn : -17.969999313354492
Train_BestReturn : -16.579999923706055
TimeSinceStart : 2988.212401151657
Exploration Critic Loss : 25345340.0
Exploitation Critic Loss : 0.07396607100963593
Exploration Model Loss : 35.80790710449219
Actor Loss : 0.42116937041282654
Eval_AverageReturn : -18.60784339904785
Eval_StdReturn : 5.243402481079102
Eval_MaxReturn : -11.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 19.607843137254903
Buffer size : 49001
Done logging...





LOGGING TO:  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam20_PointmassEasy-v0_22-11-2022_23-09-51 



########################
logging outputs to  /home/harvey/Documents/cs285/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam20_PointmassEasy-v0_22-11-2022_23-09-51
########################
Using GPU id 0


********** Iteration 0 ************

Training agent...

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.004460
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1
TimeSinceStart : 0.00446009635925293
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1
Done logging...




********** Iteration 1000 ************

Training agent...

Beginning logging procedure...
Timestep 1001
mean reward (100 episodes) -49.599998
best mean reward -inf
running time 19.823594
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 1001
Train_AverageReturn : -49.599998474121094
TimeSinceStart : 19.823594331741333
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 1001
Done logging...




********** Iteration 2000 ************

Training agent...

Beginning logging procedure...
Timestep 2001
mean reward (100 episodes) -49.799999
best mean reward -inf
running time 39.284007
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     300 / 1000At timestep:     350 / 1000At timestep:     400 / 1000At timestep:     450 / 1000At timestep:     500 / 1000At timestep:     550 / 1000At timestep:     600 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     850 / 1000At timestep:     900 / 1000At timestep:     950 / 1000At timestep:     1000 / 1000Train_EnvstepsSoFar : 2001
Train_AverageReturn : -49.79999923706055
TimeSinceStart : 39.28400707244873
Eval_AverageReturn : -50.0
Eval_StdReturn : 0.0
Eval_MaxReturn : -50.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 50.0
Buffer size : 2001
Done logging...




********** Iteration 3000 ************

Training agent...

Beginning logging procedure...
Timestep 3001
mean reward (100 episodes) -43.761192
best mean reward -inf
running time 90.610025
At timestep:     50 / 1000At timestep:     100 / 1000At timestep:     150 / 1000At timestep:     200 / 1000At timestep:     250 / 1000At timestep:     288 / 1000At timestep:     334 / 1000At timestep:     384 / 1000At timestep:     434 / 1000At timestep:     484 / 1000At timestep:     519 / 1000At timestep:     551 / 1000At timestep:     601 / 1000At timestep:     633 / 1000At timestep:     666 / 1000At timestep:     716 / 1000At timestep:     766 / 1000At timestep:     797 / 1000At timestep:     847 / 1000At timestep:     897 / 1000At timestep:     947 / 1000At timestep:     997 / 1000At timestep:     1047 / 1000Train_EnvstepsSoFar : 3001
Train_AverageReturn : -43.761192321777344
TimeSinceStart : 90.61002492904663
Exploration Critic Loss : 629814.9375
Exploitation Critic Loss : 0.029943164438009262
Exploration Model Loss : 34.68378829956055
Actor Loss : 1.1643941402435303
Eval_AverageReturn : -45.21739196777344
Eval_StdReturn : 7.717604637145996
Eval_MaxReturn : -30.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 45.52173913043478
Buffer size : 3001
Done logging...




********** Iteration 4000 ************

Training agent...

Beginning logging procedure...
Timestep 4001
mean reward (100 episodes) -38.180000
best mean reward -38.180000
running time 143.257638
At timestep:     39 / 1000At timestep:     79 / 1000At timestep:     129 / 1000At timestep:     179 / 1000At timestep:     229 / 1000At timestep:     279 / 1000At timestep:     329 / 1000At timestep:     371 / 1000At timestep:     421 / 1000At timestep:     471 / 1000At timestep:     491 / 1000At timestep:     535 / 1000At timestep:     560 / 1000At timestep:     589 / 1000At timestep:     604 / 1000At timestep:     650 / 1000At timestep:     700 / 1000At timestep:     745 / 1000At timestep:     782 / 1000At timestep:     832 / 1000At timestep:     882 / 1000At timestep:     928 / 1000At timestep:     978 / 1000At timestep:     1019 / 1000Train_EnvstepsSoFar : 4001
Train_AverageReturn : -38.18000030517578
Train_BestReturn : -38.18000030517578
TimeSinceStart : 143.2576379776001
Exploration Critic Loss : 9246101.0
Exploitation Critic Loss : 0.10456299036741257
Exploration Model Loss : 52.14476013183594
Actor Loss : 0.9024221897125244
Eval_AverageReturn : -41.875
Eval_StdReturn : 10.429574966430664
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 42.458333333333336
Buffer size : 4001
Done logging...




********** Iteration 5000 ************

Training agent...

Beginning logging procedure...
Timestep 5001
mean reward (100 episodes) -24.580000
best mean reward -24.580000
running time 203.071811
At timestep:     50 / 1000At timestep:     72 / 1000At timestep:     95 / 1000At timestep:     131 / 1000At timestep:     181 / 1000At timestep:     199 / 1000At timestep:     237 / 1000At timestep:     267 / 1000At timestep:     317 / 1000At timestep:     358 / 1000At timestep:     389 / 1000At timestep:     411 / 1000At timestep:     461 / 1000At timestep:     489 / 1000At timestep:     539 / 1000At timestep:     572 / 1000At timestep:     606 / 1000At timestep:     656 / 1000At timestep:     682 / 1000At timestep:     705 / 1000At timestep:     730 / 1000At timestep:     758 / 1000At timestep:     802 / 1000At timestep:     838 / 1000At timestep:     888 / 1000At timestep:     927 / 1000At timestep:     977 / 1000At timestep:     1027 / 1000Train_EnvstepsSoFar : 5001
Train_AverageReturn : -24.579999923706055
Train_BestReturn : -24.579999923706055
TimeSinceStart : 203.07181119918823
Exploration Critic Loss : 14533642.0
Exploitation Critic Loss : 0.10110817849636078
Exploration Model Loss : 68.55303955078125
Actor Loss : 0.735587477684021
Eval_AverageReturn : -36.0
Eval_StdReturn : 11.301074981689453
Eval_MaxReturn : -17.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 36.67857142857143
Buffer size : 5001
Done logging...




********** Iteration 6000 ************

Training agent...

Beginning logging procedure...
Timestep 6001
mean reward (100 episodes) -18.820000
best mean reward -18.820000
running time 269.308408
At timestep:     18 / 1000At timestep:     37 / 1000At timestep:     59 / 1000At timestep:     87 / 1000At timestep:     111 / 1000At timestep:     126 / 1000At timestep:     145 / 1000At timestep:     194 / 1000At timestep:     244 / 1000At timestep:     278 / 1000At timestep:     310 / 1000At timestep:     360 / 1000At timestep:     408 / 1000At timestep:     438 / 1000At timestep:     461 / 1000At timestep:     485 / 1000At timestep:     506 / 1000At timestep:     534 / 1000At timestep:     559 / 1000At timestep:     582 / 1000At timestep:     604 / 1000At timestep:     639 / 1000At timestep:     666 / 1000At timestep:     698 / 1000At timestep:     747 / 1000At timestep:     778 / 1000At timestep:     809 / 1000At timestep:     835 / 1000At timestep:     856 / 1000At timestep:     883 / 1000At timestep:     913 / 1000At timestep:     931 / 1000At timestep:     954 / 1000At timestep:     982 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 6001
Train_AverageReturn : -18.81999969482422
Train_BestReturn : -18.81999969482422
TimeSinceStart : 269.30840849876404
Exploration Critic Loss : 5021310.0
Exploitation Critic Loss : 0.16091077029705048
Exploration Model Loss : 22.46678924560547
Actor Loss : 0.6562660336494446
Eval_AverageReturn : -28.0
Eval_StdReturn : 9.68061351776123
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 28.942857142857143
Buffer size : 6001
Done logging...




********** Iteration 7000 ************

Training agent...

Beginning logging procedure...
Timestep 7001
mean reward (100 episodes) -17.420000
best mean reward -17.420000
running time 339.586150
At timestep:     27 / 1000At timestep:     57 / 1000At timestep:     87 / 1000At timestep:     116 / 1000At timestep:     166 / 1000At timestep:     202 / 1000At timestep:     237 / 1000At timestep:     287 / 1000At timestep:     304 / 1000At timestep:     333 / 1000At timestep:     365 / 1000At timestep:     393 / 1000At timestep:     410 / 1000At timestep:     439 / 1000At timestep:     464 / 1000At timestep:     494 / 1000At timestep:     513 / 1000At timestep:     530 / 1000At timestep:     573 / 1000At timestep:     595 / 1000At timestep:     630 / 1000At timestep:     647 / 1000At timestep:     672 / 1000At timestep:     693 / 1000At timestep:     724 / 1000At timestep:     756 / 1000At timestep:     791 / 1000At timestep:     820 / 1000At timestep:     843 / 1000At timestep:     867 / 1000At timestep:     905 / 1000At timestep:     931 / 1000At timestep:     981 / 1000At timestep:     1021 / 1000Train_EnvstepsSoFar : 7001
Train_AverageReturn : -17.420000076293945
Train_BestReturn : -17.420000076293945
TimeSinceStart : 339.586149930954
Exploration Critic Loss : 11779717.0
Exploitation Critic Loss : 0.11978016048669815
Exploration Model Loss : 28.951583862304688
Actor Loss : 0.6729681491851807
Eval_AverageReturn : -29.08823585510254
Eval_StdReturn : 9.147058486938477
Eval_MaxReturn : -16.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 30.029411764705884
Buffer size : 7001
Done logging...




********** Iteration 8000 ************

Training agent...

Beginning logging procedure...
Timestep 8001
mean reward (100 episodes) -17.299999
best mean reward -17.299999
running time 408.560630
At timestep:     14 / 1000At timestep:     39 / 1000At timestep:     86 / 1000At timestep:     119 / 1000At timestep:     139 / 1000At timestep:     164 / 1000At timestep:     198 / 1000At timestep:     232 / 1000At timestep:     268 / 1000At timestep:     282 / 1000At timestep:     311 / 1000At timestep:     326 / 1000At timestep:     368 / 1000At timestep:     392 / 1000At timestep:     404 / 1000At timestep:     441 / 1000At timestep:     483 / 1000At timestep:     518 / 1000At timestep:     543 / 1000At timestep:     563 / 1000At timestep:     591 / 1000At timestep:     616 / 1000At timestep:     632 / 1000At timestep:     657 / 1000At timestep:     685 / 1000At timestep:     712 / 1000At timestep:     726 / 1000At timestep:     750 / 1000At timestep:     800 / 1000At timestep:     823 / 1000At timestep:     844 / 1000At timestep:     872 / 1000At timestep:     896 / 1000At timestep:     919 / 1000At timestep:     956 / 1000At timestep:     974 / 1000At timestep:     994 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 8001
Train_AverageReturn : -17.299999237060547
Train_BestReturn : -17.299999237060547
TimeSinceStart : 408.5606300830841
Exploration Critic Loss : 19262652.0
Exploitation Critic Loss : 0.099107526242733
Exploration Model Loss : 26.59857177734375
Actor Loss : 0.6406668424606323
Eval_AverageReturn : -25.6842098236084
Eval_StdReturn : 9.248343467712402
Eval_MaxReturn : -11.0
Eval_MinReturn : -49.0
Eval_AverageEpLen : 26.68421052631579
Buffer size : 8001
Done logging...




********** Iteration 9000 ************

Training agent...

Beginning logging procedure...
Timestep 9001
mean reward (100 episodes) -17.049999
best mean reward -17.049999
running time 479.328959
At timestep:     40 / 1000At timestep:     88 / 1000At timestep:     108 / 1000At timestep:     125 / 1000At timestep:     141 / 1000At timestep:     170 / 1000At timestep:     214 / 1000At timestep:     235 / 1000At timestep:     263 / 1000At timestep:     292 / 1000At timestep:     337 / 1000At timestep:     375 / 1000At timestep:     420 / 1000At timestep:     470 / 1000At timestep:     498 / 1000At timestep:     528 / 1000At timestep:     553 / 1000At timestep:     576 / 1000At timestep:     600 / 1000At timestep:     643 / 1000At timestep:     658 / 1000At timestep:     700 / 1000At timestep:     737 / 1000At timestep:     766 / 1000At timestep:     808 / 1000At timestep:     842 / 1000At timestep:     871 / 1000At timestep:     897 / 1000At timestep:     918 / 1000At timestep:     949 / 1000At timestep:     969 / 1000At timestep:     984 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 9001
Train_AverageReturn : -17.049999237060547
Train_BestReturn : -17.049999237060547
TimeSinceStart : 479.3289587497711
Exploration Critic Loss : 29070304.0
Exploitation Critic Loss : 0.07574485242366791
Exploration Model Loss : 25.8873291015625
Actor Loss : 0.6476627588272095
Eval_AverageReturn : -29.484848022460938
Eval_StdReturn : 10.278312683105469
Eval_MaxReturn : -14.0
Eval_MinReturn : -50.0
Eval_AverageEpLen : 30.454545454545453
Buffer size : 9001
Done logging...




********** Iteration 10000 ************

Training agent...

Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -17.520000
best mean reward -17.049999
running time 546.021789
At timestep:     46 / 1000At timestep:     70 / 1000At timestep:     88 / 1000At timestep:     111 / 1000At timestep:     154 / 1000At timestep:     170 / 1000At timestep:     204 / 1000At timestep:     220 / 1000At timestep:     243 / 1000At timestep:     263 / 1000At timestep:     291 / 1000At timestep:     307 / 1000At timestep:     322 / 1000At timestep:     341 / 1000At timestep:     381 / 1000At timestep:     402 / 1000At timestep:     426 / 1000At timestep:     461 / 1000At timestep:     505 / 1000At timestep:     528 / 1000At timestep:     561 / 1000At timestep:     580 / 1000At timestep:     608 / 1000At timestep:     648 / 1000At timestep:     670 / 1000At timestep:     694 / 1000At timestep:     718 / 1000At timestep:     740 / 1000At timestep:     771 / 1000At timestep:     789 / 1000At timestep:     804 / 1000At timestep:     853 / 1000At timestep:     898 / 1000At timestep:     917 / 1000At timestep:     942 / 1000At timestep:     975 / 1000At timestep:     990 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 10001
Train_AverageReturn : -17.520000457763672
Train_BestReturn : -17.049999237060547
TimeSinceStart : 546.0217893123627
Exploration Critic Loss : 30099684.0
Exploitation Critic Loss : 0.08248019218444824
Exploration Model Loss : 25.64276123046875
Actor Loss : 0.6100692749023438
Eval_AverageReturn : -25.578947067260742
Eval_StdReturn : 9.826472282409668
Eval_MaxReturn : -14.0
Eval_MinReturn : -48.0
Eval_AverageEpLen : 26.57894736842105
Buffer size : 10001
Done logging...




********** Iteration 11000 ************

Training agent...

Beginning logging procedure...
Timestep 11001
mean reward (100 episodes) -17.309999
best mean reward -17.049999
running time 616.441490
At timestep:     28 / 1000At timestep:     43 / 1000At timestep:     58 / 1000At timestep:     75 / 1000At timestep:     90 / 1000At timestep:     104 / 1000At timestep:     120 / 1000At timestep:     140 / 1000At timestep:     155 / 1000At timestep:     177 / 1000At timestep:     190 / 1000At timestep:     218 / 1000At timestep:     244 / 1000At timestep:     283 / 1000At timestep:     303 / 1000At timestep:     320 / 1000At timestep:     347 / 1000At timestep:     364 / 1000At timestep:     384 / 1000At timestep:     403 / 1000At timestep:     421 / 1000At timestep:     440 / 1000At timestep:     465 / 1000At timestep:     488 / 1000At timestep:     513 / 1000At timestep:     543 / 1000At timestep:     567 / 1000At timestep:     602 / 1000At timestep:     625 / 1000At timestep:     643 / 1000At timestep:     674 / 1000At timestep:     688 / 1000At timestep:     710 / 1000At timestep:     730 / 1000At timestep:     748 / 1000At timestep:     770 / 1000At timestep:     785 / 1000At timestep:     813 / 1000At timestep:     858 / 1000At timestep:     889 / 1000At timestep:     913 / 1000At timestep:     930 / 1000At timestep:     953 / 1000At timestep:     987 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 11001
Train_AverageReturn : -17.309999465942383
Train_BestReturn : -17.049999237060547
TimeSinceStart : 616.4414896965027
Exploration Critic Loss : 28603136.0
Exploitation Critic Loss : 0.1237800344824791
Exploration Model Loss : 25.55401611328125
Actor Loss : 0.5593181252479553
Eval_AverageReturn : -21.399999618530273
Eval_StdReturn : 6.993330478668213
Eval_MaxReturn : -12.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 22.4
Buffer size : 11001
Done logging...




********** Iteration 12000 ************

Training agent...

Beginning logging procedure...
Timestep 12001
mean reward (100 episodes) -17.180000
best mean reward -17.049999
running time 688.131766
At timestep:     16 / 1000At timestep:     45 / 1000At timestep:     62 / 1000At timestep:     78 / 1000At timestep:     95 / 1000At timestep:     134 / 1000At timestep:     162 / 1000At timestep:     189 / 1000At timestep:     209 / 1000At timestep:     227 / 1000At timestep:     262 / 1000At timestep:     285 / 1000At timestep:     302 / 1000At timestep:     317 / 1000At timestep:     331 / 1000At timestep:     347 / 1000At timestep:     363 / 1000At timestep:     389 / 1000At timestep:     413 / 1000At timestep:     447 / 1000At timestep:     492 / 1000At timestep:     517 / 1000At timestep:     538 / 1000At timestep:     554 / 1000At timestep:     572 / 1000At timestep:     587 / 1000At timestep:     607 / 1000At timestep:     636 / 1000At timestep:     652 / 1000At timestep:     672 / 1000At timestep:     686 / 1000At timestep:     712 / 1000At timestep:     747 / 1000At timestep:     773 / 1000At timestep:     798 / 1000At timestep:     818 / 1000At timestep:     842 / 1000At timestep:     859 / 1000At timestep:     874 / 1000At timestep:     888 / 1000At timestep:     916 / 1000At timestep:     935 / 1000At timestep:     952 / 1000At timestep:     990 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 12001
Train_AverageReturn : -17.18000030517578
Train_BestReturn : -17.049999237060547
TimeSinceStart : 688.1317656040192
Exploration Critic Loss : 37621156.0
Exploitation Critic Loss : 0.13217408955097198
Exploration Model Loss : 25.522430419921875
Actor Loss : 0.6277021169662476
Eval_AverageReturn : -21.422222137451172
Eval_StdReturn : 7.549801826477051
Eval_MaxReturn : -13.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 22.42222222222222
Buffer size : 12001
Done logging...




********** Iteration 13000 ************

Training agent...

Beginning logging procedure...
Timestep 13001
mean reward (100 episodes) -17.549999
best mean reward -17.049999
running time 760.336047
At timestep:     19 / 1000At timestep:     45 / 1000At timestep:     76 / 1000At timestep:     94 / 1000At timestep:     116 / 1000At timestep:     134 / 1000At timestep:     162 / 1000At timestep:     185 / 1000At timestep:     203 / 1000At timestep:     227 / 1000At timestep:     255 / 1000At timestep:     283 / 1000At timestep:     311 / 1000At timestep:     348 / 1000At timestep:     370 / 1000At timestep:     388 / 1000At timestep:     401 / 1000At timestep:     419 / 1000At timestep:     436 / 1000At timestep:     459 / 1000At timestep:     475 / 1000At timestep:     506 / 1000At timestep:     532 / 1000At timestep:     563 / 1000At timestep:     581 / 1000At timestep:     605 / 1000At timestep:     627 / 1000At timestep:     646 / 1000At timestep:     673 / 1000At timestep:     704 / 1000At timestep:     726 / 1000At timestep:     753 / 1000At timestep:     787 / 1000At timestep:     821 / 1000At timestep:     837 / 1000At timestep:     865 / 1000At timestep:     896 / 1000At timestep:     917 / 1000At timestep:     947 / 1000At timestep:     964 / 1000At timestep:     987 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 13001
Train_AverageReturn : -17.549999237060547
Train_BestReturn : -17.049999237060547
TimeSinceStart : 760.3360466957092
Exploration Critic Loss : 35722840.0
Exploitation Critic Loss : 0.08286287635564804
Exploration Model Loss : 25.511001586914062
Actor Loss : 0.5260300636291504
Eval_AverageReturn : -23.0
Eval_StdReturn : 5.785861015319824
Eval_MaxReturn : -12.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 24.0
Buffer size : 13001
Done logging...




********** Iteration 14000 ************

Training agent...

Beginning logging procedure...
Timestep 14001
mean reward (100 episodes) -17.420000
best mean reward -17.049999
running time 829.979080
At timestep:     15 / 1000At timestep:     34 / 1000At timestep:     56 / 1000At timestep:     72 / 1000At timestep:     85 / 1000At timestep:     100 / 1000At timestep:     117 / 1000At timestep:     139 / 1000At timestep:     173 / 1000At timestep:     189 / 1000At timestep:     217 / 1000At timestep:     239 / 1000At timestep:     257 / 1000At timestep:     280 / 1000At timestep:     297 / 1000At timestep:     315 / 1000At timestep:     328 / 1000At timestep:     340 / 1000At timestep:     356 / 1000At timestep:     372 / 1000At timestep:     394 / 1000At timestep:     415 / 1000At timestep:     439 / 1000At timestep:     455 / 1000At timestep:     477 / 1000At timestep:     492 / 1000At timestep:     506 / 1000At timestep:     519 / 1000At timestep:     543 / 1000At timestep:     557 / 1000At timestep:     577 / 1000At timestep:     597 / 1000At timestep:     611 / 1000At timestep:     626 / 1000At timestep:     656 / 1000At timestep:     680 / 1000At timestep:     694 / 1000At timestep:     710 / 1000At timestep:     737 / 1000At timestep:     758 / 1000At timestep:     769 / 1000At timestep:     801 / 1000At timestep:     824 / 1000At timestep:     838 / 1000At timestep:     860 / 1000At timestep:     880 / 1000At timestep:     898 / 1000At timestep:     909 / 1000At timestep:     924 / 1000At timestep:     972 / 1000At timestep:     991 / 1000At timestep:     1015 / 1000Train_EnvstepsSoFar : 14001
Train_AverageReturn : -17.420000076293945
Train_BestReturn : -17.049999237060547
TimeSinceStart : 829.9790797233582
Exploration Critic Loss : 24717344.0
Exploitation Critic Loss : 0.1511278599500656
Exploration Model Loss : 25.507034301757812
Actor Loss : 0.5890128016471863
Eval_AverageReturn : -18.519229888916016
Eval_StdReturn : 6.561807632446289
Eval_MaxReturn : -10.0
Eval_MinReturn : -47.0
Eval_AverageEpLen : 19.51923076923077
Buffer size : 14001
Done logging...




********** Iteration 15000 ************

Training agent...

Beginning logging procedure...
Timestep 15001
mean reward (100 episodes) -17.660000
best mean reward -17.049999
running time 902.840932
At timestep:     18 / 1000At timestep:     47 / 1000At timestep:     59 / 1000At timestep:     89 / 1000At timestep:     121 / 1000At timestep:     137 / 1000At timestep:     153 / 1000At timestep:     187 / 1000At timestep:     206 / 1000At timestep:     223 / 1000At timestep:     241 / 1000At timestep:     265 / 1000At timestep:     283 / 1000At timestep:     299 / 1000At timestep:     317 / 1000At timestep:     341 / 1000At timestep:     357 / 1000At timestep:     388 / 1000At timestep:     423 / 1000At timestep:     443 / 1000At timestep:     458 / 1000At timestep:     483 / 1000At timestep:     533 / 1000At timestep:     564 / 1000At timestep:     579 / 1000At timestep:     601 / 1000At timestep:     626 / 1000At timestep:     646 / 1000At timestep:     675 / 1000At timestep:     708 / 1000At timestep:     729 / 1000At timestep:     765 / 1000At timestep:     782 / 1000At timestep:     803 / 1000At timestep:     823 / 1000At timestep:     849 / 1000At timestep:     868 / 1000At timestep:     881 / 1000At timestep:     906 / 1000At timestep:     920 / 1000At timestep:     936 / 1000At timestep:     959 / 1000At timestep:     985 / 1000At timestep:     1017 / 1000Train_EnvstepsSoFar : 15001
Train_AverageReturn : -17.65999984741211
Train_BestReturn : -17.049999237060547
TimeSinceStart : 902.840931892395
Exploration Critic Loss : 14697726.0
Exploitation Critic Loss : 0.1471017599105835
Exploration Model Loss : 25.505111694335938
Actor Loss : 0.560887336730957
Eval_AverageReturn : -22.113636016845703
Eval_StdReturn : 7.717206001281738
Eval_MaxReturn : -11.0
Eval_MinReturn : -49.0
Eval_AverageEpLen : 23.113636363636363
Buffer size : 15001
Done logging...




********** Iteration 16000 ************

Training agent...

Beginning logging procedure...
Timestep 16001
mean reward (100 episodes) -17.520000
best mean reward -17.049999
running time 975.329732
At timestep:     24 / 1000At timestep:     46 / 1000At timestep:     62 / 1000At timestep:     78 / 1000At timestep:     98 / 1000At timestep:     116 / 1000At timestep:     141 / 1000At timestep:     161 / 1000At timestep:     191 / 1000At timestep:     207 / 1000At timestep:     225 / 1000At timestep:     246 / 1000At timestep:     261 / 1000At timestep:     286 / 1000At timestep:     299 / 1000At timestep:     313 / 1000At timestep:     335 / 1000At timestep:     355 / 1000At timestep:     385 / 1000At timestep:     410 / 1000At timestep:     437 / 1000At timestep:     454 / 1000At timestep:     471 / 1000At timestep:     512 / 1000At timestep:     532 / 1000At timestep:     550 / 1000At timestep:     567 / 1000At timestep:     587 / 1000At timestep:     604 / 1000At timestep:     627 / 1000At timestep:     653 / 1000At timestep:     676 / 1000At timestep:     692 / 1000At timestep:     714 / 1000At timestep:     740 / 1000At timestep:     763 / 1000At timestep:     801 / 1000At timestep:     825 / 1000At timestep:     844 / 1000At timestep:     868 / 1000At timestep:     893 / 1000At timestep:     918 / 1000At timestep:     936 / 1000At timestep:     961 / 1000At timestep:     979 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 16001
Train_AverageReturn : -17.520000457763672
Train_BestReturn : -17.049999237060547
TimeSinceStart : 975.3297317028046
Exploration Critic Loss : 15115264.0
Exploitation Critic Loss : 0.10486388206481934
Exploration Model Loss : 25.504592895507812
Actor Loss : 0.6056328415870667
Eval_AverageReturn : -21.086956024169922
Eval_StdReturn : 5.977589130401611
Eval_MaxReturn : -12.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 22.08695652173913
Buffer size : 16001
Done logging...




********** Iteration 17000 ************

Training agent...

Beginning logging procedure...
Timestep 17001
mean reward (100 episodes) -17.850000
best mean reward -17.049999
running time 1046.295552
At timestep:     22 / 1000At timestep:     38 / 1000At timestep:     62 / 1000At timestep:     93 / 1000At timestep:     108 / 1000At timestep:     132 / 1000At timestep:     162 / 1000At timestep:     189 / 1000At timestep:     207 / 1000At timestep:     233 / 1000At timestep:     252 / 1000At timestep:     275 / 1000At timestep:     293 / 1000At timestep:     309 / 1000At timestep:     334 / 1000At timestep:     347 / 1000At timestep:     371 / 1000At timestep:     386 / 1000At timestep:     400 / 1000At timestep:     426 / 1000At timestep:     446 / 1000At timestep:     465 / 1000At timestep:     483 / 1000At timestep:     510 / 1000At timestep:     531 / 1000At timestep:     552 / 1000At timestep:     572 / 1000At timestep:     606 / 1000At timestep:     624 / 1000At timestep:     647 / 1000At timestep:     673 / 1000At timestep:     689 / 1000At timestep:     704 / 1000At timestep:     731 / 1000At timestep:     747 / 1000At timestep:     770 / 1000At timestep:     791 / 1000At timestep:     815 / 1000At timestep:     826 / 1000At timestep:     851 / 1000At timestep:     870 / 1000At timestep:     897 / 1000At timestep:     929 / 1000At timestep:     947 / 1000At timestep:     977 / 1000At timestep:     997 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 17001
Train_AverageReturn : -17.850000381469727
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1046.295551776886
Exploration Critic Loss : 18415150.0
Exploitation Critic Loss : 0.06681762635707855
Exploration Model Loss : 25.504592895507812
Actor Loss : 0.503553032875061
Eval_AverageReturn : -20.617021560668945
Eval_StdReturn : 5.297743797302246
Eval_MaxReturn : -10.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 21.617021276595743
Buffer size : 17001
Done logging...




********** Iteration 18000 ************

Training agent...

Beginning logging procedure...
Timestep 18001
mean reward (100 episodes) -18.100000
best mean reward -17.049999
running time 1118.426012
At timestep:     18 / 1000At timestep:     49 / 1000At timestep:     66 / 1000At timestep:     86 / 1000At timestep:     109 / 1000At timestep:     139 / 1000At timestep:     160 / 1000At timestep:     175 / 1000At timestep:     191 / 1000At timestep:     212 / 1000At timestep:     236 / 1000At timestep:     262 / 1000At timestep:     284 / 1000At timestep:     312 / 1000At timestep:     339 / 1000At timestep:     367 / 1000At timestep:     385 / 1000At timestep:     399 / 1000At timestep:     422 / 1000At timestep:     448 / 1000At timestep:     475 / 1000At timestep:     494 / 1000At timestep:     508 / 1000At timestep:     532 / 1000At timestep:     551 / 1000At timestep:     570 / 1000At timestep:     596 / 1000At timestep:     611 / 1000At timestep:     623 / 1000At timestep:     648 / 1000At timestep:     663 / 1000At timestep:     687 / 1000At timestep:     698 / 1000At timestep:     728 / 1000At timestep:     753 / 1000At timestep:     779 / 1000At timestep:     805 / 1000At timestep:     830 / 1000At timestep:     858 / 1000At timestep:     878 / 1000At timestep:     898 / 1000At timestep:     915 / 1000At timestep:     938 / 1000At timestep:     953 / 1000At timestep:     969 / 1000At timestep:     986 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 18001
Train_AverageReturn : -18.100000381469727
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1118.426011800766
Exploration Critic Loss : 41467248.0
Exploitation Critic Loss : 0.07069660723209381
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.4917941689491272
Eval_AverageReturn : -20.446807861328125
Eval_StdReturn : 5.1356425285339355
Eval_MaxReturn : -10.0
Eval_MinReturn : -30.0
Eval_AverageEpLen : 21.4468085106383
Buffer size : 18001
Done logging...




********** Iteration 19000 ************

Training agent...

Beginning logging procedure...
Timestep 19001
mean reward (100 episodes) -17.650000
best mean reward -17.049999
running time 1189.889767
At timestep:     23 / 1000At timestep:     41 / 1000At timestep:     66 / 1000At timestep:     82 / 1000At timestep:     114 / 1000At timestep:     132 / 1000At timestep:     154 / 1000At timestep:     173 / 1000At timestep:     194 / 1000At timestep:     224 / 1000At timestep:     238 / 1000At timestep:     255 / 1000At timestep:     272 / 1000At timestep:     300 / 1000At timestep:     320 / 1000At timestep:     340 / 1000At timestep:     373 / 1000At timestep:     387 / 1000At timestep:     408 / 1000At timestep:     422 / 1000At timestep:     445 / 1000At timestep:     464 / 1000At timestep:     478 / 1000At timestep:     493 / 1000At timestep:     515 / 1000At timestep:     552 / 1000At timestep:     571 / 1000At timestep:     593 / 1000At timestep:     614 / 1000At timestep:     641 / 1000At timestep:     667 / 1000At timestep:     695 / 1000At timestep:     711 / 1000At timestep:     727 / 1000At timestep:     739 / 1000At timestep:     772 / 1000At timestep:     791 / 1000At timestep:     808 / 1000At timestep:     828 / 1000At timestep:     845 / 1000At timestep:     866 / 1000At timestep:     883 / 1000At timestep:     902 / 1000At timestep:     917 / 1000At timestep:     939 / 1000At timestep:     960 / 1000At timestep:     980 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 19001
Train_AverageReturn : -17.649999618530273
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1189.8897666931152
Exploration Critic Loss : 35620988.0
Exploitation Critic Loss : 0.07761041820049286
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.5318899750709534
Eval_AverageReturn : -20.0
Eval_StdReturn : 5.642103672027588
Eval_MaxReturn : -11.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 21.0
Buffer size : 19001
Done logging...




********** Iteration 20000 ************

Training agent...

Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -17.510000
best mean reward -17.049999
running time 1262.266481
At timestep:     22 / 1000At timestep:     39 / 1000At timestep:     71 / 1000At timestep:     88 / 1000At timestep:     107 / 1000At timestep:     123 / 1000At timestep:     134 / 1000At timestep:     154 / 1000At timestep:     184 / 1000At timestep:     199 / 1000At timestep:     215 / 1000At timestep:     228 / 1000At timestep:     251 / 1000At timestep:     273 / 1000At timestep:     300 / 1000At timestep:     313 / 1000At timestep:     328 / 1000At timestep:     347 / 1000At timestep:     362 / 1000At timestep:     378 / 1000At timestep:     407 / 1000At timestep:     427 / 1000At timestep:     438 / 1000At timestep:     462 / 1000At timestep:     487 / 1000At timestep:     508 / 1000At timestep:     524 / 1000At timestep:     541 / 1000At timestep:     558 / 1000At timestep:     585 / 1000At timestep:     601 / 1000At timestep:     636 / 1000At timestep:     657 / 1000At timestep:     680 / 1000At timestep:     700 / 1000At timestep:     711 / 1000At timestep:     738 / 1000At timestep:     755 / 1000At timestep:     781 / 1000At timestep:     799 / 1000At timestep:     816 / 1000At timestep:     837 / 1000At timestep:     851 / 1000At timestep:     864 / 1000At timestep:     896 / 1000At timestep:     913 / 1000At timestep:     928 / 1000At timestep:     940 / 1000At timestep:     959 / 1000At timestep:     993 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 20001
Train_AverageReturn : -17.510000228881836
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1262.266480922699
Exploration Critic Loss : 33881492.0
Exploitation Critic Loss : 0.06178513169288635
Exploration Model Loss : 25.504592895507812
Actor Loss : 0.5541239976882935
Eval_AverageReturn : -18.80392074584961
Eval_StdReturn : 6.087658882141113
Eval_MaxReturn : -10.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.80392156862745
Buffer size : 20001
Done logging...




********** Iteration 21000 ************

Training agent...

Beginning logging procedure...
Timestep 21001
mean reward (100 episodes) -17.559999
best mean reward -17.049999
running time 1335.103996
At timestep:     27 / 1000At timestep:     56 / 1000At timestep:     79 / 1000At timestep:     107 / 1000At timestep:     117 / 1000At timestep:     148 / 1000At timestep:     169 / 1000At timestep:     193 / 1000At timestep:     214 / 1000At timestep:     231 / 1000At timestep:     250 / 1000At timestep:     267 / 1000At timestep:     294 / 1000At timestep:     327 / 1000At timestep:     350 / 1000At timestep:     367 / 1000At timestep:     386 / 1000At timestep:     405 / 1000At timestep:     429 / 1000At timestep:     457 / 1000At timestep:     472 / 1000At timestep:     496 / 1000At timestep:     509 / 1000At timestep:     523 / 1000At timestep:     545 / 1000At timestep:     559 / 1000At timestep:     602 / 1000At timestep:     624 / 1000At timestep:     665 / 1000At timestep:     682 / 1000At timestep:     703 / 1000At timestep:     719 / 1000At timestep:     739 / 1000At timestep:     758 / 1000At timestep:     779 / 1000At timestep:     794 / 1000At timestep:     808 / 1000At timestep:     828 / 1000At timestep:     849 / 1000At timestep:     864 / 1000At timestep:     901 / 1000At timestep:     926 / 1000At timestep:     939 / 1000At timestep:     957 / 1000At timestep:     973 / 1000At timestep:     987 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 21001
Train_AverageReturn : -17.559999465942383
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1335.1039962768555
Exploration Critic Loss : 33399220.0
Exploitation Critic Loss : 0.07819244265556335
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.4727684259414673
Eval_AverageReturn : -20.404254913330078
Eval_StdReturn : 7.088077068328857
Eval_MaxReturn : -9.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 21.404255319148938
Buffer size : 21001
Done logging...




********** Iteration 22000 ************

Training agent...

Beginning logging procedure...
Timestep 22001
mean reward (100 episodes) -17.549999
best mean reward -17.049999
running time 1405.425124
At timestep:     16 / 1000At timestep:     36 / 1000At timestep:     48 / 1000At timestep:     66 / 1000At timestep:     83 / 1000At timestep:     103 / 1000At timestep:     123 / 1000At timestep:     144 / 1000At timestep:     169 / 1000At timestep:     196 / 1000At timestep:     211 / 1000At timestep:     229 / 1000At timestep:     248 / 1000At timestep:     275 / 1000At timestep:     299 / 1000At timestep:     327 / 1000At timestep:     342 / 1000At timestep:     357 / 1000At timestep:     382 / 1000At timestep:     400 / 1000At timestep:     422 / 1000At timestep:     439 / 1000At timestep:     461 / 1000At timestep:     483 / 1000At timestep:     495 / 1000At timestep:     516 / 1000At timestep:     539 / 1000At timestep:     560 / 1000At timestep:     583 / 1000At timestep:     600 / 1000At timestep:     615 / 1000At timestep:     636 / 1000At timestep:     658 / 1000At timestep:     687 / 1000At timestep:     707 / 1000At timestep:     730 / 1000At timestep:     773 / 1000At timestep:     790 / 1000At timestep:     811 / 1000At timestep:     824 / 1000At timestep:     846 / 1000At timestep:     871 / 1000At timestep:     886 / 1000At timestep:     908 / 1000At timestep:     924 / 1000At timestep:     941 / 1000At timestep:     959 / 1000At timestep:     983 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 22001
Train_AverageReturn : -17.549999237060547
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1405.4251244068146
Exploration Critic Loss : 29899944.0
Exploitation Critic Loss : 0.07756301760673523
Exploration Model Loss : 25.504592895507812
Actor Loss : 0.48327359557151794
Eval_AverageReturn : -19.53061294555664
Eval_StdReturn : 5.2024407386779785
Eval_MaxReturn : -11.0
Eval_MinReturn : -42.0
Eval_AverageEpLen : 20.53061224489796
Buffer size : 22001
Done logging...




********** Iteration 23000 ************

Training agent...

Beginning logging procedure...
Timestep 23001
mean reward (100 episodes) -17.580000
best mean reward -17.049999
running time 1477.904721
At timestep:     15 / 1000At timestep:     29 / 1000At timestep:     44 / 1000At timestep:     83 / 1000At timestep:     100 / 1000At timestep:     116 / 1000At timestep:     139 / 1000At timestep:     155 / 1000At timestep:     180 / 1000At timestep:     199 / 1000At timestep:     222 / 1000At timestep:     243 / 1000At timestep:     270 / 1000At timestep:     286 / 1000At timestep:     310 / 1000At timestep:     336 / 1000At timestep:     357 / 1000At timestep:     371 / 1000At timestep:     384 / 1000At timestep:     403 / 1000At timestep:     424 / 1000At timestep:     451 / 1000At timestep:     466 / 1000At timestep:     480 / 1000At timestep:     501 / 1000At timestep:     529 / 1000At timestep:     552 / 1000At timestep:     597 / 1000At timestep:     625 / 1000At timestep:     640 / 1000At timestep:     658 / 1000At timestep:     674 / 1000At timestep:     695 / 1000At timestep:     718 / 1000At timestep:     741 / 1000At timestep:     761 / 1000At timestep:     782 / 1000At timestep:     807 / 1000At timestep:     826 / 1000At timestep:     843 / 1000At timestep:     866 / 1000At timestep:     886 / 1000At timestep:     899 / 1000At timestep:     918 / 1000At timestep:     942 / 1000At timestep:     970 / 1000At timestep:     990 / 1000At timestep:     1008 / 1000Train_EnvstepsSoFar : 23001
Train_AverageReturn : -17.579999923706055
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1477.9047214984894
Exploration Critic Loss : 29383210.0
Exploitation Critic Loss : 0.16853448748588562
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.4956923723220825
Eval_AverageReturn : -20.0
Eval_StdReturn : 6.133922100067139
Eval_MaxReturn : -12.0
Eval_MinReturn : -44.0
Eval_AverageEpLen : 21.0
Buffer size : 23001
Done logging...




********** Iteration 24000 ************

Training agent...

Beginning logging procedure...
Timestep 24001
mean reward (100 episodes) -17.760000
best mean reward -17.049999
running time 1549.046685
At timestep:     34 / 1000At timestep:     57 / 1000At timestep:     72 / 1000At timestep:     88 / 1000At timestep:     125 / 1000At timestep:     142 / 1000At timestep:     162 / 1000At timestep:     185 / 1000At timestep:     198 / 1000At timestep:     212 / 1000At timestep:     228 / 1000At timestep:     256 / 1000At timestep:     276 / 1000At timestep:     289 / 1000At timestep:     308 / 1000At timestep:     333 / 1000At timestep:     358 / 1000At timestep:     373 / 1000At timestep:     395 / 1000At timestep:     421 / 1000At timestep:     437 / 1000At timestep:     457 / 1000At timestep:     473 / 1000At timestep:     500 / 1000At timestep:     517 / 1000At timestep:     539 / 1000At timestep:     558 / 1000At timestep:     577 / 1000At timestep:     594 / 1000At timestep:     605 / 1000At timestep:     626 / 1000At timestep:     650 / 1000At timestep:     671 / 1000At timestep:     691 / 1000At timestep:     707 / 1000At timestep:     731 / 1000At timestep:     748 / 1000At timestep:     766 / 1000At timestep:     782 / 1000At timestep:     800 / 1000At timestep:     818 / 1000At timestep:     832 / 1000At timestep:     852 / 1000At timestep:     868 / 1000At timestep:     893 / 1000At timestep:     907 / 1000At timestep:     927 / 1000At timestep:     944 / 1000At timestep:     960 / 1000At timestep:     975 / 1000At timestep:     996 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 24001
Train_AverageReturn : -17.760000228881836
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1549.0466845035553
Exploration Critic Loss : 31847120.0
Exploitation Critic Loss : 0.0770329087972641
Exploration Model Loss : 25.504592895507812
Actor Loss : 0.5343066453933716
Eval_AverageReturn : -18.44230842590332
Eval_StdReturn : 5.0551323890686035
Eval_MaxReturn : -10.0
Eval_MinReturn : -36.0
Eval_AverageEpLen : 19.442307692307693
Buffer size : 24001
Done logging...




********** Iteration 25000 ************

Training agent...

Beginning logging procedure...
Timestep 25001
mean reward (100 episodes) -17.809999
best mean reward -17.049999
running time 1621.456452
At timestep:     15 / 1000At timestep:     38 / 1000At timestep:     56 / 1000At timestep:     84 / 1000At timestep:     102 / 1000At timestep:     123 / 1000At timestep:     140 / 1000At timestep:     157 / 1000At timestep:     185 / 1000At timestep:     207 / 1000At timestep:     220 / 1000At timestep:     240 / 1000At timestep:     258 / 1000At timestep:     277 / 1000At timestep:     292 / 1000At timestep:     315 / 1000At timestep:     335 / 1000At timestep:     351 / 1000At timestep:     371 / 1000At timestep:     394 / 1000At timestep:     418 / 1000At timestep:     430 / 1000At timestep:     454 / 1000At timestep:     467 / 1000At timestep:     492 / 1000At timestep:     511 / 1000At timestep:     537 / 1000At timestep:     556 / 1000At timestep:     576 / 1000At timestep:     599 / 1000At timestep:     628 / 1000At timestep:     650 / 1000At timestep:     680 / 1000At timestep:     693 / 1000At timestep:     715 / 1000At timestep:     732 / 1000At timestep:     756 / 1000At timestep:     779 / 1000At timestep:     799 / 1000At timestep:     822 / 1000At timestep:     846 / 1000At timestep:     862 / 1000At timestep:     881 / 1000At timestep:     894 / 1000At timestep:     910 / 1000At timestep:     927 / 1000At timestep:     953 / 1000At timestep:     978 / 1000At timestep:     993 / 1000At timestep:     1013 / 1000Train_EnvstepsSoFar : 25001
Train_AverageReturn : -17.809999465942383
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1621.4564518928528
Exploration Critic Loss : 31849792.0
Exploitation Critic Loss : 0.08934879302978516
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.45166516304016113
Eval_AverageReturn : -19.260000228881836
Eval_StdReturn : 4.4578471183776855
Eval_MaxReturn : -11.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 20.26
Buffer size : 25001
Done logging...




********** Iteration 26000 ************

Training agent...

Beginning logging procedure...
Timestep 26001
mean reward (100 episodes) -17.730000
best mean reward -17.049999
running time 1695.049244
At timestep:     16 / 1000At timestep:     34 / 1000At timestep:     49 / 1000At timestep:     64 / 1000At timestep:     87 / 1000At timestep:     105 / 1000At timestep:     123 / 1000At timestep:     152 / 1000At timestep:     175 / 1000At timestep:     197 / 1000At timestep:     219 / 1000At timestep:     240 / 1000At timestep:     257 / 1000At timestep:     282 / 1000At timestep:     298 / 1000At timestep:     314 / 1000At timestep:     328 / 1000At timestep:     351 / 1000At timestep:     370 / 1000At timestep:     385 / 1000At timestep:     404 / 1000At timestep:     424 / 1000At timestep:     437 / 1000At timestep:     456 / 1000At timestep:     472 / 1000At timestep:     496 / 1000At timestep:     516 / 1000At timestep:     542 / 1000At timestep:     570 / 1000At timestep:     586 / 1000At timestep:     606 / 1000At timestep:     626 / 1000At timestep:     654 / 1000At timestep:     669 / 1000At timestep:     684 / 1000At timestep:     698 / 1000At timestep:     711 / 1000At timestep:     725 / 1000At timestep:     739 / 1000At timestep:     758 / 1000At timestep:     776 / 1000At timestep:     798 / 1000At timestep:     818 / 1000At timestep:     831 / 1000At timestep:     846 / 1000At timestep:     864 / 1000At timestep:     892 / 1000At timestep:     914 / 1000At timestep:     930 / 1000At timestep:     948 / 1000At timestep:     972 / 1000At timestep:     998 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 26001
Train_AverageReturn : -17.729999542236328
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1695.049243927002
Exploration Critic Loss : 34441168.0
Exploitation Critic Loss : 0.07489442825317383
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.5816778540611267
Eval_AverageReturn : -18.20754623413086
Eval_StdReturn : 4.314777374267578
Eval_MaxReturn : -12.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 19.20754716981132
Buffer size : 26001
Done logging...




********** Iteration 27000 ************

Training agent...

Beginning logging procedure...
Timestep 27001
mean reward (100 episodes) -17.340000
best mean reward -17.049999
running time 1767.666315
At timestep:     22 / 1000At timestep:     38 / 1000At timestep:     56 / 1000At timestep:     92 / 1000At timestep:     112 / 1000At timestep:     125 / 1000At timestep:     140 / 1000At timestep:     166 / 1000At timestep:     185 / 1000At timestep:     203 / 1000At timestep:     219 / 1000At timestep:     235 / 1000At timestep:     251 / 1000At timestep:     265 / 1000At timestep:     292 / 1000At timestep:     307 / 1000At timestep:     329 / 1000At timestep:     353 / 1000At timestep:     372 / 1000At timestep:     398 / 1000At timestep:     426 / 1000At timestep:     441 / 1000At timestep:     457 / 1000At timestep:     479 / 1000At timestep:     500 / 1000At timestep:     524 / 1000At timestep:     545 / 1000At timestep:     561 / 1000At timestep:     578 / 1000At timestep:     591 / 1000At timestep:     608 / 1000At timestep:     627 / 1000At timestep:     650 / 1000At timestep:     674 / 1000At timestep:     694 / 1000At timestep:     708 / 1000At timestep:     737 / 1000At timestep:     763 / 1000At timestep:     789 / 1000At timestep:     809 / 1000At timestep:     826 / 1000At timestep:     841 / 1000At timestep:     863 / 1000At timestep:     885 / 1000At timestep:     898 / 1000At timestep:     919 / 1000At timestep:     946 / 1000At timestep:     962 / 1000At timestep:     977 / 1000At timestep:     992 / 1000At timestep:     1004 / 1000Train_EnvstepsSoFar : 27001
Train_AverageReturn : -17.34000015258789
Train_BestReturn : -17.049999237060547
TimeSinceStart : 1767.6663150787354
Exploration Critic Loss : 31162116.0
Exploitation Critic Loss : 0.08124799281358719
Exploration Model Loss : 25.504592895507812
Actor Loss : 0.4395827651023865
Eval_AverageReturn : -18.6862735748291
Eval_StdReturn : 5.050684928894043
Eval_MaxReturn : -11.0
Eval_MinReturn : -35.0
Eval_AverageEpLen : 19.686274509803923
Buffer size : 27001
Done logging...




********** Iteration 28000 ************

Training agent...

Beginning logging procedure...
Timestep 28001
mean reward (100 episodes) -17.020000
best mean reward -17.020000
running time 1841.282389
At timestep:     13 / 1000At timestep:     54 / 1000At timestep:     70 / 1000At timestep:     85 / 1000At timestep:     101 / 1000At timestep:     118 / 1000At timestep:     140 / 1000At timestep:     159 / 1000At timestep:     175 / 1000At timestep:     194 / 1000At timestep:     211 / 1000At timestep:     228 / 1000At timestep:     252 / 1000At timestep:     271 / 1000At timestep:     292 / 1000At timestep:     306 / 1000At timestep:     319 / 1000At timestep:     343 / 1000At timestep:     367 / 1000At timestep:     383 / 1000At timestep:     403 / 1000At timestep:     419 / 1000At timestep:     442 / 1000At timestep:     464 / 1000At timestep:     481 / 1000At timestep:     500 / 1000At timestep:     521 / 1000At timestep:     559 / 1000At timestep:     578 / 1000At timestep:     599 / 1000At timestep:     617 / 1000At timestep:     641 / 1000At timestep:     655 / 1000At timestep:     672 / 1000At timestep:     687 / 1000At timestep:     711 / 1000At timestep:     729 / 1000At timestep:     744 / 1000At timestep:     767 / 1000At timestep:     783 / 1000At timestep:     800 / 1000At timestep:     824 / 1000At timestep:     844 / 1000At timestep:     858 / 1000At timestep:     885 / 1000At timestep:     906 / 1000At timestep:     925 / 1000At timestep:     942 / 1000At timestep:     958 / 1000At timestep:     979 / 1000At timestep:     997 / 1000At timestep:     1014 / 1000Train_EnvstepsSoFar : 28001
Train_AverageReturn : -17.020000457763672
Train_BestReturn : -17.020000457763672
TimeSinceStart : 1841.2823886871338
Exploration Critic Loss : 32789468.0
Exploitation Critic Loss : 0.09341538697481155
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.4453077018260956
Eval_AverageReturn : -18.5
Eval_StdReturn : 5.209089756011963
Eval_MaxReturn : -12.0
Eval_MinReturn : -40.0
Eval_AverageEpLen : 19.5
Buffer size : 28001
Done logging...




********** Iteration 29000 ************

Training agent...

Beginning logging procedure...
Timestep 29001
mean reward (100 episodes) -17.260000
best mean reward -17.020000
running time 1914.201585
At timestep:     15 / 1000At timestep:     31 / 1000At timestep:     57 / 1000At timestep:     82 / 1000At timestep:     104 / 1000At timestep:     117 / 1000At timestep:     132 / 1000At timestep:     149 / 1000At timestep:     168 / 1000At timestep:     184 / 1000At timestep:     197 / 1000At timestep:     221 / 1000At timestep:     244 / 1000At timestep:     267 / 1000At timestep:     283 / 1000At timestep:     297 / 1000At timestep:     322 / 1000At timestep:     345 / 1000At timestep:     368 / 1000At timestep:     387 / 1000At timestep:     406 / 1000At timestep:     423 / 1000At timestep:     441 / 1000At timestep:     458 / 1000At timestep:     471 / 1000At timestep:     491 / 1000At timestep:     526 / 1000At timestep:     545 / 1000At timestep:     565 / 1000At timestep:     588 / 1000At timestep:     604 / 1000At timestep:     626 / 1000At timestep:     642 / 1000At timestep:     661 / 1000At timestep:     679 / 1000At timestep:     694 / 1000At timestep:     708 / 1000At timestep:     734 / 1000At timestep:     768 / 1000At timestep:     786 / 1000At timestep:     801 / 1000At timestep:     820 / 1000At timestep:     839 / 1000At timestep:     869 / 1000At timestep:     888 / 1000At timestep:     906 / 1000At timestep:     918 / 1000At timestep:     932 / 1000At timestep:     947 / 1000At timestep:     965 / 1000At timestep:     991 / 1000At timestep:     1009 / 1000Train_EnvstepsSoFar : 29001
Train_AverageReturn : -17.260000228881836
Train_BestReturn : -17.020000457763672
TimeSinceStart : 1914.2015850543976
Exploration Critic Loss : 31125084.0
Exploitation Critic Loss : 0.09231763333082199
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.5315062999725342
Eval_AverageReturn : -18.403846740722656
Eval_StdReturn : 5.024017810821533
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.403846153846153
Buffer size : 29001
Done logging...




********** Iteration 30000 ************

Training agent...

Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -17.940001
best mean reward -17.020000
running time 1988.777927
At timestep:     14 / 1000At timestep:     31 / 1000At timestep:     46 / 1000At timestep:     63 / 1000At timestep:     75 / 1000At timestep:     99 / 1000At timestep:     112 / 1000At timestep:     128 / 1000At timestep:     160 / 1000At timestep:     182 / 1000At timestep:     201 / 1000At timestep:     217 / 1000At timestep:     233 / 1000At timestep:     252 / 1000At timestep:     272 / 1000At timestep:     297 / 1000At timestep:     313 / 1000At timestep:     334 / 1000At timestep:     351 / 1000At timestep:     369 / 1000At timestep:     384 / 1000At timestep:     394 / 1000At timestep:     420 / 1000At timestep:     439 / 1000At timestep:     458 / 1000At timestep:     477 / 1000At timestep:     492 / 1000At timestep:     509 / 1000At timestep:     530 / 1000At timestep:     548 / 1000At timestep:     575 / 1000At timestep:     593 / 1000At timestep:     609 / 1000At timestep:     629 / 1000At timestep:     652 / 1000At timestep:     671 / 1000At timestep:     694 / 1000At timestep:     708 / 1000At timestep:     725 / 1000At timestep:     747 / 1000At timestep:     771 / 1000At timestep:     787 / 1000At timestep:     811 / 1000At timestep:     836 / 1000At timestep:     857 / 1000At timestep:     878 / 1000At timestep:     905 / 1000At timestep:     925 / 1000At timestep:     947 / 1000At timestep:     962 / 1000At timestep:     981 / 1000At timestep:     995 / 1000At timestep:     1011 / 1000Train_EnvstepsSoFar : 30001
Train_AverageReturn : -17.940000534057617
Train_BestReturn : -17.020000457763672
TimeSinceStart : 1988.777926683426
Exploration Critic Loss : 34583992.0
Exploitation Critic Loss : 0.0801411122083664
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.4194255769252777
Eval_AverageReturn : -18.075471878051758
Eval_StdReturn : 4.259724140167236
Eval_MaxReturn : -9.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 19.07547169811321
Buffer size : 30001
Done logging...




********** Iteration 31000 ************

Training agent...

Beginning logging procedure...
Timestep 31001
mean reward (100 episodes) -17.150000
best mean reward -17.020000
running time 2063.825636
At timestep:     18 / 1000At timestep:     35 / 1000At timestep:     58 / 1000At timestep:     73 / 1000At timestep:     98 / 1000At timestep:     119 / 1000At timestep:     144 / 1000At timestep:     156 / 1000At timestep:     173 / 1000At timestep:     201 / 1000At timestep:     224 / 1000At timestep:     241 / 1000At timestep:     266 / 1000At timestep:     284 / 1000At timestep:     305 / 1000At timestep:     328 / 1000At timestep:     345 / 1000At timestep:     360 / 1000At timestep:     380 / 1000At timestep:     405 / 1000At timestep:     428 / 1000At timestep:     444 / 1000At timestep:     462 / 1000At timestep:     485 / 1000At timestep:     505 / 1000At timestep:     527 / 1000At timestep:     547 / 1000At timestep:     566 / 1000At timestep:     583 / 1000At timestep:     601 / 1000At timestep:     617 / 1000At timestep:     651 / 1000At timestep:     665 / 1000At timestep:     682 / 1000At timestep:     707 / 1000At timestep:     731 / 1000At timestep:     745 / 1000At timestep:     764 / 1000At timestep:     780 / 1000At timestep:     795 / 1000At timestep:     829 / 1000At timestep:     846 / 1000At timestep:     863 / 1000At timestep:     890 / 1000At timestep:     916 / 1000At timestep:     941 / 1000At timestep:     956 / 1000At timestep:     977 / 1000At timestep:     996 / 1000At timestep:     1016 / 1000Train_EnvstepsSoFar : 31001
Train_AverageReturn : -17.149999618530273
Train_BestReturn : -17.020000457763672
TimeSinceStart : 2063.8256361484528
Exploration Critic Loss : 36792616.0
Exploitation Critic Loss : 0.07301940023899078
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.5209678411483765
Eval_AverageReturn : -19.31999969482422
Eval_StdReturn : 4.751589298248291
Eval_MaxReturn : -11.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 20.32
Buffer size : 31001
Done logging...




********** Iteration 32000 ************

Training agent...

Beginning logging procedure...
Timestep 32001
mean reward (100 episodes) -16.400000
best mean reward -16.400000
running time 2140.944856
At timestep:     14 / 1000At timestep:     30 / 1000At timestep:     43 / 1000At timestep:     68 / 1000At timestep:     81 / 1000At timestep:     103 / 1000At timestep:     122 / 1000At timestep:     142 / 1000At timestep:     163 / 1000At timestep:     185 / 1000At timestep:     202 / 1000At timestep:     216 / 1000At timestep:     236 / 1000At timestep:     257 / 1000At timestep:     285 / 1000At timestep:     299 / 1000At timestep:     317 / 1000At timestep:     338 / 1000At timestep:     357 / 1000At timestep:     376 / 1000At timestep:     410 / 1000At timestep:     424 / 1000At timestep:     444 / 1000At timestep:     471 / 1000At timestep:     487 / 1000At timestep:     503 / 1000At timestep:     522 / 1000At timestep:     550 / 1000At timestep:     563 / 1000At timestep:     576 / 1000At timestep:     602 / 1000At timestep:     627 / 1000At timestep:     646 / 1000At timestep:     662 / 1000At timestep:     685 / 1000At timestep:     704 / 1000At timestep:     720 / 1000At timestep:     747 / 1000At timestep:     780 / 1000At timestep:     814 / 1000At timestep:     827 / 1000At timestep:     850 / 1000At timestep:     866 / 1000At timestep:     886 / 1000At timestep:     908 / 1000At timestep:     929 / 1000At timestep:     943 / 1000At timestep:     972 / 1000At timestep:     1001 / 1000Train_EnvstepsSoFar : 32001
Train_AverageReturn : -16.399999618530273
Train_BestReturn : -16.399999618530273
TimeSinceStart : 2140.944855928421
Exploration Critic Loss : 32285964.0
Exploitation Critic Loss : 0.07839365303516388
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.5051460862159729
Eval_AverageReturn : -19.428571701049805
Eval_StdReturn : 5.714285850524902
Eval_MaxReturn : -12.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 20.428571428571427
Buffer size : 32001
Done logging...




********** Iteration 33000 ************

Training agent...

Beginning logging procedure...
Timestep 33001
mean reward (100 episodes) -17.200001
best mean reward -16.400000
running time 2226.229045
At timestep:     17 / 1000At timestep:     43 / 1000At timestep:     64 / 1000At timestep:     86 / 1000At timestep:     108 / 1000At timestep:     138 / 1000At timestep:     153 / 1000At timestep:     182 / 1000At timestep:     208 / 1000At timestep:     227 / 1000At timestep:     244 / 1000At timestep:     261 / 1000At timestep:     283 / 1000At timestep:     307 / 1000At timestep:     334 / 1000At timestep:     352 / 1000At timestep:     373 / 1000At timestep:     391 / 1000At timestep:     409 / 1000At timestep:     429 / 1000At timestep:     449 / 1000At timestep:     467 / 1000At timestep:     482 / 1000At timestep:     495 / 1000At timestep:     526 / 1000At timestep:     540 / 1000At timestep:     558 / 1000At timestep:     575 / 1000At timestep:     596 / 1000At timestep:     629 / 1000At timestep:     640 / 1000At timestep:     657 / 1000At timestep:     675 / 1000At timestep:     690 / 1000At timestep:     706 / 1000At timestep:     723 / 1000At timestep:     742 / 1000At timestep:     763 / 1000At timestep:     783 / 1000At timestep:     798 / 1000At timestep:     810 / 1000At timestep:     830 / 1000At timestep:     851 / 1000At timestep:     880 / 1000At timestep:     899 / 1000At timestep:     911 / 1000At timestep:     922 / 1000At timestep:     948 / 1000At timestep:     964 / 1000At timestep:     983 / 1000At timestep:     1005 / 1000Train_EnvstepsSoFar : 33001
Train_AverageReturn : -17.200000762939453
Train_BestReturn : -16.399999618530273
TimeSinceStart : 2226.229045152664
Exploration Critic Loss : 33773756.0
Exploitation Critic Loss : 0.09287439286708832
Exploration Model Loss : 25.504592895507812
Actor Loss : 0.5138081312179565
Eval_AverageReturn : -18.705883026123047
Eval_StdReturn : 5.1078338623046875
Eval_MaxReturn : -10.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 19.705882352941178
Buffer size : 33001
Done logging...




********** Iteration 34000 ************

Training agent...

Beginning logging procedure...
Timestep 34001
mean reward (100 episodes) -17.250000
best mean reward -16.400000
running time 2311.871051
At timestep:     17 / 1000At timestep:     33 / 1000At timestep:     50 / 1000At timestep:     68 / 1000At timestep:     91 / 1000At timestep:     104 / 1000At timestep:     116 / 1000At timestep:     139 / 1000At timestep:     151 / 1000At timestep:     163 / 1000At timestep:     184 / 1000At timestep:     202 / 1000At timestep:     221 / 1000At timestep:     234 / 1000At timestep:     252 / 1000At timestep:     268 / 1000At timestep:     283 / 1000At timestep:     297 / 1000At timestep:     311 / 1000At timestep:     327 / 1000At timestep:     340 / 1000At timestep:     364 / 1000At timestep:     383 / 1000At timestep:     401 / 1000At timestep:     419 / 1000At timestep:     447 / 1000At timestep:     474 / 1000At timestep:     487 / 1000At timestep:     514 / 1000At timestep:     532 / 1000At timestep:     547 / 1000At timestep:     573 / 1000At timestep:     586 / 1000At timestep:     610 / 1000At timestep:     637 / 1000At timestep:     660 / 1000At timestep:     679 / 1000At timestep:     698 / 1000At timestep:     711 / 1000At timestep:     734 / 1000At timestep:     748 / 1000At timestep:     770 / 1000At timestep:     783 / 1000At timestep:     808 / 1000At timestep:     818 / 1000At timestep:     830 / 1000At timestep:     849 / 1000At timestep:     866 / 1000At timestep:     882 / 1000At timestep:     904 / 1000At timestep:     925 / 1000At timestep:     955 / 1000At timestep:     984 / 1000At timestep:     1002 / 1000Train_EnvstepsSoFar : 34001
Train_AverageReturn : -17.25
Train_BestReturn : -16.399999618530273
TimeSinceStart : 2311.871051311493
Exploration Critic Loss : 31584222.0
Exploitation Critic Loss : 0.08125820010900497
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.45081496238708496
Eval_AverageReturn : -17.55555534362793
Eval_StdReturn : 5.090538501739502
Eval_MaxReturn : -9.0
Eval_MinReturn : -29.0
Eval_AverageEpLen : 18.555555555555557
Buffer size : 34001
Done logging...




********** Iteration 35000 ************

Training agent...

Beginning logging procedure...
Timestep 35001
mean reward (100 episodes) -17.389999
best mean reward -16.400000
running time 2399.261485
At timestep:     15 / 1000At timestep:     29 / 1000At timestep:     48 / 1000At timestep:     76 / 1000At timestep:     90 / 1000At timestep:     106 / 1000At timestep:     130 / 1000At timestep:     148 / 1000At timestep:     164 / 1000At timestep:     183 / 1000At timestep:     204 / 1000At timestep:     220 / 1000At timestep:     235 / 1000At timestep:     251 / 1000At timestep:     268 / 1000At timestep:     280 / 1000At timestep:     306 / 1000At timestep:     323 / 1000At timestep:     339 / 1000At timestep:     366 / 1000At timestep:     385 / 1000At timestep:     408 / 1000At timestep:     425 / 1000At timestep:     442 / 1000At timestep:     459 / 1000At timestep:     487 / 1000At timestep:     521 / 1000At timestep:     540 / 1000At timestep:     557 / 1000At timestep:     574 / 1000At timestep:     591 / 1000At timestep:     604 / 1000At timestep:     624 / 1000At timestep:     642 / 1000At timestep:     664 / 1000At timestep:     678 / 1000At timestep:     709 / 1000At timestep:     734 / 1000At timestep:     747 / 1000At timestep:     764 / 1000At timestep:     787 / 1000At timestep:     800 / 1000At timestep:     826 / 1000At timestep:     841 / 1000At timestep:     856 / 1000At timestep:     891 / 1000At timestep:     907 / 1000At timestep:     937 / 1000At timestep:     957 / 1000At timestep:     979 / 1000At timestep:     998 / 1000At timestep:     1018 / 1000Train_EnvstepsSoFar : 35001
Train_AverageReturn : -17.389999389648438
Train_BestReturn : -16.399999618530273
TimeSinceStart : 2399.2614846229553
Exploration Critic Loss : 34719368.0
Exploitation Critic Loss : 0.08237913250923157
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.5821062922477722
Eval_AverageReturn : -18.576923370361328
Eval_StdReturn : 5.474929332733154
Eval_MaxReturn : -11.0
Eval_MinReturn : -34.0
Eval_AverageEpLen : 19.576923076923077
Buffer size : 35001
Done logging...




********** Iteration 36000 ************

Training agent...

Beginning logging procedure...
Timestep 36001
mean reward (100 episodes) -17.690001
best mean reward -16.400000
running time 2483.193408
At timestep:     27 / 1000At timestep:     55 / 1000At timestep:     76 / 1000At timestep:     88 / 1000At timestep:     110 / 1000At timestep:     128 / 1000At timestep:     146 / 1000At timestep:     161 / 1000At timestep:     174 / 1000At timestep:     192 / 1000At timestep:     214 / 1000At timestep:     229 / 1000At timestep:     242 / 1000At timestep:     254 / 1000At timestep:     269 / 1000At timestep:     303 / 1000At timestep:     320 / 1000At timestep:     335 / 1000At timestep:     352 / 1000At timestep:     377 / 1000At timestep:     396 / 1000At timestep:     416 / 1000At timestep:     437 / 1000At timestep:     459 / 1000At timestep:     485 / 1000At timestep:     503 / 1000At timestep:     521 / 1000At timestep:     538 / 1000At timestep:     559 / 1000At timestep:     575 / 1000At timestep:     588 / 1000At timestep:     604 / 1000At timestep:     627 / 1000At timestep:     645 / 1000At timestep:     666 / 1000At timestep:     692 / 1000At timestep:     709 / 1000At timestep:     723 / 1000At timestep:     749 / 1000At timestep:     763 / 1000At timestep:     789 / 1000At timestep:     806 / 1000At timestep:     827 / 1000At timestep:     847 / 1000At timestep:     864 / 1000At timestep:     883 / 1000At timestep:     900 / 1000At timestep:     916 / 1000At timestep:     934 / 1000At timestep:     950 / 1000At timestep:     968 / 1000At timestep:     983 / 1000At timestep:     999 / 1000At timestep:     1012 / 1000Train_EnvstepsSoFar : 36001
Train_AverageReturn : -17.690000534057617
Train_BestReturn : -16.399999618530273
TimeSinceStart : 2483.1934084892273
Exploration Critic Loss : 36740400.0
Exploitation Critic Loss : 0.09604807943105698
Exploration Model Loss : 25.504592895507812
Actor Loss : 0.43154123425483704
Eval_AverageReturn : -17.740739822387695
Eval_StdReturn : 4.546814441680908
Eval_MaxReturn : -11.0
Eval_MinReturn : -33.0
Eval_AverageEpLen : 18.74074074074074
Buffer size : 36001
Done logging...




********** Iteration 37000 ************

Training agent...

Beginning logging procedure...
Timestep 37001
mean reward (100 episodes) -17.820000
best mean reward -16.400000
running time 2569.776145
At timestep:     16 / 1000At timestep:     34 / 1000At timestep:     67 / 1000At timestep:     83 / 1000At timestep:     105 / 1000At timestep:     119 / 1000At timestep:     139 / 1000At timestep:     162 / 1000At timestep:     184 / 1000At timestep:     200 / 1000At timestep:     215 / 1000At timestep:     238 / 1000At timestep:     255 / 1000At timestep:     267 / 1000At timestep:     281 / 1000At timestep:     298 / 1000At timestep:     323 / 1000At timestep:     344 / 1000At timestep:     365 / 1000At timestep:     384 / 1000At timestep:     404 / 1000At timestep:     424 / 1000At timestep:     442 / 1000At timestep:     457 / 1000At timestep:     476 / 1000At timestep:     500 / 1000At timestep:     525 / 1000At timestep:     539 / 1000At timestep:     561 / 1000At timestep:     580 / 1000At timestep:     599 / 1000At timestep:     621 / 1000At timestep:     635 / 1000At timestep:     662 / 1000At timestep:     678 / 1000At timestep:     701 / 1000At timestep:     720 / 1000At timestep:     735 / 1000At timestep:     756 / 1000At timestep:     775 / 1000At timestep:     790 / 1000At timestep:     815 / 1000At timestep:     830 / 1000At timestep:     844 / 1000At timestep:     866 / 1000At timestep:     883 / 1000At timestep:     908 / 1000At timestep:     923 / 1000At timestep:     945 / 1000At timestep:     965 / 1000At timestep:     985 / 1000At timestep:     1006 / 1000Train_EnvstepsSoFar : 37001
Train_AverageReturn : -17.81999969482422
Train_BestReturn : -16.399999618530273
TimeSinceStart : 2569.776144504547
Exploration Critic Loss : 36075680.0
Exploitation Critic Loss : 0.07348088920116425
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.5097188353538513
Eval_AverageReturn : -18.346153259277344
Eval_StdReturn : 4.080368518829346
Eval_MaxReturn : -11.0
Eval_MinReturn : -32.0
Eval_AverageEpLen : 19.346153846153847
Buffer size : 37001
Done logging...




********** Iteration 38000 ************

Training agent...

Beginning logging procedure...
Timestep 38001
mean reward (100 episodes) -17.780001
best mean reward -16.400000
running time 2654.962008
At timestep:     23 / 1000At timestep:     35 / 1000At timestep:     49 / 1000At timestep:     61 / 1000At timestep:     78 / 1000At timestep:     95 / 1000At timestep:     113 / 1000At timestep:     124 / 1000At timestep:     137 / 1000At timestep:     157 / 1000At timestep:     175 / 1000At timestep:     197 / 1000At timestep:     207 / 1000At timestep:     228 / 1000At timestep:     246 / 1000At timestep:     266 / 1000At timestep:     284 / 1000At timestep:     301 / 1000At timestep:     324 / 1000At timestep:     336 / 1000At timestep:     356 / 1000At timestep:     371 / 1000At timestep:     388 / 1000At timestep:     407 / 1000At timestep:     421 / 1000At timestep:     432 / 1000At timestep:     454 / 1000At timestep:     474 / 1000At timestep:     495 / 1000At timestep:     515 / 1000At timestep:     531 / 1000At timestep:     551 / 1000At timestep:     571 / 1000At timestep:     588 / 1000At timestep:     604 / 1000At timestep:     619 / 1000At timestep:     642 / 1000At timestep:     659 / 1000At timestep:     673 / 1000At timestep:     684 / 1000At timestep:     698 / 1000At timestep:     716 / 1000At timestep:     738 / 1000At timestep:     758 / 1000At timestep:     785 / 1000At timestep:     812 / 1000At timestep:     841 / 1000At timestep:     863 / 1000At timestep:     881 / 1000At timestep:     908 / 1000At timestep:     927 / 1000At timestep:     952 / 1000At timestep:     970 / 1000At timestep:     987 / 1000At timestep:     1010 / 1000Train_EnvstepsSoFar : 38001
Train_AverageReturn : -17.780000686645508
Train_BestReturn : -16.399999618530273
TimeSinceStart : 2654.9620082378387
Exploration Critic Loss : 31599832.0
Exploitation Critic Loss : 0.08134069293737411
Exploration Model Loss : 25.504592895507812
Actor Loss : 0.4807649552822113
Eval_AverageReturn : -17.363636016845703
Eval_StdReturn : 4.395715713500977
Eval_MaxReturn : -9.0
Eval_MinReturn : -28.0
Eval_AverageEpLen : 18.363636363636363
Buffer size : 38001
Done logging...




********** Iteration 39000 ************

Training agent...

Beginning logging procedure...
Timestep 39001
mean reward (100 episodes) -17.129999
best mean reward -16.400000
running time 2750.040163
At timestep:     21 / 1000At timestep:     41 / 1000At timestep:     62 / 1000At timestep:     82 / 1000At timestep:     98 / 1000At timestep:     114 / 1000At timestep:     127 / 1000At timestep:     146 / 1000At timestep:     162 / 1000At timestep:     179 / 1000At timestep:     199 / 1000At timestep:     213 / 1000At timestep:     242 / 1000At timestep:     266 / 1000At timestep:     281 / 1000At timestep:     304 / 1000At timestep:     323 / 1000At timestep:     337 / 1000At timestep:     359 / 1000At timestep:     373 / 1000At timestep:     391 / 1000At timestep:     408 / 1000At timestep:     428 / 1000At timestep:     444 / 1000At timestep:     463 / 1000At timestep:     481 / 1000At timestep:     509 / 1000At timestep:     526 / 1000At timestep:     549 / 1000At timestep:     574 / 1000At timestep:     591 / 1000At timestep:     605 / 1000At timestep:     635 / 1000At timestep:     655 / 1000At timestep:     677 / 1000At timestep:     691 / 1000At timestep:     714 / 1000At timestep:     730 / 1000At timestep:     741 / 1000At timestep:     767 / 1000At timestep:     781 / 1000At timestep:     794 / 1000At timestep:     814 / 1000At timestep:     828 / 1000At timestep:     846 / 1000At timestep:     864 / 1000At timestep:     882 / 1000At timestep:     914 / 1000At timestep:     932 / 1000At timestep:     945 / 1000At timestep:     958 / 1000At timestep:     976 / 1000At timestep:     991 / 1000At timestep:     1003 / 1000Train_EnvstepsSoFar : 39001
Train_AverageReturn : -17.1299991607666
Train_BestReturn : -16.399999618530273
TimeSinceStart : 2750.040163040161
Exploration Critic Loss : 36626428.0
Exploitation Critic Loss : 0.10177366435527802
Exploration Model Loss : 25.50457763671875
Actor Loss : 0.44784870743751526
Eval_AverageReturn : -17.574073791503906
Eval_StdReturn : 4.680938243865967
Eval_MaxReturn : -10.0
Eval_MinReturn : -31.0
Eval_AverageEpLen : 18.574074074074073
Buffer size : 39001
Done logging...




********** Iteration 40000 ************

Training agent...

Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -16.959999
best mean reward -16.400000
running time 2837.309673
At timestep:     22 / 1000At timestep:     42 / 1000At timestep:     57 / 1000At timestep:     76 / 1000At timestep:     92 / 1000At timestep:     108 / 1000At timestep:     119 / 1000At timestep:     139 / 1000At timestep:     155 / 1000At timestep:     179 / 1000At timestep:     191 / 1000At timestep:     208 / 1000At timestep:     227 / 1000At timestep:     251 / 1000At timestep:     271 / 1000At timestep:     287 / 1000At timestep:     302 / 1000At timestep:     329 / 1000At timestep:     344 / 1000At timestep:     363 / 1000At timestep:     380 / 1000At timestep:     396 / 1000At timestep:     418 / 1000At timestep:     431 / 1000At timestep:     444 / 1000At timestep:     460 / 1000At timestep:     486 / 1000At timestep:     506 / 1000At timestep:     523 / 1000At timestep:     545 / 1000At timestep:     560 / 1000At timestep:     578 / 1000At timestep:     595 / 1000At timestep:     614 / 1000At timestep:     625 / 1000At timestep:     647 / 1000At timestep:     660 / 1000At timestep:     672 / 1000At timestep:     691 / 1000At timestep:     716 / 1000At timestep:     735 / 1000At timestep:     760 / 1000At timestep:     784 / 1000At timestep:     800 / 1000At timestep:     816 / 1000At timestep:     836 / 1000At timestep:     854 / 1000At timestep:     873 / 1000At timestep:     896 / 1000At timestep:     913 / 1000At timestep:     947 / 1000At timestep:     965 / 1000At timestep:     985 / 1000At timestep:     1007 / 1000